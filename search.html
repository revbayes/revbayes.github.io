<!doctype html>
<html lang="en">
<link rel="icon" type="image/png" href="/assets/img/favicon.png" >
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="search-domain" value="https://revbayes.github.io/">
    <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet">
    <link rel="stylesheet" href="/assets/css/syntax.css">
    <link rel="stylesheet" type="text/css" href="/assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="/assets/css/main.css" />
    <title>RevBayes: Search Results</title>
  </head>
  <body>
    <div class="container">
      <nav class="navbar navbar-default navbar-fixed-top">
  <div class="container-fluid">
    <div class="navbar-header">
      <a href="/" class="pull-left">
        
        <img class="navbar-logo" src="/assets/img/aquabayes-desaturated.png" alt="RevBayes Home" />
        
      </a>

      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar" align="right"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="/download">Download</a></li>
        <li><a href="/tutorials/">Tutorials</a></li>
        <li><a href="/documentation/">Documentation</a></li>
        <li><a href="/interfaces">Interfaces</a></li>
        <li><a href="/workshops/">Workshops</a></li>
        <li><a href="/jobs/">Jobs</a></li>
        <li><a href="/developer/">Developer</a></li>
        <li><class="header-search">
  <form class="header-search-form" action="/search.html" method="get">
    <input type="text" id="search-box-nav" placeholder="Search..." name="query" style="border-color:black;background-color:white;height:32px;">
    <button type="submit" style="color:black; border-color:black;width:32px;height:32px;padding-top:4px">
        <img src="https://revbayes.github.io/assets/img/search.png" height="22px" width="28px"/>
    </button>
  </form>
</div>

</li>
      </ul>
      <!-- <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form> -->
    </div>
  </div>
</nav>

      <!-- adds margin to main content -->
      <div class="container" style="padding-left:20px padding-right:20px;">
          <h1 class="maintitle">

Search Results

</h1>
<hr>

<!-- List where search results will be rendered -->
<div class="container">
    <h3>Search</h3> 
    <class="header-search">
  <form class="header-search-form" action="/search.html" method="get">
    <input type="text" id="search-box-page" placeholder="Search..." name="query" style="border-color:black;background-color:white;width:80%;height:32px;">
    <button type="submit" style="color:black; border-color:black;width:32px;height:32px;padding-top:4px;">
        <img src="assets/img/search.png" height="22px" width="28px"/>
    </button>
  </form>
</div>


    <div class="container">
        <ul id="search-results"></ul>
    </div>
</div>
<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container-fluid">
    <div class="navbar-header">
      <a href="/" class="pull-left">
        
        <img class="navbar-logo" src="/assets/img/aquabayes-desaturated.png" alt="RevBayes Home" />
        
      </a>

      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar" align="right"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="/download">Download</a></li>
        <li><a href="/tutorials/">Tutorials</a></li>
        <li><a href="/documentation/">Documentation</a></li>
        <li><a href="/interfaces">Interfaces</a></li>
        <li><a href="/workshops/">Workshops</a></li>
        <li><a href="/jobs/">Jobs</a></li>
        <li><a href="/developer/">Developer</a></li>
        <li><class="header-search">
  <form class="header-search-form" action="/search.html" method="get">
    <input type="text" id="search-box-nav" placeholder="Search..." name="query" style="border-color:black;background-color:white;height:32px;">
    <button type="submit" style="color:black; border-color:black;width:32px;height:32px;padding-top:4px">
        <img src="https://revbayes.github.io/assets/img/search.png" height="22px" width="28px"/>
    </button>
  </form>
</div>

</li>
      </ul>
      <!-- <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form> -->
    </div>
  </div>
</nav>


<script>
  // Template to generate the JSON to search
  window.store = {
    
      "404-html": {
        "title": "404",
        "content": "404!",
        "url": "/404.html",
        "index": ""
      }
      ,
    
      "tutorials-coalescent-constant-html": {
        "title": "Constant Coalescent Model",
        "content": "OverviewThis exercise describes how to run a simple coalescent analysis in RevBayes.The simplest coalescent model is one with a constant population size through time.This population size is estimated solely based on the waiting times between coalescent events.The CoalescentThe coalescent process constructs a tree backwards in time.Starting from the samples, lineages are merged (i.e., coalesced), always two at a time.Under the coalescent process, the waiting time between two coalescent events is exponentially distributed and depends on the number of ‘active’ lineages and the effective population size $N_e$.Active lineages are the ones that can coalesce, the number is reduced by one with every coalescent event.The coalescent process was first introduced by Kingman in 1982 for a constant population size (Kingman 1982).In the constant coalescent process, a single population size is assumed for the whole tree.The relationship betweeen coalescent waiting times and effective population size is defined through the coalescent rate: $c = \\frac{k (k-1)}{2N_e}$ with $k$ being the number of currently active lineages and $N_e$ being the effective population size.In , a general scheme is shown.Waiting times are in between coalescent events.Schematic figure of a coalescent tree and the different times associated with it. $w_k$ are the waiting times with $k$ active lineages, $t_{c,k}$ are the coalescent events at the beginning of such a coalescent interval. Here, an example of a constant population size trajectory is shown. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Inference Example  For your info  The entire process of the estimation can be executed by using the mcmc_isochronous_Constant.Rev script that you can download on the left side of the page.Save it in your scripts directory.You can type the following command into RevBayes:  source(\"scripts/mcmc_isochronous_Constant.Rev\")    We will walk you through every single step in the following section.For every MCMC analysis, convergence assessment is an important step.In the tutorial Convergence assessment, you can find information and instructions on how to run the convergence assessment with the R package convenience.It is generally recommended to have at least two replicates per MCMC analysis to be able to compare convergence between runs.Thus, we first set the number of replicates:NUM_REPLICATES      = 2In the beginning, we also define a few variables for running the MCMC.These are the number of iterations, and the so-called “thinning” which we use to say that we want to sample every $10^{th}$ iteration.NUM_MCMC_ITERATIONS = 10000THINNING            = 10We also need to create vectors for the monitors and moves of the MCMC.Moves are functions that propose new parameter values in your MCMC, based on the current value.These newly proposed parameters can either be accepted or rejected.By repeating this process many times, we obtain samples from the posterior distribution.See for example Introduction to MCMC for more information on the acceptance / rejection procedure.Monitors are later used to track the progress of your analysis, but are also needed to write output files.moves     = VectorMoves()monitors  = VectorMonitors()Read the dataStart by reading in the aligned sequences of the isochronous horse data.sequences &lt;- readDiscreteCharacterData(\"data/horses_isochronous_sequences.fasta\")You will also need the names of the taxa and their number.taxa &lt;- sequences.taxa()n_taxa &lt;- taxa.size()The Tree ModelFor the constant coalescent model, only one population size is estimated.For this population size, a prior needs to be set.Without knowing much about the population size of our horse sample, we set a uniform prior.pop_size ~ dnUniform(0,1E8)You may realize that in the full script, we initialize the population size to have a first value of $100000$.Later in the tutorial, we will constrain the root age of the tree to be inside the interval $[250 000, 500 000]$.In order for our first proposed tree to comply with this constraint, an initial value of $100000$ proved to lead to reasonable initial proposals.pop_size.setValue(100000)We also add a move for the population size.Here, we chose a scaling move which means that the current values is multiplied by a scaling factor to propose a new value.See for example Introduction to MCMC using RevBayes for information on moves.moves.append( mvScale(pop_size, lambda=0.1, tune=true, weight=2.0) )Now, we will instantiate the stochastic node for the tree.The dnCoalescent distribution should be used for a constant coalescent process.It takes a value for the population size (theta) and the taxa as input.psi ~ dnCoalescent(theta=pop_size, taxa=taxa)We calibrate the tree based on the root age.We chose a Normal distribution with a mean of $375 000$ and a standard deviation of $60 000$.As mentioned above, the root age will be constrained to the interval $[250 000, 500 000]$.As we have access to the original analysis from Vershinina et al. (2021), we could see that this should be the rough range of the root.root_age := psi.rootAge()diff &lt;- (500000 - 250000)/2.0obs_root_age ~ dnNormal(mean = root_age, sd = 60000, min = root_age - diff, max = root_age + diff)obs_root_age.clamp(375000)We should also add moves for the tree.These include moves on a single branch, subtrees or the whole tree.Here, the weight of the different moves is based on the number of taxa.If a move changes a single branch (e.g. mvNNI), it will be applied more often and thus have a higher weight than a move which changes the whole tree (e.g. mvTreeScale).moves.append( mvNarrow(psi, weight=n_taxa) )moves.append( mvNNI(psi, weight=n_taxa) )moves.append( mvFNPR(psi, weight=n_taxa/4.0) )moves.append( mvSubtreeScale(psi, weight=n_taxa/5.0) )moves.append( mvNodeTimeSlideUniform(psi, weight=n_taxa) )moves.append( mvRootTimeScaleBactrian(psi, weight=n_taxa/5.0) )moves.append( mvTreeScale(psi, weight=n_taxa/5.0) )Substitution Model and other parametersFinally, sequence data should be added to the analysis. Here, we assume a GTR+$\\Gamma$+I substitution model, but you can of course use others. Have a look at the Nucleotide substitution models tutorial to see how you can define different substitution models.For the GTR model, we need to add exchangeability rates (er) and stationary frequences (pi).Of course, we also add moves for these.er_prior &lt;- v(1,1,1,1,1,1)pi_prior &lt;- v(1,1,1,1)er ~ dnDirichlet(er_prior)pi ~ dnDirichlet(pi_prior)moves.append( mvBetaSimplex(er, weight=3) )moves.append( mvDirichletSimplex(er, weight=1) )moves.append( mvBetaSimplex(pi, weight=2) )moves.append( mvDirichletSimplex(pi, weight=1) )This is everything needed for the Q matrix of the GTR model.Q := fnGTR(er,pi)For the $\\Gamma$ extension to the GTR model, we need to draw the site rates (sr) from a discretized Gamma function with two parameters.Here, we use alpha for both parameters.We also add a scaling move for alpha.alpha ~ dnUniform( 0.0, 1E6 )alpha.setValue( 1.0 )sr := fnDiscretizeGamma( alpha, alpha, 4 )moves.append( mvScale(alpha, weight=2.0) )We draw the proportion of invariant sites (p_inv) from a Beta distribution and add a sliding window move.p_inv ~ dnBeta(1,1)moves.append( mvSlide(p_inv) )The last step is to set the clock rate.We draw it from a log uniform distribution here.Again, we know from the original analysis (Vershinina et al. 2021) that the true value should be around $4.68*10^{-8}$ and thus set the lower bound of the distribution to $1*10^{-12}$ and the upper bound to $1*10^{-4}$.We also initialize the value to be equal to the original analysis.Then, we  add a scaling move for the clock rate.clock ~ dnLoguniform(1e-12,1e-4)clock.setValue(4.68e-8)moves.append( mvScale(clock, weight=2.0) )Additionally, we add a scaling move which makes sure to regulate clock rate and the root age.This needs to be done as root age and clock rate are intertwined and can not be clearly seperated.Here, whenever the clock rate will be increased, the root age will be decreased.Note that you could also calibrate the clock rate instead of the root age of the tree as we do it here.up_down_move = mvUpDownScale(weight=5.0)up_down_move.addVariable(clock,up=TRUE)up_down_move.addVariable(psi,up=FALSE)moves.append( up_down_move )The final dnPhyloCTMC function combines all of the previous defined parameters.We also need to clamp the sequence data.seq ~ dnPhyloCTMC(tree=psi, Q=Q, siteRates=sr, pInv=p_inv, type=\"DNA\", branchRates=clock)seq.clamp(sequences)Finalize and run the analysisIn the end, we need to wrap our model.mymodel = model(psi)Now, we add some monitors.The mnModel monitor keeps track of all model parameters and thus is written into our main .log file.With mnFile, you can keep track of the trees or parameters that you would like to keep in an extra file.mnScreen is responsible for having output printed directly to your screen.This output will not per se be saved in a file.monitors.append( mnModel(filename=\"output/horses_iso_Constant.log\",printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_Constant.trees\",psi,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_Constant_NE.log\",pop_size,printgen=THINNING) )monitors.append( mnScreen(pop_size, root_age, printgen=100) )The final step is to run the mcmc.Make sure to set combine=\"mixed\" for the output of the two replicates to be combined in the end.mymcmc = mcmc(mymodel, monitors, moves, nruns=NUM_REPLICATES, combine=\"mixed\")mymcmc.burnin(NUM_MCMC_ITERATIONS*0.1,100)mymcmc.run(NUM_MCMC_ITERATIONS, tuning = 100)Check ConvergenceTo check whether your analysis has converged, you can use the R package convenience.Have a look at the Convergence assessment tutorial.ResultsAfter running your analysis, you can plot the results using the R package RevGadgets.See the RevGadgets Github repository for information on how to install the package.After installing the package, open R or RStudio and set the tutorial directory as your working directory.You can plot the RevBayes output as follows:library(RevGadgets)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"num_grid_points = 500max_age_iso = 5e5min_age = 0spacing = \"equal\"population_size_log = \"output/horses_iso_Constant_NE.log\"df &lt;- processPopSizes(population_size_log, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points, max_age = max_age_iso, min_age = min_age, spacing = spacing)p &lt;- plotPopSizes(df) + ggplot2::coord_cartesian(ylim = c(1e3, 1e8))ggplot2::ggsave(\"figures/horses_iso_Constant.png\", p)Your output should look roughly like the following figure.Example output from plotting the constant coalescent analysis run in this exercise. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Next ExerciseWhen you are done, have a look at the next exercise.  The Skyline model",
        "url": "/tutorials/coalescent/Constant.html",
        "index": "false"
      }
      ,
    
      "tutorials-coalescent-gmrf-html": {
        "title": "Coalescent Models with GMRF",
        "content": "OverviewThis tutorial describes how to run a Coaelescent Skyline Analysis with a Gaussian Markov Random Field (GMRF) prior in RevBayes.This is a special case of a skyline plot.The most notable difference to the previous exercise is that the population size is autocorrelated, i.e., each population size has a prior distribution that is centered on the population size from the previous, more recent, interval.This leads to a smoothing effect of the population size trajectory with adjacent intervals most likely having similar population size values.In case of a strong signal from the data indicating a change in population size, however, this can also be reflected in the resulting trajectory.Here, the intervals additionally are equally spaced and thus their start and end points are independent from the coalescent events (see  for a hypothetical example).  Hypothetical example of a Bayesian skyline plot with equally sized intervals, independent from the number of coalescent events (equal-sized). $w_k$ are the waiting times with $k$ active lineages, $t_{c,k}$ are the coalescent events at the beginning of such a coalescent interval. $t_{i,j}$ mark the points of interval change.  Here, the intervals are chosen to be equal-sized and the change-points are independent from the coalescent events. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Likelihood CalculationWe assume that the phylogeny of the samples is known.There are $n$ samples, with $k$ active lineages at the current point in time $t$.Time starts at $t = 0$.The waiting times between coalescent events $w_k$ are exponentially distributed with rate $c = \\frac{k (k-1)}{2N_e(t)}$ with $N_e$ being the population size.The likelihood for a Skyline Plot is the product of the probability density functions of the coalescent waiting times, which are calculated as follows:\\[p(w_k | t_k) = \\frac{k (k -1)}{2N_e(t_k + w_k)} exp \\left[ \\int_{t_k}^{t_k+w_k} \\frac{k (k -1)}{2N_e(t)} dt \\right]\\]Each $t_k$ is the beginning of the respective kth coalescent interval.The waiting times $w_k$ refer to the waiting time starting when there are $k$ active lineages.In the case of interval times not being dependent on coalescent events, a likelihood is calculated for each interval.This likelihood is the product of the likelihoods of the coalescent events in the specific interval and the likelihood of no coalescent event happening in the remaining time until the next interval.As the intervals can be considered independent from each other, the likelihood of the complete demographic function is the product of the likelihoods of each interval.Inference Example  For your info  The entire process of the GMRF based estimation can be executed by using the mcmc_isochronous_GMRF.Rev script in the scripts folder.You can type the following command into RevBayes:  source(\"scripts/mcmc_isochronous_GMRF.Rev\")    We will walk you through the script in the following section.We will mainly highlight the parts of the script that change compared to the Constant coalescent model and the Skyline model.Read the dataRead in the data as described in the first exercise.The GMRF ModelFor the GMRF model, you need to decide on the number of intervals.These are equally distributed in time.For a simple, quickly computed, example, we choose $10$ intervals here.Later, feel free to try the analysis with more intervals, for example $100$.NUM_INTERVALS = 10You will also need to define the points of change in time to reflect the equal size of the intervals.Here, we define the maximal age to be $500000$ which should cover the whole tree (based on our results from the previous exercises).Further backwards in time the population size is thought to be in equilibrium and to be equal to the population size of the most ancient interval.The first interval (automatically) starts at $t = 0$, the other starting points depend on the number of intervals and the maximal age.MAX_AGE = 500000for (i in 1:(NUM_INTERVALS-1)) {    changePoints[i] &lt;- i * ((MAX_AGE)/NUM_INTERVALS)}For each interval, a population size will be estimated.In this case, the most recent population size (population_size_at_present) is treated differently to the other population sizes.This is due to the fact that all other population size priors depend on the one more recent.For population_size_at_present we assume the same prior distribution and initial value as in the previous exercises.population_size_at_present ~ dnUniform(0,1E8)population_size_at_present.setValue(100000)Note that we apply three different moves to the recent population size here: a scaling move (mvScaleBactrian), a mirror move (mvMirrorMultiplier) and a random dive move (mvRandomDive).The scaling move multiplies the currently proposed value with a scaling factor, the mirror move “mirrors” a value from a normal distribution on the other side of the posterior mean and the random dive move is a different type of scaling move.moves.append( mvScaleBactrian(population_size_at_present,weight=5) )moves.append( mvMirrorMultiplier(population_size_at_present,weight=5) )moves.append( mvRandomDive(population_size_at_present,weight=5) )In the GMRF model implemented in RevBayes, we do not set priors on the remaining population sizes, but on the log-scale differences between population sizes (delta_log_population_size).This way, the estimated parameters can be treated as independent even with the auto-correlation of the population sizes and MCMC sampling is easier.The overall variability of the trajectory is controlled by the standard deviation of the Normal distribution from which these delta_log_population_size values are drawn.This standard deviation is the product of a global scale parameter and its hyperprior.Therefore, you first need to define the hyperprior for the global scale parameter. You can get the appropriate value for this hyperprior dependent on the number of change-points by using the R package RevGadgets.Here, we ran the function setMRFGlobalScaleHyperpriorNShifts(9, \"GMRF\") to know the value of $0.1203$ (remember that there are nine change-points with ten intervals).To have a prior distribution on delta_log_population_size which favors autocorrelation, but allows for sudden changes, a halfCauchy distribution is chosen for the global scale.We also add a scaling move to the global scale.population_size_global_scale_hyperprior &lt;- 0.1203population_size_global_scale ~ dnHalfCauchy(0,1)moves.append( mvScaleBactrian(population_size_global_scale,weight=5.0) )The standard deviation of the aforementioned Normal distribution of the delta_log_population_size values can now be defined by multiplying the global scale hyperprior with the global scale.Here, we achieve all desired properties (favoring similar values but allowing for flexibility) by multiplying a halfCauchy(0,1) distribution with the value of the hyperprior ($0.1203$) that we calculated before.This is just a hierarchical way of defining a halfCauchy(0,$0.1203$) distribution.We add a sliding move to the delta_log_population_size values.Note that in RevBayes the standard deviation (sd) is used as input for the Normal distribution instead of the variance of the distribution (which would be the square of the standard deviation).for (i in 1:(NUM_INTERVALS-1)) {  # non-centralized parameterization  delta_log_population_size[i] ~ dnNormal( mean=0, sd=population_size_global_scale*population_size_global_scale_hyperprior )  # Make sure values initialize to something reasonable  delta_log_population_size[i].setValue(runif(1,-0.1,0.1)[1])  moves.append( mvSlideBactrian(delta_log_population_size[i], weight=5) )}Finally, the different population sizes need to be combined to form the vector of population sizes that we want to see as result in the end.In RevBayes, a function for this kind of assembly is implemented: fnassembleContinuousMRF.Note that you could also define the first value on a log scale, but then need to adjust the value of initialValueIsLogScale to be TRUE.This analysis is based on a GMRF of order $1$.Higher order GMRF also exist, but will not be discussed here.population_size := fnassembleContinuousMRF(population_size_at_present,delta_log_population_size,initialValueIsLogScale=FALSE,order=1)For this kind of analysis, the mvEllipticalSliceSamplingSimple move has to be added.Without it, convergence would be difficult to achive.Of course, we also need to add moves for the different population size parameters.# Move all field parameters in one gomoves.append( mvEllipticalSliceSamplingSimple(delta_log_population_size,weight=5,tune=FALSE) )# joint sliding moves of all vector elementsmoves.append( mvVectorSlide(delta_log_population_size, weight=10) )# up-down slide of the entire vector and the rate at presentrates_up_down_move = mvUpDownScale(weight=10.0)rates_up_down_move.addVariable(population_size_at_present,FALSE)rates_up_down_move.addVariable(delta_log_population_size,TRUE)moves.append( rates_up_down_move )# shrink expand movesmoves.append( mvShrinkExpand( delta_log_population_size, sd=population_size_global_scale, weight=10 ) )If you are interested in more details on why the analysis is set up in this way, have a look at the Episodic Diversification Rate Estimation tutorial where an analysis is performed with a similar model.Also, the Magee et al. (2020) paper provides further background information on these kinds of analysis.The TreeNow, we will instantiate the stochastic node for the tree.Similar to the skyline exercise, we use the dnCoalescentSkyline distribution for the tree.In the GMRF case, however, the method is not based on events, but has specified intervals.psi ~ dnCoalescentSkyline(theta=population_size, times=changePoints, method=\"specified\", taxa=taxa)In order to be able to later plot and analyze the population size curve, we can retrieve the resulting interval times as for the skyline exercise.Here, they should not change, so you might as well omit this line.interval_times := psi.getIntervalAges()Again, we constrain the root age as before and add the same moves for the tree.Substitution Model and other parametersThis part is also taken from the constant coalescent exercise.Finalize and run the analysisIn the end, we need to wrap our model as before.Finally, we add the monitors and then run the MCMC.Remember to change the file names to avoid overwriting your previous results.monitors.append( mnModel(filename=\"output/horses_iso_GMRF.log\",printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_GMRF.trees\",psi,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_GMRF_NEs.log\",population_size,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_GMRF_times.log\",interval_times,printgen=THINNING) )monitors.append( mnScreen(population_size, root_age, printgen=100) )mymcmc = mcmc(mymodel, monitors, moves)mymcmc.burnin(NUM_MCMC_ITERATIONS*0.1,100)mymcmc.run(NUM_MCMC_ITERATIONS, tuning = 100)ResultsAfter running your analysis, you can plot the results using the R package RevGadgets.library(RevGadgets)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"num_grid_points = 500max_age_iso = 5e5min_age = 0spacing = \"equal\"population_size_log = \"output/horses_iso_GMRF_NEs.log\"interval_change_points_log = \"output/horses_iso_GMRF_times.log\"df &lt;- processPopSizes(population_size_log, interval_change_points_log, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points, max_age = max_age_iso, min_age = min_age, spacing = spacing)p &lt;- plotPopSizes(df) + ggplot2::coord_cartesian(ylim = c(1e3, 1e8))ggplot2::ggsave(\"figures/horses_iso_GMRF.png\", p)Example output from plotting the GMRF analysis run in this exercise. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.The Horseshoe Markov Random Field PriorRelated to the GMRF, there also is the Horseshoe Markov Random Field (HSMRF) prior.It can be seen as a more generalized version of the GMRF with the difference lying in the definition of the standard deviation of the values in the intervals (Faulkner et al. 2020; Magee et al. 2020).It is even more flexible, because each interval has an additional variable assigned to the variation.Thus, you don’t only change the global scale of variability, but the local scale in each interval.The GMRF can be seen as a special case, where this local scale value is set to $1$.In the tutorial Episodic Diversification Rate Estimation, the HSMRF prior is applied to the estimation of diversification rates.Have a look at the Specifying the model section and try to change the respective lines in your current script to follow the HSMRF procedure.Do your results look different?HintThe lines you should look at are lines 67 to 73 in the script.There, you can change the way the standard deviation of the population sizes per interval is calculated.It now has an additional value sigma_population_size for each interval.This is the local scale parameter.for (i in 1:(NUM_INTERVALS-1)) {  # Variable-scaled variances for hierarchical horseshoe  sigma_population_size[i] ~ dnHalfCauchy(0,1)  # Make sure values initialize to something reasonable  sigma_population_size[i].setValue(runif(1,0.005,0.1)[1])  # moves on the single sigma values  moves.append( mvScaleBactrian(sigma_population_size[i], weight=5) )  # non-centralized parameterization of horseshoe  delta_log_population_size[i] ~ dnNormal( mean=0, sd=sigma_population_size[i]*population_size_global_scale*population_size_global_scale_hyperprior )  # Make sure values initialize to something reasonable  delta_log_population_size[i].setValue(runif(1,-0.1,0.1)[1])  moves.append( mvSlideBactrian(delta_log_population_size[i], weight=5) )}Remember to change the hyperprior value (setMRFGlobalScaleHyperpriorNShifts(9, \"HSMRF\") in RevGadgets) and to add extra moves.In case you prefer to download a whole HSMRF script to compare it to the GMRF script, have a look at the HSMRF.Next ExerciseWhen you are done, have a look at the next exercise.  The Skyfish modelAlternative ImplementationsA different model applying a GMRF Prior is the Skygrid model (Gill et al. 2012).It is based on the Skyride model (Minin et al. 2008) which is described in the Skyline tutorial.The Skyride model does have coalescent event based change-points though, the Skygrid model has independent change-points.The degree of smoothing is being regulated by a precision parameter which differs from the global scale parameter presented in this tutorial.If you would like to have an example of what a RevBayes script with this prior can look like, have a look at  the example script for a Skygrid analysis.",
        "url": "/tutorials/coalescent/GMRF.html",
        "index": "false"
      }
      ,
    
      "tutorials-coalescent-gmrf-treebased-html": {
        "title": "Coalescent Models from Trees with GMRF",
        "content": "OverviewThis tutorial describes how to run the Coalescent Skyline Analysis from trees with a Gaussian Markov Random Field (GMRF) prior in RevBayes.Here, the trees and population sizes will not be estimated simultaneously, but it is assumed that a single tree or a sample of trees is used as input instead of a sequence alignment.Inference Example  For your info  The entire process of the GMRF based estimation from trees can be executed by using the mcmc_isochronous_GMRF_treebased.Rev script in the scripts folder.The alternative script mcmc_isochronous_GMRF_maptreebased.Rev provides an example on how to compute a maximum a posteriori (MAP) tree in RevBayes an run the analysis based on that single tree.You can type the following commands into RevBayes:  source(\"scripts/mcmc_isochronous_GMRF_treebased.Rev\")source(\"scripts/mcmc_isochronous_GMRF_maptreebased.Rev\")    We will walk you through the first of the scripts in the following section.We will mainly highlight the parts of the script that change compared to the other exercises.You can have another look at the other exercises, they are listed under prerequisites on the left.Read the dataRead in the output from the constant coalescent exercise.If you haven’t done the exercise, please download the file horses_iso_Constant.trees into an “output” folder in your tutorial directory.# Read in the tree sampletreetrace = readTreeTrace(\"output/horses_iso_Constant.trees\", treetype = \"clock\", burnin = 0.1)trees = treetrace.getTrees()# Get the taxataxa &lt;- trees[1].taxa()# Get the number of taxa. We need these later on.n_taxa &lt;- taxa.size()The GMRF ModelIn contrast to the GMRF exercise, we do not set the maximal age, but take the maximal root age of all trees included in the sample.MAX_AGE &lt;- 0for (T in trees) {    MAX_AGE &lt;- max( [MAX_AGE,T.rootAge()] )}The remaining part of the model is the same as in the GMRF exercise.The TreeNow, we will instantiate the stochastic node for the tree.Similar to the skyline exercise, we use the dnCoalescentSkyline distribution for the tree.In the GMRF case, however, the method is not based on events, but has specified intervals.Also, with having a sample of trees, we use this distribution as prior and need to clamp the tree sample.tree_prior = dnCoalescentSkyline(theta=population_size, times=changePoints, method=\"specified\", taxa=taxa)psi ~ dnEmpiricalSample( tree_prior)psi.clamp(trees)The interval times are the defined change-points.If you would like to read them out later, you can set them here.interval_times := changePointsAgain, we constrain the root age as before and add the same moves for the tree.Working with one tree as inputAs mentioned above, you can also have a single tree as input.In this case, you can read it in with readTrees.Here, we first generate the maximum a posteriori (MAP) tree from the tree sample.trees = readTreeTrace(\"output/horses_iso_Constant.trees\", treetype = \"clock\", burnin = 0.1)maptree = mapTree(trace=trees, conditionalAges=TRUE)taxa &lt;- maptree.taxa()The remaining part of the script is similar to having a tree sample.The node for the tree is just a little simpler:psi ~ dnCoalescentSkyline(theta=population_size, times=changePoints, method=\"specified\", taxa=taxa)psi.clamp(maptree)Finalize and run the analysisIn the end, we need to wrap our model as before.Finally, we add the monitors and then run the MCMC.Remember to change the file names to avoid overwriting your previous results.monitors.append( mnModel(filename=\"output/horses_iso_GMRF_treebased.log\",printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_GMRF_treebased_NEs.log\",population_size,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_GMRF_treebased_times.log\",interval_times,printgen=THINNING) )monitors.append( mnScreen(population_size, printgen=100) )mymcmc = mcmc(mymodel, monitors, moves)mymcmc.burnin(NUM_MCMC_ITERATIONS*0.1,100)mymcmc.run(NUM_MCMC_ITERATIONS, tuning = 100)ResultsAfter running your analysis, you can plot the results using the R package RevGadgets.library(RevGadgets)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"num_grid_points = 500max_age_iso = 5e5min_age = 0spacing = \"equal\"population_size_log = \"output/horses_iso_GMRF_treebased_NEs.log\"interval_change_points_log = \"output/horses_iso_GMRF_treebased_times.log\"df &lt;- processPopSizes(population_size_log, interval_change_points_log, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points, max_age = max_age_iso, min_age = min_age, spacing = spacing)p &lt;- plotPopSizes(df) + ggplot2::coord_cartesian(ylim = c(1e3, 1e8))ggplot2::ggsave(\"figures/horses_iso_GMRF_treebased.png\", p)Example output from plotting the GMRF analysis run in this exercise. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Next ExerciseWhen you are done, have a look at the next exercise.  A piecewise model",
        "url": "/tutorials/coalescent/GMRF_treebased.html",
        "index": "false"
      }
      ,
    
      "tutorials-coalescent-hsmrf-html": {
        "title": "Coalescent Models with HSMRF",
        "content": "OverviewThis page provides you with the script for a Horseshoe Markov Random Field (HSMRF) skyline analysis on the left side.It is in addition to the Gaussian Markov Random Field Prior tutorial.ResultsAfter running your analysis, you can plot the results using the R package RevGadgets.Example output from plotting the HSMRF analysis. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.",
        "url": "/tutorials/coalescent/HSMRF.html",
        "index": "false"
      }
      ,
    
      "tutorials-coalescent-skyfish-html": {
        "title": "The Skyfish Model",
        "content": "OverviewThis tutorial describes how to run a Coaelescent Skyline Analysis with the Skyfish model in RevBayes.The most notable difference to the previous exercise is that the number of intervals is estimated in the analysis.In the Skyfish model, we use a compound poisson process (CPP) prior with a poisson prior on the number of interval change-points, a uniform prior on the position of the change-points, and an autocorrelated lognormal prior on the population size.In this case, a reversible jump MCMC (rjMCMC) is needed to be able to sample from the posterior distribution.In RevBayes, this functionality is implemented in the dnAutocorrelatedEvent distribution.Inference Example  For your info  The entire process of the Skyfish estimation can be executed by using the mcmc_isochronous_SkyfishAC.Rev script in the scripts folder.You can type the following command into RevBayes:  source(\"scripts/mcmc_isochronous_SkyfishAC.Rev\")    We will walk you through the script in the following section.We will mainly highlight the parts of the script that change compared to the GMRF model.Read the dataRead in the data as described in the first exercise.The Skyfish ModelFor the Skyfish model, you need to set a maximal age.Here, we define the maximal age to be $500000$ which should cover the whole tree (based on our results from the previous exercises).Further backwards in time the population size is thought to be in equilibrium and to be equal to the population size of the most ancient interval.The first interval (automatically) starts at $t = 0$, the other starting points depend on the number of intervals and the maximal age.MAX_AGE = 500000We also need to set an estimated root age which will be used for the initialization of the population size values.ESTIMATED_ROOT_AGE &lt;- 375000EXPECTED_POP_SIZE = ESTIMATED_ROOT_AGE/2We need some additional parameters for the analysis to work.H is a parameter governing the standard deviation of the population size (see below).It is chosen so that $95\\%$ of the prior probability span two orders of magnitude (see Höhna et al. (2017) for more details).ac_sigma defines the standard deviation of the autocorrelation.We draw it from an exponential distribution with rate MAX_AGE / 8.H = 0.587405ac_sigma ~ dnExponential( 0.25 * MAX_AGE / 2 )ac_sigma.setValue( 1.0 / MAX_AGE )We add a scaling move for ac_sigma.moves.append( mvScale(ac_sigma, weight=2) )In the next step, we set up the dnAutocorrelatedEvent distribution which combines population sizes, the position of interval change-points, and the number of change-points in a single distribution.For the eventDistribution parameter, a distribution on natural numbers has to be chosen.These “events” are the number of change-points seperating intervals in this case and not coalescent events.In this example, we use a Poisson distribution with an expected value of $10$.The valueDistribution is a vector of prior distributions for the population sizes and the interval times.We chose a uniform distribution for the positioning of interval change-points.For the first population size, we chose a lognormal distribution based on the expected population size and a standard deviation of 2*H.We also should call the variables by their names.The minNumberEvents are $1$ population size and $0$ times corresponding to interval changes.We expect to always have one more population size value than change-points.For autocorrelationTypes we set the population size to be autocorrelated through a lognormal distribution (ACLN).The autocorrelation of the population size should be time-dependent, thus we need to set the second value of the autocorrelationDependencies to \"time\".Finally, autocorrelation of the population size is governed by it standard deviation defined by the ac_sigma  parameter.The interval change-points are not autocorrelated, thus we set the respective values to \"NONE\", \"none\", and $0$.In the last line, we add the name of the variable to sort the values of all parameters by.In our case, we want the population size values to be sorted by time.events ~ dnAutocorrelatedEvent(eventDistribution = dnPoisson(lambda=10),                               valueDistribution =[dnUniform(0.0,MAX_AGE),                                                   dnLognormal( ln(EXPECTED_POP_SIZE), sd=2*H )],                               names=[\"time\",\"theta\"],                               minNumberEvents=[0,1],                               autocorrelationTypes=[\"NONE\",\"ACLN\"],                               autocorrelationDependencies=[\"none\",\"time\"],                               autocorrelationSigmas=[0,ac_sigma],                               sort = \"time\")For the dnAutocorrelatedEvent distribution, we add specific moves.# apply a move that adds and removes pairs of theta+timemoves.append( mvMultiValueEventBirthDeath(events, weight=50) )# add a move that changes the theta variablesmoves.append( mvMultiValueEventScale(events, name=\"theta\", lambda=1.0, weight=10, tune=!FALSE) )moves.append( mvMultiValueEventSlide(events, name=\"theta\", lambda=1.0, weight=10, tune=!FALSE) )# add a move that changes the time variablesmoves.append( mvMultiValueEventSlide(events, name=\"time\", lambda=10.0, weight=10, tune=!FALSE) )moves.append( mvMultiValueEventScale(events, name=\"time\", lambda=0.5, weight=10, tune=!FALSE) )Finally, we need to track the different parameters by assigning them to variables.n_events := events.getNumberOfEvents()population_size := events.getRealPosValues(name=\"theta\")changePoints := events.getRealPosValues(name=\"time\")The TreeNow, we will instantiate the stochastic node for the tree.psi ~ dnCoalescentSkyline(theta=population_size, times=changePoints, method=\"specified\", taxa=taxa)In our example, we realized that it is difficult to find a starting tree with this model.We just set the maximum a posteriori (MAP) tree of the analysis with the Constant model as our starting tree.trees = readTreeTrace(\"output/horses_iso_Constant.trees\", treetype = \"clock\", burnin = 0.1)maptree = mapTree(trace=trees, conditionalAges=TRUE)psi.setValue( maptree )root_age := psi.rootAge()For consistency with the other exercises, we set the interval_times to be the changePoints variable that we declared above.You might as well omit this line, but then track the changePoints in your MCMC monitors.interval_times := changePointsAgain, we constrain the root age as before and add the same moves for the tree.Substitution Model and other parametersThis part is taken from the previous exercises.Finalize and run the analysisIn the end, we need to wrap our model as before.Finally, we add the monitors and then run the MCMC.Remember to change the file names to avoid overwriting your previous results.monitors.append( mnModel(filename=\"output/horses_iso_SkyfishAC.log\",printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_SkyfishAC_nevents.log\",n_events,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_SkyfishAC.trees\",psi,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_SkyfishAC_NEs.log\",population_size,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_SkyfishAC_times.log\",interval_times,printgen=THINNING) )monitors.append( mnScreen(n_events, root_age, printgen=100) )mymcmc = mcmc(mymodel, monitors, moves, nruns=NUM_REPLICATES, combine=\"mixed\")mymcmc.burnin(NUM_MCMC_ITERATIONS*0.1,100)mymcmc.run(NUM_MCMC_ITERATIONS, tuning = 100)ResultsAfter running your analysis, you can plot the results using the R package RevGadgets.library(RevGadgets)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"num_grid_points = 500max_age_iso = 5e5min_age = 0spacing = \"equal\"population_size_log = \"output/horses_iso_SkyfishAC_NEs.log\"interval_change_points_log = \"output/horses_iso_SkyfishAC_times.log\"df &lt;- processPopSizes(population_size_log, interval_change_points_log, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points, max_age = max_age_iso, min_age = min_age, spacing = spacing)p &lt;- plotPopSizes(df) + ggplot2::coord_cartesian(ylim = c(1e3, 1e8))ggplot2::ggsave(\"figures/horses_iso_SkyfishAC.png\", p)Example output from plotting the Skyfish analysis. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Next ExerciseWhen you are done, have a look at the next exercise.  The GMRF model with trees as input data",
        "url": "/tutorials/coalescent/Skyfish.html",
        "index": "false"
      }
      ,
    
      "tutorials-coalescent-skyline-html": {
        "title": "Skyline Models",
        "content": "Skyline PlotsSkyline Plots are models for how population size changes through time.The classical skyline plot (Pybus et al. 2000) provided the first implementation of this idea.For each interval between two coalescent events, an effective population size was estimated.This led to a plot looking very similar to a skyline, thus giving the method its name (see  for a hypothetical example).The generalized skyline plot (Strimmer and Pybus 2001) aimed at reducing the noise from analyzing every single interval by grouping several coalescent events into one interval.This created a smoother curve.First, these models were used for maximum likelihood (ML) estimation of population sizes through time.By now, several extensions allowing for Bayesian estimation have been published (see for example Drummond et al. (2005), Heled and Drummond (2008), Minin et al. (2008)).In RevBayes, a Skyline plot method is implemented with constant or linear population size intervals.The length of the skyline intervals can either be defined by a specific number of intervals ending at coalescent events, or alternatively be chosen individually without depending on the coalescent events.In this exercise, each interval will group five coalescent events.Have a look at , for a hypothetical example with three events per interval.Hypothetical example of a classical skyline plot. $w_k$ are the waiting times with $k$ active lineages, $t_{c,k}$ are the coalescent events at the beginning of such a coalescent interval. Here, a population size for each interval between coalescent events is calculated. The bold line represents the maximum likelihood estimate of the population size.Hypothetical example of a Bayesian skyline plot with the interval length dependent on the number of coalescent events (coalescent event based).  $w_k$ are the waiting times with $k$ active lineages, $t_{c,k}$ are the coalescent events at the beginning of such a coalescent interval. $t_{i,j}$ mark the points of interval change.  Here, the change-points are coalescent event based, i.e., dependent on coalescent events. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Likelihood CalculationWe assume that the phylogeny of the samples is known.There are $n$ samples, with $k$ active lineages at the current point in time $t$.Time starts at $t = 0$.The waiting times between coalescent events $w_k$ are exponentially distributed with rate $c = \\frac{k (k-1)}{2N_e(t)}$ with $N_e$ being the population size.The likelihood for a given piecewise-constant population size trajectory is computed as the product of the probability density functions of the coalescent waiting times, which are calculated as follows:\\[p(w_k | t_{c,k}) = \\frac{k (k -1)}{2N_e(t_{c,k} + w_k)} exp \\left[ \\int_{t_{c,k}}^{t_{c,k}+w_k} \\frac{k (k -1)}{2N_e(t)} dt \\right]\\]Each $t_{c,k}$ is the beginning of the respective kth coalescent interval.The waiting times $w_k$ refer to the waiting time starting when there are $k$ active lineages.Inference Example  For your info  The entire process of the skyline estimation can be executed by using the mcmc_isochronous_Skyline.Rev script that you can download on the left side of the page.Save it in your scripts directory.You can type the following command into RevBayes:  source(\"scripts/mcmc_isochronous_Skyline.Rev\")    We will walk you through the script in the following section.We will mainly highlight the parts of the script that change compared to the constant coalescent model.Read the dataRead in the data as described in the first exercise.The Skyline ModelFor the skyline model, you need to decide on the number of intervals.In this case, we would like to have five coalescent events per interval, so we divide the number of coalescent events by five.With $n$ taxa, we expect $(n-1)$ coalescent events, until there is only one lineage left.NUM_INTERVALS = ceil((n_taxa - 1) / 5)For each interval, a population size will be estimated.Choose a prior and add a move for each population size.In the following loop, we also set the number of coalescent events per interval by dividing the number of coalescent events by the number of intervals.Remember that we chose the number of intervals based on our wish to have $5$ events per interval.for (i in 1:NUM_INTERVALS) {    pop_size[i] ~ dnUniform(0,1E6)    pop_size[i].setValue(100000)    moves.append( mvScale(pop_size[i], lambda=0.1, tune=true, weight=2.0) )    num_events[i] &lt;- ceil( (n_taxa-1) / NUM_INTERVALS )}The TreeNow, we will instantiate the stochastic node for the tree.The Skyline version of the Coalescent distribution function dnCoalescentSkyline takes the vector of population sizes and the taxa as input. By chosing methods=\"events\", the interval lengths will be chosen based on the number of events and we have to add the number of coalescent events to the input.psi ~ dnCoalescentSkyline(theta=pop_size, events_per_interval=num_events, method=\"events\", taxa=taxa)For later plotting and analyzing the population size curve, we need to retrieve the resulting interval times.interval_times := psi.getIntervalAges()For this analysis, we constrain the root age as before and add the same moves for the tree.Substitution Model and other parametersThis part is also taken from the constant coalescent exercise.Finalize and run the analysisFinally, we need to wrap our model as before.We add the monitors and then run the MCMC.Here, an additional monitor is added for the interval times.monitors.append( mnModel(filename=\"output/horses_iso_Skyline.log\",printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_Skyline.trees\",psi,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_Skyline_NEs.log\",pop_size,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_Skyline_times.log\",interval_times,printgen=THINNING) )monitors.append( mnScreen(pop_size, root_age, printgen=100) )ResultsAfter running your analysis, you can plot the results again using the R package RevGadgets.library(RevGadgets)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"num_grid_points = 500max_age_iso = 5e5min_age = 0spacing = \"equal\"population_size_log = \"output/horses_iso_Skyline_NEs.log\"interval_change_points_log = \"output/horses_iso_Skyline_times.log\"df &lt;- processPopSizes(population_size_log, interval_change_points_log, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points, max_age = max_age_iso, min_age = min_age, spacing = spacing)p &lt;- plotPopSizes(df) + ggplot2::coord_cartesian(ylim = c(1e3, 1e8))ggplot2::ggsave(\"figures/horses_iso_Skyline.png\", p)Example output from plotting the coalescent Skyline analysis run in this exercise. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Next ExerciseWhen you are done, have a look at the next exercise.  The Gaussian Markov Random Field (GMRF) modelAlternative PriorsThere are many different ways of defining priors for the population sizes.Here, we chose to draw the population sizes from a Uniform distribution and to treat the intervals as independent and identically distributed (iid).In other software, the default priors can be defined differently.For example, for the Bayesian Skyline Plot (Drummond et al. 2005), all but the first population size are drawn from an exponential distribution.The mean of this distribution is set to be the previous population size.This means that the population sizes in neighbouring intervals are correlated.For the first population size, a log uniform distribution was chosen.Additionally, the number of coalescent events per interval is not fixed, but estimated during the MCMC.In a Skyride analysis (Minin et al. 2008), the population size is directly estimated on a log scale.The intervals also are correlated, but a Gaussian Markov Random Field (GMRF) prior is used with the degree of smoothing being regulated by a precision parameter.In this approach, each interval includes one coalescent event, similar to the classical Skyline plot.The smoothing effect, however, reduces the noise in the resulting population size trajectory.For the Extended Bayesian Skyline Plot (Heled and Drummond 2008), the number of change-points is estimated by stochastic search variable selection.The intervals are considered to be iid.Also, the default demographic function usually is piecewise linear and not piecewise constant as in this tutorial.The estimation of the number of intervals can also be done slightly differently, as we show in the Skyfish tutorial.You can try and change the priors now accordingly.If you would like to have an example of what a RevBayes script with these different priors can look like, have a look at  the example script for a BSP analysis,  the example script for a Skyride analysis, or  the example script for an EBSP analysis. Note that the .Rev script does not apply stochastic variable search as in the original publication to determine the number of interval change-points. The script instead uses reversible jump MCMC.All three examples have the interval change-points on coalescent events (what we call “coalescent event based”, as in the skyline model presented here).In the next tutorial (The GMRF model), we use intervals with changing points independent from coalescent events (“specified”).",
        "url": "/tutorials/coalescent/Skyline.html",
        "index": "false"
      }
      ,
    
      "tutorials-morph-tree-v2-html": {
        "title": "Discrete morphology - Multistate Characters",
        "content": "IntroductionMorphological data is commonly used for estimating phylogenetic trees from fossils. This tutorial will focus on estimating phylogenetic trees from discrete characters, those characters which can be broken into non-overlapping character states. This type of data has been used for estimation of phylogenetic trees for many years. In the past twenty years, Bayesian methods for estimating phylogeny from this type of data have become increasingly common.This tutorial will give an overview of common models and assumptions when estimating a tree from discrete morphological data. We will use a dataset from (Zamora et al. 2013). This dataset contains 27 extinct echinoderm taxa and 60 binary and multistate characters.Overview of Discrete Morphology ModelsAs technologies for obtaining low-cost and high-throughput nucleotide sequence data have become available, many scientists have become reliant on molecular data for phylogenetics. However, morphological data remain the only direct observations we have of most extinct organisms, and are an independent data source for understanding phylogeny. Many of the phylogenetic methods we will discuss in this tutorial were invented for use with sequence data. However, these methods are still very useful for discrete morphological data. We will examine some common assumptions for modeling data in a phylogenetic context, then move on to look at relaxing these assumptions.Modeling discrete morphological data requires an understanding of the underlying properties of the data. When we work with molecular data, we know a priori that certain types of changes are more likely than others. For example, changes within a type of base (purine and pyrimidine) are much more likely than changes between types of bases. This information can be used to add parameters to the phylogenetic model. There are no equivalent and generalizable truths across characters in a morphological data matrix. For example, while 0 and 1 are commonly coded to “presence” and “absence”, this is not always the case, nor are all characters atomized at the same magnitude. For instance, at one character, changing character states may not reflect a large amount of genetic changes. Theca shape (character 2 in the Zamora et al. 2013 dataset), for example appears quite labile. At another, the changes to the character state may reflect a rearrangement of genetic elements, or might have larger ramifications for the organism’s life and behavior. Character 38, the central plate of the lintel, may be one such character, as it changes seldom.When we work with morphological data in a Bayesian context, we are performing these analyses after a long history of workers performing phylogenetic analysis in a maximum parsimony framework. Under maximum parsimony, trees are proposed. The number of changes in the data implied by the tree are then counted. The tree implying the fewest changes is considered the best. There may be multiple most parsimonious trees in a dataset. Parsimony has been the dominant method for estimating phylogenetic trees from discrete morphological data. Characters that cannot be used to discriminate between tree topologies are not typically collected by workers using parsimony. For example, characters that do not vary are not collected, as they all have the same length (0 steps) on a tree. Likewise, autapomorphies are typically not collected. As we will see later, this has ramifications for how we model the data.Graphical model showing the Mk model (left panel) and corresponding Rev code (right panel).For many years, parsimony was the only way to estimate a phylogenetic tree from morphological data. In 2001, Paul Lewis published the Mk model of morphological evolution. The Mk model (Lewis 2001) is a generalization of the Jukes-Cantor model (Jukes and Cantor 1969) of nucleotide sequence evolution. This model, while simple, has allowed researchers to access the toolkit of phylogenetic methods available to researchers working with other discretely-valued data, such as nucleotides or amino acids.The Mk ModelAs mentioned above, the Mk model is a generalization of the JC model. This model assumes that all transitions between character states are equal, and that all characters in the matrix have the same transition matrix. The transition matrix for a binary trait looks like so:\\[Q = \\begin{pmatrix} -\\mu_0 &amp; \\mu_{01} \\\\\\mu_{10} &amp; -\\mu_1  &amp;\\\\\\end{pmatrix} \\mbox{  ,}\\]In this matrix, $\\mu$ represents the transition probability between the two states that follow it. A transition matrix for multistate data simply expands.\\[Q = \\begin{pmatrix} -\\mu_0 &amp; \\mu_{01} &amp; \\mu_{02} &amp; \\mu_{03} \\\\\\mu_{10} &amp; -\\mu_1  &amp; \\mu_{12} &amp; \\mu_{13} \\\\\\mu_{20} &amp; \\mu_{21} &amp; -\\mu_2  &amp; \\mu_{23} \\\\\\mu_{30} &amp; \\mu_{31} &amp; \\mu_{32} &amp; -\\mu_3\\end{pmatrix} \\mbox{  ,}\\]However, the Mk model sets transitions to be equal from any state to any other state. In that sense, our multistate matrix really looks like this:\\[Q = \\begin{pmatrix} -(k-1)\\mu &amp; \\mu &amp; \\mu &amp; \\mu \\\\\\mu &amp; -(k-1)\\mu  &amp; \\mu &amp; \\mu \\\\\\mu &amp; \\mu &amp; -(k-1)\\mu  &amp; \\mu \\\\\\mu &amp; \\mu &amp; \\mu &amp; -(k-1)\\mu \\\\\\end{pmatrix} \\mbox{  ,}\\]You might notice that these transition rates are not different than what we might expect from an equal-weights parsimony matrix. In practice, the Mk model makes very few assumptions due to the complexity and non-generalizability of morphological data.This model may strike some readers as too simplistic to be adequate for morphological data. However, Bayesian methods are less likely to be mislead by homoplasy than is parsimony (Felsenstein 1983). More recent work has demonstrated that the model outperforms parsimony in many situations, particularly those in which there is high homoplasy (Wright and Hillis 2014), with empirical work demonstrating that it fits many datasets reasonably well (Wright et al. 2016).In the first part of this tutorial, we will estimate a tree under the Mk model as proposed by Lewis (2001). We will then relax core parameters of the model.Ascertainment BiasOne remaining component of the model we have not yet discussed is ascertainment bias. Because workers using parsimony do not collect invariant characters and seldom collect autapomorphies, our data are biased. Imagine, for a moment, that you were to measure the average height in a room. But first, you asked the 10 shortest people to leave. Your estimate of the average height would be too tall! In effect, this happens in the morphological data, as well. Because the characters with the fewest changes are not collected, we over estimate the amount of evolutionary change on the tree. At the time of publication, Lewis (2001) also included a correction factor for this bias.These original corrections involved simulating parsimony non-informative characters along each proposed tree. These would be used to normalize the likelihood value. While this procedure is statistically valid, it is a bit slow. There are multiple ways to perform this correction (Allman and Rhodes 2008). RevBayes uses a dynamic likelihood approach to avoid repeated simulations.Example: Inferring a Phylogeny of Extinct Cinctans Using the Mk ModelTutorial FormatThis tutorial follows a specific format for issuing instructions and information.  The boxed instructions guide you to complete tasks that are not part of the RevBayes syntax, but rather direct you to create directories or files or similar.Information describing the commands and instructions will be written in paragraph-form before or after they are issued.All command-line text, including all Rev syntax, are given inmonotype font. Furthermore, blocks of Rev code that are needed to build the model, specify the analysis, or execute the run are given in separate shaded boxes. For example, we will instruct you to create a constant node called example that is equal to 1.0 using the &lt;- operator like this:example &lt;- 1.0It is important to be aware that some PDF viewers may render some characters given as differently. Thus, if you copy and paste text from this PDF, you may introduce some incorrect characters. Because of this, we recommend that you type the instructions in this tutorial or copy them from the scripts provided.Data and FilesGetting Started  First create a directory for this tutorial and name it RB_MultistateCharacters_Tutorial, or any name you like. Navigate to this new directory and create subdirectories called data and scripts.  Download the data file called Cinctans.nex and save it to your data directory.Creating Rev Files  In this exercise, we advise you to record all your commands via a text editor in a set of files – corresponding to the different modules of the model – that will be easily managed and interchanged. Call them mcmc_mk_tutorial.Rev, mk_model_tree.Rev, mk_model_gamma.Rev, and place them in scripts.When you execute RevBayes in this tutorial, you will do so within the main directory  (RB_MultistateCharacters_Tutorial), thus, if you are using a Unix-based operating system, we recommend that you add the RevBayes binary to your path. Alternatively make sure that you set the  working directory to the directory you stored the scripts and data in.In the following sections you will begin the mcmc_mk_tutorial.Rev file and write the Rev commands for loading in the taxon list and managing the data matrices. Then, starting in , you will move on to specifying each of the Mk model components, in files mk_model_tree.Rev and mk_model_gamma.Rev. Once the model specifications are complete, you will complete the MCMC script with the instructions given in .Load Data MatricesRevBayes uses the function readDiscreteCharacterData() to load a data matrix to the workspace from a formatted file. This function can be used for both molecular sequences and discrete morphological characters. Import the morphological character matrix and assign it the variable morpho.morpho &lt;- readDiscreteCharacterData(\"data/Cinctans.nex\")Create Helper VariablesWe will dig into the model momentarily. But first, we will create some variables that are used in our analysis, but are not parameters. We will assign these variables with the constant node assignment operator, &lt;-. Even though these values are used in our scripts, they are not parameters of the model.We will first create a constant node called num_taxa that is equal to the number of species in our analysis (27). We will also create a constant node called num_branches representing the number of branches in the tree, and one of the taxon names. This list will be used to initialize the tree.num_taxa &lt;- morpho.size()num_branches &lt;- 2 * num_taxa - 2taxa &lt;- morpho.names()Next, create a workspace variable called moves, a vector containing all of the MCMC moves used to propose new states for every stochastic node in the model graph.moves = VectorMoves()One important distinction here is that moves is part of the RevBayes workspace and not the hierarchical model. Thus, we use the workspace assignment operator = instead of the constant node assignment &lt;-.Provided that you started RevBayes from the correct directory, you can then use the source()function to load the model files into RevBayes from your Rev-script files (mk_model_tree.Rev and mk_model_gamma.Rev ).source(\"scripts/mk_model_tree.Rev\")source(\"scripts/mk_model_gamma.Rev\")The Mk ModelIn this section you will move on to specifying the Mk model components, starting in file mk_model_tree.Rev. First, we will create a joint prior on the branch lengths.br_len_lambda ~ dnExp(0.2)moves.append(mvScale(br_len_lambda, weight=2))This prior specifies that branch lengths will be drawn from an exponential distribution with parameter 0.2.Now, we combine the branch lengths with a uniform prior on topology to make a tree. The uniform prior simply means no tree is more likely a priori than any other. This can be easily changed, for example, to use a starting tree. We then specify MCMC moves on the topology, NNI and SPR. These moves propose new topologies. In this way, we propose and evaluate new sets of relationships. We perform these moves frequently because these parameters are really important. We will also move each of the branch lengths each iteration. The scale move scales the current branch length. Finally, we monitor the tree length by adding a deterministic node with the operator :=. This is a quantity many biologists are interested in.phylogeny ~ dnUniformTopologyBranchLength(taxa, branchLengthDistribution=dnExponential(br_len_lambda))moves.append(mvNNI(phylogeny, weight=num_branches/2.0))moves.append(mvSPR(phylogeny, weight=num_branches/10.0))moves.append(mvBranchLengthScale(phylogeny, weight=num_branches))tree_length := phylogeny.treeLength()Move on to file mk_model_gamma.Rev, where we will add Gamma-distributed rate variation and specify moves on the parameter of the Gamma distribution.alpha_morpho ~ dnUniform( 0, 1E6 )rates_morpho := fnDiscretizeGamma( alpha_morpho, alpha_morpho, 4 )moves.append(mvScale(alpha_morpho, lambda=1, weight=2.0))Next we will create a $Q$-matrix. Recall that the Mk model is simply a generalization of the JC model. Therefore, we will create a $Q$-matrix using fnJC, which initializes $Q$-matrices with equal transition probabilities between all states. Since we have multistate data, we need to specify different $Q$-matrices for the different number of character states. For example, it would not make sense to model a 5-state character using a model saying there are only two character states.To do this, we have written a loop in which we break up the data set into partitions according to the number of character states that character has. Then, we specify a $Q$-matrix in the correct dimensions. We do not retain any partitions that do not have any characters. For example, if we tried to partition the characters with 4 states, and there were none, we would not create a $Q$-matrix.Then, we combine each partition, Gamma-distributed rate heterogeneity, and the tree together into what is called the phyloCTMC. This is the joint set of model parameters that will be used the model these data. Each partition is then clamped to its model.n_max_states &lt;- 7idx = 1morpho_bystate[1] &lt;- morphofor (i in 2:n_max_states) {    morpho_bystate[i] &lt;- morpho                                # make local tmp copy of data    morpho_bystate[i].setNumStatesPartition(i)                 # only keep character blocks with state space equal to size i    nc = morpho_bystate[i].nchar()                             # get number of characters per character size with i-sized states    if (nc &gt; 0) {                                              # for non-empty character blocks        q[idx] &lt;- fnJC(i)                                      # make i-by-i rate matrix        m_morph[idx] ~ dnPhyloCTMC( tree=phylogeny,                                    Q=q[idx],                                    nSites=nc,                                    siteRates=rates_morpho,                                    type=\"Standard\")           # create model of evolution for the character block        m_morph[idx].clamp(morpho_bystate[i])                  # attach the data        idx = idx + 1                                          # increment counter        idx    }}We see some familiar pieces: tree, $Q$-matrix and rates_morpho.We also have two new keywords: data type and coding. The data type argument specifies the type of data - in our case, “Standard”, the specification for morphology.All of the components of the model are now specified.Complete MCMC AnalysisCreate Model ObjectBack in file mcmc_mk_tutorial.Rev, we can now create our workspace model variable with our fully specified model DAG. We will do this with the model() function and provide a single node in the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph and allows us to pass the model to various functions that are specific to our MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our Rev-script is to specify the monitors and output file names. For this, we use the vector called monitors that will take each sample and record or output our MCMC.monitors = VectorMonitors()The first monitor we create will monitor every named random variable in our model graph. This will include every stochastic and deterministic node using the mnModel monitor. The only parameter that is not included in the mnModel is the tree topology. Therefore, the parameters in the file written by this monitor are all numerical parameters written to a tab-separated text file that can be opened by accessory programs for evaluating such parameters. We will also name the output file for this monitor and indicate that we wish to sample our MCMC every 10 cycles.monitors.append( mnModel(filename=\"output/mk_gamma.log\", printgen=10))The mnFile monitor writes any parameter we specify to file. Thus, if we only cared about the branch lengths and nothing else (this is not a typical or recommended attitude for an analysis this complex) we wouldn’t use the mnModel monitor above and just use the mnFile monitor to write a smaller and simpler output file. Since the tree topology is not included in the mnModel monitor (because it is not numerical), we will use mnFile to write the tree to file by specifying our phylogeny variable in the arguments.monitors.append( mnFile(filename=\"output/mk_gamma.trees\", printgen=10, phylogeny))The third monitor we will add to our analysis will print information to the screen. Like with mnFile we could tell mnScreen which parameters we’d like to see updated on the screen.monitors.append( mnScreen(printgen=100))Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now create the workspace variable that defines our MCMC run. We do this using the mcmc() function that simply takes the three main analysis components as arguments. Furthermore, we will perform and combine two independent MCMC runs to ensure proper convergence.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")The MCMC object that we named mymcmc has a member method called .run(). This will execute our analysis and we will set the chain length to 10000 cycles using the generations option.mymcmc.run(generations=10000, tuningInterval=200)Once our Markov chain has terminated, we will want RevBayes to close. Tell the program to quit using the q() function.q()  You made it! Save all of your files.Execute the MCMC AnalysisWe will now execute an MCMC analysis.  Begin by running the RevBayes executable on your MCMC script. In Unix systems, from the RB_MultistateCharacters_Tutorial folder directory, type thefollowing in your terminal (if the RevBayes binary is in your path) :rb scripts/mcmc_mk_tutorial.RevWhen the analysis is complete, RevBayes will quit and you will have a new directory called output that will contain all of the files you specified with the monitors.We can look at the log files in the software Tracer. We can also calculate several different types of summary trees:trace = readTreeTrace(\"output/mk_gamma.trees\")      # Read in the tree tracemccTree(trace, file=\"output/mk_gamma.mcc.tre\" )     # Construct and save the maximum clade credibility (MCC) treeRevBayes can calculate MCC trees, MAP trees, and consensus trees. Have each person at your table try one, and see how they differ.Choose Your Own AdventureFor this next portion of the tutorial, we will choose and edit some pieces of the model. Feel free to add your own innovations. For this, you will create copies of your previous Rev scripts, and modify them. Make sure to change the file names in the source() function accordingly before running the MCMC!Lognormally-distributed among-character rate variationAs we discussed, there is reason to believe a lognormal distribution may fit morphological data better than a gamma. RevBayes makes it very natural to discretize any distribution you might like to work with.rates_morpho := fnDiscretizeDistribution( dnLognormal(ln(alpha_morpho), 0.01), 4 )Can you figure out which line you might replace with this code? Do it, and change your output files to indicate that this is estimation uses a lognormal distribution. Re-run your Rev script.Ascertainment BiasAs discussed earlier in the section Ascertainment_Bias, we also need to correct for ascertainment bias.  Create a copy of your previous Rev script, and call it mkv_model_gamma.Rev.You will need to modify the Revcode provided in this section in this file.In RevBayes it is actually very simple to add a correction for ascertainment bias. You only need to set the option coding=\"variable\" in the dnPhyloCTMC. Coding specifieswhat type of ascertainment bias is expected. We are using the variable correction, as we have no invariant character in our matrix. If we also lacked parsimony non-informative characters, we would use the coding informative.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q_morpho, type=\"Standard\", coding=\"variable\")Relaxing Character State SymmetryThe Mk model makes a number of assumptions, but one that may strike youas particularly unrealistic is the assumption that characters are equally likely to change from any one state to any other. That means that a trait is as likely to be gained as lost. While this may hold true for some traits, we expect that it may be untrue for many others.RevBayes has functionality to allow us to relax this assumption (Nylander et al. 2004). We dothis by specifying a beta prior on state frequencies. Stationary frequencies impact how likely we are to see changes in a character. For example, it may be very likely, in a character, to change from 0 to 1. But if the frequency of 0 is very low, we will still seldom see this change.We can think of a Q matrix as looking like so:\\[Q = \\begin{pmatrix} -\\mu_0\\pi0 &amp; \\mu_{01}\\pi0 \\\\\\mu_{10}\\pi1 &amp; -\\mu_1\\pi1  &amp;\\\\\\end{pmatrix} \\mbox{  ,}\\]In which the probability of changing states depends not solely on the transition probability, but also the frequency of the starting state. For example, if we have a rare character state, we do not expect to see many transitions from the rare state to another. $\\pi$ is the value chosen to represent state frequency commonly in phylogenetic models.We can exploit the relationship between state frequencies and observedchanges to allow for variable Q-matrices across characters. To do this, we generate a beta distribution on state frequencies, and use the state frequencies from that distribution to generate a series of Q-matrices used to evaluate our data (Pagel and Meade 2004; Nylander et al. 2004; Wright et al. 2016).This type of model is called a mixture model. There are assumed tobe subdivisions in the data, which may require different parameters (inthis case, state frequencies). These subdivisions are not defined apriori. This model has previously been shown to be effective for arange of empirical and simulated datasets (Wright et al. 2016).Graphical model demonstrating thediscretized beta distribution for allowing variable state frequencies.Modifying the Rev-script  Make a copy of the Rev script you made earlier. Call itmkv_model_gamma_discretized.Rev. This new script willcontain the new model parameters and models.We will use a discretized beta distribution to place a prior on the state frequencies.The beta distribution has two parameters, $\\alpha$ and $\\beta$. These twoparameters specify the shape of the distribution. State frequencies willbe evaluated according to this distribution, in the same way that ratevariation is evaluated according to the gamma distribution. Thediscretized distribution is split into multiple classes, each with itsown set of frequencies for the 0 and 1 characters. The number of classescan vary; we have chosen 4 for tractability. Note that we need to make sure that this discretization results in a symmetric model, therefore we will use only one parameter for the beta distribution: beta_scale such that $\\alpha = \\beta$.To simplify the analysis, and make it tractable within the time period, we will only use the binary characters.morpho.setNumStatesPartition(2)Next, we will set up the prior on $\\beta$.num_cats = 4beta_scale ~ dnLognormal( 0.0, sd=2*0.587405 )moves.append( mvScale(beta_scale, lambda=1, weight=5.0 ) )Above, we initialized the number of categories, the parameters of thebeta distribution, and the moves on these parameters.Next, we set the categories to each represent a quadrant of the betadistribution specified by beta_scale.cats := fnDiscretizeBeta(beta_scale, beta_scale, num_cats)If you were to print the cats variable, you would see a list of statefrequencies like so:[ 0.011, 0.236, 0.764, 0.989 ]Using these state frequencies, we will generate a new vector of Q-matrices. Because we are varying the state frequencies, we must use a Q-matrix generation function that allows for state frequencies to vary asa parameter. We will, therefore, use the fnF81 function.for (i in 1:cats.size()){Q[i] := fnF81(simplex(abs(1-cats[i]), cats[i]))}Additionally, in RevBayes we need to specify the probabilities that a site evolves according to oneof the Q-matrices. For this model the probabilities must be equal because we need to guarantee thatthe model is symmetric. Thus, we use a simplex function to create a vector that sums to 1.0.  mat_prior &lt;- rep(1,num_cats)  matrix_probs ~ dnDirichlet(mat_prior)  moves.append( mvBetaSimplex(matrix_probs, weight=3.0) )  moves.append( mvDirichletSimplex(matrix_probs, weight=1.5) )Finally, we make the CTMC for the model:phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q, type=\"Standard\", coding=\"variable\", siteMatrices=matrix_probs)phyMorpho.clamp(morpho)This model can be embedded in the for loop seen in  to work with multi-state characters. An example is provided in script mkv_model_gamma_discretized_multistate.Rev.",
        "url": "/tutorials/morph_tree/V2.html",
        "index": "true"
      }
      ,
    
      "tutorials-mcmc-archery-html": {
        "title": "Introduction to MCMC using RevBayes",
        "content": "OverviewThis tutorial is intended to provide a introduction to the basics ofMarkov chain Monte Caro (MCMC) using the Metropolis-Hastings algorithm.This will provide a brief introduction to MCMC moves as well as priordistributions. We begin with a simple example of estimating theprobability distribution of an archer’s ability to shoot at a target,and the distance those arrows land from the center. We will simulatedata using this example and attempt to estimate the posteriordistribution using a variety of MCMC moves.Modeling an Archer’s Shots on a TargetRepresentation of the archery data used in this tutorial.Each yellow dot represents the position of an arrow shot by an archer.The distance of each arrow from the the center of the target is assumed to be exponentially distributed with mean $\\mu$.We’ll begin our exploration of Bayesian inference with a simple archerymodel. For this model, there is an unknown archer shooting $n$ arrows ata target (see ). The distance $d$ of each arrowfrom the target’s center is measured. Let’s assume that the distance ofeach arrow from the bullseye follows an exponentialdistribution—i.e., $d\\sim\\mbox{Exp}(\\mu^{-1})$. This implies the archer has an inherent ability to shoot arrows at an average distance $\\mu$. Then, the probability density of each arrow distance $d_i$ is\\[\\begin{aligned}P(d_i \\mid \\mu) = \\frac{1}{\\mu} e^{-d_i/\\mu}.\\end{aligned}\\]Simple intuition suggests that, given that we observe $n$ arrows, a goodestimate of $\\mu$ is the average of all the arrow distances$\\bar d = \\frac{1}{n}\\sum_{i=1}^n d_i$. Indeed this is the maximumlikelihood estimate! In fact, given $n$ arrows whose distances follow anexponential distribution, it turns out that the observed average$\\bar d$ follows a gamma distribution, with parameters $n$ and $n/\\mu$,\\[\\begin{aligned}P(\\bar d \\mid \\mu,n) = \\frac{(n/\\mu)^n}{\\Gamma(n)} {\\bar d}\\,^{n-1}e^{-n\\bar d /\\mu}.\\end{aligned}\\]In this case, the average $\\bar d$ acts as a sufficient statistic for$\\mu$. This means that it tells just as much about $\\mu$ as thecollection of individual arrow distances. Therefore, we will use aGamma$(n, n/\\mu)$ distribution on $\\bar d$ as the likelihood of ourdata.From Bayes’ theorem, the posterior distribution of $\\mu$ given$\\bar d$, $P(\\mu \\mid \\bar d)$, is:\\[\\begin{aligned}P\\left(\\mu \\mid \\bar d\\right) = \\frac{P\\left(\\bar d \\mid \\mu\\right) \\times P\\left(\\mu\\right)}{P\\left(\\bar d\\right)}\\end{aligned}\\]Where $P(\\mu \\mid \\bar d)$ is our posterior distribution, $P(\\bar d \\mid \\mu)$ is our likelihood or data distribution, $P(\\mu)$ is our prior distribution, and $P(\\bar d)$ is our marginal likelihood. The take-home message here is that, if we’re interested in doing Bayesian inference for the archery model, we need to specify a likelihood function and a prior distribution for $\\mu$. In virtually all practical cases, we cannot compute the posterior distribution directly and instead use numerical procedures, such as a Markov chain Monte Carlo (MCMC) algorithm. Therefore, we will also have to write a MCMC algorithm that samples parameter values in the frequency of their posterior probability.We’ll use a simple exponential distribution as a prior on the parameter of the model, $\\mu$. The exponential distribution has one parameter $\\alpha$ representing our prior belief about the mean arrow distance . Different choices for $\\alpha$ represent different prior beliefs.Exponential distribution with one parameter $\\alpha$. This distribution is used as a prior distribution on the average arrow distance $\\mu$.Here we show different curves for the exponential distribution when using different parameters. shows the graphical model for the archery model. This nicely visualizes the dependency structure in the model. We see that the parameter $\\alpha$ is drawn in a solid square, representing that this variable is constant (i.e., it takes a “known” value). Following the graph in , we see an arrow connecting $\\alpha$ and the variable $\\mu$. That simply means that $\\mu$ depends on $\\alpha$. More specifically, $\\mu$ is a stochastic variable (shown as a solid circle) that is drawn from an exponential distribution with parameter $\\alpha$. Another constant variable, $n$, represents the number of shots taken by the archer. Finally, we have the observed data $\\bar d$ which is drawn from a gamma distribution with parameters $\\mu$ and $n$, as can be seen by the arrows pointing from those parameters to $d$. Furthermore, the solid circle of $\\bar d$ is shaded which means that the variable has data attached to it.Graphical model for the archery model.Writing MCMC from ScratchTutorial FormatThis tutorial follows a specific format for issuing instructions andinformation.The boxed instructions guide you to complete tasks that are not part of the RevBayes syntax, but rather direct you to create directories or files or similar.Information describing the commands and instructions will be written in paragraph-form before or after they are issued.All command-line text, including all Rev syntax, are given in monotype font. Furthermore, blocks of Rev code that are needed to build the model, specify the analysis, or execute the run are given in separate shaded boxes. For example, we will instruct you to create a new variable called n that is equal to 10 using the = operator like this:n = 10Create Your Script FileMake yourself familiar with the example script called archery_MH.Rev which shows the code for the following sections. Then, start a new and empty script in your text editor and follow each step provided as below.Name the script file my_archery_MH.Rev or anything you’d like.The Metropolis-Hastings AlgorithmThough RevBayes implements efficient and easy-to-use Markov chain Monte Carlo (MCMC) algorithms, we’ll begin by writing one ourselves to gain a better understanding of the moving parts. The Metropolis-Hastings MCMC algorithm (Metropolis et al. 1953) (Hastings 1970) proceeds as follows:      Generate initial values for the parameters of the model (in thiscase, $\\mu$).        Propose a new value (which we’ll call $\\mu^\\prime$) for someparameters of the model, (possibly) based on their current values        Calculate the acceptance probability, $R$, according to:\\[\\begin{aligned}R &amp;= \\text{min}\\left\\{1, \\frac{P(\\bar d \\mid \\mu^\\prime)}{P(\\bar d \\mid \\mu)} \\times \\frac{P(\\mu^\\prime)}{P(\\mu)} \\times \\frac{q(\\mu)}{q(\\mu^\\prime)} \\right\\}\\\\  &amp;= \\text{min}\\left\\{1, \\text{likelihood ratio} \\times \\text{prior ratio} \\times \\text{proposal ratio} \\right\\},\\end{aligned}\\]    where the proposal ratio (also called the Hastings ratio) ensures the correct target density, even if the move is biased.        Generate a uniform random number $u$ between 1 and 0.          if $u&lt;R$:              then accept the move and set $\\mu = \\mu^\\prime$.            else (if $u \\geq R$):              the value of $\\mu$ does not change and the move is rejected:$\\mu = \\mu$.                  Record the values of the parameters.        Return to step 2 many, many times, keeping track of the value of $\\mu$.  Reading in the dataSince we do not have access to archery data, we will simulate the the shots of our archer using the simulation tools in RevBayes. By simulating the data, we can also evaluate how well our moves and prior model perform—i.e., how robust and accurate our estimators are. After completing this exercise, feel free to repeat it and alter the true values to see how they influence the posterior distribution.# Simulate some data (i.e. shoot some arrows)# First we need the number of arrows to shootn = 10# Then we need a true mean distancemu_true = 1# Simulate the observed mean distance of the arrows shot from a gamma distributionarrow_mean = rgamma(1, n, n/mu_true)[1]The Rev code above uses the rgamma() function to simulate a singleobserved arrow_mean from $n=10$ arrows shot on target. The [1]following the rgamma() function is needed because this function alwaysreturns a vector even when we only request a single value. Thus, inorder to treat arrow_mean as a single value, we have to request thefirst element of the vector returned by that function.Initializing the Markov chainWe have to start the MCMC off with some initial values for all of theparameters. One way to do this is to randomly draw values of theparameters (just $\\mu$, in this case) from the prior distribution. We’llassume a simple exponential prior distribution; that is, one with$\\alpha = 1$.# Initialize the chain with some starting valuealpha = 1.0mu = rexp(1, alpha)[1]Likelihood functionNext we will specify the likelihood function, which will compute theprobability of our data given the prior model. We use the gammadistribution for the likelihood. Since the likelihood is defined onlyfor values of $\\mu$ greater than 0, we return a likelihood of 0.0 if$\\mu$ is negative:# Define the likelihood function on the meanfunction likelihood(mu){    if(mu &lt; 0.0)        return 0.0    return dgamma(arrow_mean, n, n/mu, log=false)}In Rev, we can create a user-defined function using thefunction keyword. In our function definition above, likelihood()takes a single value as an argument that is expected to be the mean($\\mu$) value. All other parameters in our function are expected to bedefined before likelihood() is called.Prior distributionSimilarly, we need to define a function for the prior distribution.Here, we use the exponential probability distribution for the prior on$\\mu$:# Define the prior function on the meanfunction prior(mu){    if(mu &lt; 0.0)        return 0.0    return dexp(mu, alpha, log=false)}Monitoring parameter valuesAdditionally, we are going to monitor, i.e. store, parameter values into a file during the MCMC simulation. For this file we need to write the columnheaders in the first line of our output file, which we will namearchery_MH.log (you may have to change the newline characters from\\n to \\r\\n if you’re using a Windows operating system.):# Prepare a file to log our sampleswrite(\"iteration\",\"mu\",\"\\n\",file=\"archery_MH.log\")write(0,mu,\"\\n\",file=\"archery_MH.log\",append=TRUE)We’ll also monitor the parameter values to the screen, so let’s printthe initial values:# Print the initial values to the screenprint(\"iteration\",\"mu\")print(0,mu)Writing the Metropolis-Hastings AlgorithmAt long last, we can write our MCMC algorithm. First, we define howoften we print to file (i.e., monitor); this is called thinning if we do not choose to save every value of our parameter to file. If we set the variable printgen=1, then we will store the parameter values at every iteration; if we instead choose printgen=10, then we’ll only save the values every $10^{th}$ step in our Markov chain.# Write the MH algorithm    printgen = 10We will repeat this resampling procedure many times and iterate the MCMCusing a for loop (e.g., step 6 in The Metropolis-Hastings Algorithm Section). We will startthis part by defining the number of iterations for our MCMC ( reps =10000), and writing the first line of our ‘for‘ loop. We’ll also definea variable delta (explained momentarily).reps = 10000delta = 1for(rep in 1:reps){In Rev, the contents of every for loop must be enclosed within a setof ‘curly braces’ . Our loop will not be complete until we finish it andadd the closing brace. Additionally, it is good style to make our loopsreadable by indenting the contents within the curly braces. We recommendthat you use 4 spaces to represent these indents.For our MCMC algorithm, the first thing we do is generate a new value of$\\mu^\\prime$ to evaluate (step 2 of the The Metropolis-Hastings Algorithm Section). We’ll propose a new value of $\\mu$ bydrawing a random number from a uniform window and then adding thisrandom number to the current value (i.e. centered on the previous value). Thevalue of delta defines the width of the uniform window from which wedraw new values. Thus, if delta is large, then the proposed values aremore likely to be very different from the current value of $\\mu$.Conversely, if delta is small, then the proposed values are morelikely to be very close to the current value of $\\mu$. By changing thevalue of delta we can tune the behavior of the proposal, and thereforedelta is called a tuning parameter.# Propose a new value of pmu_prime &lt;- mu + runif(n=1,-delta,delta)[1]Next, we compute the proposed likelihood and prior probabilities usingthe functions we defined above, as well as the acceptance probability,$R$ (step 3 of the The Metropolis-Hastings Algorithm Section):# Compute the acceptance probabilityR = ( likelihood(mu_prime)/likelihood(mu) ) * ( prior(mu_prime)/prior(mu) )Then, we accept the proposal with probability $R$ and reject otherwise(step 4 of the The Metropolis-Hastings Algorithm Section):# Accept or reject the proposalu = runif(1,0,1)[1]if(u &lt; R){# Accept the proposal    mu = mu_prime }Finally, we store the current value of $\\mu$ in our log file (step5 of the the The Metropolis-Hastings Algorithm Section). Here, we actually check if we want to store the value during this iteration.if ( (rep % printgen) == 0 ) {    # Write the samples to a file    write(rep,mu,\"\\n\",file=\"archery_MH.log\",append=TRUE)    # Print the samples to the screen    print(rep,mu)}and close the for loop}Execute the MCMC Analysis {#subsect:Exercise-RunMCMC}Now that you have defined your model and written functions to compute the probability and sample values of $\\mu$, you are now ready to run your analysis.Begin by running the RevBayes executable. You can do this by navigating to the folder containing your RevBayes executable and running it. If you’re on a Unix system you can do this by typing:rbAlternatively, if you are on a Unix system and the RevBayes binary is in your path, you only have to type the following from any directory:rbNow you can run your RevBayes script:source(\"my_archery_MH.Rev\")Exercise 1      Write and execute the script outlined above, which you can give anyname you like (there is also an example filecalled archery_MH.Rev).        The .log file will contain samples from the posterior distributionof the model. Open the file in Tracer to learn aboutvarious features of the posterior distribution, for example: theposterior mean or the 95% credible interval.  Pretty awesome, right?Below we show an example of the obtained output inTracer. Specifically,  showsthe sample trace (left) and the estimated posterior distribution of$\\mu$ (right). There are other parameters, such as the posterior meanand the 95% HPD (highest posterior density) interval, that you canobtain from Tracer.The Trace of sample from an MCMC simulation. Right: The approximated posterior probability distribution for $\\mu$.More on Moves: Tuning and WeightsIn the previous example we hard coded a single move updating thevariable $\\mu$ by drawing a new value from a sliding window. There areother ways how to propose new values; some of which are more efficientthan others.First, let us rewrite the MCMC loop so that instead we call a function,which we name move_slide for simplicity, that performs the move:Slide moveNow we need to actually write the move_slide function. We mostly justcopy the code we had before into a dedicated functionThere are a few things to consider in the function move_slide. First,we do not have a return value because the move simply changes thevariable $\\mu$ if the move is accepted. Second, in addition to thetuning parameter delta, we expect an argument called weight whichwill tell us how often we want to use this move. Otherwise, thisfunction does exactly the same what was inside the for loop previously.(Note that you need to define this function before the for loop in yourscript).Experiment with different values for delta and see how the effectivesample size (ESS) changes.There is, a priori, no good method for knowing what values of delta are most efficient. However, there are some algorithms implemented in RevBayes, called auto-tuning, that will estimate good values for delta.Scaling moveAs another move we will write a scaling move. The scaling move proposesan update by drawing a random number from a $Uniform(-0.5,0.5)$distribution, exponentiating the random number, and then multiplyingthis scaling factor by the current value. An interesting feature of thismove is that it is not symmetrical and thus needs a Hastings ratio (thisis the same as the proposal ratio given in Section[sect:MH_algorithm]). The Hastings ratio is rather trivial in thiscase, and one only needs to multiply the acceptance rate by the scalingfactor.As before, this move has a tuning parameter called lambda.The sliding-window and scaling moves are very common and popular moves in RevBayes. The code examples here are actually showing the exact same equation as implemented internally. It will be very useful for you to understand these moves.Exercise 2      Rewrite your previous script to include these two different moves,and re-run the script to estimate the posterior distribution of$\\mu$ again.        Use only a single move and set printgen=1. Which move has the bestESS?        How does the ESS change if you use tuning parameter valuesdelta=10 or delta=0.1 for the sliding-window move? What aboutlambda=10 or lambda=0.1 for the scaling move?        You can keep track of your results using the following table.  The Metropolis-Hastings Algorithm with the Real RevBayesWe’ll now specify the exact same model in Rev using the built-inmodeling functionality and moves. It turns out that the ‘Rev‘ code tospecify the above model is extremely simple and similar to the one weused before. Again, we start by “reading in” (i.e. making up) ourdata.# Simulate some data (i.e. shoot some arrows)# First we need the number of arrows to shootn = 10# Then we need some true mean distancemu_true = 1# Simulate the observed mean distance of the arrows we shotarrow_mean = rgamma(1, n, n/mu_true)[1]Now we specify our prior model.# Specify the prior distributionalpha &lt;- 1.0mu ~ dnExponential(alpha)One difference between RevBayes and the MH algorithm that we wroteabove is that many MCMC proposals are already built-in, but we have tospecify them before we run the MCMC. We usually define (at least) onemove per parameter immediately after we specify the prior distributionfor that parameter.# Define a move for our parameter, mumoves[1] = mvSlide(mu, delta=1, weight=1.0)Next, our likelihood model.# Specify the likelihood modeld_bar ~ dnGamma(n, n/mu)d_bar.clamp(arrow_mean)We wrap our full Bayesian model into one model object (this is aconvenience to keep the entire model in a single object, and is moreuseful when we have very large models):# Construct the full modelmy_model = model(mu)We use “monitors” to keep track of parameters throughout the MCMC. Thetwo kinds of monitors we use here are the mnModel, which writesparameters to a specified file, and the mnScreen, which simply outputssome parts of the model to screen (as a sort of progress bar).# Make the monitors to keep track of the MCMCmonitors[1] = mnModel(filename=\"archery_RB.log\", printgen=10)monitors[2] = mnScreen(printgen=1000, mu)Finally, we assemble the analysis object (which contains the model, themonitors, and the moves) and execute the run using the .run command:# Make the analysis objectanalysis = mcmc(my_model, monitors, moves)# Run the MCMCanalysis.run(100000)# Show how the moves performedanalysis.operatorSummary()Open the resulting archery_RB.log file in Tracer.Do the posterior distributions for the parameter $\\mu$ look the same as the ones we got from our first analysis?Hopefully, you’ll note that this Rev model is substantially simplerand easier to read than the MH algorithm script we began with. Perhapsmore importantly, this Rev analysis is orders of magnitude fasterthan our own script, because it makes use of extremely efficientprobability calculations built-in to RevBayes (rather than the ones wehacked together in our own algorithm).Exercise 3      Run the built-in MCMC and compare the results to your own MCMC. Arethe posterior estimates the same? Are the acceptance rates of themoves similar?        Add a second move moves[2] = mvScale(mu,lambda=0.1,weight=1.0)        Run the analysis again and compare the output.        Have a look at how the acceptance rate changes for different valuesof the tuning parameters.  Influence of the PriorSo far we have used a fairly simple exponential prior with $\\alpha = 1$.However, we have not explored what impact this prior has on our estimateof $\\mu$, or whether it is an appropriate prior distribution. If theprior is very informative, then our posterior distribution will berelatively similar to our prior beliefs. In order to explore theinformativeness of the prior, we can change the true value of $\\mu$ sothat it is very different from our prior belief. If we are still able torecover the correct value of $\\mu$, then we can say that our prior isfairly uninformative.If we find that our prior distribution is very informative, we have twooptions for minimizing sensitivity to the prior. First, we can use aless informative prior distribution. For example, since our data isexponentially distributed, a good choice for an uninformative prior is a$Gamma(0,0)$ distribution (this is the Jeffreys prior). Unfortunately,this prior distribution is improper (it does not integrate to 1), andso we can’t use it in RevBayes. However we can approximate this priordistribution by using very small parameter values, e.g. $Gamma(0.001,0.001)$. As you can see in , comparedto the exponential distribution, the $Gamma(0.001, 0.001)$ distribution ismuch more “flat”.Comparison of exponential distribution with $\\alpha = 1$ and uninformative gamma distribution with parameters $\\alpha=0.001$ and $\\beta=0.001$.The second and simplest way we can overcome the informativeness of theprior is to increase the amount of data we collect. We can do that inour example by increasing the number of arrows we shoot.Exercise 4      Increase the true mean arrow distance so that it is significantlylarger than $\\alpha$. How does this impact your estimate of $\\mu$?        Now use an uninformative $Gamma(0.001, 0.001)$ prior for $\\mu$. Didyour estimate of $\\mu$ improve?        Increase the number of arrows shot. How does this change the shapeand scale of the posterior distribution?  ",
        "url": "/tutorials/mcmc/archery.html",
        "index": "true"
      }
      ,
    
      "tutorials-morph-ase-ase-html": {
        "title": "Discrete morphology - Ancestral State Estimation",
        "content": "IntroductionExample: Ancestral state estimation with a simple equal rates Markov (ERM) modelWe are interested, in general, in the evolution of morphological characters in primates.In this example we will look at the evolution of morphological character solitariness in primates.We have two different types of solitariness: group living (0) and solitary living(1).You can try different morphological characters too.Specifically, we want to know what the ancestral state of all primates is.Furthermore, we want to know if solitariness is evolving under an equal-rates model, an unequal rates model, an irreversible model, or any other type of transition model.Specifying the ERM ModelWe will start this tutorial with the simple equal rates Markov (ERM) model with two states, $k=2$ (Pagel 1994; Maddison 1994; Lewis 2001).Thus, we will follow the Discrete morphology - Tree Inference Tutorial very closely and refer you to that tutorial for more information.Let us start with defining the rate matrix $Q$ for this 2-state model:\\(Q = \\begin{pmatrix} -\\mu_1 &amp; \\mu_{12} \\\\\\mu_{21} &amp; -\\mu_2 \\\\\\end{pmatrix} \\mbox{  .}\\)Remember, the ERM model sets transitions to be equal from any state to any other state.In that sense, our 2-state matrix really looks like this:\\(Q = \\begin{pmatrix} -(k-1)\\mu &amp; \\mu \\\\\\mu &amp; -(k-1)\\mu \\\\\\end{pmatrix} \\mbox{  .}\\)Because this is a Jukes-Cantor-like model (Jukes and Cantor 1969), state frequencies do not vary as a model parameter.A visualization of this simple model can be seen in .Graphical model showing the ERM model (left panel). Rev code specifying the ERM model is on the right-hand panel.We will first perform a phylogenetic analysis using the ERM model.In further sections, we will explore how to relax key assumptions of the ERM model.Example: Ancestral State Estimation Using the ERM ModelIn this example, we will use the solitariness data applied to a phylogeny of primates.Remember, this example provides you with how to perform the analysis and you should try the same analysis with other morphological characters.Data and Files  Create a directory called RB_DiscreteMorphology_RateASE_Tutorial (or any name you like).  Make sure that you have the data files copied into a subdirectory called data: .Getting Started  Create a new directory (in RB_DiscreteMorphology_RateASE_Tutorial) called scripts`.(If you do not have this folder, please refer to the directions in section .)When you execute RevBayes in this exercise, you will do so within the main directory you created (RB_DiscreteMorphology_RateASE_Tutorial).Creating Rev FilesFor complex models and analyses, it is best to create Rev script files that will contain all of the model parameters, moves, and functions.In this exercise, you will work primarily in your text editor and create a set of files that will be easily managed and interchanged.In this first section, you will write the following files from scratch and save them in the scripts directory:  mcmc_ase_ERM.Rev: the Rev-script file that loads the data, specifies the model describing discrete morphological character change (binary characters), and specifies the monitors and MCMC sampler.All of the files that you will create are also provided in the this RevBayes tutorial.Please refer to these files to verify or troubleshoot your own scripts.  Open your text editor and create the master Rev file called mcmc_ase_ERM.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.The file you will begin in this section will be the one you load into RevBayes when you have completed all of the components of the analysis.In this section you will begin the file and write the Rev commands for loading in the taxon list and managing the data matrices.Then, starting in section , you will move on to write each of the model components.After writing all the components for your model, you will complete the Rev script with the instructions given in section .Load Data MatricesRevBayes uses the function readDiscreteCharacterData() to load a data matrix to the workspace from a formatted file.This function can be used for both molecular sequences and discrete morphological characters.Import the morphological character matrix and assign it to the variable morpho.morpho &lt;- readDiscreteCharacterData(\"data/primates_solitariness.nex\")Create Helper VariablesBefore we begin writing the Rev scripts for each of the model components, we need to instantiate a couple “helper variables” that will be used by downstream parts of our model specification files.Create vectors of moves and monitorsmoves = VectorMoves()monitors = VectorMonitors()The ERM ModelFirst, we read in the tree topology:phylogeny &lt;- readTrees(\"data/primates_tree.nex\")[1]Next, we will create a Q matrix.Recall that the ERM model has equal rates to transition between any states.Therefore, we fist define a global rate variable $\\mu$.We will use an exponential prior with a mean of 10 events along this given phylogeny.If you have an idea for another reasonable number of events, then feel free to change this prior mean.Note that if you are uncertain, it is better to specify a slightly too high than too low prior mean.rate_pr := phylogeny.treeLength() / 10mu ~ dnExp(rate_pr)Since $\\mu$ is a rate parameter, we will apply a scaling move to update it.moves.append( mvScale(mu, lambda=1, weight=2.0) )Next, we specify our vector of rates.We need to do this because our rate matrix requires a vector of rates.Since we use the ERM model, all rates in this vector are identical.We could simply write rate := [mu,mu], however, we want our script to be more flexible if we wanted to analyze multistate characters.So we use for convenience a loop over the number of rates, which is $K*(K-1)$ rates where $K$ is the number of states.NUM_STATES = 2NUM_RATES = NUM_STATES * (NUM_STATES-1)for ( i in 1:NUM_RATES ) {    rate[i] := mu}Now that we have our rates, we can specify the morphological rate matrix.We will use the fnFreeK function for this.Q_morpho := fnFreeK( rate, rescale=false )Lastly, we set up the CTMC.This should be familiar from the Nucleotide substitution models tutorial.We see some familiar pieces: tree and Q matrix.We also have one new keywords: data type.The data type argument specifies the type of data - in our case, “Standard”, the specification for morphology.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, Q=Q_morpho, type=\"Standard\")phyMorpho.clamp(morpho)All of the components of the model are now specified.Complete MCMC AnalysisCreate Model ObjectWe can now create our workspace model variable with our fully specified model DAG.We will do this with the model() function and provide a single node in the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph and allows us to pass the model to various functions that are specific to our MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our Rev script file is to specify the monitors and output file names.The first monitor we will create will monitor every named random variable in our model graph.This will include every stochastic and deterministic node using the mnModel monitor.In this case, it will only be our rate variable $\\mu$.It is still useful to specify the model monitor this way for later extensions of the model.We will also name the output file for this monitor and indicate that we wish to sample our MCMC every 10 cycles.monitors.append( mnModel(filename=\"output/solitariness_ERM.log\", printgen=10) )The second monitor we will add to our analysis will print information to the screen.Like with mnFile we must tell mnScreen which parameters we’d like to see updated on the screen.monitors.append( mnScreen(printgen=100) )The third and final monitor might be new to you: the mnJointConditionalAncestralState monitor computes and writes the ancestral states to file.monitors.append( mnJointConditionalAncestralState(tree=phylogeny,                                                   ctmc=phyMorpho,                                                   filename=\"output/solitariness_ERM.states.txt\",                                                   type=\"Standard\",                                                   printgen=1,                                                   withTips=true,                                                   withStartStates=false) )The core arguments this monitor needs are a tree object (tree=phylogeny),the phylogenetic model (ctmc=phyMorpho), an output filename (filename=\"output/solitariness_ERM.states.txt\"),the data type for the characters (type=\"Standard\"), and the sampling frequency (printgen=10}.The final argument, withTips=true, indicates that we do wish to record the tip states because we didn’t know all tip values and might be interested in the most plausible values.The monitor will produce a joint sample of ancestral states, where every ancestral state is conditional on the drawn value of its parent node state (except for the root node),storing the samples every 10 iterations to the file \"output/solitariness_ERM.states.txt\".Viewing the states file, we seeIteration\tend_1\tend_2\tend_3\tend_4\tend_5\t...0\t0\t0\t1\t0\t1\t...1\t0\t0\t1\t0\t1\t...2\t0\t0\t0\t0\t1\t...3\t0\t0\t1\t0\t1\t...4\t0\t0\t1\t0\t1\t...5\t0\t0\t0\t1\t1\t...6\t0\t0\t1\t0\t1\t...7\t0\t1\t1\t0\t1\t...8\t0\t0\t1\t1\t1\t...9\t0\t0\t0\t0\t1\t...10\t0\t0\t1\t0\t1\t......Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now create the workspace variable that defines our MCMC run.We do this using the mcmc() function that simply takes the three main analysis components as arguments.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")The MCMC object that we named mymcmc has a member method called .run().This will execute our analysis and we will set the chain length to 25000 cycles using the generations option.mymcmc.run(generations=25000, tuningInterval=200)Once our Markov chain has terminated, we will process the ancestral state samples.This function will compute the posterior probabilities of the ancestral states from the samples.Later we can visualize our ancestral states.anc_states = readAncestralStateTrace(\"output/solitariness_ERM.states.txt\")anc_tree = ancestralStateTree(tree=phylogeny, ancestral_state_trace_vector=anc_states, include_start_states=false, file=\"output/solitariness_ase_ERM.tree\", burnin=0.25, summary_statistic=\"MAP\", site=1)Finally we can close RevBayes.Tell the program to quit using the q() function.q()  You made it! Save your file.Execute the MCMC AnalysisWith all the parameters specified and all analysis components in place, you are now ready to run your analysis.The Rev script you just created will be used by RevBayes and loaded in the appropriate order.  Begin by running the RevBayes executable.Provided that you started RevBayes from the correct directory (RB_DiscreteMorphology_RateASE_Tutorial),you can then use the source() function to feed RevBayes your master script file (mcmc_ase_ERM.Rev).source(\"scripts/mcmc_ase_ERM.Rev\")This will execute the analysis and you should see the following output (though not the exact same values):   Processing file \"scripts/mcmc_ase_ERM.Rev\"   Successfully read one character matrix from file 'data/primates_solitariness.nex'   Attempting to read the contents of file \"primates_tree.nex\"   Successfully read file   Running MCMC simulation   This simulation runs 2 independent replicates.   The simulator uses 1 different moves in a random move schedule with 2 moves per iterationIter        |      Posterior   |     Likelihood   |          Prior   |    elapsed   |        ETA   |----------------------------------------------------------------------------------------------------0           |       -27.0579   |       -31.6633   |        4.60534   |   00:00:00   |   --:--:--   |100         |       -27.0182   |       -30.8742   |        3.85594   |   00:00:00   |   --:--:--   |200         |       -26.3264   |       -30.6164   |        4.29003   |   00:00:01   |   00:02:04   |300         |       -26.3876   |       -30.5523   |        4.16473   |   00:00:02   |   00:02:44   |...When the analysis is complete, RevBayes will quit and you will have a new directory called output that will contain all of the files you specified with the monitors (Section ).Plotting the tree with ancestral statesWe will now switch to R using the package RevGadgets (see the Introduction to RevGadgets Tutorial for an overview, Tribble et al. (2022)).Now that we have our posterior distribution of ancestral states, we want to visualize those results.This section will aim to produce a pdf containing figures for the ancestral state estimates.We have written a little R package called \\RevGadgets that can be used to visualize the output of \\RevBayes.  Start R from the same working directory as you started RevBayes.This should be the directory where you now have you directory called output with the MCMC output files.First, we need to load the R package RevGadgetslibrary(RevGadgets)Second, we specify the name of the tree file.tree_file = \"output/solitariness_ase_ERM.tree\"Then, you plot the tree with ancestral states nicely mapped onto it.You may want to experiment with some of the settings to make the plot look prettier.For example, if you set show_posterior_legend=TRUE and node_size_range=c(1, 3),then the size of the circles will represent the posterior probability.# process the ancestral statesase &lt;- processAncStates(tree_file,                       # Specify state labels.                       # These numbers correspond to                       # your input data file.                         state_labels = c(\"0\" = \"no\", \"1\" = \"yes\"))# produce the plot object, showing MAP states at nodes.# color corresponds to state, size to the state's posterior probabilityp &lt;- plotAncStatesMAP(t = ase,                      tree_layout = \"rect\",                      tip_labels_size = 1) +     # modify legend location using ggplot2     theme(legend.position = c(0.92,0.81))Finally, we save the output into a PDF.ggsave(paste0(\"primates_solitariness_ASE_ERM_MAP.pdf\"), p, width = 11, height = 9)You can also plot the ancestral states as a pie chart, which nicely shows the uncertainty.p &lt;- plotAncStatesPie(t = ase,                      tree_layout = \"rect\",                      tip_labels_size = 1) +     # modify legend location using ggplot2     theme(legend.position = c(0.92,0.81))ggsave(paste0(\"primates_solitariness_ASE_ERM_Pie.pdf\"), p, width = 11, height = 9)  You can also find all these commands in the file called plot_anc_states.R which you can run as a script in R. shows the result of this analysis.Ancestral state estimation of solitariness in primates.You might observe that there is considerable uncertainty in the ancestral state  at the root node.In the following exercise we’ll relax the assumption of equal rates.  Click below to begin the next exercise!  The unequal rates model",
        "url": "/tutorials/morph_ase/ase.html",
        "index": "true"
      }
      ,
    
      "tutorials-morph-ase-ase-free-html": {
        "title": "Discrete morphology - Ancestral State Estimation with the independent rates model",
        "content": "Example: Unequal Transition RatesThe instantaneous rate matrix encodes the transition rates between all pairs of evolutionary states.It is important to emphasize that all rate matrices are assertions about how morphological evolution operates.Depending on how one populates the rate matrix elements, different evolutionary hypotheses may be expressed.When we model the evolution of morphological data, unlike nucleotide data, each change may require a sequence of intermediate changes.Getting to one state may require going through another.In short, it is probably not likely that one single model describes all characters well.The ERM model makes a number of assumptions, but one that may strike you as unrealistic is the assumption that characters are equally likely to change from any one state to any other state.That means that a trait is as likely to be gained as lost.While this may hold true for some traits, we expect that it may be untrue for many others.$$Q = \\begin{pmatrix}  &amp; \\mu_1 \\mu_2 &amp; -\\end{pmatrix}$$RevBayes has functionality to allow us to relax this assumption.For example, we can define the rates    rates := [ [0.0,  mu_1],        [ mu_2,  0.0] ]        and then create the rate matrix    Q := fnFreeK(rates)        which corresponds to the model$$Q = \\begin{pmatrix}    &amp; \\mu_1 \\mu_2 &amp; -\\end{pmatrix}$$This is the independent rates model (Pagel 1994; Maddison 1994; Schluter et al. 1997), which we will explore in this tutorial.  Make a copy of the MCMC and model files you just made.Call them mcmc_ase_ERM.Rev and `model_ase_FreeK.Rev.These will contain the new model parameters and models.Modifying the MCMC SectionAt each place in which the output files are specified in the MCMC file, change the output path so you don’t overwrite the output from the previous exercise.For example, you might call your output file output/solitariness_ase_freeK.log.Change source statement to indicate the new model file.Modifying the Model SectionOur goal here is to create a rate matrix with 2 free parameters.We will assume an exponential prior distribution for each of the rates.Thus, we start be specifying the rate of this exponential prior distribution.A good guess might be that 10 events happened along the tree, so the rate should be the tree-length divided by 10.rate_pr := phylogeny.treeLength() / 10Now we can create our two independent rate variables drawn from an identical exponential distributionNUM_RATES = NUM_STATES * (NUM_STATES-1)for ( i in 1:NUM_RATES ) {    rate[i] ~ dnExp(rate_pr)    moves.append( mvScale( rate[i], weight=2 ) )}Next, we put all the rates together into our rate matrix.Don’t forget to say that we do not rescale the rate matrix (rescale=false).We would only rescale if we use relative rates.Q_morpho := fnFreeK( rate, rescale=false )In this model, we also decide to specify an additional parameter for the root state frequencies instead of assuming the root state to be drawn from the stationary distribution.We will use a Dirichlet prior distribution for the root state frequencies.rf_prior &lt;- [1,1]rf ~ dnDirichlet( rf_prior )moves.append( mvBetaSimplex( rf, weight=2 ) )moves.append( mvDirichletSimplex( rf, weight=2 ) )We need to modify the dnPhyloCTMC to pass in our new root frequencies parameter.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, Q=Q_morpho, rootFrequencies=rf, type=\"Standard\")phyMorpho.clamp(morpho)  Now you are done with your unequal rates model. Give it a run!Plotting the tree with ancestral statesAs before in the Discrete morphology - Ancestral State Estimation tutorial, we will use R and the package RevGadgets (see the Introduction to RevGadgets Tutorial for an overview, Tribble et al. (2022)) to plot the ancestral state estimates.  Adapt your previous R script plot_anc_states.RStart R from the same working directory as you started RevBayes.Plot the ancestral state estimates.Ancestral state estimates under the independent rates model. Note that the root state has changed compared to the ERM analysis!Next, we want to actually see the estimated rates.We can do this nicely in RevGadgets (see the Introduction to RevGadgets Tutorial, Tribble et al. (2022)):library(RevGadgets)library(ggplot2)# specify the input filefile &lt;- paste0(\"output/solitariness_freeK.log\")# read the trace and discard burnintrace_quant &lt;- readTrace(path = file, burnin = 0.25)# produce the plot object, showing the posterior distributions of the rates.p &lt;- plotTrace(trace = trace_quant, vars = paste0(\"rate[\",1:2,\"]\"))[[1]] +     # modify legend location using ggplot2     theme(legend.position = c(0.88,0.85))ggsave(paste0(\"Primates_solitariness_rates_freeK.pdf\"), p, width = 5, height = 5)Posterior distribution of rates of morphological evolution.We also see some clear evidence that the rates of gain and loss are not equal.This gives as a first indication that the free rates model should be supported over the equal rates model.We observe  that the rate of gain rate[1] is very low compared to the rate of loss rate[2].  Does this correspond to our observation of changes in ancestral state estimates, i.e., did we see more losses than gains?Computing Bayes factors to test for model supportLet us now actually test if the independent rates model is supported using statistical model testing.Have a look at the General Introduction to Model selection tutorial for more information.  Copy and change your two MCMC scripts mcmc_ase_ERM.Rev and mcmc_ase_freeK.Rev.You need to exchange the MCMC algorithm with the power posterior algorithm (Höhna et al. 2021).Remove all lines after mymodel = model(phylogeny).Then, you need to construct the powerPosterior, which works analogous to an MCMC.In fact, it perform cats=63 MCMC runs.We chose cats=63 as a conservative estimate.Since this performs 64 MCMC simulations with 1000 iterations each, this can take a little while.### Compute power posterior distributions```pow_p = powerPosterior(mymodel, moves, monitors, \"output/\"+CHARACTER+\"_ERM.out\", cats=63, sampleFreq=10)pow_p.burnin(generations=2000,tuningInterval=250)pow_p.run(generations=1000)The next step is to summarize the power posterior distribution, first using stepping stone sampling,### Use stepping-stone sampling to calculate marginal likelihoodsss = steppingStoneSampler(file=\"output/\"+CHARACTER+\"_ERM.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")ss.marginal()and then using path sampling.### Use path-sampling to calculate marginal likelihoodsps = pathSampler(file=\"output/\"+CHARACTER+\"_ERM.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")ps.marginal()You should see the following output&gt; source(\"scripts/ml_ase_ERM.Rev\")   Processing file \"scripts/ml_ase_ERM.Rev\"   Successfully read one character matrix from file 'data/primates_solitariness.nex'   Attempting to read the contents of file \"primates_tree.nex\"   Successfully read fileRunning burn-in phase of Power Posterior sampler for 2000 iterations.The simulator uses 1 different moves in a random move schedule with 2 moves per iterationProgress:0---------------25---------------50---------------75--------------100********************************************************************Running power posterior analysis ...Step   1 / 64\t\t****************************************Step   2 / 64\t\t****************************************Step   3 / 64\t\t****************************************...Step 63 / 64\t\t****************************************Step 64 / 64\t\t****************************************   -31.7236   -31.72593Note that last two numbers; these are your marginal likelihood estimates.Now, write another script to compute the power posterior distribution for the independent rates (freeK) model (see the ml_ase_freeK.Rev script). Then use stepping stone sampling and/or path sampling to calculate the marginal likelihood.  Compute the Bayes factor! Which model is supported?  Click below to begin the next exercise!  Testing for irreversible evolution",
        "url": "/tutorials/morph_ase/ase_free.html",
        "index": "true"
      }
      ,
    
      "tutorials-morph-ase-ase-irreversible-html": {
        "title": "Discrete morphology - Ancestral State Estimation and Irreversibility",
        "content": "Reversible-jump MCMC to test for irreversibilityIn the previous section we assumed that there are 2 different rates, which are all $&gt;0$.Now, we will apply a reversible-jump MCMC (missing reference) to test if any of the rates is significantly larger than $0$.  Make a copy of the Rev script file you just made.Call them `mcmc_ase_freeK_RJ.Rev.This will contain the new model parameters and models.Modifying the MCMC SectionAt each place in which the output files are specified in the MCMC section, change the output path so you don’t overwrite the output from the previous exercise.For example, you might call your output file output/solitariness_ase_irrev.log.Modifying the Model SectionThe only part in the model section that we are going to modify is the prior distributions and moves on the rate parameters.We will assume the same rate for the exponential prior distribution as before.rate_pr := phylogeny.treeLength() / 10Next, we specify that we have a 0.5 probability, a priori, that a rate is equal to 0.mix_pr &lt;- 0.5We will specify again our rates within a for-loop, which makes changing this script to other number of character states very easy.We will explain each element within in the for-loop below.for ( i in 1:NUM_RATES ) {    rate[i] ~ dnRJMixture(0.0, dnExp(rate_pr), p=mix_pr)    prob_rate[i] := ifelse( rate[i] == 0, 1.0, 0.0 )    moves.append( mvScale( rate[i], weight=2 ) )    moves.append( mvRJSwitch( rate[i], weight=2 ) )}First, we can create our reversible-jump distributions dnRJMixture, which take in a constant value, 0.0 in this case, and a distribution dnExp(rate_pr).Thus, the value is either drawn to be exactly equal to the constant value (0.0 here), or drawn from the base distribution (the exponential distribution in this case).The last argument specifies the probability of the constant value p=mix_pr, which is important later for model comparison.Second, since we are interested in the probability that a rate is equal to 0.0, we want to compute this posterior probability directly.Therefore, we will use the ifelse function, which will return 1.0 if the rate is equal to 0.0, and 0.0 otherwise (if the rate is unequal to 0.0).Hence, the frequency with which we sample a 1.0 gives us the posterior probability that a given rate is equal to 0.0.Third, we also need to specify specific moves that jump in parameter dimension.We will use the mvRJSwitch move that changes the value to be either equal to the constant valueprovided from the dnRJMixture or a value drawn from the base distribution (the exponential distribution).Additionally, we also need to specify moves that change the rates if they are not equal to 0.0.As usual, we use the standard scaling moves.  This is all that you need to do for this ``fancy’’ reversible-jump model. Give it a try!Evaluate and Summarize Your ResultsVisualizing Ancestral State EstimatesWe have previously seen in Discrete morphology - Ancestral State Estimation and Discrete morphology - Ancestral State Estimation with the independent rates model how the ancestral states of the simple model equal rates Markov(ERM) model and the independent rates model look.You should repeat plotting the ancestral states now also for the irreversible (irrev) analyses.My output is shown in Ancestral state estimates of solitariness under the irreversible model. You might see that some ancestral states have changed.Next, we also want to see if there was support for the irreversible model.Therefore, we will plot the probability that a rate was equal to 0.0.You can do this nicely in RevGadgets (Tribble et al. 2022)library(RevGadgets)library(ggplot2)CHARACTER  &lt;- \"solitariness\"# specify the input filefile &lt;- paste0(\"output/\",CHARACTER,\"_irrev.log\")# read the trace and discard burnintrace_qual &lt;- readTrace(path = file, burnin = 0.25)# produce the plot object, showing the posterior distributions of the rates.p &lt;- plotTrace(trace = trace_qual,          vars = paste0(\"prob_rate[\",1:NUM_RATES,\"]\"))[[1]] +     # modify legend location using ggplot2     theme(legend.position = c(0.85,0.85))ggsave(paste0(\"Primates_\",CHARACTER,\"_irrev.pdf\"), p, width = 5, height = 5)Probability that a rate was equal to 0.0. We see ambiguous support that the rate of gain was equal to 0.0. Conversely, we don’t see any strong support that the rate of gain was not 0.0! We clearly see decisive support that the rate of loss was not equal to 0.0.  Perform this test for irreversibility with different prior means, e.g., expecting 1 or 100 events along the tree.Does this have an impact on your conclusion about irreversible evolution?  Click below to begin the next exercise!  Stochastic Character Mapping and Testing for Rate Variation",
        "url": "/tutorials/morph_ase/ase_irreversible.html",
        "index": "true"
      }
      ,
    
      "tutorials-morph-ase-ase-mammals-html": {
        "title": "Discrete morphology - Ancestral State Estimation (Mammals &amp; Placenta Type)",
        "content": "IntroductionExample: Ancestral state estimation with a simple equal rates modelIn this example we will look at the evolution of morphological character placenta type in placental mammals.We have three different types of placenta: Epitheliochorial(1), Endotheliochorial(2), Hemochorial(3) ( (missing reference)).We are interested, in general, in the evolution of placenta type in placental mammals.Specifically, we want to know what the ancestral state of all placental mammals is.Furthermore, we want to know if placenta type is evolving under an equal-rates model, an unequal rates model, an irreversible model, or any other type of transition model.Visualization of different placenta types. Reproduced from (missing reference).Specifying the Mk ModelWe will start this tutorial with the simple Mk model with three states, $k=3$ (Lewis 2001).Thus, we will follow the Discrete morphology - Tree Inference Tutorial very closely and refer you to that tutorial for more information.Let us start with defining the rate matrix $Q$ for this 3-state model:\\(Q = \\begin{pmatrix} -\\mu_1 &amp; \\mu_{12} &amp; \\mu_{13} \\\\\\mu_{21} &amp; -\\mu_2  &amp; \\mu_{23} \\\\\\mu_{31} &amp; \\mu_{32} &amp; -\\mu_3\\end{pmatrix} \\mbox{  .}\\)Remember, the Mk model sets transitions to be equal from any state to any other state.In that sense, our 3-state matrix really looks like this:\\(Q = \\begin{pmatrix} -(k-1)\\mu &amp; \\mu &amp; \\mu \\\\\\mu &amp; -(k-1)\\mu  &amp; \\mu \\\\\\mu &amp; \\mu &amp; -(k-1)\\mu \\\\\\end{pmatrix} \\mbox{  .}\\)Because this is a Jukes-Cantor-like model (Jukes and Cantor 1969), state frequencies do not vary as a model parameter.A visualization of this simple model can be seen in .Graphical model showing the Mk model (left panel). Rev code specifying the Mk model is on the right-hand panel.We will first perform a phylogenetic analysis using the Mk model.In further sections, we will explore how to relax key assumptions of the Mk model.Example: Ancestral State Estimation Using the Mk ModelIn this example, we will use the placenta type data applied to a thinned phylogeny of placental mammals.We have thinned the phylogeny only so that it will run considerably fast for this tutorial.Our actual analysis uses the full dataset of $\\sim 5000$ taxa.Data and Files  Create a directory called RB_DiscreteMorphology_RateASE_Tutorial (or any name you like).  Make sure that you have the data files copied into a subdirectory called data: .Getting Started  Create a new directory (in RB_DiscreteMorphology_RateASE_Tutorial) called scripts`.(If you do not have this folder, please refer to the directions in section .)When you execute RevBayes in this exercise, you will do so within the main directory you created (RB_DiscreteMorphology_RateASE_Tutorial).Creating Rev FilesFor complex models and analyses, it is best to create Rev script files that will contain all of the model parameters, moves, and functions.In this exercise, you will work primarily in your text editor and create a set of files that will be easily managed and interchanged.In this first section, you will write the following files from scratch and save them in the scripts directory:  mcmc_ase_mk.Rev: the Rev-script file that loads the data, specifies the model describing discrete morphological character change (binary characters), and specifies the monitors and MCMC sampler.All of the files that you will create are also provided in the this RevBayes tutorial.Please refer to these files to verify or troubleshoot your own scripts.  Open your text editor and create the master Rev file called mcmc_ase_Mk.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.The file you will begin in this section will be the one you load into RevBayes when you have completed all of the components of the analysis.In this section you will begin the file and write the Rev commands for loading in the taxon list and managing the data matrices.Then, starting in section , you will move on to writing module files for each of the model components.Once the model files are complete, you will return to editing mcmc_ase_Mk.Rev and complete the Rev script with the instructions given in section .Load Data MatricesRevBayes uses the function readDiscreteCharacterData() to load a data matrix to the workspace from a formatted file.This function can be used for both molecular sequences and discrete morphological characters.Import the morphological character matrix and assign it to the variable morpho.morpho &lt;- readDiscreteCharacterData(\"data/mammals_thinned_placenta_type.nex\")Create Helper VariablesBefore we begin writing the Rev scripts for each of the model components, we need to instantiate a couple ``helper variables’’ that will be used by downstream parts of our model specification files.Create vectors of moves and monitorsmoves = VectorMoves()monitors = VectorMonitors()The Mk ModelFirst, we read in the tree topology:phylogeny &lt;- readTrees(\"data/mammals_thinned.tree\")[1]Next, we will create a Q matrix.Recall that the Mk model is simply a generalization of the JC model.Therefore, we will create a 3x3 Q matrix using fnJC, which initializes $Q$-matrices with equal transition probabilities between all states.Q_morpho &lt;- fnJC(3)Now that we have the basics of the model specified, we will specify the only parameter of the model, $\\mu$.The parameter specifies all the rates of morphological evolution:mu_morpho ~ dnExponential( 1.0 )Since $\\mu$ is a rate parameter, we will apply a scaling move to update it.moves.append( mvScale(mu_morpho,lambda=1, weight=2.0) )Lastly, we set up the CTMC.This should be familiar from the Nucleotide substitution models tutorial.We see some familiar pieces: tree and Q matrix.We also have two new keywords: data type and coding.The data type argument specifies the type of data - in our case, “Standard”, the specification for morphology.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, branchRates=mu_morpho, Q=Q_morpho, type=\"Standard\")phyMorpho.clamp(morpho)All of the components of the model are now specified.Complete MCMC AnalysisCreate Model ObjectWe can now create our workspace model variable with our fully specified model DAG.We will do this with the model() function and provide a single node in the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph and allows us to pass the model to various functions that are specific to our MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our Rev script file is to specify the monitors and output file names.The first monitor we will create will monitor every named random variable in our model graph.This will include every stochastic and deterministic node using the mnModel monitor.In this case, it will only be our rate variable $\\mu$.It is still useful to specify the model monitor this way for later extensions of the model.We will also name the output file for this monitor and indicate that we wish to sample our MCMC every 10 cycles.monitors.append( mnModel(filename=\"output/mk.log\", printgen=10) )The second monitor we will add to our analysis will print information to the screen.Like with mnFile we must tell mnScreen which parameters we’d like to see updated on the screen.monitors.append( mnScreen(printgen=100) )The third and final monitor might be new to you: the mnJointConditionalAncestralState monitor computes and writes the ancestral states to file.monitors.append( mnJointConditionalAncestralState(tree=phylogeny,                                                   ctmc=phyMorpho,                                                   filename=\"output/mk.states.txt\",                                                   type=\"Standard\",                                                   printgen=1,                                                   withTips=true,                                                   withStartStates=false) )The core arguments this monitor needs are a tree object (tree=phylogeny),the phylogenetic model (ctmc=phyMorpho), an output filename (filename=\"output/mk.states.txt\"),the data type for the characters (type=\"Standard\"), and the sampling frequency (printgen=10}.The final argument, withTips=true, indicates that we do wish to record the tip states because we didn’t know all tip values and might be interested in the most plausible values.The monitor will produce a joint sample of ancestral states, where every ancestral state is conditional on the drawn value of its parent node state (except for the root node),storing the samples every 10 iterations to the file \"output/mk.states.txt\".Viewing the states file, we seeIteration\tend_1\tend_2\tend_3\tend_4\tend_5\t...0\t2\t3\t3\t3\t3\t...1\t2\t3\t3\t3\t3\t...2\t2\t3\t3\t3\t3\t...3\t2\t3\t3\t3\t3\t...4\t2\t3\t3\t3\t3\t...5\t2\t3\t3\t3\t3\t...6\t2\t3\t3\t3\t3\t...7\t2\t3\t3\t3\t3\t...8\t2\t3\t3\t3\t3\t...9\t2\t3\t3\t3\t3\t...10\t2\t3\t3\t3\t3\t......Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now create the workspace variable that defines our MCMC run.We do this using the mcmc() function that simply takes the three main analysis components as arguments.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")The MCMC object that we named mymcmc has a member method called .run().This will execute our analysis and we will set the chain length to 10000 cycles using the generations option.mymcmc.run(generations=10000, tuningInterval=200)Once our Markov chain has terminated, we will process the ancestral state samples.This function will compute the posterior probabilities of the ancestral states from the samples.Later we can visuallize our ancestral states.anc_states = readAncestralStateTrace(\"output/mk.states.txt\")anc_tree = ancestralStateTree(tree=phylogeny, ancestral_state_trace_vector=anc_states, include_start_states=false, file=\"output/ase_mk.tree\", burnin=0.25, summary_statistic=\"MAP\", site=1)writeNexus( anc_tree, filename=\"output/ase_mk.tree\" )Finally we can close RevBayes.Tell the program to quit using the q() function.q()  You made it! Save your file.Execute the MCMC AnalysisWith all the parameters specified and all analysis components in place, you are now ready to run your analysis.The Rev script you just created will be used by RevBayes and loaded in the appropriate order.  Begin by running the RevBayes executable.Provided that you started RevBayes from the correct directory (RB_DiscreteMorphology_RateASE_Tutorial),you can then use the source() function to feed RevBayes your master script file (mcmc_ase_mk.Rev).source(\"scripts/mcmc_ase_mk.Rev\")This will execute the analysis and you should see the following output (though not the exact same values):   Processing file \"scripts/mcmc_ase_mk.Rev\"   Successfully read one character matrix from file 'data/mammals_thinned_placenta_type.nex'   Attempting to read the contents of file \"mammals_thinned.tree\"   Successfully read file   Running MCMC simulation   This simulation runs 2 independent replicates.   The simulator uses 1 different moves in a random move schedule with 2 moves per iterationIter        |      Posterior   |     Likelihood   |          Prior   |    elapsed   |        ETA   |----------------------------------------------------------------------------------------------------0           |       -359.492   |       -359.194   |      -0.298007   |   00:00:00   |   --:--:--   |100         |       -62.0258   |       -62.0245   |     -0.0013356   |   00:00:03   |   --:--:--   |200         |       -62.3349   |       -62.3336   |    -0.00123787   |   00:00:05   |   00:02:00   |300         |        -62.005   |       -62.0024   |    -0.00256593   |   00:00:07   |   00:01:49   |400         |       -61.3682   |       -61.3664   |    -0.00177658   |   00:00:09   |   00:01:43   |500         |        -61.683   |       -61.6807   |    -0.00235834   |   00:00:12   |   00:01:48   |...When the analysis is complete, RevBayes will quit and you will have a new directory called output that will contain all of the files you specified with the monitors (Section ).Plotting the tree with ancestral statesWe will now switch to R using the package RevGadgets.Make sure that you have the package installed, usinglibrary(devtools)install_github(\"GuangchuangYu/ggtree\")install_github(\"revbayes/RevGadgets\")Now that we have our posterior distribution of ancestral states, we want to visualize those results.This section will aim to produce a pdf containing figures for the ancestral state estimates.We have written a little R package called \\RevGadgets that can be used to visualize the output of \\RevBayes.  Start R from the same working directory as you started RevBayes.This should be the directory where you now have you directory called output with the MCMC output files.First, we need to load the R package RevGadgetslibrary(RevGadgets)Second, we specify the name of the tree file.tree_file = \"output/ase_mk.tree\"Then, you plot the tree with ancestral states nicely mapped onto it.You may want to experiment with some of the settings to make the plot look prettier.For example, if you set show_posterior_legend=TRUE and node_size_range=c(1, 3),then the size of the circles will represent the posterior probability.g &lt;- plot_ancestral_states(tree_file, summary_statistic=\"MAP\",                      tip_label_size=1,                      xlim_visible=NULL,                      node_label_size=0,                      show_posterior_legend=FALSE,                      node_size_range=c(2.5, 2.5),                      alpha=0.75)Finally, we save the output into a PDF.ggsave(\"Mammals_ASE_MK.pdf\", g, width = 11, height = 9)  You can also find all these commands in the file called plot_anc_states.R which you can run as a script in R. shows the result of this analysis.Ancestral state estimation of the placenta type.Example: Unequal Transition Rates  Make a copy of the MCMC and model files you just made.Call them mcmc_ase_mk.Rev and `model_ase_FreeK.Rev.These will contain the new model parameters and models.The Mk model makes a number of assumptions, but one that may strike you as unrealisticis the assumption that characters are equally likely to change from any one state to any other state.That means that a trait is as likely to be gained as lost.While this may hold true for some traits, we expect that it may be untrue for many others.RevBayes has functionality to allow us to relax this assumption.Modifying the MCMC SectionAt each place in which the output files are specified in the MCMC file, change the output path so you don’t overwrite the output from the previous exercise.For example, you might call your output file output/ase_freeK.log.Change source statement to indicate the new model file.Modifying the Model SectionOur goal here is to create a rate matrix with 6 free parameters.We will assume an exponential prior distribution for each of the rates.Thus, we start be specifying the rate of this exponential prior distribution.A good guess might be that 10 events happened along the tree, so the rate should be the tree-length divided by 10.rate_pr := phylogeny.treeLength() / 10Now we can create our six independent rate variables drawn from an identical exponential distributionrate_12 ~ dnExponential(rate_pr)rate_13 ~ dnExponential(rate_pr)rate_21 ~ dnExponential(rate_pr)rate_23 ~ dnExponential(rate_pr)rate_31 ~ dnExponential(rate_pr)rate_32 ~ dnExponential(rate_pr)As usual, we will apply a scaling move to each of the rate variables.moves.append( mvScale( rate_12, weight=2 ) )moves.append( mvScale( rate_13, weight=2 ) )moves.append( mvScale( rate_21, weight=2 ) )moves.append( mvScale( rate_23, weight=2 ) )moves.append( mvScale( rate_31, weight=2 ) )moves.append( mvScale( rate_32, weight=2 ) )Next, we put all the rates together into our rate matrix.Don’t forget to say that we do not rescale the rate matrix (rescale=false).We would only rescale if we use relative rates.Q_morpho := fnFreeK( [ rate_12, rate_13, rate_21, rate_23, rate_31, rate_32 ], rescale=false )In this model, we also decide to specify an additional parameter for the root state frequencies instead of assuming the root state to be drawn from the stationary distribution.We will use a Dirichlet prior distribution for the root state frequencies.rf_prior &lt;- [1,1,1]rf ~ dnDirichlet( rf_prior )moves.append( mvBetaSimplex( rf, weight=2 ) )moves.append( mvDirichletSimplex( rf, weight=2 ) )We need to modify the dnPhyloCTMC to pass in our new root frequencies parameter.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, Q=Q_morpho, rootFrequencies=rf, type=\"Standard\")  Now you are done with your unequal rates model. Give it a run!Reversible-jump MCMC to test for irreversibilityIn the previous section we assumed that there are 6 different rates, which are all $&gt;0$.Now, we will apply a reversible-jump MCMC (missing reference) to test if any of the rates is significantly larger than $0$.  Make a copy of the Rev script file you just made.Call them `mcmc_ase_freeK_RJ.Rev.This will contain the new model parameters and models.Modifying the MCMC SectionAt each place in which the output files are specified in the MCMC section, change the output path so you don’t overwrite the output from the previous exercise.For example, you might call your output file output/freeK_ASE.log.Modifying the Model SectionThe only part in the model section that we are going to modify is the prior distributions and moves on the rate parameters.We will assume the same rate for the exponential prior distribution as before.rate_pr := phylogeny.treeLength() / 10Next, we specify that we have a 0.5 probability, a priori, that a rate is equal to 0.mix_pr &lt;- 0.5Now we can create our reversible-jump distributions, which take in a constant value, 0.0 in this case, and a distribution.Thus, the value is either drawn to be exactly equal to the constant value (0.0 here), or drawn from the base distribution (the exponential distribution in this case).rate_12 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_13 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_21 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_23 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_31 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_32 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)Since we are interested in the probability that a rate is equal to 0.0, we want to compute this posterior probability directly.Therefore, we will use the ifelse function, which will return 1.0 if the rate is equal to 0.0, and 0.0 otherwise (if the rate is unequal to 0.0).Hence, the frequency with which we sample a 1.0 gives us the posterior probability that a given rate is equal to 0.0.prob_rate_12 := ifelse( rate_12 == 0, 1.0, 0.0 )prob_rate_13 := ifelse( rate_13 == 0, 1.0, 0.0 )prob_rate_21 := ifelse( rate_21 == 0, 1.0, 0.0 )prob_rate_23 := ifelse( rate_23 == 0, 1.0, 0.0 )prob_rate_31 := ifelse( rate_31 == 0, 1.0, 0.0 )prob_rate_32 := ifelse( rate_32 == 0, 1.0, 0.0 )We also need to specify specific moves that ``jump’’ in parameter dimension.We will use the mvRJSwitch move that changes the value to be either equal to the constant valueprovided from the dnRJMixture or a value drawn from the base distribution (the exponential distribution).moves.append( mvRJSwitch( rate_12, weight=2 ) )moves.append( mvRJSwitch( rate_13, weight=2 ) )moves.append( mvRJSwitch( rate_21, weight=2 ) )moves.append( mvRJSwitch( rate_23, weight=2 ) )moves.append( mvRJSwitch( rate_31, weight=2 ) )moves.append( mvRJSwitch( rate_32, weight=2 ) )Additionally, we also need to specify moves that change the rates if they are not equal to 0.0.As usual, we use the standard scaling moves.moves.append( mvScale( rate_12, weight=2 ) )moves.append( mvScale( rate_13, weight=2 ) )moves.append( mvScale( rate_21, weight=2 ) )moves.append( mvScale( rate_23, weight=2 ) )moves.append( mvScale( rate_31, weight=2 ) )moves.append( mvScale( rate_32, weight=2 ) )  This is all that you need to do for this ``fancy’’ reversible-jump model. Give it a try!Evaluate and Summarize Your ResultsVisualizing Ancestral State EstimatesWe have previously seen in  how the ancestral states of the simple model look.You should repeat plotting the ancestral states now also for the freeK and freeK_RJ analyses.My output is shown in Ancestral state estimates of placenta type under the freeK model.You should observe that the estimated root states have changed!",
        "url": "/tutorials/morph_ase/ase_mammals.html",
        "index": "true"
      }
      ,
    
      "workshops-australia2018-html": {
        "title": "Bayesian phylogenetics and macroevolution in RevBayes",
        "content": "",
        "url": "/workshops/australia2018.html",
        "index": ""
      }
      ,
    
      "workshops-barcelona2019-html": {
        "title": "Phylogenomics and Population Genomics&amp;#58; Inference and Applications",
        "content": "",
        "url": "/workshops/barcelona2019.html",
        "index": ""
      }
      ,
    
      "workshops-berkeley2018-html": {
        "title": "Phylogenetic graphical models and Bayesian inference",
        "content": "",
        "url": "/workshops/berkeley2018.html",
        "index": ""
      }
      ,
    
      "tutorials-model-selection-bayes-factors-bf-intro-html": {
        "title": "General Introduction to Model selection",
        "content": "OverviewThis tutorial provides the third protocol from our recent publication(Höhna et al. 2017). The first protocol is described in the Substitution model tutorialand the second protocol is described in the Partitioned data analysis tutorial.This tutorial demonstrates some general principles of Bayesian modelcomparison, which is based on estimating the marginal likelihood ofcompeting models and then comparing their relative fit to the data usingBayes factors.IntroductionFor most sequence alignments, several (possibly many) substitutionmodels of varying complexity are plausible a priori. We therefore needa way to objectively identify the model that balances estimation biasand inflated error variance associated with under- andover-parameterized models, respectively. Increasingly, model selectionis based on Bayes factors [e.g., (Suchard et al. 2001; Lartillot 2006; Xie et al. 2011; Baele et al. 2012; Baele et al. 2013)], whichinvolves first calculating the marginal likelihood of each candidatemodel and then comparing the ratio of the marginal likelihoods for theset of candidate models.Given two models, $M_0$ and $M_1$, the Bayes-factor comparison assessingthe relative fit of each model to the data, $BF(M_0,M_1)$, is:\\[BF(M_0,M_1) = \\frac{\\mbox{posterior odds}}{\\mbox{prior odds}}.\\]The posterior odds is the posterior probability of $M_0$ given the data,$\\mathbf X$, divided by the posterior probability of $M_1$ given the data:\\[\\mbox{posterior odds} = \\frac{\\mathbb{P}(M_0 \\mid \\mathbf X)}{\\mathbb{P}(M_1 \\mid \\mathbf X)},\\]and the prior odds is the prior probability of $M_0$ divided by theprior probability of $M_1$:\\[\\mbox{prior odds} = \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)}.\\]Thus, the Bayes factor measures the degree to which the data alter our beliefregarding the support for $M_0$ relative to $M_1$ (Lavine and Schervish 1999):\\[\\begin{equation}BF(M_0,M_1) = \\frac{\\mathbb{P}(M_0 \\mid \\mathbf X)}{\\mathbb{P}(M_1 \\mid \\mathbf X)} \\div \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)}. \\tag{Bayes Factor}\\label{eq:BF}\\end{equation}\\]Note that interpreting Bayes factors involves some subjectivity. Thatis, it is up to you to decide the degree of your belief in $M_0$relative to $M_1$. Despite the absence of an absolutely objectivemodel-selection threshold, we can refer to the scale [outlined by(Jeffreys 1961)] that provides a “rule-of-thumb” for interpreting thesemeasures ().            Strength of evidence      BF($M_0$,$M_1$)**      log(BF($M_0$,$M_1$))      $log_{10}(BF(M_0$,$M_1))$                  Negative (supports $M_1$)      $&lt;1$      $&lt;0$      $&lt;0$              Barely worth mentioning      $1$ to $3.2$      $0$ to $1.16$      $0$ to $0.5$              Substantial      $3.2$ to $10$      $1.16$ to $2.3$      $0.5$ to $1$              Strong      $10$ to $100$      $2.3$  to $4.6$      $1$ to $2$              Decisive      $&gt;100$      $&gt;4.6$      $&gt;2$      The scale for interpreting Bayes factors by Harold (Jeffreys 1961).Unfortunately, it is generally not possible to directly calculate theposterior odds to prior odds ratios. However, we can further define theposterior odds ratio as:\\[\\begin{aligned}\\frac{\\mathbb{P}(M_0 \\mid \\mathbf X)}{\\mathbb{P}(M_1 \\mid \\mathbf X)} = \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)} \\frac{\\mathbb{P}(\\mathbf X \\mid M_0)}{\\mathbb{P}(\\mathbf X \\mid M_1)},\\end{aligned}\\]where $\\mathbb{P}(\\mathbf X \\mid M_i)$ is the marginal likelihood ofthe data (this may be familiar to you as the denominator of BayesTheorem, which is variously referred to as the model evidence orintegrated likelihood). Formally, the marginal likelihood is theprobability of the observed data ($\\mathbf X$) under a given model($M_i$) that is averaged over all possible values of the parameters ofthe model ($\\theta_i$) with respect to the prior density on $\\theta_i$\\[\\begin{equation}\\mathbb{P}(\\mathbf X \\mid M_i) = \\int \\mathbb{P}(\\mathbf X \\mid \\theta_i) \\mathbb{P}(\\theta_i)dt.\\tag{Marginal Likelihood}\\label{eq:marginal_likelihood}\\end{equation}\\]This makes it clear that more complex (parameter-rich) models arepenalized by virtue of the associated prior: each additional parameterentails integration of the likelihood over the corresponding priordensity. If you refer back to equation \\eqref{eq:BF}, you can see that, withvery little algebra, the ratio of marginal likelihoods is equal to theBayes factor:\\[\\begin{equation}BF(M_0,M_1) = \\frac{\\mathbb{P}(\\mathbf X \\mid M_0)}{\\mathbb{P}(\\mathbf X \\mid M_1)} = \\frac{\\mathbb{P}(M_0 \\mid \\mathbf X)}{\\mathbb{P}(M_1 \\mid \\mathbf X)} \\div \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)}. \\label{eq:bf_Formula}\\end{equation}\\]Therefore, we can perform a Bayes factor comparison of two models bycalculating the marginal likelihood for each one. Alas, exact solutionsfor calculating marginal likelihoods are not known for phylogeneticmodels (see equation \\eqref{eq:marginal_likelihood}), thus we must resort to numericalintegration methods to estimate or approximate these values. In thisexercise, we will estimate the marginal likelihood for each partitionscheme using both the stepping-stone (Xie et al. 2011; Fan et al. 2011) and pathsampling estimators (Lartillot 2006; Baele et al. 2012).Estimating the Marginal LikelihoodWe will estimate the marginal likelihood of a given model using a‘stepping-stone’ (or ‘path-sampling’) algorithm. These algorithms aresimilar to the familiar MCMC algorithms, which are intended to samplefrom (and estimate) the joint posterior probability of the modelparameters. Stepping-stone algorithms are like a series of MCMCsimulations that iteratively sample from a specified number ofdistributions that are discrete steps between the posterior and theprior probability distributions. The basic idea is to estimate theprobability of the data for all points between the posterior and theprior—effectively summing the probability of the data over the priorprobability of the parameters to estimate the marginal likelihood.Technically, the steps correspond to a series of powerPosteriors(),where the likelihood is iteratively raised to a series of numbersbetween 1 and 0 (Figure [fig:ss]). When the likelihood is raised tothe power of 1 (typically the first stepping stone), samples are drawnfrom the (untransformed) posterior. By contrast, when the likelihood israised to the power of 0 (typically the last stepping stone), samplesare drawn from the prior. To perform a stepping-stone simulation, weneed to specify (1) the number of stepping stones (power posteriors)that we will use to traverse the path between the posterior and theprior (e.g., we specify 50 or 100 stones),(2) the spacing of the stones between the posterior and prior(e.g., we may specify that the stones aredistributed according to a beta distribution), (3) the number of samples(and their thinning) to be drawn from each stepping stone, and (4) thedirection we will take (i.e., from theposterior to the prior or vice versa).   Estimating marginal likelihoods usingstepping-stone simulation. Estimating the marginal likelihood involvesintegrating the likelihood of the data over the entire prior probabilitydensity for the model parameters.MCMC algorithms target the posteriorprobability density, which is typically concentrated in a small regionof the prior probability density (A).Accordingly, standard MCMCsimulation cannot provide unbiased estimates of the marginal likelihoodbecause it will typically fail to explore most of the prior density.(B)Stepping-stone algorithms estimate the marginal likelihood by means of aseries of MCMC-like simulations, where the likelihood is iterativelyraised to a series of powers, effectively forcing the simulation to morefully explore the prior density of the model parameters.Here, sixuniformly spaced stones span the posterior, where the power posterior is$\\beta=6/6=1$, to the prior, where the power posterior is $\\beta=0/6=0$.This method computes a vector of powers from a beta distribution, thenexecutes an MCMC run for each power step while raising the likelihood tothat power. In this implementation, the vector of powers starts with 1,sampling the likelihood close to the posterior and incrementallysampling closer and closer to the prior as the power decreases.The following procedure for estimating marginal likelihoods is valid forany model in RevBayes. First, we create the variable containing the power-posterioranalysis. This requires that we provide a model and vector of moves, aswell as an output file name. The cats argument sets the number ofstepping stones.pow_p = powerPosterior(mymodel, moves, monitors, \"output/model1.out\", cats=50) We can start the power-posterior analysis by first burning in the chainand and discarding the first 10000 states. This will help ensure thatanalysis starts from a region of high posterior probability, rather thanfrom some random point.pow_p.burnin(generations=10000,tuningInterval=1000)Now execute the run with the .run() function:pow_p.run(generations=1000)  Once the power posteriors have been saved to file, create a steppingstone sampler. This function can read any file of power posteriors andcompute the marginal likelihood using stepping-stone sampling.ss = steppingStoneSampler(file=\"output/model1.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")These commands will execute a stepping-stone simulation with 50 steppingstones, sampling 1000 states from each step. Compute the marginallikelihood under stepping-stone sampling using the member functionmarginal() of the ss variable.ss.marginal() Path sampling is an alternative to stepping-stone sampling and alsotakes the same power posteriors as input.ps = pathSampler(file=\"output/model1.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")Compute the marginal likelihood under stepping-stone sampling using themember function marginal() of the ps variable and record the valuein Table [tab:ml_cytb].ps.marginal() We have kept this description of how to use stepping-stone-sampling andpath-sampling very generic and did not provide the information about themodel here. Our main motivation is to show that the marginal likelihoodestimation algorithms are independent of the model. Thus, you can applythese algorithms to any model, e.g., relaxedclock models and birth-death models, as well. For a more concrete example including data and Rev script, please see the follow-up tutorial Model selection of common substitution models for one locus.Computing Bayes Factors and Model SelectionNow that we have estimates of the marginal likelihood for each of ourthe candidate substitution models, we can evaluate their relative fit tothe datasets using Bayes factors. Phylogenetic programs log-transformthe likelihood values to avoidunderflow:multiplying likelihoods (numbers $&lt; 1$) generates numbers that are toosmall to be held in computer memory. Accordingly, we need to use adifferent form of equation [bfFormula] to calculate the ln-Bayesfactor (we will denote this value $\\mathcal{K}$):\\[\\begin{equation}\\mathcal{K}=\\ln[BF(M_0,M_1)] = \\ln[\\mathbb{P}(\\mathbf X \\mid M_0)]-\\ln[\\mathbb{P}(\\mathbf X \\mid M_1)],\\label{LNbfFormula}\\end{equation}\\]where $\\ln[\\mathbb{P}(\\mathbf X \\mid M_0)]$ is the marginal lnLestimate for model $M_0$. The value resulting from equation[LNbfFormula] can be converted to a raw Bayes factor by simply takingthe exponent of $\\cal{K}$\\[\\begin{equation}BF(M_0,M_1) = e^{\\cal{K}}.\\label{LNbfFormula2}\\end{equation}\\]Alternatively, you can directly interpret the strength of evidence in favor of $M_0$ in logspace by comparing the values of $\\cal{K}$ to the appropriate scale(Table [bftable], second column). In this case, we evaluate $\\cal{K}$in favor of model $M_0$ against model $M_1$ so that:  if $\\mathcal{K} &gt; 1$, model $M_0$ is preferredif $\\mathcal{K} &lt; -1$, model $M_1$ is preferred.Thus, values of $\\mathcal{K}$ around 0 indicate that there is nopreference for either model.Using the values you entered in Table [tab:ml_cytb] and equation[LNbfFormula], calculate the ln-Bayes factors (using $\\mathcal{K}$)for each model comparison. Enter your answers in Table [bfTable2]using the stepping-stone and the path-sampling estimates of the marginallog-likelihoods.For your consideration…In this tutorial you have learned how to use RevBayes to assess therelative fit of a pool of candidate substitution models to a givensequence alignment. Typically, once we have identified the “best”substitution model for our alignment, we would then proceed to use thismodel for inference. Technically, this is a decision to condition ourinferences on the selected model, which explicitly assumes that itprovides a reasonable description of the process that gave rise to ourdata. However, there are several additional issues to consider beforeproceeding along these lines, which we briefly mention below.Accommodating Model UncertaintyIn some or many situations the number of possible models to compare islarge, e.g., choosing all possiblecombinations of substitution models (Huelsenbeck et al. 2004). Furthermore,imagine, for example, that there are several (possibly many) alternativemodels that provide a similarly good fit to our given dataset. In suchscenarios, conditioning inference on any single model (even the‘best’) ignores uncertainty in the chosen model, which can causeestimates to be biased. This is the issue of model uncertainty. TheBayesian framework provides a natural approach for accommodating modeluncertainty by means of model averaging; we simply adopt theperspective that models (like standard parameters) are random variables,and integrate the inference over the distribution of candidate models.We will demonstrate how to accommodate model uncertainty usingRevBayes in a separate tutorial, for example, Model averaging of substitution models.Assessing Model AdequacyIn this tutorial, we used Bayes factors to assess the fit of varioussubstitution models to our sequence data, effectively establishing therelative rank of the candidate models. Even if we have successfullyidentified the very best model from the pool of candidates, however, thepreferred model may nevertheless be woefully inadequate in an absolutesense. For this reason, it is important to consider model adequacy:whether a given model provides a reasonable description of the processthat gave rise to our sequence data. We can assess the absolute fit of amodel to a given dataset using posterior predictive simulation. Thisapproach is based on the following premise: if the candidate modelprovides a reasonable description of the process that gave rise to ourdataset, then we should be able to generate data under this model thatresemble our observed data. We will demonstrate how to assess modeladequacy using RevBayes in a separate tutorial,Assessing Phylogenetic Reliability Using RevBayes and $P^{3}$.",
        "url": "/tutorials/model_selection_bayes_factors/bf_intro.html",
        "index": "true"
      }
      ,
    
      "tutorials-model-selection-bayes-factors-bf-partition-model-html": {
        "title": "Model selection of partition models",
        "content": "OverviewYou should read first the General Introduction to Model selection tutorial, which explains the theory and standard algorithms for estimating marginal likelihoods and Bayes factors.Additionally, you may want to work through the Model selection of common substitution models for one locus tutorial,which estimates marginal likelihoods for different substitution models for one locus, before attempting this tutorial.Comparing Partitioned Models Using Bayes FactorsFor this tutorial you should download the sequence data, ITS, matK, and rbcL. These new sequence data are for the genus Fagus, the beeches.Data partitions allow us to apply different substitution models to different loci in order to accommodate process heterogeneity (variation in the substitution process among sequences). The substitution models may be of the same form (i.e., they may all be GTR models), or of entirely different forms (i.e, some may be HKY, while others are GTR). Just because two loci have the same form of substitution model does not necessarily mean they share the same substitution models; for example, we determined that the GTR model is preferred for each of the loci above, but it is possible that the stationary frequencies and relative rate parameters for these loci are different (i.e., they have different substitution models).According to our previous analysis, we could partition our Fagus data so that each locus has the same or different substitution model parameters. Each of these choices imply different phylogenetic models, and thus we can choose among partitioned models using Bayes factors.The Uniform Partitioned ModelIf we assigned the same GTR+G to each locus, we would be assuming that the process of evolution is the same among loci (we often call this the “uniform model”). We can specify this uniform partition model by using the same $Q$ matrix and ASRV model for each alignment. Open the file marginal_likelihood_partition_1.Rev and examine how we specify this model. In particular, note that dnPhyloCTMC models all use the same Q matrix and site_rates:seq_ITS ~ dnPhyloCTMC(tree=phylogeny, Q=Q, type=\"DNA\", siteRates=site_rates)seq_ITS.clamp(data_ITS) # attach the observed dataseq_matK ~ dnPhyloCTMC(tree=phylogeny, Q=Q, type=\"DNA\", siteRates=site_rates)seq_matK.clamp(data_matK) # attach the observed dataseq_rbcL ~ dnPhyloCTMC(tree=phylogeny, Q=Q, type=\"DNA\", siteRates=site_rates)seq_rbcL.clamp(data_rbcL) # attach the observed dataThe Saturated ModelAt the opposite end of the spectrum, the “saturated” model applies a different substitution model to each locus, and each locus receives its own subset-specific rate multiplier (with the contraint that the mean rate is 1!). Open the script marginal_likelihood_partition_5.Rev to see how this model is specified. Notice how the subset-specific rates are specified:num_sites[1] = data_ITS.nchar()num_sites[2] = data_matK.nchar()num_sites[3] = data_rbcL.nchar()relative_rates ~ dnDirichlet(v(1,1,1))moves.append( mvBetaSimplex(relative_rates, weight=1.0) )subset_rates := relative_rates * sum(num_sites) / num_sites(The last line forces the subset_rates to have a mean of 1.)Notice also that each dnPhyloCTMC is receiving a different Q, subset_rates site_rates argument:seq_ITS ~ dnPhyloCTMC(tree=phylogeny, branchRates=subset_rates[1],                      Q=Q_ITS, type=\"DNA\", siteRates=site_rates_ITS)seq_ITS.clamp(data_ITS) # attach the observed dataseq_matK ~ dnPhyloCTMC(tree=phylogeny, branchRates=subset_rates[2],                       Q=Q_matK, type=\"DNA\", siteRates=site_rates_matK)seq_matK.clamp(data_matK) # attach the observed dataseq_rbcL ~ dnPhyloCTMC(tree=phylogeny, branchRates=subset_rates[3],                       Q=Q_rbcL, type=\"DNA\", siteRates=site_rates_rbcL)seq_rbcL.clamp(data_rbcL) # attach the observed dataIn-Class Exercises  Download the Rev script associated with your assigned partition model. Note how this script implements the particular partition model and the power posterior analysis.  Execute your Rev script. This can take a long time; please be patient! Once your stepping-stone analysis is complete, record your result on the Google spreadsheet.",
        "url": "/tutorials/model_selection_bayes_factors/bf_partition_model.html",
        "index": "true"
      }
      ,
    
      "tutorials-model-selection-bayes-factors-bf-subst-model-html": {
        "title": "Model selection of common substitution models for one locus",
        "content": "OverviewThis tutorial provides the third protocol from our recent publication (Höhna et al. 2017).The first protocol is described in the Nucleotide substitution modelsand the second protocol is described in the Partitioned data analysis.You should read first the General Introduction to Model selection tutorial, which explains the theory andstandard algorithms for estimating marginal likelihoods and Bayes factors.Substitution ModelsThe models we use here are equivalent to the models described in theprevious exercise on substitution models (continuous time Markovmodels). To specify the model please consult the previous exercise.Specifically, you will need to specify the following substitutionmodels:  Jukes-Cantor (JC) substitution model (Jukes and Cantor 1969)  Hasegawa-Kishino-Yano (HKY) substitution model (Hasegawa et al. 1985)  General-Time-Reversible (GTR) substitution model (Tavaré 1986)  Gamma (+G) model for among-site rate variation (Yang 1994)  Invariable-sites (+I) model (Hasegawa et al. 1985)Just to be safe, it is better to clear the workspace (if you did notjust restart RevBayes):clear()Now set up the model as in the previous exercise. You should start withthe simple Jukes-Cantor substitution model. Setting up the modelrequires:  Loading the data and retrieving useful variables about it(e.g., number of sequences andtaxon names).  Specifying the instantaneous-rate matrix of the substitution model.  Specifying the tree model including branch-length variables.  Creating a random variable for the sequences that evolved underthe PhyloCTMC.  Clamping the data.  Creating a model object.  Specifying the moves for parameter updates.The following procedure for estimating marginal likelihoods is valid forany model in RevBayes. You will need to repeat this later for othermodels. First, we create the variable containing the power-posterioranalysis. This requires that we provide a model and vector of moves, aswell as an output file name. The cats argument sets the number ofstepping stones.pow_p = powerPosterior(mymodel, moves, monitors, \"output/model1.out\", cats=50)We can start the power-posterior analysis by first burning in the chainand and discarding the first 10000 states. This will help ensure thatanalysis starts from a region of high posterior probability, rather thanfrom some random point.pow_p.burnin(generations=10000,tuningInterval=1000)Now execute the run with the .run() function:pow_p.run(generations=1000)Once the power posteriors have been saved to file, create a steppingstone sampler. This function can read any file of power posteriors andcompute the marginal likelihood using stepping-stone sampling.ss = steppingStoneSampler(file=\"output/model1.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")These commands will execute a stepping-stone simulation with 50 steppingstones, sampling 1000 states from each step. Compute the marginallikelihood under stepping-stone sampling using the member functionmarginal() of the ss variable and record the value in Table[tab:ml_cytb].ss.marginal()Path sampling is an alternative to stepping-stone sampling and alsotakes the same power posteriors as input.ps = pathSampler(file=\"output/model1.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")Compute the marginal likelihood under stepping-stone sampling using themember function marginal() of the ps variable and record the valuein Table [tab_ml_subst_models].ps.marginal()We have kept this description of how to use stepping-stone-sampling andpath-sampling very generic and did not provide the information about themodel here. Our main motivation is to show that the marginal likelihoodestimation algorithms are independent of the model. Thus, you can applythese algorithms to any model, e.g., relaxedclock models and birth-death models, as well.Exercises  Compute the marginal likelihoods of the cytb alignment for thefollowing substitution models:          Jukes-Cantor (JC) substitution model      Hasegawa-Kishino-Yano (HKY) substitution model      General-Time-Reversible (GTR) substitution model      GTR with gamma distributed-rate model (GTR+G)      GTR with invariable-sites model (GTR+I)      GTR+I+G model        Enter the marginal likelihood estimate for each model in thecorresponding cell of Table [tab:ml_cytb].  Which is the best fitting substitution model?            Model      Path-Sampling      Stepping-Stone-Sampling                  JC ($M_1$)                            HKY ($M_2$)                            GTR ($M_3$)                            GTR+$\\Gamma$ ($M_4$)                            GTR+I ($M_5$)                            GTR+$\\Gamma$+I ($M_6$)                    Marginal likelihoods for different substitution models.  Now you can continue to the next tutorial: Model selection of partition models or Model averaging of substitution models",
        "url": "/tutorials/model_selection_bayes_factors/bf_subst_model.html",
        "index": "true"
      }
      ,
    
      "tutorials-model-selection-bayes-factors-bf-subst-model-rj-html": {
        "title": "Model averaging of substitution models",
        "content": "OverviewYou should read first the General Introduction to Model selection tutorial, which explains the theory and standard algorithms for estimating marginal likelihoods and Bayes factors.Additionally, you may want to work through the Model selection of common substitution models for one locus tutorial,which estimates marginal likelihoods for different substitution models for one locus, before attempting this tutorial.Bayesian Model AveragingSometimes, the data are indecisive about which model is preferred by Bayes factor.We call this phenomenon model uncertainty because we’re actually uncertain about which model is the best description of the process that generated our data. The natural Bayesian solution to this problem is simply to treat the model itself as a random variable, which averages parameter estimates (including the tree, branch lengths, and all substitution model parameters) over the uncertainty in the model itself. We accomplish this (generally) using a special “reversible-jump” MCMC algorithm (also known “rjMCMC”, “transdimensional MCMC”, or “the Green algorithm”) which adds, removes, or combines parameters to move between models.The state space of potential models is vast, so we’ll restrict ourselves to a very particular set of models, in particular, we’re going to average over the “named” members of the GTR models (the ones you learned specifically in class) and models with and without Gamma-distributed ASRV.Averaging over $Q$-matricesWe average over $Q$-matrices by including all of the relevant parameters ($\\kappa$, $\\pi$, $r$) in our model, and using a model indicator to indicate which parameters to include in the model. For example:kappa ~ dnExp(1)moves.append( mvScale(kappa, weight=5.0) )pi ~ dnDirichlet(v(1,1,1,1))moves.append( mvBetaSimplex(pi, weight=5.0) )er ~ dnDirichlet(v(1,1,1,1,1,1))moves.append( mvBetaSimplex(er, weight=5.0) )Q_JC  &lt;- fnJC(4)Q_K80 := fnK80(kappa)Q_F81 := fnF81(pi)Q_HKY := fnHKY(kappa, pi)Q_GTR := fnGTR(er, pi)Q_vec := v(Q_JC, Q_K80, Q_F81, Q_HKY, Q_GTR)model_indicator ~ dnCategorical(simplex(1,1,1,1,1))moves.append( mvRandomGeometricWalk(model_indicator, weight=10.0, tune=FALSE)Q := Q_vec[model_indicator]In this case, we have a vector of $Q$ matrices that are assembled from the relevant parameters, and another parameter (the indicator) that allows us to move between $Q$ matrices!Reversible-Jump for Gamma-distributed ASRVIncluding reversible-jump for Gamma-distributed ASRV is more straightforward:alpha ~ dnReversibleJumpMixture(1E8, dnUniform(0,1E8), 0.5)alpha.setValue(1.0)moves.append( mvRJSwitch(alpha, weight=10.0) )moves.append( mvScale(alpha, weight=10.0) )alpha_indicator := ifelse(alpha == 1E8, 0, 1)site_rates := fnDiscretizeGamma(alpha, alpha, 4)Here, we draw alpha from a “reversibe jump mixture”, which specifies the value of alpha when ASRV is “turned off” (the first argument), the prior distribution from which alpha is drawn when the ASRV is “turned on” (the second argument), and the prior probability that ASRV is “turned on” (the final argument). The alpha_indicator parameter will have a value of 1 when ASRV is “turned on” and a value of 0 when it is “turned off”. We’re using a value of alpha=1e8 to approximate “no rate variation”, because, as $\\alpha \\rightarrow \\infty$, the Gamma-model collapse to a spike at 1 (i.e., approximately no rate variation):Gamma distribution for different choices of $\\alpha$.Using reversible jump, we can actually estimate the posterior probability of each model! In this case, the posterior probability of a model for a particular locus is the frequency with which it is sampled in the posterior distribution; in this case, the posterior probability of the rate-variable model is the fraction of MCMC samples that aren’t 10000, which is also the posterior mean value of the alpha_indicator parameter!In-Class Exercises  Download and run the model_average_primates_cytb.Rev script. Examine the posterior distributions of the model_indicator and alpha_indicator parameters in Tracer. What is the posterior probability that our model includes Gamma-distributed rate variation? What is the $Q$ matrix with the highest posterior probability? What substitution models are in the 95% credible set?  Repeat the above exercise for matK and rbcL by making the appropriate changes to the model_average_primates_cytb.Rev script. Are these results consistent with the Bayes factors we computed in the first section of the tutorial?",
        "url": "/tutorials/model_selection_bayes_factors/bf_subst_model_rj.html",
        "index": "true"
      }
      ,
    
      "tutorials-mcmc-binomial-html": {
        "title": "Introduction to MCMC using RevBayes",
        "content": "OverviewThis very basic tutorial provides an introduction to Bayesian inferenceand Markov chain Monte Carlo (MCMC) algorithms. The tutorial explainsthe fundamental concepts of an MCMC algorithm, such as moves andmonitors, which are ubiquitous in every other tutorial. After thetutorial you should be somewhat familiar with Bayesian inference(e.g., what is a prior distribution,posterior distribution, and likelihood function) and MCMC simulation(e.g., what are moves and monitors and why dowe need them).This tutorial comes with a recorded video walkthrough. The video corresponding to each section of the exercise is linked next to the section title. The full playlist is available here: A Coin Flipping (Binomial) Model We’ll begin our exploration of Bayesian inference with a simplecoin-flipping model. In this model, we imagine flipping a coin $n$ timesand count the number of heads, $x$; each flip comes up heads withprobability $p$. This model gives rise to the Binomial probabilitydistribution, with parameters $n$ and $p$: \\(\\begin{aligned}P(x \\mid n,p) = {n \\choose x}p^x(1-p)^{n-x}\\end{aligned}\\) Simpleintuition suggests that, given that we observe $x$ heads in $n$ cointosses, the maximum-likelihood estimate (MLE) of $p$ is simply$\\frac{x}{n}$: if we flip a coin 100 times and observe 70 heads, weassume the probability the coin comes up heads is$\\frac{70}{100} = 0.7$. This is indeed the maximum likelihood estimate!From Bayes’ theorem, the posterior distribution of $p$ given $x$,$P(p \\mid x)$, is: \\(\\begin{aligned} \\overbrace{P(p \\mid x)}^{\\text{posterior distribution}} = \\frac{\\overbrace{P(x \\mid p)}^{\\text{likelihood}} \\times \\overbrace{P(p)}^{\\text{prior}}}{\\underbrace{P(x)}_{\\text{marginal likelihood}}} \\end{aligned}\\)The take-home message here is that if we’re interested in doingBayesian inference for the coin flipping model, we need to specify alikelihood function and a prior distribution for $p$. In virtuallyall practical cases, we cannot compute the posterior distributiondirectly and instead use numerical procedures, such as a Markov chainMonte Carlo (MCMC) algorithm. Therefore, we will also have to write anMCMC algorithm that samples parameter values in the frequency of theirposterior probability.We’ll use a simple beta distribution as a prior on the parameter of themodel, $p$. The beta distribution has two parameters, $\\alpha$ and$\\beta$ (). Different choices for$\\alpha$ and $\\beta$ represent different prior beliefs.Distribution with two parameters,$\\alpha$ and $\\beta$. This distribution is used as a prior distributionon the probability parameter $p$ of observing a head. Here we showdifferent curves for the beta distribution when using differentparameters. shows the graphical model for thebinomial model. This nicely visualizes the dependency structure in themodel. We see that the two parameters $\\alpha$ and $\\beta$ are drawn insolid squares, representing that these variables are constant. Fromthese two variables, we see arrows going into the variable $p$. Thatsimply means that $p$ depends on $\\alpha$ and $\\beta$. Morespecifically, $p$ is a stochastic variable (shown as a solid circle) anddrawn from a beta distribution with parameters $\\alpha$ and $\\beta$.Then, we have another constant variable, $n$. Finally, we have theobserved data $x$ which is drawn from a Binomial distribution withparameters $p$ and $n$, as can be seen by the arrows going into $x$.Furthermore, the solid circle of $x$ is shaded which means that thevariable has data attached to it.Graphical model for the binomial model.Writing an MCMC from Scratch Make yourself familiar with the example script calledBinomial_MH_algorithm.Rev which shows the code for the followingsections. Then, start a new and empty script and follow each stepprovided in this tutorial.The Metropolis-Hastings AlgorithmThough RevBayes implements efficient and easy-to-use Markov chainMonte Carlo algorithms, we’ll begin by writing one ourselves to gain abetter understanding of the moving parts. The Metropolis-Hastings MCMCalgorithm (missing reference) proceeds as follows:      Generate initial values for the parameters of the model (in thiscase, $p$).        Propose a new value (which we’ll call $p^\\prime$) for someparameters of the model, (possibly) based on their current values        Calculate the acceptance probability, $R$, according to:\\(\\begin{aligned}\t\tR = \\text{min}\\left\\{1, \\frac{P(x \\mid p^\\prime)}{P(x \\mid p)} \\times \\frac{P(p^\\prime)}{P(p)} \\times \\frac{q(p)}{q(p^\\prime)} \\right\\}\t\\end{aligned}\\)        Generate a uniform random number between 1 and 0. If it is less than$R$, accept the move (set $p = p^\\prime$). Otherwise, keep thecurrent value of $p$.        Record the values of the parameters.        Return to step 2 many many times, keeping track of the value of $p$.  Reading in the dataActually, in this case, we’re just going to make up some data on thespot. Feel free to alter these values to see how they influence theposterior distribution# Make up some coin flips!# Feel free to change these numbersn &lt;- 100 # the number of flipsx &lt;- 63\t# the number of headsInitializing the Markov chainWe have to start the MCMC off with some initial parameter values. Oneway to do this is to randomly draw values of the parameters (just $p$,in this case) from the prior distribution. We’ll assume a “flat” betaprior distribution; that is, one with parameters $\\alpha = 1$ and$\\beta = 1$.# Initialize the chain with starting valuesalpha &lt;- 1beta  &lt;- 1p &lt;- rbeta(n=1,alpha,beta)[1]Likelihood functionWe also need to specify the likelihood function. We use the binomialprobability for the likelihood function. Since the likelihood is definedonly for values of $p$ between 0 and 1, we return 0.0 if $p$ is outsidethis range:# specify the likelihood functionfunction likelihood(p) {    if(p &lt; 0 || p &gt; 1)        return 0    l = dbinomial(x,p,n,log=false)    return l}Prior distributionSimilarly, we need to specify a function for the prior distribution.Here, we use the beta probability distribution for the prior on $p$:# specify the prior functionfunction prior(p) {    if(p &lt; 0 || p &gt; 1)        return 0    pp = dbeta(p,alpha,beta,log=false)    return pp}Monitoring parameter valuesAdditionally, we are going to monitor,i.e., store, parameter values into a fileduring the MCMC simulation. For this file we need to write the columnheaders:# Prepare a file to log our sampleswrite(\"iteration\",\"p\",\"\\n\",file=\"binomial_MH.log\")write(0,p,\"\\n\",file=\"binomial_MH.log\",append=TRUE)(You may have to change the newline characters to\\backslashr\\backslashn if you’re using a Windows operating system.)We’ll also monitor the parameter values to the screen, so let’s printthe initial values:# Print the initial values to the screenprint(\"iteration\",\"p\")print(0,p)Writing the MH AlgorithmAt long last, we can write our MCMC algorithm. First, let us define thefrequency how often we print to file(i.e., monitor), which is also often calledthinning. If we set the variable printgen to 1, then we will store theparameter values every single iteration; if we choose printgen=10instead, then only every $10^{th}$ iteration.printgen = 10We will repeat this resampling procedure many times (here, 10000), anditerate the MCMC using a for loop:# Write the MH algorithmreps = 10000for(rep in 1:reps){(remember to close your for loop at the end).The first thing we do in the first generation is generate a new value of$p^\\prime$ to evaluate. We’ll propose a new value of $p$ from a uniformdistribution between 0 and 1. Note that in this first example we do notcondition new parameter values on the current value.    # Propose a new value of p    p_prime &lt;- runif(n=1,0.0,1.0)[1]Next, we compute the proposed likelihood and prior probabilities, aswell as the acceptance probability, $R$:    # Compute the acceptance probability    R &lt;- ( likelihood(p_prime) / likelihood(p) ) * ( prior(p_prime) / prior(p) )Then, we accept the proposal with probability $R$ and reject otherwise:    # Accept or reject the proposal    u &lt;- runif(1,0,1)[1]    if(u &lt; R){        # Accept the proposal        p &lt;- p_prime    }Finally, we store the current value of $p$ in our log file. Here, weactually check if we want to store the value during this iteration.    if ( (rep % printgen) == 0 ) {        # Write the samples to a file        write(rep,p,\"\\n\",file=\"binomial_MH.log\",append=TRUE)        # Print the samples to the screen        print(rep,p)    }} # end MCMCVisualizing the samples of an MCMC simulationBelow we show an example of the obtained output inTracer. Specifically,  showsthe sample trace (left) and the estimated posterior distribution of $p$(right). There are other parameters, such as the posterior mean and the95% HPD (highest posterior density) interval, that you can obtain fromTracer.  Left: the Trace of samples from an MCMC simulation. Right: the approximated posteriorprobability distribution for $p$.More on Moves: Tuning and weightsIn the previous example we hard coded a single move updating thevariable $p$ by drawing a new value from a uniform(0,1) distribution.There are actually many other ways how to propose new values; some ofwhich are more efficient than others.First, let us rewrite the MCMC loop so that we use instead a function,which we call move_uniform for simplicity, that performs the move:for (rep in 1:reps){    # call uniform move    move_uniform(1)    if ( (rep % printgen) == 0 ) {        # Write the samples to a file        write(rep,p,\"\\n\",file=\"binomial_MH.log\",append=TRUE)    }} # end MCMCThis loop looks already much cleaner.Uniform moveNow we need to actually write the move_uniform function. We mostlyjust copy the code we had before into a dedicated functionfunction move_uniform( Natural weight) {    for (i in 1:weight) {        # Propose a new value of p        p_prime &lt;- runif(n=1,0.0,1.0)[1]        # Compute the acceptance probability        R &lt;- ( likelihood(p_prime) / likelihood(p) ) * ( prior(p_prime) / prior(p) )        # Accept or reject the proposal        u &lt;- runif(1,0,1)[1]        if (u &lt; R){            # Accept the proposal            p &lt;- p_prime        } else {            # Reject the proposal            # (we don't have to do anything here)        }    }}There are a few things to consider in the function move_uniform.First, we do not have a return value because the move simply changes thevariable $p$ if the move is accepted. Second, we expect an argumentcalled weight which will tell us how often we want to use this move.Otherwise, this function does exactly the same what was inside the forloop previously.(Note that you need to define this function before the for loop in yourscript).Sliding-window moveAs a second move we will write a sliding-window move. The sliding-windowmoves propose an update by drawing a random number from a uniformdistribution and then adding this random number to the current value(i.e., centered on the previous value).function move_slide( RealPos delta, Natural weight) {    for (i in 1:weight) {        # Propose a new value of p        p_prime &lt;- p + runiform(n=1,-delta,delta)[1]        # Compute the acceptance probability        R &lt;- ( likelihood(p_prime) / likelihood(p) ) * ( prior(p_prime) / prior(p) )        # Accept or reject the proposal        u &lt;- runif(1,0,1)[1]        if (u &lt; R) {            # Accept the proposal            p &lt;- p_prime        } else {            # Reject the proposal            # (we don't have to do anything here)        }    }}In addition to the weight of the move, this move has another argument,delta. The argument delta defines the width of the uniform windowfrom which we draw new values. Thus, if delta is large, then theproposed values are more likely to be very different from the currentvalue of $p$. Conversely, if delta is small, then the proposed valuesare more likely to be very close to the current value of $p$.Experiment with different values for delta and check how the effectivesample size (ESS) changes.There is, a priori, no good method for knowing what values of deltaare most efficient. However, there are some algorithms implemented inRevBayes, called auto-tuning, that will estimate good values fordelta.Scaling moveAs a third and final move we will write a scaling move. The scaling moveproposes an update by drawing a random number from a uniform(-0.5,0.5)distribution, exponentiating the random number, and then multiplyingthis scaling factor by the current value. An interesting feature of thismove is that it is not symmetrical and thus needs a Hastings ratio. TheHastings ratio is rather trivial in this case, and one only needs tomultiply the acceptance rate by the scaling factor.function move_scale( RealPos lambda, Natural weight) {    for (i in 1:weight) {        # Propose a new value of p        sf &lt;- exp( lambda * ( runif(n=1,0,1)[1] - 0.5 ) )        p_prime &lt;- p * sf        # Compute the acceptance probability        R &lt;- ( likelihood(p_prime) / likelihood(p) ) * ( prior(p_prime) / prior(p) ) * sf        # Accept or reject the proposal        u &lt;- runif(1,0,1)[1]        if (u &lt; R){            # Accept the proposal            p &lt;- p_prime        } else {            # Reject the proposal            # (we don't have to do anything here)        }    }}As before, this move has a tuning parameter called lambda.The sliding-window and scaling moves are very common and popular movesin RevBayes. The code examples here are actually showing the exactsame equation as implemented internally. It will be very useful for youto understand these moves.However, this MCMC algorithm is very specific to our binomial modeland thus hard to extend (also it’s pretty inefficient!).The Metropolis-Hastings Algorithm with the Real RevBayesThe video walkthrough for this section is in two parts.Part 1 Part 2 We’ll now specify the exact same model in Rev using the built-inmodeling functionality. It turns out that the Rev code to specify theabove model is extremely simple and similar to the one we used before.Again, we start by “reading in” (i.e., making up) our data.# Make up some coin flips!# Feel free to change these numbersn &lt;- 100 # the number of flipsx &lt;- 63\t# the number of headsNow we specify our prior model.# Specify the prior distributionalpha &lt;- 1beta  &lt;- 1p ~ dnBeta(alpha,beta)One difference between RevBayes and the MH algorithm that we wroteabove is that many MCMC proposals are already built-in, but we have tospecify them before we run the MCMC. We usually define (at least) onemove per parameter immediately after we specify the prior distributionfor that parameter.# Define a move for our parameter, pmoves[1] = mvSlide(p,delta=0.1,weight=1)Next, our likelihood model.# Specify the likelihood modelk ~ dnBinomial(p, n)k.clamp(x)We wrap our full Bayesian model into one model object (this is aconvenience to keep the entire model in a single object, and is moreuseful when we have very large models):# Construct the full modelmy_model = model(p)We use “monitors” to keep track of parameters throughout the MCMC. Thetwo kinds of monitors we use here are the mnModel, which writesparameters to a specified file, and the mnScreen, which simply outputssome parts of the model to screen (as a sort of progress bar).# Make the monitors to keep track of the MCMCmonitors[1] = mnModel(filename=\"binomial_MCMC.log\", printgen=10, separator = TAB)monitors[2] = mnScreen(printgen=100, p)Finally, we assemble the analysis object (which contains the model, themonitors, and the moves) and execute the run using the .run command:# Make the analysis objectanalysis = mcmc(my_model, monitors, moves)# Run the MCMCanalysis.run(100000)# Show how the moves performedanalysis.operatorSummary()Open the resulting binomial_MCMC.log file in Tracer. Do theposterior distributions for the parameter $p$ look the same as the oneswe got from our first analysis?Hopefully, you’ll note that this Rev model is substantially simplerand easier to read than the MH algorithm script we began with. Perhapsmore importantly, this Rev analysis is orders of magnitude fasterthan our own script, because it makes use of extremely efficientprobability calculations built-in to RevBayes (rather than the ones wehacked together in our own algorithm).Exercises for the MCMC TutorialExercise 1: Performing your first simple MCMC simulation      Look into the Binomial_MH_algorithm.Rev script and make yourselffamiliar with it. All the commands should be explained in the textof the tutorial.        Execute the script Binomial_MH_algorithm.Rev.        The .log file will contain samples from the posterior distributionof the model! Open the file in Tracerto learn aboutvarious features of the posterior distribution, for example: theposterior mean or the 95% credible interval.        Save the MCMC trace plot and posterior distribution into a PDF file.  Exercise 2: Different MCMC strategies (moves)      Now look into the script called Binomial_MH_algorithm_moves.Revwhich shows the 3 different types of moves described inthis tutorial.        Run the script to estimate the posterior distribution of $p$ again.        Look at the output in Tracer.        Use only a single move and set printgen=1. Which move has the bestESS?        How does the ESS change if you use a delta=10 for thesliding-window move?    Add to each move a counter variable that counts how often the movewas accepted. For example:    if (u &lt; R){# Accept the proposalp &lt;- p_prime++num_sliding_move_accepted}        Have a look at how the acceptance rate changes for different valuesof the tuning parameters.Exercise 3: MCMC in RevBayes      Run the built-in MCMC (Binomial_MCMC.Rev) and compare the resultsto your own MCMC. Are the posterior estimates the same? Are the ESSvalues similar? Which script was the fastest?        Next, add a second move moves[2] =mvScale(p,lambda=0.1,tune=true,weight=1.0) just after thefirst one. Run the analysis again and compare it to the original one.Did the second move help with mixing?        Finally, run a pre-burnin usinganalysis.burnin(generations=10000,tuningInterval=200) just beforeyou call analysis.run(100000). This will auto-tune the tuningparameters (e.g., delta and lambda)so that the acceptance ratio is between 0.4 and 0.5.        What are the tuned values for delta and lambda? Did theauto-tuning increase the ESS?  Exercise 4: Approximating the posterior distributionModify the script Binomial_MCMC.Rev. Assume you flipped a coin 100times and got 34 heads. Run the MCMC for 100,000 generations, printingevery 100 samples to the file.      What is the posterior mean estimate of p? The 95% credibleinterval?        Pretend you flipped the coin 900 more times, for a total of1000 flips. Among those 1000 flips, you observed 340 heads (changeyour script accordingly!). What is your posterior mean estimate of pnow? How has the 95% credible interval changed? Provide an intuitiveexplanation for this change.  Exercise 5: Exploring prior sensitivity and MCMC settingsPlay around with various parts of the model to develop on intuition forboth the Bayesian model and the MCMC algorithm. For example, how doesthe posterior distribution change as you increase the number of coinflips (say, increase both the number of flips and the number of heads byan order of magnitude)? How does the estimated posterior distributionchange if you change the prior model parameters, $\\alpha$ and $\\beta$(i.e., is the model prior sensitive)? Doesthe prior sensitivity depend on the sample size? Are the posteriorestimates sensitive to the length of the MCMC? Do you think this MCMChas been run sufficiently long, or should you run it longer? Try toanswer some of these questions and explain your findings.",
        "url": "/tutorials/mcmc/binomial.html",
        "index": "true"
      }
      ,
    
      "tutorials-biogeo-biogeo-dating-html": {
        "title": "Biogeographic dating using the DEC model",
        "content": "IntroductionThis tutorial describes how to apply a biogeographic dating analysis.To do so, we will jointly estimate phylogeny and biogeography. Onebenefit of this joint inference strategy is that the biogeographic analysis will intrinsicallyaccommodate phylogenetic uncertainty, both in terms of topology andbranch lengths. Another is that paleogeographic evidence has thepotential provide information about the geological timing of speciationevents in the phylogeny (Ho et al. 2015). Finally, biogeographic data may lendsupport to certain phylogenetic relationships that have poor resolutionotherwise.Modeling the interactions between phylogeny, biogeography, and paleogeography should also, somehow, inform our posterior estimates of divergence times throughout the clade.After all, an island cannot be colonized before it comes into existence.But encoding this intuition into our phylogenetic model requires some special considerations.As mentioned in the Simple DEC model tutorial, Hawaiian silverswords arenested within the subtribe Madiinae, alongside thetarweeds, a clade of plants inhabiting in western North America. Fossilpollen evidence indicates that Madiinae diversifiedduring a period of aridification from 15–5 Ma in the western regions ofNorth America Baldwin et al. (1991). It’s clear that silverswords colonizedHawaii from western North America, but the timing of the event isdifficult to estimate. Even though the oldest Hawaiian island theyinhabit is Kauai, it is possible that silverswords first colonized olderislands in the Emperor Island chain that predate the formation of Kauai (&gt;5 Ma).This makes the application of standard node-based biogeographiccalibrations challenging, because it would require a strong assumptionabout when and how many times the oldest silversword lineages colonizedKauai. Did silverswords colonize Kauai once directly from the Californiacoast? Or did the colonize the younger islands multiple times from olderislands in the chain? And did the event occur immediately after Kauaisurfaced or much later? Because we cannot observe the timing and natureof this dispersal event directly, we will integrate over all possible evolutionaryhistories using process-based biogeographic dating method described inLandis (2017).Cartoon of biogeographictransition probabilities as functions of geological time, and how thatrelates to speciation times. (a) Areas split, dispersal before split,positive probability; (b) Areas split, dispersal after split, zeroprobability; (c) Areas merge, dispersal after merge, positiveprobability; (d) Areas merge, dispersal before merge, zero probabilty.Original figure and details regarding cartoon assumptions are found in Landis (2017).The basic idea is that an empirically informed epoch model is capable ofcreating conditions that favor key evolutionary transitions to occurduring one time interval over another. Unlike the time-homogeneousprobabilities that arise from, say, a molecular substitution process,these age-dependent transition probabilities may identify rate fromtime, and thus generate information about branch lengths in units ofabsolute time (). A biogeographicprocess that is constrained by paleogeographic connectivity iswell-suited to this purpose.Note: like all dating methods, including node calibration methods, tipdating methods, and fossilized birth death dating methods, process-basedbiogeographic dating estimates are prior sensitive and datasetdependent. Applying this model to alternative data sets should be donewith care!Another note: This tutorial describes an analysis that is similar, but not identical, to the analysis conducted in Landis et al. (2018).Much of this tutorial will be similar to the previous sections, exceptwe are adding a birth-death process and a molecular substitution processto the model graph.AnalysisTo use date the silversword radiation using biogeography, it isnecessary that we transition from our simpler 4-area model to a richer6-area model (see ). The mainland area (Z)is necessary to force the silversword and tarweed clade to originateapart from the islands. The area corresponding to the older island chain(R) is necessary because we do not know a priori whethersilverswords colonized the modern islands directly from the mainland (Z$\\rightarrow$ K), or first colonized R and only later dispersed into theyounger islands any number of times (Z $\\rightarrow$ R $\\rightarrow$ K).Thus, adding these two areas allows the silversword origin time toprecede the formation of Kauai when the dispersal rate is large.Additionally, we will add three tarweed taxa to our dataset, increasingthe total number of taxa to 38. We’ll use a molecular alignment for theinternal transcribed spacer (ITS) to estimate the phylogeny, which is a657bp non-coding locus that is historically important for plantsystematics. Because the locus is relatively short, it will also leaveus with a fair amount of phylogenetic uncertainty in branch length andtopology estimates. However, because we’re estimating phylogeny andbiogeography, it will be correctly incorporated into our ancestral rangeestimates.As usual, we’ll begin by creating variables to manage our input andoutput filesrange_fn = \"data/n6/silversword.n6.range.nex\"mol_fn = \"data/n6/silversword.mol.nex\"tree_fn = \"data/n6/silversword.tre\"out_fn = \"output/epoch_phy\"geo_fn = \"data/n6/hawaii.n6\"times_fn = geo_fn + \".times.txt\"dist_fn = geo_fn + \".distances.txt\"Add the analysis helper variablesn_gen = 25000    # more parameters, longer run!Read in the molecular alignmentdat_mol = readDiscreteCharacterData(mol_fn)Read in the species ranges for six areasdat_range_01 = readDiscreteCharacterData(range_fn)Compute the number of ranges when ranges may only be one or two areas insizen_areas &lt;- dat_range_01.nchar()max_areas &lt;- 2n_states &lt;- 0for (k in 0:max_areas) n_states += choose(n_areas, k)Then format the dataset for the reduced state spacedat_range_n = formatDiscreteCharacterData(dat_range_01, \"DEC\", n_states)Record the complete list of range descriptions to filestate_desc = dat_range_n.getStateDescriptions()state_desc_str = \"state,range\\n\"for (i in 1:state_desc.size()){    state_desc_str += (i-1) + \",\" + state_desc[i] + \"\\n\"}write(state_desc_str, file=out_fn+\".state_labels.txt\")Read the minimum and maximum ages of the island complexestime_bounds &lt;- readDataDelimitedFile(file=times_fn, delimiter=\" \")n_epochs &lt;- time_bounds.size()Read in the connectivity matrices between the six areasfor (i in 1:n_epochs) {  epoch_fn[i] = geo_fn + \".connectivity.\" + i + \".txt\"  connectivity[i] &lt;- readDataDelimitedFile(file=epoch_fn[i], delimiter=\" \")}Read the geographical distances between areasdistances &lt;- readDataDelimitedFile(file=dist_fn, delimiter=\" \")Remember that we are estimating the phylogeny as part of this analysis.In general, it is possible that certain combinations of phylogeny,biogeography, and paleogeography have zero-valued likelihoods should theepoch model introduce reducible rate matrix structures [see thesupplemental of (missing reference)]. The initial MCMC state, however, must havea non-zero probability for it to work properly. Although it may not beneeded, we will provide tree_init as a starting tree forthe tree variable that we will create to be safe.tree_init = readTrees(tree_fn)[1]We will record some basic information about the taxon set, the number oftaxa, and the number of branches in the treetaxa = tree_init.taxa()n_taxa = taxa.size()n_branches = 2 * n_taxa - 2The Tree ModelBecause we will estimate the topology and branch lengths parameters, thetree variable must be declared as a stochastic node with aprior distribution. For this, we’ll use a constant rate birth-deathprocess.Assign root age with a maximum age of 15 Ma to reflect the ecological root age calibration for Californian tarweeds (Baldwin and Sanderson 1998). No assumption is madeabout the minimum root age.root_age ~ dnUniform(0, 15)Add a move to update the root agemoves = VectorMoves()moves.append( mvScale(root_age, weight=5) )Assign the proportion of sampled taxa (we have a non-uniform samplingscheme, but this should suffice).rho &lt;- 35/50Assign the birth and death priors. It is important to note that thebirth and death priors induce a root age distribution through thebirth-death process.birth ~ dnExp(10)moves.append( mvScale(birth, weight=2) )death ~ dnExp(10)moves.append( mvScale(death, weight=2) )Instantiate a tree variable generated by a birth-death processtree ~ dnBDP(lambda=birth, mu=death, rho=rho, rootAge=root_age, taxa=taxa)Add topology and branch length movesmoves.append( mvNNI(tree, weight=n_branches/2) )moves.append( mvFNPR(tree, weight=n_branches/8) )moves.append( mvNodeTimeSlideUniform(tree, weight=n_branches/2) )moves.append( mvSubtreeScale(tree, weight=n_branches/8) )moves.append( mvTreeScale(tree, root_age, weight=n_branches/8) )Provide a starting tree to ensure the biogeographic model has non-zerolikelihoodtree.setValue(tree_init)root_age.setValue(tree_init.rootAge())The molecular modelTo inform our branch lengths (in relative time units) and our topology,we will specify a simple HKY+$\\Gamma4$+UCLN model of molecularsubstitution (Hasegawa et al. 1985; Yang and Nielsen 1998; Drummond et al. 2006).First specify a base rate for the molecular clock. This prior is uniformover orders of magnitude, between $10^{-6}$ and $10^3$, and was chosento minimize its influence on the tree height.rate_mol ~ dnLoguniform(1E-6, 1E0)rate_mol.setValue(1E-2)moves.append( mvScale(rate_mol, lambda=0.2, weight=4) )moves.append( mvScale(rate_mol, lambda=1.0, weight=2) )Assign log-normal relaxed clock rate multipliers to each branch in thetree. These priors have a mean of 1 so each branch prefers a strictclock model in the absence of data.branch_sd &lt;- 1.0branch_mean &lt;- 0.0 - 0.5 * branch_sd^2for (i in 1:n_branches) {    branch_rate_multiplier[i] ~ dnLognormal(mean=branch_mean, sd=branch_sd)    branch_rates[i] := rate_mol * branch_rate_multiplier[i]    moves.append( mvScale(branch_rate_multiplier[i]) )}moves.append( mvVectorScale(branch_rate_multiplier, weight=3) )Now we’ll create an HKY rate matrix. First, we create agamma-distributed transition-transversion (Ts/Tv) rate ratio with priorwith mean equal to onekappa ~ dnGamma(2,2)moves.append( mvScale(kappa) )then create a flat Dirichlet prior on the base frequencies over A, C, G,and Tbf ~ dnDirichlet([1,1,1,1])moves.append( mvSimplexElementScale(bf, alpha=10, weight=2) )and, finally, combine the base frequencies and Ts/Tv rate ratio to buildthe rate matrixQ_mol := fnHKY(kappa, bf)Next, we’ll create a $+\\Gamma4$ across-site rate variation model. First,we need a parameter to control the amount of site rate variationalpha ~ dnUniform(0,50)moves.append( mvScale(alpha) )and a discretized Gamma distribution with four categoriessite_rates := fnDiscretizeGamma(alpha, alpha, 4)The distribution of site rates categories has mean equal to one andvariance equal to $1/\\alpha$. When alpha grows small, theamount of site rate heterogeneity increases. When alpha islarge, the variance shrinks to zero, and the site rate multipliers ofsite_rates converge to the value 1.Finally, we’ll create our molecular model of substitutionm_mol ~ dnPhyloCTMC(Q=Q_mol,                    tree=tree,                    branchRates=branch_rates,                    siteRates=site_rates,                    type=\"DNA\",                    nSites=dat_mol.nchar())and attach the ITS alignmentm_mol.clamp(dat_mol)The biogeographic modelThe biogeographic model is identical to that described in Section , so redundant details are omitted here.First, create the biogeographic rate parameter.rate_bg ~ dnLoguniform(1E-4,1E2)rate_bg.setValue(1E-2)moves.append( mvScale(rate_bg, lambda=0.2, weight=4) )moves.append( mvScale(rate_bg, lambda=1.0, weight=2) )The relative dispersal rate is fixed to 1dispersal_rate &lt;- 1.0the distance scale parameterdistance_scale ~ dnUnif(0,20)distance_scale.setValue(0.001)moves.append( mvScale(distance_scale, weight=3) )Next, create dispersal rates that are functions of distance between allpairs of areas, but between areas that exist during epochi!for (i in 1:n_epochs) {  for (j in 1:n_areas) {    for (k in 1:n_areas) {     dr[i][j][k] &lt;- 0.0     if (connectivity[i][j][k] &gt; 0) {       dr[i][j][k] := dispersal_rate * exp(-distance_scale * distances[j][k])     }    }  }}Create the extirpation rateslog_sd &lt;- 0.5log_mean &lt;- ln(1) - 0.5*log_sd^2extirpation_rate ~ dnLognormal(mean=log_mean, sd=log_sd)moves.append( mvScale(extirpation_rate, weight=2) )for (i in 1:n_epochs) {  for (j in 1:n_areas) {    for (k in 1:n_areas) {      er[i][j][k] &lt;- 0.0    }    er[i][j][j] := extirpation_rate  }}Build a rate matrix for each time intervalfor (i in 1:n_epochs) {  Q_DEC[i] := fnDECRateMatrix(dispersalRates=dr[i],                          extirpationRates=er[i],                          maxRangeSize=max_areas)}Treat epoch times as random variables, except the present is always thepresent (or is it?).for (i in 1:n_epochs) {  time_max[i] &lt;- time_bounds[i][1]  time_min[i] &lt;- time_bounds[i][2]  if (i != n_epochs) {    epoch_times[i] ~ dnUniform(time_min[i], time_max[i])    epoch_width = time_bounds[i][1] - time_bounds[i][2]    moves.append( mvSlide(epoch_times[i], delta=epoch_width/2) )  } else {    epoch_times[i] &lt;- 0.0  }}Wrap the vector of rate matrices with the fnEpoch rategenerator functionQ_DEC_epoch := fnEpoch(Q=Q_DEC, times=epoch_times, rates=rep(1, n_epochs))Here, we treat the probability of different types of cladogenetic eventsas a random variable to be estimate.clado_event_types &lt;- [ \"s\", \"a\" ]p_sympatry ~ dnUniform(0,1)p_allopatry := abs(1.0 - p_sympatry)moves.append( mvSlide(p_sympatry, delta=0.1, weight=2) )clado_event_probs := simplex(p_sympatry, p_allopatry)P_DEC := fnDECCladoProbs(eventProbs=clado_event_probs,                         eventTypes=clado_event_types,                         numCharacters=n_areas,                         maxRangeSize=max_areas)Based on fossil pollen evidence, force range state and the root of thetree to be the mainland area (Z)rf_DEC_tmp &lt;- rep(0, n_states)rf_DEC_tmp[n_areas+1] &lt;- 1  # Mainland (Z) is the only possible starting staterf_DEC &lt;- simplex(rf_DEC_tmp)Create the phylogenetic model of range evolutionm_bg ~ dnPhyloCTMCClado(tree=tree,                           Q=Q_DEC_epoch,                           cladoProbs=P_DEC,                           branchRates=rate_bg,                           rootFrequencies=rf_DEC,                           type=\"NaturalNumbers\",                           nSites=1)        Attach the species range dataset to the modelm_bg.clamp(dat_range_n)To easily identify interactions between the posterior estimates ofisland ages and divergence times, we’ll create a deterministic node tomonitor the age of the silversword radiation. First, create adeterministic node to monitor the crown age of the silversword radiationingroup_clade &lt;- clade(\"Wilkesia_hobdyi\",                       \"Dubautia_reticulata\",                       \"Dubautia_microcephala\",                       \"Argyroxiphium_caliginis\")ingroup_age := tmrca(tree, ingroup_clade)Next, create a vector of variables to report the posterior probabilitythat the clade originates before a given island. When thefirst argument in of the ifelse function returnstrue, the node has value 1 and 0otherwise. Thus, the mean of this variable gives the posteriorprobability that the inequality is satisfied.for (i in 1:n_epochs) {  ingroup_older_island[i] := ifelse(ingroup_age &gt; epoch_times[i], 1, 0)}Create the standard monitors. One difference is that themnFile monitor will now record the posterior distributionfor the tree variable, whereas the previous two tutorialsassumed tree was fixed.monitors = VectorMonitors()monitors.append( mnScreen(printgen=100, ingroup_age) )monitors.append( mnModel(file=out_fn+\".model.log\", printgen=100) )monitors.append( mnFile(tree, filename=out_fn+\".tre\", printgen=100) )monitors.append( mnJointConditionalAncestralState(tree=tree,                                                  ctmc=m_bg,                                                  type=\"NaturalNumbers\",                                                  withTips=true,                                                  withStartStates=true,                                                  filename=out_fn+\".states.log\",                                                  printgen=100) )monitors.append( mnStochasticCharacterMap(ctmc=m_bg,                                          filename=out_fn+\".stoch.log\",                                          printgen=100) )Because ingroup_older_island does not contribute to themodel likelihood, it must be manually introduced to the model object.Compose the model object.mymodel = model(m_bg, ingroup_older_island)Create the MCMC object and run the analysis.mymcmc = mcmc(mymodel, moves, monitors)mymcmc.run(n_gen)Dated maximum clade credibility tree with ancestral state estimates for the “simple_phy” analysis. Nodes represent ancestral ranges before and after cladogenetic events. Pie slices are proportional to the posterior probability of that ancestral range. Colorsof markers indicate the range state. Only the first, second, and third most probable ranges are shown, with less probable ranges represented in gray (…). Vertical dashed lines indicate the range of possible island formation times.ResultsExample results are located at andTo understand the influence of the epoch model on ancestral range anddivergence time estimation, it is important to run addition analyseswith alternative settings. Scripts to jointly estimate molecularevolution, historical biogeographic, and phylogenetic parameters areavailable as scripts/run_simple_phy.Rev andscripts/run_epoch_phy.Rev. The “epoch” analysis isidentical to the analysis just described. The “simple” analysis issimilar to the “epoch” analysis, except it substitutes thepaleogeography-aware model of range evolution (see Section ) for a paleogeography-naive model (see Section).Dated maximum clade credibility tree with ancestral state estimates for the “epoch_phy” analysis. Nodes represent ancestral ranges before and after cladogenetic events. Pie slices are proportional to the posterior probability of that ancestral range. Colorsof markers indicate the range state. Only the first, second, and third most probable ranges are shown, with less probable ranges represented in gray (…). Vertical dashed lines indicate the range of possible island formation times.We see that simple analysis () estimates theancestral range at the root of the clade as Maui+Mainland (MZ) or Hawaii+Mainland (HZ). This isunrealistic, both because of the extreme distance between those areas,but also the simple analysis estimates the root age to be 10.3 (HPD95%4.6, 15.0) Ma, well before Maui originated. (Date estimates are reportedin the simple_phy.mcc.tre andsimple_phy.model.log files.) The simple model also infersKauai+Maui (KM) as the ancestral range of living silverswords and aingroup crown age of 6.4 (HPD95% 2.2, 11.0) Ma, which is impossibly ancientgiven the islands’ ages.The epoch analysis () produces more sensibleancestral range estimates, with Kauai being colonized first, and youngerislands only being colonized as they become available. The crown age ofsilverswords is estimated as 3.3 (HPD95% 1.6, 5.0) Ma. When comparingthe results to the earlier fixed-phylogeny epoch results in , we recover a greater role forcladogenesis for the younger speciation events. These two analyses onlydiffer in terms of whether the phylogeny is fixed or estimated, so it islikely a result of phylogenetic error in the fixed tree.Plot of posterior samples for island ages and the origin time of living silverswords.The colors black, blue, red, orange, and green correspond to the origination times of Kaui, Oahu, Maui, Hawaii, and the silversword clade, respectively.Theleft panel ignores paleogeography, allowing silverswords to originatewell before the formation of Kauai (epoch_times[1]).Theright panel conditions of paleogeography, which prefers a silverswordcrown age that follows the formation of Kauai.Plot of posterior samples for island ages and the origin time ofliving silverswords. The colors black, blue, red, orange, and greencorrespond to the origination times of Kaui, Oahu, Maui, Hawaii, and thesilversword clade, respectively. The left panel ignores paleogeography,allowing silverswords to originate well before the formation of Kauai(epoch_times[1]). The right panel conditions of paleogeography, which prefers a silversword crown age that follows the formation of Kauai.In Tracer, one can look at the sampled posterior of island ages incomparison the origination time of crown silverswords (). The left panel shows the simple analysis, wherecrown silverswords often originate before the formation of Kauai. Theright panel shows that crown silverswords probably originated before theformation of Maui, but after the formation of Kauai.            Model      $P(a_S&gt;a_K)$      $P(a_S&gt;a_O)$      $P(a_S&gt;a_M)$      $P(a_S&gt;a_H)$                  simple      0.68      0.94      1.00      1.00              epoch      0.04      0.59      0.99      1.00      Posterior probability that the age of crown silverswords ($a_S$) is  older than the origination times of K, O, M, and H  ($a_K, a_O, a_M, a_H$, respectively). The “simple” model (Left)  ignores paleogeography while the “epoch” model (Right) conditions on  it.By tabulating the results of the deterministic variableingroup_older_island, we measure the posteriorprobability that crown silverswords originated before or after eachparticular epoch in the model (). Treating$Prob&gt;0.95$ as significant support for an evolutionary outcome, the epochmodel produces strong support that crown silverswords originated afterthe formation of Kauai, $Prob(a_S &gt; a_K) = 1.0 - 0.04 &gt; 0.95$ but less supportthat they originated after the formation of Oahu,$Prob(a_S &gt; a_O) = 1.0 - 0.59 &lt; 0.95$.",
        "url": "/tutorials/biogeo/biogeo_dating.html",
        "index": "true"
      }
      ,
    
      "tutorials-biogeo-biogeo-epoch-html": {
        "title": "Advanced phylogenetic analysis using the DEC model",
        "content": "IntroductionIn the Simple phylogenetic analysis using the DEC model tutorial, we went through the exercise of setting up theinstantaneous rate matrix and cladogenetic transition probabilities for a simpleDEC model.In this tutorial, we will set up a more complex model considering the geological histories in the biogeographic inference. We will learn how to set up an epoch model.An improved DEC analysisIn this section, we’ll introduce a suite of model features that lendtowards more realistic biogeographic analyses. Topics include applyingrange size constraints, stratified (or epoch) models ofpaleoconnectivity, function-valued dispersal rates, and incorporatinguncertainty in paleogeographic event time estimates. These modificationsshould produce more realistic ancestral range estimates, e.g. that avolcanic island may only be colonized once it has formed, and thatdistance should have some bearing on dispersal rate.To accomplish this, we’ll incorporate (paleo-)geographical data for theHawaiian archipelago, summarized in . Even thoughwe will continue to use four areas (K, O, M, H) in this section, we willuse all six areas (R, K, O, M, H, Z) in , hencethe full table is given for future reference.            area      code      $a_{max}$      $a_{min}$      $g_{\\bullet R}$      $g_{\\bullet K}$      $g_{\\bullet O}$      $g_{\\bullet M}$      $g_{\\bullet H}$      $g_{\\bullet Z}$                  Older islands      R      -      -      -      261      406      500      680      3900              Kauai      K      5.15      5.05      -      -      145      239      419      3900              Oahu      O      3.7      2.2      -      -      -      059      239      3900              Maui Nui      M      1.8      1.3      -      -      -      -      082      3900              Hawaii      H      0.7      0.3      -      -      -      -      -      3900              Mainland      Z      -      -      -      -      -      -      -      -      Hawaiian paleogeographic data. The six areas are given in Simple phylogenetic analysis using the DEC model.   Ages $a_{max}$ and $a_{min}$ report the maximum  and minimum origination times for the given island [adapted from  (Lim and Marshall 2017)]. Note that these ages are younger (less conservative) than the ages used in Landis et al. (2018). Distances $g_{ij}$ report the shortest geographical  distance from the coast of the row’s area to the column’s area  (measured at present).AnalysisStart by creating variables for the tree file, the range data, and theoutput prefix    range_fn = \"data/n4/silversword.n4.range.nex\"    tree_fn = \"data/n4/silversword.tre\"    out_fn = \"output/epoch\"The paleogeographical information from  is encodedin three files named hawaii.n4.times.txt,hawaii.n4.distances.txt, andhawaii.n4.connectivity.*.txt.    geo_fn = \"data/n4/hawaii.n4\"    times_fn = geo_fn + \".times.txt\"    dist_fn = geo_fn + \".distances.txt\"Create vectors that will contain all of ourmoves and monitors vectors, respectively.    moves = VectorMoves()    monitors = VectorMonitors()Read in the presence-absence range characters and record the number ofareas in the dataset    dat_range_01 = readDiscreteCharacterData(range_fn)    n_areas &lt;- dat_range_01.nchar()Often, biogeographers wish to limit to the maximum allowable range size.This prohibits widespread species ranges and reduces the total number ofrange states in the analysis, thus improving computational efficiency.We will restrict ranges from including more than two areas. The totalnumber of ranges equals $\\sum_{k=0}^m {n \\choose k}$ where $n$ is thetotal number of areas, $m$ is the maximum number of permissible areas,and ${n}\\choose{k}$ is the number of ways to sample $k$ unorderedareas from a pool of $n$ areas. For $n=4$ and $m=2$, this equals${4 \\choose 0} + {4 \\choose 1} + {4 \\choose 2} = 1 + 4 + 6 = 11$states.First, compute the number of states    max_areas &lt;- 2    n_states &lt;- 0    for (k in 0:max_areas) n_states += choose(n_areas, k)then use n_states to format the dataset for the reducedstate space    dat_range_n = formatDiscreteCharacterData(dat_range_01, \"DEC\", n_states)Our state space now includes only 11 states ($\\emptyset$, K, O, M, H,KO, KM, OM, KH, OH, MH).Record the complete list of range descriptions to file    state_desc = dat_range_n.getStateDescriptions()    state_desc_str = \"state,range\\n\"    for (i in 1:state_desc.size())    {        state_desc_str += (i-1) + \",\" + state_desc[i] + \"\\n\"    }    write(state_desc_str, file=out_fn+\".state_labels.txt\")As with the previous analysis, we’ll brazenly assume we know the datedspecies phylogeny without error.    tree &lt;- readTrees(tree_fn)[1]Next, we’ll read and structure our paleogeographic data. Read in thelist of minimum and maximum ages of island formation    time_bounds &lt;- readDataDelimitedFile(file=times_fn, delimiter=\" \")    n_epochs &lt;- time_bounds.size()Read in the vector of matrices that describe the connectivity betweenareas over time. Note, there is one connectivity matrix per epoch,ordered from oldest to youngest.    for (i in 1:n_epochs) {      epoch_fn[i] = geo_fn + \".connectivity.\" + i + \".txt\"      connectivity[i] &lt;- readDataDelimitedFile(file=epoch_fn[i], delimiter=\" \")    }The area connectivity file for the third epoch (when K, O, and M exist,but not H) contains    1 1 1 0    1 1 1 0    1 1 1 0    0 0 0 0Dispersal events between, say, K and M will be penalized by distancerather than be forbidden by non-connectivity.Read in the matrix of distances between all pairs of areas (km). Forsimplicity, we will assume that distances remained constant acrossepochs, even though these distances certainly varied over time.    distances &lt;- readDataDelimitedFile(file=dist_fn, delimiter=\" \")The distances files contains    001 145 239 419    145 001 059 239    239 059 001 082    419 239 082 001and, if we assume the relative distance between islands remains roughlyconstant over time, then one set of distances is suitable for use forall four epochs.Next, we’ll build an enhanced DEC model. Like before, we’ll define therate matrix in terms of relative rates, then rescale the entire matrixwith the biogeographic rate scaling parameter rate_bg.    rate_bg ~ dnLoguniform(1E-4,1E2)    rate_bg.setValue(1E-2)And create a move to update the base rate of anagenetic change    moves.append( mvSlide(rate_bg, weight=4) )Fix the base dispersal rate to 1    dispersal_rate &lt;- 1.0Dispersal rates might make use of some extrinsic information, such asgeographical distances between areas (MacArthur and Wilson 1967; Webb and Ree 2012). Wemodel this as $d_{ij} = \\exp(-a g_{ij})$ where $g_{ij}$ is thegeographical distance between areas $i$ and $j$ and $a$ is a parameterthat scales distance. Note that all dispersal rates are equal when$a=0$. Add a distance scale parameter    distance_scale ~ dnUnif(0,20)    distance_scale.setValue(0.01)    moves.append( mvScale(distance_scale, weight=3) )Now we can assign rates that are functions of distance between all pairsof areas, but also over all epochs. To accomplish this,notice we now have an outer loop over the number of epochs,n_epochs. This is used to construct a vector of dispersalmatrices, one matrix per epoch. It is crucial to note that all ofelements are assigned the value 0.0 unless the if-statementif (connectivity[i][j][k] &gt; 0)  evaluates totrue. That is, dispersal rates between areas jand k for epoch i are non-zero if and only ifthe connectivity matrix element connectivity[i][j][k]has a positive value! When this condition is met, the dispersal rate isdetermined by the exponential function of inverse distance given above.    for (i in 1:n_epochs) {      for (j in 1:n_areas) {        for (k in 1:n_areas) {          dr[i][j][k] &lt;- 0.0          if (connectivity[i][j][k] &gt; 0) {            dr[i][j][k]  := dispersal_rate * exp(-distance_scale * distances[j][k])          }        }      }    }We will assign the same extirpation prior as was done in the simpleanalysis in the previous section    log_sd &lt;- 0.5    log_mean &lt;- ln(1) - 0.5*log_sd^2    extirpation_rate ~ dnLognormal(mean=log_mean, sd=log_sd)    moves.append( mvScale(extirpation_rate, weight=2) )and then provide the appropriate extirpation matrix structure    for (i in 1:n_epochs) {      for (j in 1:n_areas) {        for (k in 1:n_areas) {          er[i][j][k] &lt;- 0.0        }        er[i][j][j] := extirpation_rate      }    }Now we have a vector of dispersal rates, dr, and an vectorof extirpation rates, er, in stored in the RevBayesworkspace. We’ll use these to create a vector of four DEC rate matrices,one for each epoch.    for (i in 1:n_epochs) {      Q_DEC[i] := fnDECRateMatrix(dispersalRates=dr[i],                                  extirpationRates=er[i],                                  maxRangeSize=max_areas)    }Next, we need to define breakpoints for when the underlyingpaleogeographic state/connectivity changes. In our case, we’ll definethe epoch breakpoints as uniformly distributed random variables that arebounded by the minimum and maximum age estimates for when each newisland complex formed (). This is easily doneusing a for loop over the number of epochs. Note, we define the end ofthe final epoch as the present.    for (i in 1:n_epochs) {      time_max[i] &lt;- time_bounds[i][1]      time_min[i] &lt;- time_bounds[i][2]      if (i != n_epochs) {          epoch_times[i] ~ dnUniform(time_min[i], time_max[i])          moves.append( mvSlide(epoch_times[i], delta=(time_max[i]-time_min[i])/2) )      } else {          epoch_times[i] &lt;- 0.0      }    }Now that we have variables for the timing (epoch_times)and character (Q_DEC via connectivity) ofpaleogeographic change throughout the Hawaiian archipelago, we’re readyto unify these objects with the fnEpoch function. Thisfunction requires a vector of rate matrices, a vector of epoch endtimes, and a vector of rate multipliers as arguments. Internally, thefunction computes the appropriate probabilities for state transitionsalong branches according under a piecewise constant continuous-timeMarkov chain. The important consequence of using an epoch model is thattransition probabilities for anagenetic events depend on the geologicalage of the branch.    Q_DEC_epoch := fnEpoch(Q=Q_DEC, times=epoch_times, rates=rep(1,n_epochs))Here, we treat the probability of different types of cladogenetic eventsas a random variables to be estimated.    clado_event_types &lt;- [ \"s\", \"a\" ]    p_sympatry ~ dnUniform(0,1)    p_allopatry := abs(1.0 - p_sympatry)    clado_type_probs := simplex(p_sympatry, p_allopatry)    moves.append( mvSlide(p_sympatry, weight=2) )    P_DEC := fnDECCladoProbs(eventProbs=clado_type_probs,                             eventTypes=clado_event_types,                             numCharacters=n_areas,                             maxRangeSize=max_areas)For this dataset, we assume cladogenetic probabilities are constant withrespect to geological time. Using time-dependent cladogeneticprobabilities (fnEpochCladoProbs) and mixtures ofcladogenetic probabilities (fnMixtureCladoProbs) will becovered in future tutorials.Among the four areas, only Kauai existed at the provided originationtime of the clade, so will set it as the only valid starting statethrough the root frequency distribution.    rf_DEC_tmp &lt;- rep(0, n_states)    rf_DEC_tmp[2] &lt;- 1    rf_DEC &lt;- simplex(rf_DEC_tmp)We have created all the necessary model variables. Now we can create thephylogenetic model of anagenetic and cladogenetic character evolution.dnPhyloCTMCClado will internally make use of thetime-heterogeneous probabilities embedded in the epoch rate generator,Q_DEC_epoch.    m_bg ~ dnPhyloCTMCClado(tree=tree,                            Q=Q_DEC_epoch,                            cladoProbs=P_DEC,                            branchRates=rate_bg,                            rootFrequencies=rf_DEC,                            type=\"NaturalNumbers\",                            nSites=1)Attach the observed range data to the distribution    m_bg.clamp(dat_range_n)And the rest we’ve done before…    monitors.append( mnScreen(printgen=100, rate_bg, extirpation_rate, distance_scale) )    monitors.append( mnModel(file=out_fn+\".model.log\", printgen=10) )    monitors.append( mnFile(tree, filename=out_fn+\".tre\", printgen=10) )    monitors.append( mnJointConditionalAncestralState(tree=tree,                                                           ctmc=m_bg,                                                           type=\"NaturalNumbers\",                                                           withTips=true,                                                           withStartStates=true,                                                           filename=out_fn+\".states.log\",                                                           printgen=10) )    monitors.append( mnStochasticCharacterMap(ctmc=m_bg,                                              filename=out_fn+\".stoch.log\",                                              printgen=100) )Wrap the model graph into a model object    mymodel = model(m_bg)then build and run MCMC    mymcmc = mcmc(mymodel, moves, monitors)    mymcmc.run(5000)ResultsExample results are located at output_example/epoch.When compared to the ancestral state estimates from the “simple”analysis (Simple phylogenetic analysis using the DEC model), these results are farmore consonant with what we understand about the origination times ofthe islands (). First, this reconstruction assertsthat the clade originated in the modern Hawaiian islands at a time whenonly Kauai was above sea level. Similarly, the D.sheriffiana and D. arborea clade no longerestimates OMH as its ancestral range, since Maui and Hawaii had not yetformed 2.4 Ma. The ancestral range for the Agyroxiphiumclade does not have a Hawaiian origin, whereas previously it awarded substantial support for H or MH origins.It may be that these epoch-based estimates are relatively accurate, or they may contain artifacts as a result of assuming a fixedand errorless phylogeny. The next tutorials discuss how to jointlyestimate phylogeny and biogeography, which potentially improves theestimation of divergence times, tree topology, and ancestral ranges.Tree with ancestral state estimates for the “epoch” analysis. Nodes represent ancestral ranges before and after cladogenetic events. Pie slices are proportional to the posterior probability of that ancestral range. Colorsof markers indicate the range state. Only the first, second, and third most probable ranges are shown, with less probable ranges represented in gray (…). Vertical dashed lines indicate the range of possible island formation times.  Continue to the next tutorial: Biogeographic dating using the DEC model",
        "url": "/tutorials/biogeo/biogeo_epoch.html",
        "index": "index"
      }
      ,
    
      "tutorials-biogeo-biogeo-intro-html": {
        "title": "Introduction to phylogenetic biogeography with the DEC model",
        "content": "IntroductionMany fundamental evolutionary processes, such as adaptation, speciation,and extinction, operate in a spatial context. When the historical aspectof this spatial context cannot be observed directly, as is often thecase, biogeographic inference may be applied to estimate ancestralspecies ranges. This works by leveraging phylogenetic, molecular, andgeographical information to model species distributions as the outcomeof biogeographic processes. How to best model these processes requiresspecial consideration, such as how ranges are inherited followingspeciation events, how geological events might influence dispersalrates, and what factors affect rates of dispersal and extirpation. Amajor technical challenge of modeling range evolution is how totranslate these natural processes into stochastic processes that remaintractable for inference. This tutorial provides a brief background insome of these models, then describes how to perform Bayesian inferenceof historical biogeography using a Dispersal-Extinction-Cladogenesis (DEC) model in RevBayes.Overview of the Dispersal-Extinction-Cladogenesis ModelThe Dispersal-Extinction-Cladogenesis (DEC) process models rangeevolution as a discrete-valued process (Ree et al. 2005; Ree and Smith 2008). There arethree key components to understanding the DEC model: range characters,anagenetic range evolution, and cladogenetic range evolution ().Cartoon of behavior of the DEC model. Two anagenetic events (a,b) and fivecladogenetic (c–g) events are shown for a system with two areas. Areas are shadedwhen inhabited by a given lineage and left blank when uninhabited. Time proceedsfrom left to right. (a) Dispersal: a new area to be added to the species range.(b) Extirpation (or local extinction): the species range loses a previouslyinhabited area. (c) Narrow sympatry: When the ancestral range contains one area,both daughter lineages inherit that area. (d) Subset sympatry: When theancestral range is widespread, one daughter inherits the ancestral range and theother daughter inherits only one area. (e) Allopatry (or vicariance): When theancestral range is widespread, one daughter lineage inherits a subset of theancestral areas while the other daughter inherits all remaining ancestral areas.(f) Widespread sympatry: When the ancestral range is widespread, both daughtersinherit the ancestral range. (g) Jump dispersal (or founder speciation): Onedaughter inherits the ancestral range while the other daughter inherits a newunoccupied area.Discrete Range CharactersDEC interprets taxon ranges as presence-absence data, that is, where aspecies is observed or not observed across multiple discrete areas. Forexample, say there are three areas, A, B, and C. If a species is presentin areas A and C, then its range equals AC, which can also be encodedinto the length-3 bit vector, 101. Bit vectors may also be transformedinto an integer-valued state, e.g., the binarynumber 101 equals the integer 5. Note, add 1 to the value of theinteger-valued state to access that state from a RevBayes object, suchas a rate matrix, e.g. the range AC with integer value 5 is accessed atindex 6.            Range      Bits      Size      Integer                  $\\emptyset$      000      0      0              A      100      1      1              B      010      1      2              C      001      1      3              AB      110      2      4              AC      101      2      5              BC      011      2      6              ABC      111      3      7      : Example of discrete range representations for an analysis with areas  A, B, and C.The decimal representation of range states is rarely used in discussion,but it is useful to keep in mind when considering the total number ofpossible ranges for a species and when processing output.Anagenetic Range EvolutionIn the context of the DEC model, anagenesis refers to range evolutionthat occurs between speciation events within lineages. There are twotypes of anagenetic events, dispersal (a) and(local) extinction or exitrpation (b).Because DEC uses discrete-valued ranges, anagenesis is modeled using acontinuous-time Markov chain. This, in turn, allows us to computetransition probability of a character changing from $i$ to $j$ in time$t$ through matrix exponentiation\\(\\mathbf{P}_{ij}(t) = \\left[ \\exp \\left\\lbrace \\mathbf{Q}t \\right\\rbrace \\right]_{ij},\\)where $\\textbf{Q}$ is the instantaneous rate matrix defining the ratesof change between all pairs of characters, and $\\textbf{P}$ is thetransition probability rate matrix. The indices $i$ and $j$ representdifferent ranges, each of which is encoded as the set of areas occupiedby the species. The probability has integrated over all possiblescenarios of character transitions that could occur during $t$ so longas the chain begins in range $i$ and ends in range $j$. We can thenencode ${\\bf Q}$ to reflect the allowable classes of range evolutionevents with biologically meaningful parameters. For three areas, therates in the anagenetic rate matrix are\\[\\textbf{Q} =\t\\begin{array}{c|cccccccc}\t\t&amp; \\emptyset &amp; A &amp; B &amp; C &amp; AB &amp; AC &amp; BC &amp; ABC \\\\\t\t\\hline\t\t\\emptyset \t&amp; - \t&amp; 0 \t&amp; 0 \t&amp; 0 \t\t&amp; 0\t\t\t&amp; 0 \t\t&amp; 0 \t\t&amp; 0 \\\\\t\tA \t\t\t&amp; e_A \t&amp; - \t&amp; 0 \t&amp; 0 \t\t&amp; d_{AB}\t&amp; d_{AC} \t&amp; 0 \t\t&amp; 0 \\\\\t\tB \t\t\t&amp; e_B \t&amp; 0 \t&amp; - \t&amp; 0 \t\t&amp; d_{BA}\t&amp; 0 \t\t&amp; d_{BC} \t&amp; 0 \\\\\t\tC \t\t\t&amp; e_C \t&amp; 0 \t&amp; 0 \t&amp; - \t\t&amp; 0 \t\t&amp; d_{CA} \t&amp; d_{CB} \t&amp; 0 \\\\\t\tAB \t\t\t&amp; 0 \t&amp; e_B \t&amp; e_A \t&amp; 0 \t\t&amp; -\t\t\t&amp; 0 \t\t&amp; 0 \t\t&amp; d_{AC} + d_{BC} \\\\\t\tAC \t\t\t&amp; 0 \t&amp; e_C \t&amp; 0 \t&amp; e_A \t\t&amp; 0\t\t\t&amp; - \t\t&amp; 0 \t\t&amp; d_{AB} + d_{CB} \\\\\t\tBC \t\t\t&amp; 0 \t&amp; 0 \t&amp; e_C \t&amp; e_B \t\t&amp; 0\t\t\t&amp; 0 \t\t&amp; - \t\t&amp; d_{BA} + d_{CA} \\\\\t\tABC \t\t&amp; 0 \t&amp; 0 \t&amp; 0 \t&amp; 0 \t\t&amp; e_C \t\t&amp; e_B \t\t&amp; e_A \t\t&amp; - \\\\\t\t\t\t\t\t\t\t\t\\end{array}\\]where $e = ( e_A, e_B, e_C )$ are the (local) extinction rates per area,and $d = ( d_{AB}, d_{AC}, d_{BC}, d_{BA}, d_{CA}, d_{CB})$ are thedispersal rates between areas. Notice that the sum of rates leaving thenull range ($\\emptyset$) is zero, meaning any lineage that loses allareas in its range remains that way permanently.To build our intuition, let’s construct a DEC rate matrix in RevBayes.  Create a new directory on your computer called RB_biogeo_tutorial.  Navigate to your tutorial directory and execute the rb binary.One option for doing this is to move the rb executable to the tutorialdirectory or to create a shortcut to your executable.  Alternatively, if you are on a Unix system, and have added RevBayes to your path,you simply have to type rb in your Terminal to run the program.Assume you have three areasn_areas &lt;- 3First, create a matrix of dispersal rates between area pairs, with rates$d_{AB} = d_{AC} = \\ldots = d_{CB} = 1$.for (i in 1:n_areas) {    for (j in 1:n_areas) {        dr[i][j] &lt;- 1.0    }}Next, let’s create the extirpation rates with values $e_A=e_B=e_C=1$for (i in 1:n_areas) {    for (j in 1:n_areas) {        er[i][j] &lt;- 0.0    }    er[i][i] &lt;- 1.0}When the extirpation rate matrix is a diagonal matrix (i.e. allnon-diagonal entries are zero), extirpation rates are mutuallyindependent as in (Ree et al. 2005). More complex models that penalizewidespread ranges that span disconnected areas are explored in latersections.To continue, create the DEC rate matrix from the dispersal rates(dr) and extirpation rates (er).Q_DEC := fnDECRateMatrix(dispersalRates=dr, extirpationRates=er)Q_DEC    [ [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000 ] ,        1.0000, -3.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000 ] ,        1.0000, 0.0000, -3.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000 ] ,        1.0000, 0.0000, 0.0000, -3.0000, 0.0000, 1.0000, 1.0000, 0.0000 ] ,        0.0000, 1.0000, 1.0000, 0.0000, -4.0000, 0.0000, 0.0000, 2.0000 ] ,        0.0000, 1.0000, 0.0000, 1.0000, 0.0000, -4.0000, 0.0000, 2.0000 ] ,        0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, -4.0000, 2.0000 ] ,        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, -3.0000 ] ]Compute the anagenetic transition probabilities for a branch of length0.2.tp_DEC &lt;- Q_DEC.getTransitionProbabilities(rate=0.2)tp_DEC    [ [ 1.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],      [ 0.000, 0.673, 0.013, 0.013, 0.123, 0.123, 0.005, 0.050],      [ 0.000, 0.013, 0.673, 0.013, 0.123, 0.005, 0.123, 0.050],      [ 0.000, 0.013, 0.013, 0.673, 0.005, 0.123, 0.123, 0.050],      [ 0.000, 0.107, 0.107, 0.004, 0.502, 0.031, 0.031, 0.218],      [ 0.000, 0.107, 0.004, 0.107, 0.031, 0.502, 0.031, 0.218],      [ 0.000, 0.004, 0.107, 0.107, 0.031, 0.031, 0.502, 0.218],      [ 0.000, 0.021, 0.021, 0.021, 0.107, 0.107, 0.107, 0.616]]Notice how the structure of the rate matrix is reflected in thetransition probability matrix. For example, ranges that are separated bymultiple dispersal and extirpation events are the most improbable:transitioning from going from A to BC takes a minimum of three eventsand has probability 0.005.Also note that the probability of entering or leaving the null range iszero. By default, the RevBayes conditions the anagenetic rangeevolution process on never entering the null range when computing thetransition probabilities (nullRange=\"CondSurv\"). Thisallows the model to both simulate and infer using the same transitionprobabilities. Massana et al. (2015) first noted that the null range—anunobserved absorbing state—results in abnormal extirpation rate andrange size estimates. Their proposed solution to eliminate the nullrange from the state space is enabled with thenullRange=\"Exclude\" setting. ThenullRange=\"Include\" setting provides no special handling ofthe null range, and produces the raw probabilities of Ree et al. (2005).Cladogenetic Range EvolutionThe cladogenetic component of the DEC model describes evolutionarychange accompanying speciation events (c–g).In the context of range evolution, daughter species do not necessarilyinherit their ancestral range in an identical manner. For each internalnode in the reconstructed tree, one of several cladogenetic events canoccur, some of which are described below.Beginning with the simplest case first, suppose the range of a speciesis $A$ the moment before speciation occurs at an internal phylogeneticnode. Since the species range is size one, both daughter lineagesnecessarily inherit the ancestral species range ($A$). In DEC parlance,this is called a narrow sympatry event (c).Now, suppose the ancestral range is $ABC$. Under subset sympatry, onelineage identically inherits the ancestral species range, $ABC$, whilethe other lineage inherits only a single area, i.e. only $A$ or $B$ or$C$ (d). Under allopatric cladogenesis, theancestral range is split evenly among daughter lineages, e.g. onelineage may inherit $AB$ and the other inherits $C$(e). For widespread sympatric cladogenesis, bothlineages inherit the ancestral range, $ABC$ (f). Finally, supposing the ancestral range is $A$,jump dispersal cladogenesis results in one daughter lineage inheritingthe ancestral range $A$, and the other daughter lineage inheriting apreviously uninhabited area, $B$ or $C$ (g).See Matzke (2012) for an excellent overview of the cladogenetic statetransitions described in the literature (specifically see this figure).Make the cladogenetic probability event matrixclado_event_types = [ \"s\", \"a\" ]clado_event_probs &lt;- simplex( 1, 1 )P_DEC := fnDECCladoProbs(eventProbs=clado_event_probs,                         eventTypes=clado_event_types,                         numCharacters=n_areas)clado_event_types defines what cladogenetic event typesare used. \"a\" and \"s\" indicate allopatry andsubset sympatry, as described in Ree et al. (2005). Other cladogenetic eventsinclude jump dispersal [\"j\"] (Matzke 2014) and full sympatry[\"f\"] (Landis et al. 2013). The cladogenetic event probabilitymatrix will assume that eventProbs and eventTypes share the same order.Print the cladogenetic transition probabilitiesP_DEC       [         ( 1 -&gt; 1, 1 ) = 1.0000,         ( 2 -&gt; 2, 2 ) = 1.0000,         ( 3 -&gt; 3, 3 ) = 1.0000,         ...         ( 7 -&gt; 7, 1 ) = 0.0833,         ( 7 -&gt; 7, 2 ) = 0.0833,         ( 7 -&gt; 7, 3 ) = 0.0833       ]The cladogenetic probability matrix becomes very sparse for largenumbers of areas, so only non-zero values are shown. Each row reports atriplet of states—the ancestral state and the two daughter states—withthe probability associated with that event. Since these are properprobabilities, the sum of probabilities for a given ancestral state overall possible cladogenetic outcomes equals one.Things to ConsiderThe probabilities of anagenetic change along lineages must account forall combinations of starting states and ending states. For 3 areas,there are 8 states, and thus $8 \\times 8 = 64$ probability terms forpairs of states. For cladogenetic change, we need transitionprobabilities for all combinations of states before cladogenesis, aftercladogenesis for the left lineage, and after cladogenesis for the rightlineage. Like above, for three areas, there are 8 states, and$8 \\times 8 \\times 8 = 512$ cladogenetic probability terms.Of course, this model can be specified for more than three areas. Let’sconsider what happens to the size of Q when the number of areas$N$ becomes large. For three areas, Q is of size $8 \\times 8$. For tenareas, Q is of size $2^{10} \\times 2^{10} = 1024 \\times 1024$, whichapproaches the largest size matrices that can be exponentiated in apractical amount of time. For twenty areas, Q is size$2^{20} \\times 2^{20} \\approx 10^6 \\times 10^6$ and exponentiation isnot viable. Thus, selecting the discrete areas for a DEC analysis shouldbe done with regard to what one hopes to learn through the analysisitself.  Continue to the next tutorial: Simple phylogenetic analysis using the DEC model",
        "url": "/tutorials/biogeo/biogeo_intro.html",
        "index": "true"
      }
      ,
    
      "tutorials-biogeo-biogeo-simple-html": {
        "title": "Simple phylogenetic analysis using the DEC model",
        "content": "IntroductionIn the Introduction to phylogenetic biogeography with the DEC model tutorial, we went through the exercise of setting up theinstantaneous rate matrix and cladogenetic transition probabilities for a simpleDEC model.In this tutorial, we will complete a biogeographic analysis using an empirical dataset: the Hawaiian silverswords.A Simple DEC AnalysisThe following series of tutorials will estimate the ancestral ranges ofthe silversword alliance (Tribe Madiinae), a young anddiverse clade of about 50 species and subspecies. Although silverswordsare endemic to Hawaii, they are nested within a larger clade alongsidetarweeds, which are native to western continental North America(Baldwin et al. 1991). The size and age of the silversword clade, combined withour knowledge of Hawaiian island formation, makes it an ideal system toexplore concepts in historical biogeography and phylogeny. For furtherreading, consult Carlquist (1959) and Baldwin and Sanderson (1998).A beautiful figure of the discrete areas for the tutorial. Six areas are shown: Kauai and Niihau (K); Oahu (O); Maui-Nui, Lanai, and Molokai (M); Hawaii (H); the remaining Hawaiian islands (R); and the North American mainland (Z).For this tutorial we’ll focus entirely on the silversword alliance andthe modern Hawaiian archipelago. To begin, we’ll use just four areas, K,O, M, and H, and include areas R and Z in later analyses (). The species ranges used in this exercise followGillespie and Baldwin (2009).            Range      Areas      Size      State                  $\\emptyset$      0000      0      0              K      1000      1      1              O      0100      1      2              M      0010      1      3              H      0001      1      4              KO      1100      2      5              KM      1010      2      6              OM      0110      2      7              KH      1001      2      8              OH      0101      2      9              MH      0011      2      10              KOM      1110      3      11              KOH      1101      3      12              KMH      1011      3      13              OMH      0111      3      14              KOMH      1111      4      15      Area coding used for four areas: K is Kauai and Nihoa; O is Oahu; M  is Maui Nui, Lanai, and Molokai; H is Hawaii island.Getting Set UpWe have provided the following data files for this tutorial:  silversword.tre:Dated phylogeny of the silversword alliance.  silversword.n4.range.nex:A data matrix with the coded ranges for each species.  Once you have downloaded the zip file by clicking on the arrow above,Unzip that and navigate inside of that directory.This directory should be called revbayes_biogeo_biogeo_simple  Once inside you should see two directories: data and scripts  Once you have all of the files, open RevBayes and ensure that your working directoryis the top directory above the data folder (e.g., revbayes_biogeo_biogeo_simple).AnalysisFirst, create file management variables for input and outputrange_fn = \"data/n4/silversword.n4.range.nex\"tree_fn = \"data/n4/silversword.tre\"out_fn = \"output/simple\"then read in our character data as binary presence-absence charactersdat_range_01 = readDiscreteCharacterData(range_fn)then encode the species ranges into natural numbersdat_range_n = formatDiscreteCharacterData(dat_range_01, \"DEC\")Record the number of areas (characters) from the discrete character dataobjectn_areas = dat_range_01.nchar()You can view the taxon data to see how characters are coded both ashuman-readable presence-absence datadat_range_01[1]      Argyroxiphium_grayanum_East_Maui:        0010And as computer-readable naturalnumbersdat_range_n[1]      Argyroxiphium_grayanum_East_Maui:        3We’ll want to record the relationship between range states and rangelabels when producing an ancestral range estimate figure. First, storethe vector of range state descriptionsstate_desc = dat_range_n.getStateDescriptions()then write it to filestate_desc_str = \"state,range\\n\"for (i in 1:state_desc.size()){    state_desc_str += (i-1) + \",\" + state_desc[i] + \"\\n\"}write(state_desc_str, file=out_fn+\".state_labels.txt\")For this tutorial we’ll assume we know the dated species phylogenywithout error.tree &lt;- readTrees(tree_fn)[1]Next, we’ll build the anagenetic rate matrix for the DEC model. In itssimplest form, the rate matrix requires a dispersal rate and anextirpation rate. For this analysis, we’ll assume that all pairs ofareas share the same dispersal rate and all areas share the sameextirpation rate. To gain greater control to observe and manage priorsensitivity, we’ll reparameterize the DEC rate matrix to report therelative rates of dispersal versus extirpation events. Inorder for anagenetic event rates to be measured on an absolute timescale (e.g., in millions of years), we will also introduce abiogeographic rate parameter, similar to the molecular clock parameterused in dating analyses.First, create a parameter for the arrival rate of anagenetic rangeevolution events. We’ll apply an uninformative prior to the rate’smagnitude by first assigning it a log uniform prior.rate_bg ~ dnLoguniform(1E-4,1E2)rate_bg.setValue(1E-2)Create a moves vector and create a proposal to update the rate parameter.moves = VectorMoves()moves.append( mvSlide(rate_bg, weight=4) )This yields a uniform prior over orders of magnitude, ranging from$10^{-4}$ to $10^2$ events per million years.Because the rate matrix will describe the relative anagenetic eventrates, we can safely assume that dispersal occurs at the relative(fixed) rate of one.dispersal_rate &lt;- 1.0then create the dispersal rate matrixfor (i in 1:n_areas) {  for (j in 1:n_areas) {    dr[i][j] &lt;- dispersal_rate  }}Next, assign a prior distribution to the relative extirpation rate andassign it a move. The prior distribution of extirpation rates is givenlog_sd and log_mean values that give theprior expected value of one – i.e. the mean rate of area gain and arealoss are equal under the prior.log_sd &lt;- 0.5log_mean &lt;- ln(1) - 0.5*log_sd^2extirpation_rate ~ dnLognormal(mean=log_mean, sd=log_sd)moves.append( mvScale(extirpation_rate, weight=2) )then create a matrix of extirpation ratesfor (i in 1:n_areas) {  for (j in 1:n_areas) {    er[i][j] &lt;- 0.0         }  er[i][i] := extirpation_rate}Note that er is a diagonal matrix whose diagonal values aredetermined (:=) by the stochastic variable,extirpation_rate. We can now create our relative ratematrix, Q_DEC, with the fnDECRateMatrixfunction.Q_DEC := fnDECRateMatrix(dispersalRates=dr, extirpationRates=er)Note, fnDECRateMatrix does not rescale its elements in anyway, so transition rates share the same time scale as the underlyingtree. This scaling is in contrast to the standard molecular substitutionprocesses that are available in RevBayes, such as fnGTR,whose rates are rescaled such that the process is expected to produceone event per site per unit time.Next, we need to create the cladogenetic probability matrix.Cladogenetic event probabilities are given by a transition probabilitymatrix, not a rate matrix. First, we will provide the vector to indicatethat we wish to consider only subset sympatry and allopatry events.Next, we will create a vector of prior weights on cladogenesis eventsthat fixes all cladogenetic events to be equiprobable.clado_event_types &lt;- [ \"s\", \"a\" ]clado_event_probs &lt;- simplex(1, 1)P_DEC := fnDECCladoProbs(eventProbs=clado_event_probs,                            eventTypes=clado_event_types,                            numCharacters=n_areas)Finally, all our DEC model components are encapsulated in thednPhyloCTMCClado distribution, which is similar todnPhyloCTMC except specialized to integrate overcladogenetic events. Although this dataset has four areas, it isrecognized single character with states valued from 1 to $2^4$, hencenSites=1.m_bg ~ dnPhyloCTMCClado(tree=tree,                           Q=Q_DEC,                           cladoProbs=P_DEC,                           branchRates=rate_bg,                           nSites=1,                           type=\"NaturalNumbers\")Finally, attach the observed ranges to the model. Be sure to use thenatural number valued range characters, dat_range_n, andnot the presence-absence range characters, dat_range_01.m_bg.clamp(dat_range_n)Add the monitors.monitors = VectorMonitors()monitors.append( mnScreen(rate_bg, extirpation_rate, printgen=100) )monitors.append( mnModel(file=out_fn+\".params.log\", printgen=10) )monitors.append( mnFile(tree, file=out_fn+\".tre\", printgen=10) )monitors.append( mnJointConditionalAncestralState(tree=tree,                                                  ctmc=m_bg,                                                  filename=out_fn+\".states.log\",                                                  type=\"NaturalNumbers\",                                                  printgen=10,                                                  withTips=true,                                                  withStartStates=true) )monitors.append( mnStochasticCharacterMap(ctmc=m_bg,                                          filename=out_fn+\".stoch.log\",                                          printgen=100) )The mnJointConditionalAncestralState monitor samplesancestral states from the phylogeny, tree, according to themodel of evolution, m_bg, and stores it to the file named“simple.states.log”. Each row in the states file lists thejoint sample of ancestral states conditioned on the tip values for theentire tree (i.e. a joint ancestral state sampleconditional on the tip states). Each column correspondsto the phylogenetic node index for that particular MCMC sample. Theindex is used used to match the state samples with the tree samples,which is especially important when the topology is a random variable().The remaining tasks should be familiar from previous tutorials, so wecan proceed briskly. Prepare the model graph for analysis by creating aModel object.mymodel = model(m_bg)Create the MCMC object from the model, moves, and monitorsvariables, and run the MCMC analysis.mymcmc = mcmc(mymodel, moves, monitors)mymcmc.run(3000)ResultsAnnotated tree withancestral state estimates in FigTree.This tree wasgenerated by ancestralStateTree in RevBayes.The mostprobable end state of each branch (before cladogenesis) is shown at eachnode. Branches are labeled with the posterior probability for theancestral state on the tipwards end of the branch.After opening a new RevBayes session, create helper variables forfiles we’ll work with.out_str = \"output/simple\"out_state_fn = out_str + \".states.log\"out_tree_fn = out_str + \".tre\"out_mcc_fn = out_str + \".mcc.tre\"Build a maximum clade credibility tree from the posterior treedistribution, discarding the first 25% of samples. (Note, this step isgratuitous when we assume a fixed phylogeny, but essential when weestimate the phylogeny in ).tree_trace = readTreeTrace(file=out_tree_fn, treetype=\"clock\")tree_trace.setBurnin(0.25)n_burn = tree_trace.getBurnin()Compute and save the maximum clade credibility treemcc_tree = mccTree(tree_trace, file=out_mcc_fn)Get the ancestral state trace from simple.states.logstate_trace = readAncestralStateTrace(file=out_state_fn)Get the ancestral state tree trace from simple.tre. It isimportant to use readAncestralTreeTrace and notreadTreeTrace to properly annotate the tree with ancestralstates.tree_trace = readAncestralStateTreeTrace(file=out_tree_fn, treetype=\"clock\")Finally, compute and save the ancestral state tree assimple.ase.tre.anc_tree = ancestralStateTree(tree=mcc_tree,                              ancestral_state_trace_vector=state_trace,                              tree_trace=tree_trace,                              include_start_states=true,                              file=out_str+\".ase.tre\",                              burnin=n_burn,                              site=1)We can review the output from ancestralStateTree inFigTree().Ancestral state trees are annotated with the first three most probableancestral states along with their posterior probabilities. When the treeis a random variable, as it is in later exercises, additionalinformation about phylogenetic uncertainty is reported.Finally, it is possible to generate a figure with ancestral states that issuitable for publication using the R packageRevGadgets ().To create build this figure, open an R session and load the plottingscript with the source functionsource(\"scripts/plot_anc_state.simple.R\")Tree with ancestral state estimates for the “simple” analysis. Nodes represent ancestral ranges before and after cladogenetic events. Pie slices are proportional to the posterior probability of that ancestral range. Colorsof markers indicate the range state. Only the first, second, and third most probable ranges are shown, with less probable ranges represented in gray (…). Vertical dashed lines indicate the range of possible island formation times, however island ages do not influence the model inference for this analysis.Notice that the model prefers a widespread ancestral range for the clade(KOH, KMH, or KM) approximately four million years ago when only Kauai existed.Similar geologically unrealistic ranges are estimated for theAgyroxiphium clade (H or MH) and the D. sheriffiana and D. arborea clade (OMH, OM, M) at times when Hawaii and Maui did not exist. Theremaining tutorials will focus on improvements to the simple DEC modelpresented here.  Continue to the next tutorial: Advanced phylogenetic analysis using the DEC model",
        "url": "/tutorials/biogeo/biogeo_simple.html",
        "index": "index"
      }
      ,
    
      "tutorials-sse-bisse-intro-html": {
        "title": "Background on state-dependent diversification rate estimation",
        "content": "IntroductionThis is a general introduction to character state-dependentbranching process models, particularly as they are implemented in RevBayes. Frequently referred to asstate-dependent speciation and extinction (SSE) models, these models area birth-death process where the diversification rates are dependent onthe state of an evolving character. The original model of this typeconsidered a binary character (a trait with two discrete state values;called BiSSE, (Maddison et al. 2007). Several variants have also been developedfor other types of traits(FitzJohn 2010; Goldberg et al. 2011; Goldberg and Igić 2012; Magnuson-Ford and Otto 2012; FitzJohn 2012; Beaulieu and O’Meara 2016; Freyman and Höhna 2018).RevBayes can be used to specify a wide range of SSE models. For specific examples see these other tutorials:  BiSSE and MuSSE models: State-dependent diversification with BiSSE and MuSSE  HiSSE models: State-dependent diversification with HiSSE  ClaSSE models: State-dependent diversification with the ClaSSE model  ChromoSSE: Chromosome EvolutionBackground: The BiSSE ModelThe binary state speciation and extinction model (BiSSE) (Maddison et al. 2007)was introduced because of two problems identified by Maddison (2006).First, inferences about character state transitions based on simpletransition models [like Pagel (1999)] can be thrown off if the characteraffects rates of speciation or extinction. Second, inferences aboutwhether a character affects lineage diversification based on sisterclade comparisons (Mitter et al. 1988) can be thrown off if the transitionrates are asymmetric. BiSSE and related models are now mostly used toassess if the states of a character are associated with different ratesof speciation or extinction.RevBayes implements the extension of BiSSE to any number of discretestates–i.e., the MuSSE model in diversitree;(FitzJohn 2012). We will first describe the general theory about themodel.The theory behind state-dependent diversification modelsA schematic overview of the BiSSE model.Each lineage has a binary trait associated with it, so it is either in state 0 (blue) or state 1 (red).When a lineage is in state 0, it can either (a) speciate with rate $\\lambda_0$ which results into two descendant lineage both being in state 0; (b) go extinct with rate $\\mu_0$; or (c) transition to state 1 with rate $q_{01}$.The same types of events are possible when a lineage is in state 1 but with rates $\\lambda_1$, $\\mu_1$, and $q_{10}$, respectively.General approachThe BiSSE model assumes two discrete states(i.e., a binary character), and that thestate of each extant species is known(i.e., the discrete-valued character isobserved). The general approach adopted by BiSSE andrelated models is to derive a set of ordinary differential equations(ODEs) that describe how the probability of observing a descendant cladechanges along a branch in the observed phylogeny. Each equation in thisset describes how the probability of observing a clade changes throughtime if it is in a particular state over that time period; collectively,these equations are called $\\frac{\\mathrm{d}D_{N,i}(t)}{\\mathrm{d}t}$,where $i$ is the state of a lineage at time $t$ and $N$ is the cladedescended from that lineage.Computing the likelihood proceeds by establishing an initial valueproblem. We initialize the procedure by observing the character statesof some lineages, generally the tip states. Then starting from thoseprobabilities (e.g., species X has state 0with probability 1 at the present), we describe how those probabilitieschange over time (described by the ODEs), working our way back until wehave computed the probabilities of observing that collection of lineagesat some earlier time (e.g., the root).As we integrate from the tips to the root, we need to deal with branchescoming together at nodes. Assuming that the parent and daughter lineageshave the same state, we multiply together the probabilities that thedaughters are state $i$ and the instantaneous speciation rate$\\lambda_i$ to get the initial value for the ancestral branch subtendingthat node.Proceeding in this way down the tree results in a set of $k$probabilities at the root; these $k$ probabilities represent theprobability of observing the phylogeny conditional on the root being ineach of the states (i.e., the $i^\\text{th}$conditional probability is the probability of observing the tree giventhat the root is in state $i$). The overall likelihood of the tree is aweighted average of the $k$ probabilities at the root, where theweighting scheme represents the assumed probability that the root was ineach of the $k$ states.As with all birth-death process models, special care must be taken toaccount for the possibility of extinction. Specifically, the above ODEsmust accommodate lineages that may arise along each branch in the treethat subsequently go extinct before the present (and so are unobserved).This requires a second set of $k$ ODEs,$\\frac{ \\mathrm{d}E_{i}(t)}{\\mathrm{d}t}$, which define how theprobability of eventual extinction from state $i$ changes over time.These ODEs must be solved to compute the differential equations$\\frac{ \\mathrm{d}D_{N,i}(t)}{\\mathrm{d}t}$. We will derive both sets ofequations in the following sections.Derivation for the binary state birth-death processThe derivation here follows the original description in Maddison et al. (2007).Consider a (time-independent) birth-death process with two possiblestates (a binary character), with diversification rates\\(\\{\\lambda_0, \\mu_0\\}\\) and \\(\\{\\lambda_1, \\mu_1\\}\\).Clade probabilities, $D_{N, i}$We define $D_{N,0}(t)$ as the probability of observing lineage $N$descending from a particular branch at time $t$, given that the lineageat that time is in state 0. To compute the probability of observing thelineage at some earlier time point, $D_{N,0}(t + \\Delta t)$, weenumerate all possible events that could occur within the interval$\\Delta t$. Assuming that $\\Delta t$ is small—so that the probability ofmore than one event occurring in the interval is negligible—there arefour possible scenarios within the time interval():      nothing happens;        a transition occurs, so the state changes $0 \\rightarrow 1$;        a speciation event occurs, and the right descendant subsequentlygoes extinct before the present, or;        a speciation event occurs and the left descendant subsequently goesextinct before the present.  We are describing events within a branch of the tree (not at a node), sofor (3) and (4), we require that one of the descendant lineages goextinct before the present because we do not observe a node in the treebetween $t$ and $t + \\Delta t$.Possible events along a branch in the BiSSE model, used for deriving $D_{N,0}(t + \\Delta t)$.This is Figure 2 in Maddison et al. (2007).We can thus compute $D_{N,0}(t + \\Delta t)$ as:\\[\\begin{aligned}\tD_{N,0}(t + \\Delta t) = &amp; \\;(1 - \\mu_0 \\Delta t) \\times &amp; \\text{in all cases, no extinction of the observed lineage} \\\\&amp; \\;[  (1 - q_{01} \\Delta t)(1 - \\lambda_0 \\Delta t) D_{N,0}(t) &amp; \\text{case (1) nothing happens} \\\\&amp; \\; + (q_{01} \\Delta t) (1 - \\lambda_0 \\Delta t) D_{N,1}(t) &amp; \\text{case (2) state change but no speciation} \\\\&amp; \\; + (1 - q_{01} \\Delta t) (\\lambda_0 \\Delta t) E_0(t) D_{N,0}(t) &amp; \\text{case (3) no state change, speciation, extinction} \\\\&amp; \\; + (1 - q_{01} \\Delta t) (\\lambda_0 \\Delta t) E_0(t) D_{N,0}(t)] &amp; \\text{case (4) no state change, speciation, extinction}\\end{aligned}\\]A matching equation can be written down for $D_{N,1}(t+\\Delta t)$.To convert these difference equations into differential equations, wetake the limit $\\Delta t \\rightarrow 0$. With the notation that $i$ canbe either state 0 or state 1, and $j$ is the other state, this yields:\\[\\frac{\\mathrm{d}D_{N,i}(t)}{\\mathrm{d}t} = - \\left(\\lambda_i + \\mu_i + q_{ij} \\right) D_{N,i}(t) + q_{ij} D_{N,j}(t) + 2 \\lambda_i E_i(t) D_{N,i}(t)\\tag{1}\\label{eq:one}\\]Extinction probabilities, $E_i$To solve the above equations for $D_{N, i}$, we see that we need theextinction probabilities. Define $E_0(t)$ as the probability that alineage in state 0 at time $t$ goes extinct before the present. Todetermine the extinction probability at an earlier point,$E_0(t+\\Delta t)$, we can again enumerate all the possible events in theinterval $\\Delta t$ ():      the lineage goes extinct within the interval;        the lineage neither goes extinct nor speciates, resulting in asingle lineage that must eventually go extinct before the present;        the lineage neither goes extinct nor speciates, but there is a statechange, resulting in a single lineage that must go extinct beforethe present, or;        the lineage speciates in the interval, resulting in two lineagesthat must eventually go extinct before the present.  \\[\\begin{aligned}E_0(t + \\Delta t) = &amp;\\; \\mu_0\\Delta t +&amp; \\text{case (1) extinction in the interval} \\\\&amp; (1 - \\mu_0\\Delta t) \\times &amp; \\text{no extinction in the interval and \\dots} \\\\&amp; \\;[(1-q_{01}\\Delta t)(1-\\lambda_0 \\Delta t) E_0(t) &amp; \\text{case (2) nothing happens, but subsequent extinction} \\\\&amp; \\;+ (q_{01}\\Delta t) (1-\\lambda_0 \\Delta t) E_1(t) &amp; \\text{case (3) state change and subsequent extinction} \\\\&amp; \\;+ (1 - q_{01} \\Delta t) (\\lambda_0 \\Delta t) E_0(t)^2] &amp; \\text{case (4) speciation and subsequent extinctions}\\end{aligned}\\]Again, a matching equation for $E_1(t+\\Delta t)$ can be written down.Possible events along a branch in the BiSSE model, used for deriving $E_0(t + \\Delta t)$.This is Figure 3 in Maddison et al. (2007).To convert these difference equations into differential equations, weagain take the limit $\\Delta t \\rightarrow 0$:\\[\\frac{\\mathrm{d}E_i(t)}{\\mathrm{d}t} = \\mu_i - \\left(\\lambda_i + \\mu_i + q_{ij} \\right)E_i(t) + q_{ij} E_j(t) + \\lambda_i E_i(t)^2\\tag{2}\\label{eq:two}\\]Initial values: tips and samplingThe equations above describe how to get the answer at time$t + \\Delta t$ assuming we already have the answer at time $t$. How dowe start this process? The answer is with our character stateobservations, which are generally the tip state values. If species $s$has state $i$, then $D_{s,i}(0) = 1$ (probability is 1 at time 0 [thepresent] because we observed it for sure) and $E_i(0) = 0$ (probability0 of being extinct at the present). For all states other than $i$,$D_{s,j}(0) = 0$ and $E_j(0) = 1$.We can adjust these initial conditions to allow for incomplete sampling.If a proportion $\\rho$ of species are included on the tree, we wouldinstead set $D_{s,i}(0) = \\rho$ (probability of having state $s$ andof being on the tree) and $E_i(0) = 1-\\rho$ (probability of absent, dueto sampling rather than extinction). This simple form of incompletesampling assumes that any species is equally likely to be on the tree(FitzJohn et al. 2009).At nodesEquations \\eqref{eq:one} and \\eqref{eq:two} are the BiSSE ODEs,describing probabilities along the branches of a phylogeny. We also needto specify what happens with the clade probabilities (the $D$s) at thenodes of the tree. BiSSE assumes the ancestor (called $A$) anddescendants (called $N$ and $M$) have the same state(i.e., there is no cladogenetic characterchange). The initial value for the ancestral branch going into a node(at time $t_A$) is then the product of the final values for each of thedaughter branches coming out of that node, times the instantaneousspeciation rate (to account for the observed speciation event):\\[D_{A, i}(t_A) = D_{N, i}(t_A) D_{M, i}(t_A) \\lambda_i\\tag{3}\\label{eq:three}\\]At the rootAfter we integrate equations \\eqref{eq:one} and \\eqref{eq:two}from the tips to the root, dealing with nodes along the way viaequation \\eqref{eq:three}, we arrive at the root with the $D$ values(called $D_{R, i}$), one for each state. These need to be combinedsomehow to get the overall likelihood of the data:\\[\\text{Likelihood(tree, tip states | model)} = \\sum_i D_{R, i} \\, p_{R, i}\\]What probability weighting, $p_{R, i}$ should be used for the possibleroot states? Sometimes a fixed approach is used, assuming that the priorroot state probabilities are either all equal, or are the same as theobserved tip state frequencies, or are the equilibrium state frequenciesunder the model parameters. These assumptions do not have a real basis,however (unless there is some external data that supports them), andthey can cause trouble (Goldberg and Igić 2008). An alternative is to use theBiSSE probabilities themselves to determine the root state weightings,essentially adjusting the weightings to be most consistent with the dataand BiSSE parameters (FitzJohn et al. 2009). Perhaps better is to treat theweightings as unknown parameters to be estimated. These estimates areusually quite uncertain, but in a Bayesian framework, one can treat the$p_{R, i}$ as nuisance parameters and integrate over them.BiSSE model parameters and their interpretation            Parameter      Interpretation                  $\\Psi$      Phylogenetic tree with divergence times              $T$      Root age              $q_{01}$      Rate of transitions from 0 to 1              $q_{10}$      Rate of transitions from 1 to 0              $\\lambda_0$      Speciation rate for state 0              $\\mu_0$      Extinction rate for state 0              $\\lambda_1$      Speciation rate for state 1              $\\mu_1$      Extinction rate for state 1      Equations for the multi-state birth-death processThe entire derivation above can easily be expanded to accommodate anarbitrary number of states (FitzJohn 2012). The only extra piece issumming over all the possible state transitions. The resultingdifferential equations within the branches are:\\[\\begin{aligned}\\frac{\\mathrm{d}D_{N,i}(t)}{\\mathrm{d}t} &amp;= - \\left(\\lambda_i + \\mu_i + \\sum\\limits_{j \\neq i}^k q_{ij} \\right)D_{N,i}(t) + \\sum\\limits_{j \\neq i}^k q_{ij} D_{N,j}(t) + 2\\lambda_iE_i(t)D_{N,i}(t) \\\\\\frac{\\mathrm{d}E_i(t)}{\\mathrm{d}t} &amp;= \\mu_i - \\left(\\lambda_i + \\mu_i + \\sum\\limits_{j \\neq i}^k q_{ij} \\right)E_i(t) + \\sum\\limits_{j \\neq i}^k q_{ij} E_j(t) + \\lambda_i E_i(t)^2\\end{aligned}\\]",
        "url": "/tutorials/sse/bisse-intro.html",
        "index": "true"
      }
      ,
    
      "tutorials-sse-bisse-html": {
        "title": "State-dependent diversification with BiSSE and MuSSE",
        "content": "IntroductionThis tutorial describes how to specify character state-dependent branching process models in RevBayes (Höhna et al. 2016).For more details on the theory behind these models, please see the introductory page: Background on state-dependent diversification rate estimation.This tutorial will explain how to fit the BiSSE and MuSSE models to data using Markov chain Monte Carlo (MCMC).RevBayes is a powerful tool for SSE analyses:to specify HiSSE model, please see State-dependent diversification with HiSSE,for the ClaSSE model, please see State-dependent diversification with the ClaSSE modeland for ChromoSSE please see Chromosome Evolution.Getting Set UpWe provide the data files which we will use in this tutorial:  primates_tree.nex:Dated primate phylogeny including 233 out of 367 species. This treeis from Magnuson-Ford and Otto (2012), who took it from Vos and Mooers (2006) and thenrandomly resolved the polytomies using the method of Kuhn et al. (2011).  primates_activity_period.nex:A file with the coded character states for primate species activity time. This character has just two states: 0 = diurnal and 1 = nocturnal.  primates_solitariness.nex:A file with the coded character states for primate species social system type. This character has just two states: 0 = group living and 1 = solitary.  primates_mating_system.nex:A file with the coded character states for primate species mating-system type. This character has four states: 0 = monogamy, 1 = polygyny, 2 = polygynandry, and 3 = polyandry.  Create a new directory on your computer called RB_bisse_tutorial.  Within the RB_bisse_tutorial directory, create a subdirectory called data.Then, download the provided files and place them in the data folder.Estimating State-Dependent Speciation and Extinction under the BiSSE ModelNow let’s start to analyze an example in RevBayes using the BiSSEmodel. In RevBayes, it’s called CDBDP, meaning character dependentbirth-death process.  Navigate to the RB_bisse_tutorial directory and execute the rb binary.One option for doing this is to move the rb executable to the RB_bisse_tutorialdirectory.  Alternatively, if you are on a Unix system, and have added RevBayes to your path,you simply have to type rb in your Terminal to run the program.For this tutorial, we will specify a BiSSE model that allows for speciation and extinctionto be correlated with a particular life-history trait: the timing of activity during the day.If you open the file primates_activity_period.nexin your text editor, you will see that several species like the mantled howler monkey(Alouatta palliata) have the state 0,indicating that they are diurnal. Whereas other nocturnalspecies, like the aye-aye(Daubentonia madagascariensis) are coded with 1.We may have an a priori hypothesis that diurnal species have higher rates of speciation andby estimating the rates of lineages associated with that trait will allow us to explore thishypothesis.Once you execute RevBayes, you will be in the console. The rest of this tutorial will proceedusing the interactive console.Read in the DataFor this tutorial, we are assuming that the tree is “observed” and considered data.Thus, we will read in the dated phylogeny first.observed_phylogeny &lt;- readTrees(\"data/primates_tree.nex\")[1]Next, we will read in the observed character states for primate activity period.data &lt;- readCharacterData(\"data/primates_activity_period.nex\")It will be convenient to get the number of sampledspecies num_taxa from the tree:num_taxa &lt;- observed_phylogeny.ntips()Additionally, we initialize a variable for our vector of moves and monitors.moves    = VectorMoves()monitors = VectorMonitors()Finally, create a helper variable that specifies the number of statesthat the observed character has:NUM_STATES = 2Using this variable allows us to easily change our script and use a differentcharacter with a different number of states, essentially changing ourmodel from BiSSE (Maddison et al. 2007) to one that allows for more than 2 states–i.e.,the MuSSE model (FitzJohn 2012).Specify the ModelThe basic idea behind the model in this example is that speciation andextinction rates are dependent on a binary character, and the charactertransitions between its two possible states (Maddison et al. 2007).Priors on the RatesWe start by specifying prior distributions on the diversification rates.Here, we will assume an identical prior distribution on each of thespeciation and extinction rates. Furthermore, we will use a log-uniformdistribution as the prior distribution on each speciation andextinction rate (i.e., a uniform distribution on the log of the rates).Now we can specify our character-specific speciation and extinction rateparameters. Because we will use the same prior for each rate, it’s easyto specify them all in a for-loop. We will use a log-uniform distribution as a prioron the speciation and extinction rates. The loop also allows us to apply moves to eachof the rates we are estimating and create a vector of deterministic nodesrepresenting the rate of diversification ($\\lambda - \\mu$) associated with eachcharacter state.for (i in 1:NUM_STATES) {     ### Create a loguniform distributed variable for the diversification rate    speciation[i] ~ dnLoguniform( 1E-6, 1E2)    moves.append( mvScale(speciation[i],lambda=0.20,tune=true,weight=3.0) )    ### Create a loguniform distributed variable for the turnover rate    extinction[i] ~ dnLoguniform( 1E-6, 1E2)    moves.append( mvScale(extinction[i],lambda=0.20,tune=true,weight=3.0) )    diversification[i] := speciation[i] - extinction[i]}The stochastic nodes representing the vector of speciation rates and vector ofextinction rates have been instantiated. The software assumes that the rate in position [1] of eachvector corresponds to the rate associated with diurnal 0 lineages and the rateat position [2] of each vector is the rate associated with nocturnal 1 lineages.⇨ If RevBayes has trouble finding good starting values, then you can initialize the speciation and extinction rate as follows:speciation[i].setValue( ln(367.0/2.0) / observed_phylogeny.rootAge() )extinction[i].setValue( speciation/10.0 )You can print the current values of the speciation rate vector to your screen:speciation[ 0.267, 0.160 ]Of course, your screen output will be different from the values shown above since yourstochastic nodes were initialized with different values drawn from the exponential prior.Next we specify the transition rates between the states 0 and 1:$q_{01}$ and $q_{10}$. As a prior, we choose that each transition rateis drawn from an exponential distribution with a mean of 10 characterstate transitions over the entire tree. This is reasonable because weuse this kind of model for traits that transition not-infrequently, andit leaves a fair bit of uncertainty.Note that we will actually use a for-loop to instantiate the transition ratesso that our script will also work for non-binary characters.rate_pr := observed_phylogeny.treeLength() / 10for ( i in 1:(NUM_STATES*(NUM_STATES-1)) ) {    transition_rates[i] ~ dnExp(rate_pr)    moves.append( mvScale(transition_rates[i],lambda=0.20,tune=true,weight=3.0) )}Here, rate[1] is the rate of transition from state 0 (diurnal) to state 1 (nocturnal),and rate[2] is the rate of going from nocturnal to diurnal.Finally, we put the rates into a matrix, because this is what’s neededby the function for the state-dependent birth-death process.rate_matrix := fnFreeK( transition_rates, rescaled=false)Note that we do not “rescale” the rate matrix. Rate matrices formolecular evolution are rescaled to have an average rate of 1.0, but forthis model we want estimates of the transition rates with the same timescale as the diversification rates.Prior on the Root StateCreate a variable for the root state frequencies. We are using a flat Dirichlet distribution as the prior oneach state. There has been some discussion about this in (FitzJohn et al. 2009).You could also fix the prior probabilities for the root states to be equal(generally not recommended), or use empirical state frequencies.root_state_freq ~ dnDirichlet( rep(1, NUM_STATES) )Note that we use the rep() function which generates a vector of length NUM_STATESwith each position in the vector set to 1. Using this function and the NUM_STATESvariable allows us to easily use this Rev script as a template for a different analysisusing a character with more than two states.We will use a special move for objects that are drawn from a Dirichlet distribution:append( mvDirichletSimplex(rate_category_prior,tune=true,weight=2) )The Probability of Sampling an Extant SpeciesAll birth-death processes are conditioned on the probability a taxon is sampled in the present.We can get an approximation for this parameter by calculating the proportion of sampledspecies in our analysis.We know that we have sampled 233 out of 367 living described primate species. Toaccount for this we can set the sampling probability as a constant nodewith a value of 233/367.sampling &lt;- num_taxa / 367Root AgeThe birth-death process also depends on time to the most-recent-common ancestor–i.e.,the root. In thisexercise we use a fixed tree and thus we know the age of the tree.root_age &lt;- T.rootAge()The Time TreeNow we have all of the parameters we need to specify the full characterstate-dependent birth-death model. We initialize the stochastic noderepresenting the time tree and we create this node using the dnCDBDP() function.timetree ~ dnCDBDP( rootAge           = root_age,                    speciationRates   = speciation,                    extinctionRates   = extinction,                    Q                 = rate_matrix,                    pi                = root_state_freq,                    rho               = sampling )Now, we will fix the BiSSE time-tree to the observed values from our data files. We usethe standard .clamp() method to give the observed tree and branch times:timetree.clamp( observed_phylogeny )And then we use the .clampCharData() to set the observed states at the tips of the tree:timetree.clampCharData( data )Finally, we create a workspace object of our whole model. The model()function traverses all of the connections and finds all of the nodes wespecified.mymodel = model(timetree)You can use the .graph() method of the model object to visualize the graphical model youhave just constructed . This function writes the model DAG to a filethat can be viewed using the  program Graphviz ().The probabilistic graphical model of the character-state-dependent diversification model.This image was generated by executing the mymodel.graph(\"bisse.dot\") in RevBayes after specifying the full model DAG.Then, the resulting file can be opened in the program Graphviz.Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we set up a vector of monitors to record thestates of our Markov chain. The first monitor will model all numericalvariables; we are particularly interested in the rates of speciation,extinction, and transition.monitors.append( mnModel(filename=\"output/primates_BiSSE_activity_period.log\", printgen=1) )Optionally, we can sample ancestral states during the MCMC analysis.We need to add an additional monitor to record the state of each internal node in the tree.The file produced by this monitor can be summarized so that we can visualize the estimates of ancestral states.monitors.append( mnJointConditionalAncestralState(tree=timetree,                     cdbdp=timetree,                       type=\"Standard\",                     printgen=1,                     withTips=true,                     withStartStates=false,                     filename=\"output/primates_BiSSE_activity_period_anc_states.log\") )Similarly, you may want to add a stochastic character map.monitors.append( mnStochasticCharacterMap(cdbdp=timetree,                       filename=\"output/primates_BiSSE_activity_period_stoch_map.log\",                       printgen=1) )Then, we add a screen monitor showing some updates during the MCMCrun.monitors.append( mnScreen(printgen=10, speciation, extinction) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, wecan now set up the MCMC algorithm that will sample parameter values inproportion to their posterior probability. The mcmc() function willcreate our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=5000, tuningInterval=200)Summarize Sampled Ancestral StatesIf we sampled ancestral states during the MCMC analysis, we can use the RevGadgets (Tribble et al. 2022) R packageto plot the ancestral state reconstruction.First, though, we must summarize the sampled values in RevBayes.To do this, we first have to read in the ancestral state log file. This uses a specific function called readAncestralStateTrace().anc_states = readAncestralStateTrace(\"output/primates_BiSSE_activity_period_anc_states.log\")Now, we can write an annotated tree to a file. This function will write a tree with eachnode labeled with the maximum a posteriori (MAP) state and the posterior probabilities for eachstate.anc_tree = ancestralStateTree(tree=T,                  ancestral_state_trace_vector=anc_states,                  include_start_states=false,                  file=\"output/primates_BiSSE_anc_states_results.tree\",                  burnin=0,                  summary_statistic=\"MAP\",                  site=1)Similarly, we compute the maximum a posteriori (MAP) stochastic character map.anc_state_trace = readAncestralStateTrace(\"output/primates_BiSSE_activity_period_stoch_map.log\")characterMapTree(observed_phylogeny,                 anc_state_trace,                 character_file=\"output/primates_BiSSE_activity_period_stoch_map_character.tree\",                 posterior_file=\"output/primates_BiSSE_activity_period_stoch_map_posterior.tree\",                 burnin=0.1,                 reconstruction=\"marginal\")Visualize Estimated Ancestral StatesTo visualize the posterior probabilities of ancestral states, we will use the RevGadgets (Tribble et al. 2022) R package.  Open R.RevGadgets requires the ggtree package (Yu et al. 2017).First, install the ggtree and RevGadgets packages:install.packages(\"devtools\")library(devtools)install_github(\"GuangchuangYu/ggtree\")install_github(\"revbayes/RevGadgets\")Run this code (or use the script plot_anc_states_BiSSE.R):library(ggplot2)library(RevGadgets)# read in and process the ancestral statesbisse_file &lt;- paste0(\"output/primates_BiSSE_activity_period_anc_states_results.tree\")p_anc &lt;- processAncStates(bisse_file)# plot the ancestral statesplot &lt;- plotAncStatesMAP(p_anc,        tree_layout = \"rect\",        tip_labels_size = 1) +        # modify legend location using ggplot2        theme(legend.position = c(0.1,0.85),              legend.key.size = unit(0.3, 'cm'), #change legend key size              legend.title = element_text(size=6), #change legend title font size              legend.text = element_text(size=4))ggsave(paste0(\"BiSSE_anc_states_activity_period.png\"),plot, width=8, height=8)A visualization of the ancestral states estimated under the BiSSE model. We used the script plot_anc_states_BiSSE.R.Next, we also want to plot the stochastic character map.Use the script plot_simmap_BiSSE.R.A visualization of the stochastic character map estimated under the BiSSE model. We used the script plot_simmap_BiSSE.R.Summarizing Parameter EstimatesOur MCMC analysis generated a tab-delimited file called primates_BiSSE_activity_period.log that containsthe samples of all the numerical parameters in our model.Again, we will use the RevGadgets (Tribble et al. 2022) R package, which allow you to generate plots andvisually explore the posterior distributions of sampled parameters.  Open R.Run this code:library(RevGadgets)library(ggplot2)# read in and process the log filebisse_file &lt;- paste0(\"output/primates_BiSSE_activity_period.log\")pdata &lt;- processSSE(bisse_file)# plot the ratesplot &lt;- plotMuSSE(pdata) +        theme(legend.position = c(0.875,0.915),              legend.key.size = unit(0.4, 'cm'), #change legend key size              legend.title = element_text(size=8), #change legend title font size              legend.text = element_text(size=6))ggsave(paste0(\"BiSSE_div_rates_activity_period.png\"),plot, width=5, height=5)Visualizing posterior samples of the speciation rates associated with daily activity time with the RevGadgets (Tribble et al. 2022) R package. We used the script plot_div_rates_BiSSE.R.Evaluate Social System under the BiSSE ModelNow that you have completed the BiSSE analysis for the timing of activity for all primates,perform the same analysis using a different character. Your data directory shouldcontain the file primates_solitariness.nex, which has thecoded states for each species habitat type. This is also a binary character, where the statesare 0 = forest and 1 = savanna.  Complete the BiSSE analysis for habitat type using the same commands given above.Remember it is useful to clear the RevBayes console before starting another analysis. Do thisusing the clear() function.  While you are setting up your new analysis, substitute the character data file name so that you read indata/primates_solitariness.nex instead of primates_activity_period.nex.  It is important that you remember to also change the output file names.  View the parameter log file in Tracer after your MCMC is complete.  What is the rate of speciation associated with group living (speciation[1])? What aboutfor solitary lineages (speciation[2])?Compare the rate estimatesCompare the rates estimated when the activity time is the focal character versus when solitariness is the dependent character.You can do this by opening both files in the same tracer window. If you managed to give all the parameters the same name,it is possible to compare the estimates in the Tracer window by highlighting both files.Explore the estimates of the various parameters. Are any different? Are any the same?Why do you think you might be seeing this pattern?Evaluate Mating System under the MuSSE ModelIn RevBayes it is trivial to change the BiSSE analysis you did in the exercises above to a multi-state model that is not limited to justbinary characters. That is because the model is effectively the same, just with the variable NUM_STATES changed.For this final exercise, we will use the MuSSE model (FitzJohn 2012) to estimate the rates of speciation and extinctionassociated with the mating system state for each primate lineage.Your data directory should contain a file called primates_mating_system.nex. This is a four-statecharacter where the states are: 0 = monogamy, 1 = polygyny, 2 = polygynandry, and 3 = polyandry.  Modify the analysis you completed for the binary state characters in the BiSSE Exercise to accommodate a 4-state character.This means that you must not only change the input data file (primates_mating_system.nex),but you also need to specify NUM_STATES = 4. The rate_matrix must also be modified to accommodate 4 states.  It is important that you remember to also change the output file names.  View the parameter log file in Tracer after your MCMC is complete.  What is the diversification rate associated with each state (diversification)?  Click below to begin the next exercise!  Testing for state-dependent diversification with unobserved rate variation (HiSSE)",
        "url": "/tutorials/sse/bisse.html",
        "index": "true"
      }
      ,
    
      "workshops-bodega2019-html": {
        "title": "Bodega Applied Phylogenetics Workshop",
        "content": "",
        "url": "/workshops/bodega2019.html",
        "index": ""
      }
      ,
    
      "workshops-botany2018-html": {
        "title": "RevBayes for Botanists",
        "content": "",
        "url": "/workshops/botany2018.html",
        "index": ""
      }
      ,
    
      "tutorials-divrate-branch-specific-html": {
        "title": "Branch-Specific Diversification Rate Estimation",
        "content": "Outline: Estimating Branch-Specific Speciation &amp; Extinction RatesThis tutorial describes how to specify a branch-specificbranching-process models in RevBayes; a birth-death process wherediversification rates vary among branches, similar to Rabosky (2014).The probabilistic graphical model is given for each component of thistutorial. The goal is to obtain estimate of branch-specificdiversification rates using Markov chain Monte Carlo (MCMC).The Birth-Death-Shift ProcessAn example of a tree with a single event of diversification rate change, from$(\\lambda_1, \\mu_1)$ to $(\\lambda_2, \\mu_2)$. a) A tree showing the complete cladogenetic process, includingextinct lineages. b) The reconstructed process for the tree shown in a. shows an example in which speciation and extinction rates change among lineages.The speciation and extinction rates at the root of the tree of  are $(\\lambda_1, \\mu_1)$. There was one event of change to the speciation and extinction rates on the tree from $(\\lambda_1, \\mu_1)$ to $(\\lambda_2, \\mu_2)$.From a casual inspection of the tree, it appears that the single change in speciation and/or extinction ratein the tree of  affected the diversity. Note that the clade above the event of speciation/extinctionrate change has fewer living species, and more extinct species, than the clade that maintained the ancestralspeciation and extinction rates.This is exactly the type of situation we attempt to uncover.Here we will describe the birth-death-shift process.The parameters in this model are:  $\\lambda_i$: the rates of speciation of the $i^{\\text{th}}$ lineage  $\\mu_i$: the rates of extinction of the $i^{\\text{th}}$ lineage  $\\eta$: the rate at which speciation/extinction rates changeThe process is described as follows. In a small interval of time, $\\Delta t$, a lineage speciates with probability $\\lambda \\Delta t$, goes extinct with probability $\\mu \\Delta t$, or changes its rate with probability $\\eta \\Delta t$. When a speciation event occurs, both daughter lineages inherit the speciation and extinction rates of the parentlineage. When an event of rate change occurs, new speciation and extinction rates are drawn fromthe probability distributions, $f_{\\lambda}(\\cdot)$ and $f_{\\mu}(\\cdot)$. The affected lineage then continues, but with the modified speciation and extinction rates. When an extinction event occurs, the lineage is terminated at the event time.Samples of an MCMC analysis under a Birth-Death-Shift model. The colorsrepresent different classes of rates for the speciation and extinction rates ($\\lambda$ and$\\mu$, respectively. Note that the speciation and extinction rates at the root of the treediffer for the different MCMC samples: $(\\lambda’,\\mu’)$, $(\\lambda’’,\\mu’’)$, and $(\\lambda’’’,\\mu’’’)$.Estimating Branch-Specific Diversification RatesIn this analysis we are interested in estimating the branch-specific diversification rates.We show how to implement and use the model developed by Höhna et al. (2019)We are going to use the dnCBDP distribution which uses a finite number of rate-categoriesinstead of drawing rates from a continuous distribution directly.Here we adopt an approach using (few) discrete rate categories.This allows us to numerically integrate over all possible rate categories usinga system of differential equations originally described by Maddison et al. (2007)(see also FitzJohn et al. (2009) and FitzJohn (2010)).The numerical procedure breaks time into very small time intervals and sumsover all possible events occurring in that interval (see ).Possible scenarios that could occur over the interval $\\Delta t$ along a lineage that is observed at time $t$.To compute the probability under the birth-death-shift process, we traverse the tree from the tips to the root in small time steps, $\\Delta t$.For each step into the past, from time $t$ to time ($t+\\Delta t$), we compute the change in probability of the observed lineage by enumerating all of the possible scenarios that could occur over the interval $\\Delta t$:    (i) nothing happens,    (ii) a speciation event occurs, where the right descendant survives and the left descendant goes extinct before the present, or    (iii) a speciation event occurs, where the left descendant survives but the right goes extinct before the present, or    (iv) a diversification-rate shift from category $i$ to $j$ occurs.Color key: segment(s) of the tree within the interval $\\Delta t$ are colored blue for state $i$ and/or green for state $j$ to reflect the conditioning of the corresponding scenarios,segment(s) of the tree between $t$ and the present are colored gray because we have integrated over the $k$ discrete rate categories (no specific assignment of rate categories), andsegments of the tree between $t+\\Delta t$ and the root are colored gray because we will integrated over the $k$ discrete rate categories.You don’t need to worry about any of the technical details.It is important for you to realize that this model assumes that new rates at arate-shift event are drawn from a given (discrete) set of rates (see ).Read the treeBegin by reading in the observed tree.observed_phylogeny &lt;- readTrees(\"data/primates_tree.nex\")[1]From this tree, we can get some helpful variables:taxa &lt;- observed_phylogeny.taxa()root &lt;- observed_phylogeny.rootAge()tree_length &lt;- observed_phylogeny.treeLength()Additionally, we initialize a variable for our vector of moves and monitors.moves    = VectorMoves()monitors = VectorMonitors()Finally, we create a helper variable that specifies the number ofdiscrete rate categories, another helper variable for the total number of speciesand our constant for specifying the standard deviation of the lognormal distribution.NUM_RATE_CATEGORIES = 6NUM_TOTAL_SPECIES = 367H = 0.587405Using these variables we can easily change our script, for example,to use more or fewer categories and test the impact.Specifying the modelPriors on ratesApproximation of the continuous base distributions for the diversification-rate parameters using discrete rate categories.From left to right, we show a discretization of a lognormal distribution with k={2,4,6,8,10,20} bins.Our approach for computing the probability of the data under the lineage-specific birth-death-shiftmodel specifies k quantiles of the continuous base distributions for the speciation and extinction rates.We compute probabilities by marginalizing (averaging) over the k discrete rate categories,where the diversification rate for a given category is the median of the corresponding quantile (colored dots).This approach provides an efficient alternative to computing the continuous integral,and will provide a reliable approximation of the continuous integral when the number of categories k is sufficiently large to resemble the underlying continuous distribution.Instead of using a continuous probability distribution we will use adiscrete approximation of the distribution, as done for modeling ratevariation across sites (Yang 1994) and for modeling relaxed molecularclocks (Drummond et al. 2006). That means, we assume that the speciation ratesare drawn from one of the $N$ quantiles of the lognormal distribution.For this we will use the function fnDiscretizeDistribution which takesin a distribution as its first argument and the number of quantiles asthe second argument. The return value is a vector of quantiles. We useit as a deterministic variable and every time the parameters of the basedistribution (i.e., the lognormaldistribution in our case) change the quantiles will update automaticallyas well. Thus we only need to specify parameters for our basedistribution, the lognormal distribution.We choose a log-uniform distribution as the prior distribution for the mean parameter of the lognormal distribution.speciation_mean ~ dnLoguniform( 1E-6, 1E2)moves.append( mvScale(speciation_mean, lambda=1, tune=true, weight=2.0) )Next, we choose an exponential prior distribution with mean of $H$ for the variation in speciation rates.speciation_sd ~ dnExponential( 1.0 / H )moves.append( mvScale(speciation_sd, lambda=1, tune=true, weight=2.0) )Now, we can compute the speciation rate categories.We will use a lognormal distribution discretized into NUM_RATE_CATEGORIES quantiles and the parameters that we should created.speciation := fnDiscretizeDistribution( dnLognormal(ln(speciation_mean), speciation_sd), NUM_RATE_CATEGORIES )Similarly, we define the prior on the extinction rate in the same way as we did forthe speciation rate.extinction_mean ~ dnLoguniform( 1E-6, 1E2)extinction_mean.setValue( speciation_mean / 2.0 )moves.append( mvScale(extinction_mean, lambda=1, tune=true, weight=2.0) )However, we assume that extinction rate is the same for all categories.Therefore, we simply replicate using the rep function the extinction rate NUM_RATE_CATEGORIES times.extinction := rep( extinction_mean, NUM_RATE_CATEGORIES )Next, we need a rate parameter for the rate-shifts events. We do nothave much prior information about this rate but we can provide somerealistic ranges. For example, we can specify a uniform distribution that thegoes from 0 to 100 expected events.Remember that this is only possible if the tree is known and notestimated simultaneously because only if the tree is known, then we also know thetree length. As usual for rate parameter, we apply a scaling move to theevent_rate variable.event_rate ~ dnUniform(0.0, 100.0/tree_length)moves.append( mvScale(event_rate, lambda=1, tune=true, weight=2.0) )Additionally, we need a parameter for probability that the process starts at the root in any of the diversification-rate categories.We use a uniform/equal prior distribution on the diversification-rate categories.rate_cat_probs &lt;- simplex( rep(1, NUM_RATE_CATEGORIES) )Shifts in the Extinction RateWe might want to allow the extinction rate to change as well.As with the speciation rate, we discretize the lognormal distributioninto a finite number of rate categories.extinction_categories := fnDiscretizeDistribution( dnLognormal(ln(extinction_mean), H), NUM_RATE_CATEGORIES )Now, we must create a vector that contains each combination ofspeciation- and extinction-rates. This allows the rate of speciation tochange without changing the rate of extinction and vice versa. Theresulting vector should be $N^2$ elements long. We call these the`paired’ rate categories.k = 1for(i in 1:NUM_RATE_CATEGORIES) {    for(j in 1:NUM_RATE_CATEGORIES) {        speciation[k]   := speciation_categories[i]        extinction[k++] := extinction_categories[j]    }}Now we also need to specify a root prior for $N^2$ elements.rate_cat_probs &lt;- simplex( rep(1, NUM_RATE_CATEGORIES * NUM_RATE_CATEGORIES) )Note however, that this type of analysis will take significantly longer to run!Incomplete Taxon SamplingWe know that we have sampled 233 out of 367 living primate species. Toaccount for this we can set the sampling parameter as a constant nodewith a value of 233 / 367.rho &lt;- observed_phylogeny.ntips() / NUM_TOTAL_SPECIESRoot ageThe birth-death process requires a parameter for the root age. In thisexercise we use a fix tree and thus we know the age of the tree. Hence,we can get the value for the root from the (Magnuson-Ford and Otto 2012) tree. Thisis done using our global variable root defined above and nothing elsehas to be done here.The time treeNow we have all of the parameters we need to specify the fullbranch-specific birth-death model. We initialize the stochastic noderepresenting the time tree.timetree ~ dnCDBDP( rootAge           = root,                    speciationRates   = speciation,                    extinctionRates   = extinction,                    Q                 = fnJC(NUM_RATE_CATEGORIES),                    delta             = event_rate,                    pi                = rate_cat_probs,                    rho               = rho,                    condition         = \"time\" )And then we attach data to it.timetree.clamp(observed_phylogeny)Finally, we create a workspace object of our whole model using themodel() function.mymodel = model(speciation)The model() function traversed all of the connections and found all ofthe nodes we specified.Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we need to set up a vector of monitors torecord the states of our Markov chain. First, we will initialize themodel monitor using the mnModel function. This creates a new monitorvariable that will output the states for all model parameters whenpassed into a MCMC function.monitors.append( mnModel(filename=\"output/primates_BDS.log\",printgen=1, separator = TAB) )For summary and plotting purposes, we need to obtain the branch-specific diversification rate estimate along the tree.We will use a stochastic rate mapping algorithm Freyman and Höhna (2019).Thus, we create an mnStochasticBranchRate. The stochastic branch-rate monitordraws stochastic character maps and writes the simulated branch rates into a file.We will need this file later to estimate and visualize the posteriordistribution of the rates at the branches.monitors.append( mnStochasticBranchRate(cdbdp=timetree, printgen=1, filename=\"output/primates_BDS_rates.log\") )Finally, create a screen monitor that will report the states ofspecified variables to the screen with mnScreen:monitors.append( mnScreen(printgen=10, event_rate, speciation_mean, extinction_mean) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, wecan now set up the MCMC algorithm that will sample parameter values inproportion to their posterior probability. The mcmc() function willcreate our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=2500,tuning=200)⇨ The Rev file for performing this analysis: mcmc_BDS.RevWhen the analysis is complete, you will have the monitored files in youroutput directory. You can then visualize the branch-specific rates byplotting them using our R package RevGadgets.Just start R in the main directory for this analysis and then type the following commands:library(RevGadgets)my_tree_file = \"data/primates_tree.nex\"my_branch_rates_file = \"output/primates_BDS_rates.log\"tree_plot = plot_branch_rates_tree( tree_file=my_tree_file,                                    branch_rates_file=my_branch_rates_file)ggsave(\"BDS.pdf\", width=15, height=15, units=\"cm\")Estimated branch-specific speciation rates.Here we show the results of our example analysis. You’ll see that there is a speciation rateincrease for the New World Monkeys.Exercise 1  Run an MCMC simulation to estimate the posterior distribution of the speciation rate and extinction rate.  Visualize the branch-specific rates in R using RevGadgets.  Do you see evidence for rate decreases or increases? What is the general trend?  Run the analysis using a different number of rate categories (NUM_RATE_CATEGORIES), e.g., 4 or 10. How do the rates change?",
        "url": "/tutorials/divrate/branch_specific.html",
        "index": "true"
      }
      ,
    
      "workshops-bristol2018-html": {
        "title": "Analysing Macroevolutionary Processes using RevBayes",
        "content": "",
        "url": "/workshops/bristol2018.html",
        "index": ""
      }
      ,
    
      "workshops-bristol2018b-html": {
        "title": "Bayesian phylogenetic analysis of morphological character data using RevBayes",
        "content": "",
        "url": "/workshops/bristol2018b.html",
        "index": ""
      }
      ,
    
      "workshops-buenosaires2018-html": {
        "title": "Buenos Aires 2018 RevBayes Workshop",
        "content": "",
        "url": "/workshops/buenosaires2018.html",
        "index": ""
      }
      ,
    
      "workshops-chamela2023-html": {
        "title": "SSB 2023 Chamela Station Workshop on Phylogenetics and Comparative Method",
        "content": "Bayesian Inference and Divergence-Time Estimation (Tracy Heath)Lecture Slides:  Introduction to Bayesian Inference  Bayesian Divergence-Time EstimationWorkshop Tutorials (choose your own adventure):  Nucleotide substitution models: Phylogenetic inference of nucleotide data using RevBayes  Estimating a Time-Calibrated Phylogeny of Fossil and Extant Taxa using Morphological DataAnalysis of Biogeography (Isabel Sanmartín)All materials: https://github.com/isabelsanmartin/Phylogenetics-Workshop-ChamelaLecture Slides:  Introduction to Biogeography in RevBayesWorkshop Tutorials:  Introduction to RevBayes  Introduction to the Dispersal-Extinction-Cladogenesis (DEC) Model in RevBayes  Introduction to the Bayesian Island Biogeography (BIB) Model in RevBayesAdditional MaterialsHelpful Video Lectures  Paul Lewis’s Primer on Phylogenetics Part 1 - Trees &amp; Likelihood  Paul Lewis’s Primer on Phylogenetics Part 2 - Substitution Models  Paul Lewis’s Primer on Phylogenetics Part 3a - Bayesian Statistics &amp; MCMC  Paul Lewis’s Primer on Phylogenetics Part 3b - Bayesian PhylogeneticsVideo Tutorials  RevBayes Intro to MCMC  RevBayes Substitution Models  RevBayes Partitioned Data Analysis  RevBayes FBD Tutorial  RevBayes Convergence Assessment  RevBayes Intro to Posterior Prediction  History of Phylogenetic Comparative Methods  Introduction to RevGadgets  RevBayes BiSSE Tutorial",
        "url": "/workshops/chamela2023.html",
        "index": ""
      }
      ,
    
      "citation": {
        "title": "Citation",
        "content": "The recommended citation for the current version of the RevBayes software is:Höhna, Landis, Heath, Boussau, Lartillot, Moore, Huelsenbeck, Ronquist. 2016. RevBayes: Bayesian phylogenetic inference using graphical models and an interactive model-specification language. Systematic Biology, 65:726-736.Additionally, the appropriate citation for the probabilistic graphical model framework used in RevBayes is:Höhna, Heath, Boussau, Landis, Ronquist, Huelsenbeck. 2014. Probabilistic graphical model representation in phylogenetics. Systematic Biology 63:753–771.",
        "url": "/citation",
        "index": ""
      }
      ,
    
      "tutorials-sse-classe-html": {
        "title": "State-dependent diversification with the ClaSSE model",
        "content": "IntroductionIn the previous examples we have modeled all character state transitionsas anagenetic changes.Anagenetic changes occur along the branches of a phylogeny, within a lineage.Cladogenetic changes, on the other hand, occur at speciation events.They represent changes in a character state that may be associated with speciation eventsdue to increased reproductive isolation,for example colonizing a new geographic area or a shift in chromosome number.Note that it can be quite tricky to determine if a character state shift is a cause or a consequence of speciation, but we can at least test if state changes tend to occur in the same time window as speciation events.A major challenge for all phylogenetic models of cladogenetic character changeis accounting for unobserved speciation events due to lineages going extinctand not leaving any extant descendants (Bokma 2002),or due to incomplete sampling of lineages in the present.Teasing apartthe phylogenetic signal for cladogenetic and anagenetic processes givenunobserved speciation events is a major difficulty.Commonly used biographic models like thedispersal-extinction-cladogenesis (DEC; Ree and Smith (2008)) simply ignoreunobserved speciation events and so result in biasedestimates of cladogenetic versus anagenetic change.This bias can be avoided by using theCladogenetic State change Speciation and Extinction(ClaSSE) model (Goldberg and Igić 2012),which accounts for unobserved speciation eventsby jointly modeling both character evolutionand the phylogenetic birth-death process.ClaSSE models extend other SSE models by incorporating both cladogeneticand anagenetic character evolution.This approach has been used to model biogeographic range evolution (Goldberg et al. 2011)and chromosome number evolution (missing reference).Here we will use RevBayes to examine biogeographic range evolution in the primates.We will model biogeographic range evolution similar to a DEC model,however we will use ClaSSE to account for speciation events unobserved dueto extinction or incomplete sampling.Setting up the analysisReading in the dataBegin by reading in the observed tree.observed_phylogeny &lt;- readTrees(\"data/primates_biogeo.tre\")[1]Get the taxa in the tree. We’ll need this later on.taxa = observed_phylogeny.taxa()Now let’s read in the biogeographic range data. The areas are represented as the following character states:  0 = 00 = the null state with no range  1 = 01 = New World only  2 = 10 = Old World only  3 = 11 = both New and Old WorldFor consistency, we have chosen to use the same representation of biogeographic ranges used in the \\RevBayes biogeography/DEC tutorial.Each range is represented as both a natural number (0, 1, 2, 3) and a corresponding bitset (00, 01, 10, 11).The null state (state 0) is used in DEC models to represent a lineage that has no biogeographic range and is therefore extinct.Our model will include this null state as well, however, we will explicitly model extinction as part of the birth-deathprocess so our character will never enter state 0.data_biogeo = readCharacterDataDelimited(\"data/primates_biogeo.tsv\", stateLabels=\"0123\", type=\"NaturalNumbers\", delimiter=\"\\t\", headers=TRUE)Also we need to create the move and monitor vectors.moves = VectorMoves()monitors = VectorMonitors()Set up the extinction ratesWe are going to draw both anagenetic transition ratesand diversification rates from a lognormal distribution.The mean of the prior distribution will be$\\ln(\\frac{\\text{#Taxa}}{2}) / \\text{tree-age}$which is the expected netdiversification rate, and the SD will be 1.0 so the 95\\%prior interval ranges well over 2 orders of magnitude.num_species &lt;- 424 # approximate total number of primate speciesrate_mean &lt;- ln( ln(num_species/2.0) / observed_phylogeny.rootAge() )rate_sd &lt;- 1.0The extinction rates will be stored in a vector where each element representsthe extinction rate for the corresponding character state.We have chosen to allow a lineage to go extinct in both the New and Old Worldat the same time (like a global extinction event). As an alternative, you couldrestrict the model so that a lineage can only go extinct if it’s range is limitedto one area.extinction_rates[1] &lt;- 0.0 # the null state (state 0)extinction_rates[2] ~ dnLognormal(rate_mean, rate_sd) # extinction when the lineage is in New World (state 1)extinction_rates[3] ~ dnLognormal(rate_mean, rate_sd) # extinction when the lineage is in Old World (state 2)extinction_rates[4] ~ dnLognormal(rate_mean, rate_sd) # extinction when in both (state 3)Note \\Rev vectors are indexed starting with 1, yet our character states startat 0. So \\texttt{extinction_rate[1]} will represent the extinction rate for characterstate 0.Add MCMC moves for each extinction rate.moves.append( mvSlide( extinction_rates[2], weight=4 ) )moves.append( mvSlide( extinction_rates[3], weight=4 ) )moves.append( mvSlide( extinction_rates[4], weight=4 ) )Let’s also create a deterministic variable to monitor the overall extinction rate.total_extinction := sum(extinction_rates)Set up the anagenetic transition rate matrixFirst, let’s create the rates of anagenetic dispersal:anagenetic_dispersal_13 ~ dnLognormal(rate_mean, rate_sd) # disperse from New to Old World 01 -&gt; 11anagenetic_dispersal_23 ~ dnLognormal(rate_mean, rate_sd) # disperse from Old to New World 10 -&gt; 11Now add MCMC moves for each anagenetic dispersal rate.moves.append( mvSlide( anagenetic_dispersal_13, weight=4 ) )moves.append( mvSlide( anagenetic_dispersal_23, weight=4 ) )The anagenetic transitions will be stored in a 4 by 4instantaneous rate matrix. We will construct this byfirst creating a vector of vectors. Let’s begin byinitalizing all rates to 0.0:for (i in 1:4) {    for (j in 1:4) {        r[i][j] &lt;- 0.0    }}Now we can populate non-zero rates into the anagenetic transition rate matrix:r[2][4] := anagenetic_dispersal_13r[3][4] := anagenetic_dispersal_23r[4][2] := extinction_rates[3]r[4][3] := extinction_rates[2]Note that we have modeled the rate of 11 $\\rightarrow$ 01 (3 $\\rightarrow$ 1) as beingthe rate of going extinct in area 2, and the rate of 11 $\\rightarrow$ 10 (3 $\\rightarrow$ 2)as being the rate of going extinct in area 1.Now we pass our vector of vectors into the \\cl{fnFreeK} function to createthe instaneous rate matrix.ana_rate_matrix := fnFreeK(r, rescaled=false)Set up the cladogenetic speciation rate matrixHere we need to define each cladogenetic event type in the form[ancestor\\_state, daughter1\\_state, daughter2\\_state]and assign each cladogenetic event type a correspondingspeciation rate.The first type of cladogenetic event we’ll specify is widespread sympatry.Widespread sympatric cladogenesis is where the biogeographic range doesnot change; that is the daughter lineages inherit the same range asthe ancestor. In this example we are not going to allow the speciation events like11 $\\rightarrow$ 11, 11, as it seems biologically implausible. However if you wantedyou could add this to your model.Define the speciation rate for widespread sympatric cladogenesis events:speciation_wide_sympatry ~ dnLognormal(rate_mean, rate_sd)moves.append( mvSlide( speciation_wide_sympatry, weight=4 ) )Define the widespread sympatric cladogenetic events:clado_events[1] = [1, 1, 1] # 01 -&gt; 01, 01clado_events[2] = [2, 2, 2] # 10 -&gt; 10, 10and assign each the same speciation rate:speciation_rates[1] := speciation_wide_sympatry/2speciation_rates[2] := speciation_wide_sympatry/2Subset sympatry is where one daughter lineage inherits the fullancestral range but the other lineage inherits only a single region.speciation_sub_sympatry ~ dnLognormal(rate_mean, rate_sd)moves.append( mvSlide( speciation_sub_sympatry, weight=4 ) )Define the subset sympatry events and assign each a speciation rate:clado_events[3] = [3, 3, 1] # 11 -&gt; 11, 01clado_events[4] = [3, 1, 3] # 11 -&gt; 01, 11clado_events[5] = [3, 3, 2] # 11 -&gt; 11, 10clado_events[6] = [3, 2, 3] # 11 -&gt; 10, 11speciation_rates[3] := speciation_sub_sympatry/4speciation_rates[4] := speciation_sub_sympatry/4speciation_rates[5] := speciation_sub_sympatry/4speciation_rates[6] := speciation_sub_sympatry/4Allopatric cladogenesis is when the two daughter lineagessplit the ancestral range:speciation_allopatry ~ dnLognormal(rate_mean, rate_sd)moves.append( mvSlide( speciation_allopatry, weight=4 ) )Define the allopatric events:clado_events[7] = [3, 1, 2] # 11 -&gt; 01, 10clado_events[8] = [3, 2, 1] # 11 -&gt; 10, 01speciation_rates[7] := speciation_allopatry/2speciation_rates[8] := speciation_allopatry/2Now let’s create a deterministic variable to monitor the overall speciation rate:total_speciation := sum(speciation_rates)Finally, we construct the cladogenetic speciation ratematrix from the cladogenetic event types and the speciation rates.clado_matrix := fnCladogeneticSpeciationRateMatrix(clado_events, speciation_rates, 4)Let’s view the cladogenetic matrix to see if we have set it up correctly:clado_matrixSet up the cladogenetic character state-dependent birth-death processFor simplicity we will fix the root frequencies to be equal except for the null statewhich has probability of 0.root_frequencies &lt;- simplex([0, 1, 1, 1])rho is the probability of sampling species at the present:rho &lt;- observed_phylogeny.ntips()/num_speciesNow we construct a stochastic variable drawn from the cladogeneticcharacter state-dependent birth-death process.classe ~ dnCDCladoBDP( rootAge         = observed_phylogeny.rootAge(),                       cladoEventMap   = clado_matrix,                       extinctionRates = extinction_rates,                       Q               = ana_rate_matrix,                       delta           = 1.0,                       pi              = root_frequencies,                       rho             = rho,                       condition       = \"time\",                       taxa            = taxa )Clamp the model with the observed data.classe.clamp( observed_phylogeny )classe.clampCharData( data_biogeo )Finalize the modelJust like before, we must create a workspace model object.mymodel = model(classe)\\subsection{Set up and run the MCMC}First, set up the monitors that will output parameter values to file and screen.monitors.append( mnModel(filename=\"output/primates_ClaSSE.log\", printgen=1) )monitors.append( mnJointConditionalAncestralState(tree=observed_phylogeny, cdbdp=classe, type=\"NaturalNumbers\", printgen=1, withTips=true, withStartStates=true, filename=\"output/anc_states_primates_ClaSSE.log\") )monitors.append( mnScreen(printgen=1, speciation_wide_sympatry, speciation_sub_sympatry, speciation_allopatry, extinction_rates) )Now define our workspace MCMC object.mymcmc = mcmc(mymodel, monitors, moves)We will perform a pre-burnin to tune the proposalsand then run the MCMC. Note that for a real analysis you wouldwant to run the MCMC for many more iterations.mymcmc.burnin(generations=200,tuningInterval=5)mymcmc.run(generations=1000)Summarize ancestral statesWhen the analysis has completed you now summarize the ancestral states.The ancestral states are estimated both for the “beginning” and “end”state of each branch, so that the cladogenetic changes that occurred at speciation eventsare distinguished from the changes that occurred anagenetically along branches.Make sure the include_start_states argument is set to true.anc_states = readAncestralStateTrace(\"output/anc_states_primates_ClaSSE.log\")anc_tree = ancestralStateTree(tree=observed_phylogeny, ancestral_state_trace_vector=anc_states, include_start_states=true, file=\"output/anc_states_primates_ClaSSE_results.tree\", burnin=0, summary_statistic=\"MAP\", site=0)Plotting ancestral statesLike before, we’ll plot the ancestral statesusing the RevGadgets R package.Execute the script plot_anc_states_ClaSSE.R in R.The results can be seen in Figure \\ref{fig:results_ClaSSE}.The maximum a posteriori (MAP) estimate for each node is shown as well as the posterior probability of the states represented by the size of the dots.library(RevGadgets)tree_file = \"output/anc_states_primates_ClaSSE_results.tree\"plot_ancestral_states(tree_file, summary_statistic=\"MAPRange\",                      tip_label_size=3,                      tip_label_offset=1,                      xlim_visible=c(0,100),                      node_label_size=0,                      shoulder_label_size=0,                      include_start_states=TRUE,                      show_posterior_legend=TRUE,                      node_size_range=c(4, 7),                      alpha=0.75)output_file = \"RevBayes_Anc_States_ClaSSE.pdf\"ggsave(output_file, width = 11, height = 9)Maximum a posteriori estimate of biogeographic range evolution of the primates.The most recent common ancestor of the primates is inferred to be in the Old World (green).According to this reconstruction, approximately 70 Mya one lineage dispersed to be in both New and Old World (blue).This widespread lineage underwent allopatric cladogenesis, resulting in onedaughter lineage in the Old World and one in the New World (green).Exercise  Using either R or Tracer, visualize the posterior estimates for different types of cladogenetic events.  What kind of speciation events are most common?  As we have specified the model, we did not allow cladogenetic long  distance (jump) dispersal, for example 01 $\\rightarrow$ 01, 10.  Modify this script to include cladogenetic  long distance dispersal and calculate Bayes factors to see which model fits the data better.  How does this affect the ancestral state estimate?}",
        "url": "/tutorials/sse/classe.html",
        "index": "true"
      }
      ,
    
      "compile-linux": {
        "title": "Compile on Linux",
        "content": "NOTE: These instructions are for compiling the development branch.You can also compile with meson instead of cmake.Pre-requisitesYou will need to have a C++ compiler installed on your computer. GCC 6 (or higher) and Clang 8 (or higher) should work.You will also need to have CMake (3.5.1 or higher) and Boost (1.71 or higher) installedInstalling pre-requisites with root/administrator privelegesInstall these using your distribution’s package managerUbuntusudo apt updatesudo apt install build-essential cmake libboost-all-devCentOS 8sudo yum group install \"Development Tools\"sudo yum install cmake boost-develInstalling pre-requisites on Linux computing clustersIf you are compiling revbayes on a Linux cluster, you might need to select a version of gcc or cmake that is more recent than the default version.Most high-performance compute clusters have additional software available as “modules”.Using the module avail command followed by the name of the library will tell you if there is already a sufficiently recent version installed, thus saving you the effort of installing boost or cmake yourself:module availmodule help gcc boost cmakemodule load gccCMake, GCC and Boost are all commonly used in computational research, there will likely be a sufficiently recent version of gcc and cmake, and perhaps a recent enough version of boost.Installing pre-requisites without root/administrator privelegesIf there is no compiler, you will need your administrator to install build-essential (or equivalent package containing gcc) for you. If possible, ask them to install cmake as well.Installing cmakeThe simplest way to install cmake is to download a CMake executable.  For example:curl -O -L https://github.com/Kitware/CMake/releases/download/v3.22.1/cmake-3.22.1-linux-x86_64.tar.gztar -zxf cmake-3.22.1-linux-x86_64.tar.gzcmake-3.22.1/bin/cmake --versionecho \"cmake installed at: $(cd cmake-3.22.1/bin; pwd)/cmake\"Note the full path to the cmake executable!You may replace the call to cmake on line 161 of build.sh with this path to use your custom cmake installation.In the rare cases where the downloaded cmake executable will not run on your computer, you can also compile from source:curl -O -L https://github.com/Kitware/CMake/releases/download/v3.22.1/cmake-3.22.1.tar.gztar -xzvf cmake-3.22.1.tar.gzcd cmake-3.22.1/./bootstrap -- -DCMAKE_USE_OPENSSL=OFFmakebin/cmake --versionecho \"cmake installed at: $(cd bin; pwd)/cmake\"Note the full path to the cmake executable.Installing boostThen you can compile boost:curl -O -L https://boostorg.jfrog.io/artifactory/main/release/1.74.0/source/boost_1_74_0.tar.gztar -xzvf boost_1_74_0.tar.gzcd boost_1_74_0./bootstrap.sh --with-libraries=atomic,chrono,filesystem,system,regex,thread,date_time,program_options,math,serialization --prefix=../installed-boost-1.74.0./b2 link=static installecho -e \"\\n    BOOST root is at $(cd ../installed-boost-1.74.0; pwd)\\n\"This creates a new directory called installed-boost-1.74.0 that contains the boost installation.This directory is called the BOOST “root”.You will need the path to the BOOST root for the next step.To set up an IDE such as XCode, the following directory should be added to compiler include paths:/path/to/installed-boost-1.74.0/includeThe following directory should be added to linker library paths:/path/to/installed-boost-1.74.0/libCompileDownload RevBayes from our github repository. Clone the repository using git by running the following command in the terminal:git clone --branch development https://github.com/revbayes/revbayes.git revbayesTo compile with the system boost:cd revbayes/projects/cmake./build.shYou will likely see some compiler warnings. This is normal.To compile revbayes using a locally compiled boost, do the following. Be sure to replace the paths in the build command with those you got from boost in the previous step../build.sh -boost_root /path/to/installed-boost-1.74.0For the MPI version:./build.sh -mpi trueThis produces an executable called rb-mpi.Note that compiling the MPI version requires that an MPI library is installed. If you have root, openmpi can be install with apt or yum. If not, if can be downloaded and compiled.TroubleshootingGeneral      rb: command not found    The problem is that you tried to run RevBayes but your computer doesn’t know where the executable is. The easiest way is to add the directory in which you compiled RevBayes to your system path:    export PATH=&lt;your_revbayes_directory&gt;/projects/cmake:$PATH              Error cmake not found!    Please double check that CMake is installed.  Boost      Error can't find the libboost_filesystem.so library or Library not   loaded: libboost_filesystem.so    You need to add the boost libraries to your path variable. You may find that you have to export this LD_LIBRARY_PATH every time you open a new terminal window. To get around this, you can add this to your .bash_profile or .bashrc file (which lives in your home directory). To change this, open a new terminal window and you should be in the home directory. If you do not have either of these files, use the text editor nano to create this file type:    cd ~touch .bash_profilenano .bash_profile        Then add the following lines, replacing /root with wherever you put the boost libraries:    export LD_LIBRARY_PATH=/root/boost_1_74_0/stage/lib:$LD_LIBRARY_PATH        Then save the file using ctrl^o and hit return, then exit using ctrl^x. Now quit the Terminal app and reopen it and the boost libraries will forever be in your path.  MPI      I am using MPI RevBayes and receiving an ORTE transport error    This typically occurs if the version of MPI that is in your environment is not the same as the one you used to compile RevBayes. For example, if you compiled RevBayes  with OpenMPI, but later installed Anaconda Python, which installs a Python MPI. If you receive this error, you might consider removing alternative MPI versions from your system path while running RevBayes. You can check which version of MPI you are using with the command mpirun --version. Note that you may want to remove the build directory before restarting your build.  ",
        "url": "/compile-linux",
        "index": ""
      }
      ,
    
      "compile-osx": {
        "title": "Compile on Mac OS X",
        "content": "NOTE: These instructions are for compiling the development branch.You can also compile with meson instead of cmake.Pre-requisitesYou will need to have C++ compiler installed on your computer. GCC 6 (or higher) and Apple Clang from XCode 11 (or higher) should work. If you don’t have a C++ compiler, you should install Xcode.You will also need to have CMake (3.5.1 or higher) and Boost (1.74 or higher) installedInstalling pre-requisites with root/administrator priveleges (the usual case)The typical way to install boost and cmake is to use the homebrew package manager.If homebrew is not already installed, you can install it with:/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"You can then use homebrew to install cmake and boost:brew install cmake boostInstalling pre-requisites without root/administrator privelegesFirst you will need to install cmakeThen you can compile boost:curl -O -L https://boostorg.jfrog.io/artifactory/main/release/1.74.0/source/boost_1_74_0.tar.gztar -xzvf boost_1_74_0.tar.gzcd boost_1_74_0./bootstrap.sh --with-libraries=atomic,chrono,filesystem,system,regex,thread,date_time,program_options,math,serialization --prefix=../installed-boost-1.74.0./b2 link=static installecho -e \"\\n    BOOST root is at $(cd ../installed-boost-1.74.0; pwd)\\n\"This creates a new directory called installed-boost-1.74.0 that contains the boost installation.This directory is called the BOOST “root”.You will need the path to the BOOST root for the next step.To set up an IDE such as XCode, the following directory should be added to compiler include paths:/path/to/installed-boost-1.74.0/includeThe following directory should be added to linker library paths:/path/to/installed-boost-1.74.0/libCompileDownload RevBayes from our github repository. Clone the repository using git by running the following command in the terminalgit clone --branch development https://github.com/revbayes/revbayes.git revbayesTo compile with the system Boost library:cd revbayes/projects/cmake./build.shYou will likely see some compiler warnings (e.g. clang: warning: optimization flag '-finline-functions' is not supported). This is normal.To compile revbayes using a locally compiled boost, do the following. Be sure to replace the paths in the build command with those you got from boost in the previous step../build.sh -boost_root /path/to/installed-boost-1.74.0For the MPI version:./build.sh -mpi trueThis produces an executable called rb-mpi.Note that compiling the MPI version requires that an MPI library is installed.TroubleshootingGeneral      rb: command not found    The problem is that you tried to run RevBayes but your computer doesn’t know where the executable is. The easiest way is to add the directory in which you compiled RevBayes to your system path:    export PATH=&lt;your_revbayes_directory&gt;/projects/cmake:$PATH              Error cmake not found!    Please double check that CMake is installed. For OS X, go to step 2 above.  Boost      Error can't find the libboost_filesystem.dylib library or Library not   loaded: libboost_filesystem.dylib    You need to add the boost libraries to your path variable. You may find that you have to export this DYLD_LIBRARY_PATH every time you open a new terminal window. To get around this, you can add this to your .bash_profile or .bashrc file (which lives in your home directory). To change this, open a new terminal window and you should be in the home directory. If you do not have either of these files, use the text editor nano to create this file type:    cd ~touch .bash_profilenano .bash_profile        Then add the following lines, replacing /root with wherever you put the boost libraries:    export DYLD_LIBRARY_PATH=/root/boost_1_74_0/stage/lib:$DYLD_LIBRARY_PATH        Then save the file using ctrl^o and hit return, then exit using ctrl^x. Now quit the Terminal app and reopen it and the boost libraries will forever be in your path.  ",
        "url": "/compile-osx",
        "index": ""
      }
      ,
    
      "compile-spack": {
        "title": "Compile with Spack",
        "content": "What is Spack?Spack is a package management tool designed to support multiple versions and configurations of software on a wide variety of platforms and environments. It was designed for large supercomputing centers, where many users and application teams share common installations of software on clusters with exotic architectures, using libraries that do not have a standard ABI.For RevBayes, it handles installing and linking the Boost and MPI dependencies for you. It also handles differences in the build process between versions.Pre-requisitesYou will need to have C++ compiler installed on your computer, along with git, make, curl, and python. On OSX, XCode is required.Set upgit clone https://github.com/spack/spack.gitcd spacksource share/spack/setup-env.shspack compiler findCompile and RunLatest stable versionWithout MPIspack install revbayes ~mpispack load revbayesrbWith MPIspack install revbayes +mpispack load revbayesmpirun rb-mpiDevelopment versionWithout MPIspack install revbayes@develop ~mpispack load revbayesrbWith MPIspack install revbayes@develop +mpispack load revbayesmpirun rb-mpiOlder versionYou can install older versions by replacing @develop in the development version commands with @version. E.g.spack install revbayes@1.0.12 ~mpi.To get a list of available versions run:spack info revbayesTroubleshootingError: revbayes@develop matches multiple packages.The error message should have a list of of packages like so:Matching packages:    abe2xnq revbayes@develop%gcc@9.2.1 arch=linux-fedora31-skylake    l6sv4oz revbayes@develop%gcc@9.2.1 arch=linux-fedora31-skylakeThe text before revbayes in each line is the hash for the package. You can get more information on the difference between the two packages by doing:Package 1:spack find --deps /abe2xnqWhich shows-- linux-fedora31-skylake / gcc@9.2.1 ---------------------------revbayes@develop    boost@1.72.0        bzip2@1.0.8        zlib@1.2.11Package 2:spack find --deps /l6sv4ozWhich shows==&gt; 1 installed package-- linux-fedora31-skylake / gcc@9.2.1 ---------------------------revbayes@develop    boost@1.72.0        bzip2@1.0.8        zlib@1.2.11    openmpi@3.1.5        hwloc@1.11.11            libpciaccess@0.13.5            libxml2@2.9.9                libiconv@1.16                xz@5.2.4            numactl@2.0.12The difference here is that l6sv4oz was compiled with mpi support.We can load it withspack load /l6sv4oz",
        "url": "/compile-spack",
        "index": ""
      }
      ,
    
      "compile-windows": {
        "title": "Compile on Windows",
        "content": "NOTE: These instructions are for compiling the development branch.NOTE: Boost now needs to be at least version 1.71.You can also compile with meson instead of cmake.  Meson allows using linux to generate windows binaries (“cross-compiling”).      Download and install 64-bit cygwin (setup-x86_64.exe). Make sure you include the following packages:    (Cygwin package versions are from May 2022. Newer versions may work, but see special version notes below.  As of July 2024, RevBayes requires cmake 3.5.1 and Boost 1.71.)                            package          version                                      cmake          3.14.5-1                          cmake-debuginfo          3.14.5-1                          cmake-doc          3.14.5-1                          cmake-gui          3.14.5-1                          git          2.21.0-1                          make          4.3-1                          mingw64-x86_64-boost          1.66.0-1                          mingw64-x86_64-gcc-core          9.2.0-2                          mingw64-x86_64-gcc-g++          9.2.0-2                      For RevStudio you will also need:                            package          version                                      mingw64-x86_64-atk1.0          2.26.1-1                          mingw64-x86_64-bzip2          1.0.6-4                          mingw64-x86_64-cairo          1.14.12-1                          mingw64-x86_64-fribidi          0.19.7-1                          mingw64-x86_64-gdk-pixbuf2.0          2.36.11-1                          mingw64-x86_64-glib2.0          2.54.3-1                          mingw64-x86_64-gtk2.0          2.24.31-1                          mingw64-x86_64-gtkmm2.4          2.24.5-1                          mingw64-x86_64-pango1.0          1.40.14-1                      Notes about the versions:    Boost and CMake:    It’s important that the version of Boost that you use be supported by the version of CMake that you use. You can check this by going to the package source for the CMake version you’re using e.g. 3.14.5. Search for _Boost_KNOWN_VERSIONS and ensure your boost version appears in the list.    Occasionally, RevBayes may require a version of cmake or Boost that is not available via Cygwin. Once you have used Cygwin to install gcc, you can install these separately.    cmake has a simple and well-documented installation pathway.    Configuring Boost is less straightforward.          Download Boost and unzip the archive.  In this tutorial, I unzipped v1.82.0 to c:\\boost\\boost_1_82_0.  It may be wise not to use the most current version; at time of writing, the latest 1.85 release caused compatibility issues.      Use Open a cygwin terminal window and cd to the boost directory, here: c:/boost/boost_1_82_0      Type ./bootstrap.bat gcc to run the script with the gcc toolset.      Execute b2 toolset=gcc-13 address-model=64 architecture=x86 --build-dir=build variant=release --build-type=complete --prefix=c:/boost/boost_1_82_0/gcc --with-regex --with-program_options --with-thread --with-system --with-filesystem --with-date_time --with-serialization  install.                  The address-model=64 architecture=x86 toolset=gcc-13 options are assumed to match the configuration of your system – here, a 64-bit x86 machine – throughout this tutorial.  Use gcc -v to check which version of gcc you are using and update gcc-13 to match.                    Check that installation was successful.  If it was, C:\\boost\\boost_1_82_0\\gcc\\lib\\cmake\\Boost-1.82.0 will contain a file BoostConfig.cmake        If you see errors, you may need to use a more recent version of b2 than is bundled with Boost:          Download b2 5.2.0 or above.      Remove (or rename) the local copy of b2.exe, so the system uses the updated version installed globally above.      Run b2 --version to confirm that a version of b2 &gt;= 5.2 is available.              Update &lt;path/to/revbayes&gt;/src/CMakeLists.txt to point to Boost:   a. After the line project(RevBayes), add:  set(BOOST_ROOT \"C:/boost/boost_1_82_0/gcc\")   (to match the value specified to b2 in the prefix argument)   b. After the  line starting find_package(Boost, around line 170,  add include_directories(${Boost_INCLUDE_DIRS})            Retrieve the RevBayes sources.          Open a cygwin terminal window      Clone the git repository:         git clone https://github.com/revbayes/revbayes.git revbayes                      If you have already obtained the source code by another method,  navigate to the folder that contains it.  To get to the folder c:/RevBayes, use cd /cygdrive/c/RevBayes.        Compile RevBayes.          Open a Cygwin terminal window (cmd or PowerShell will not work!) and go to the RevBayes source directoryif you haven’t already done so, e.g.,         cd revbayes                    Next, go into the projects and then the cmake subdirectory:         cd projects/cmake                    Now you can either build the standard version         bash build.sh -DCMAKE_TOOLCHAIN_FILE=../mingw64_toolchain.cmake                or the RevStudio version         bash build.sh -DCMAKE_TOOLCHAIN_FILE=../mingw64_toolchain.cmake -cmd true                                    If you see the error build.sh: line ##: $'\\r': command not found, you need to convert to Unix line endings (\\n rather than Windows \\r\\n). Open the problematic file – here, (build.sh) – in Notepad++, use the Edit menu→EOL conversion→Unix (LF), then save the file.                    If you installed Boost manually, rather than using the Cygwin package, you will need to update src/CMakeLists.txt to tell cmake where to find Boost, as detailed above.                    If you encounter problems finding specific Boost libraries (e.g. regex),a common issue is that the boost libraries have been built using a different toolkit (e.g. a different version of gcc) to the toolkit beingused to build RevBayes.It may help to switch from the Cygwin shell to a regular command prompt,or to use VSCodeto build the project.Add the line set(Boost_DEBUG ON) to the top of your CMakeLists.txt fileto see the toolset used to build Boost (which may differ, confusingly, fromthe filenames of the compiled libraries in C:\\boost\\boost_1_82_0\\gcc\\lib).This should match the version number reported by gcc -v.                    If cmake cannot find generated_included_dirs, these were not generatedsuccessfully by &lt;revbayes&gt;/projects/cmake/regenerate.sh; head tothis folder and run bash regenerate.sh.                  Library whack-a-mole    When you try to run the executable you will likely get an error about missing libraries.    Make a new directory and put the executable in it.    Then find the library from the error message in /usr/x86_64-w64-mingw32/sys-root/mingw/bin/ and copy it to the directory you put the exectuable in. Repeat this until you stop getting error messages.    At the time this was written (RevBayes v1.1.0), this consisted of:          iconv.dll      libatk-1.0-0.dll      libboost_filesystem-mt.dll      libboost_program_options-mt.dll      libboost_system-mt.dll      libbz2-1.dll      libcairo-2.dll      libexpat-1.dll      libffi-6.dll      libfontconfig-1.dll      libfreetype-6.dll      libgcc_s_seh-1.dll      libgdk_pixbuf-2.0-0.dll      libgdk-win32-2.0-0.dll      libgio-2.0-0.dll      libglib-2.0-0.dll      libgmodule-2.0-0.dll      libgobject-2.0-0.dll      libgtk-win32-2.0-0.dll      libharfbuzz-0.dll      libintl-8.dll      libpango-1.0-0.dll      libpangocairo-1.0-0.dll      libpangoft2-1.0-0.dll      libpangowin32-1.0-0.dll      libpcre-1.dll      libpixman-1-0.dll      libpng16-16.dll      libstdc++-6.dll      libwinpthread-1.dll      zlib1.dll      ",
        "url": "/compile-windows",
        "index": ""
      }
      ,
    
      "tutorials-cont-traits-cont-trait-intro-html": {
        "title": "Introduction to Models of Continuous-Character Evolution",
        "content": "Overview: Models of Continuous-Character EvolutionIn these tutorials we will focus on a few types of models that are commonly used to study how continuous characters evolve over a phylogeny. All of the models we describe are Gaussian models, meaning that character evolution follows a normal distribution, as in the Brownian motion and Ornstein-Uhlenbeck models. Models that include stochastic jumps (Landis and Schraiber 2017) are not covered here.Models of continuous-character evolution are used to understand many interesting evolutionary phenomena, including: the rate of character evolution, the mode of character evolution, how these vary over time or among lineages, and understanding the biotic and abiotic factors that contribute to this variation.Brownian Motion modelsThe simplest models of continuous-character evolution assume that the character evolves under a Brownian motion model (missing reference). Under this model, the expected amount of character change along a branch is zero, and the variance in character change is proportional to time and a rate parameter, $\\sigma^2$. These tutorials focus on estimating the rate parameter, $\\sigma^2$, as well as how it varies over time and among lineages.(1) Estimating rates of evolutionWhat is the (global) rate of evolution for my continuous character?The simplest Brownian-motion model assumes that the continuous character evolves at a constant rate over time and among lineages. While this model is limited in terms of the evolutionary questions it is able to address, it provides a useful introduction to modeling continuous-character evolution in RevBayes.You can find examples and more information in the Simple Brownian Rate Estimation tutorial.(2) Detecting variation in rates of evolution through timeDo rates of evolution vary over time?Many hypotheses can naturally be addressed by asking whether rates of evolution vary over time, for example: are rates of evolution increased early in an adaptive radiation (Harmon et al. 2010), do rates of evolution increase over time (Blomberg et al. 2003), or do rates vary in less predictable ways?Work in Progress(3) Detecting variation in rates of evolution among lineagesIs there evidence for variation in the rate of evolution across the branches of my phylogeny?Identifying the number, location, and magnitude of shifts in rates of continuous character evolution can illuminate many evolutionary questions. Relaxing the assumption that rates are constant requires specifying a model that describes how rates vary (a ‘‘relaxed morphological clock’’). Many such models have been proposed (Eastman et al. 2011; Venditti et al. 2011); we provide an example of relaxing the morphological clock using a ‘‘random local clock’’ model, as described in Eastman et al. (2011), in the Relaxed Brownian Rate Estimation tutorial.(4) Detecting state-dependent rates of evolutionAre rates of evolution correlated with a discrete variable on my phylogeny?If rates of evolution are found to vary across branches, a natural question to ask is if some focal variable is associated with the rate variation. For example, we can test whether changes in rates are associated with habitat type in reef and non-reef dwelling fishes (Price et al. 2013; May and Moore 2020).We provide an example of fitting the state-dependent model to discrete- and continuous-character data in the State-Dependent Brownian Rate Estimation tutorial.(5) Detecting correlated evolutionAre characters X and Y evolutionarily correlated?Often we would like to know whether two (or more) continuous characters are correlated, and if they are, how strongly (Felsenstein 1985). Additionally, we may view correlations among continuous characters as nuisance parameters: perhaps we are interested in estimating how rates of evolution vary among lineages or over time from multiple characters, and are concerned that failing to model correlations among characters will mislead us (Adams et al. 2017).In the Multivariate Brownian Motion tutorial, we will provide examples for working with multivariate Brownian motion models to test hypotheses about correlations, and to estimate the strength of correlations among many continuous characters.Ornstein-Uhlenbeck modelsAnother major class of Gaussian models are the Ornstein-Uhlenbeck models, sometimes referred to as the Hansen models (Hansen 1997; Butler and King 2004). These models can describe the evolution of a continuous character under stabilizing selection. These tutorials focus on estimating the optimum parameter, $\\theta$, as well as how it varies among lineages.(1) Estimating evolutionary optimaWhat is the optimal value for the continuous character?The simplest Ornstein-Uhlenbeck model assumes that the continuous character evolves at a constant rate, and is drawn toward an optimal value, $\\theta$, that is assumed to be the same over time and across branches.You can find examples and more information in the Simple Ornstein-Uhlenbeck Models tutorial.(2) Detecting shifts in evolutionary optimaHave optimal phenotypes changed over evolutionary history?Optimal traits may change as species evolve over the adaptive landscape. Relaxing the assumption that the optimal value is fixed across the tree requires specifying a model that describes how theta varies.In the Relaxed Ornstein-Uhlenbeck Models tutorial, we provide an example of relaxing the assumption that optima are homogeneously across branches using a ‘‘random local clock’’ model, which is spiritually similar to the one described in Uyeda and Harmon (2014).",
        "url": "/tutorials/cont_traits/cont_trait_intro.html",
        "index": "true"
      }
      ,
    
      "tutorials-morph-ase-corr-html": {
        "title": "Discrete morphology - Correlation among Characters",
        "content": "OverviewIn this tutorial we want to test for correlation between two discrete characters.In this primates dataset, we have the following discrete morpholigical characters: activity period, habitat, solitariness, terrestrially, number of males per group, mating system and diet (see Introduction to Discrete Morphology Evolution for more details).For example, the habitat and terrestrially characters are very likely to be dependent on another.But what about the other characters and how do we test for correlation?It actually turns out to be very similar to our Discrete morphology - Stochastic Character Mapping and Hidden Rates analysis.The Correlated Evolution ModelIn this exercise, let us assume that we have two binary characters A and B.To test for correlation, we need to combine these two characters into a single one.That means, we will expand the state space to 00, 10, 01 and 11 where the first position corresponds to the state of character A and the second position corresponds to character B.If both characters evolve independently and according to rates of gain $\\alpha_A$ and $\\alpha_B$ and rates of loss $\\beta_A$ and $\\beta_B$, respectively, then we can write the rate matrix as follows (Pagel and Meade 2004).\\[Q = \\begin{smallmatrix} &amp; \\begin{smallmatrix}00 &amp; 10 &amp; 01 &amp; 11\\end{smallmatrix} \\\\\\begin{smallmatrix}00\\\\10\\\\01\\\\11\\end{smallmatrix} &amp;   \\left(\\begin{smallmatrix}- &amp; \\alpha_{A} &amp; \\alpha_{B} &amp; 0 \\\\ \\beta_{A} &amp; - &amp; 0 &amp; \\alpha_{B}\\\\ \\beta_{B} &amp; 0 &amp; - &amp; \\alpha_{A} \\\\ 0 &amp; \\beta_{A} &amp; \\beta_{B} &amp; -  \\end{smallmatrix}\\right)\\\\\\end{smallmatrix}\\]Now if we assume instead that rates of gain and loss for both characters actually depend on the state of the other character, then we would obtain the full rate matrix of\\[Q = \\begin{smallmatrix} &amp; \\begin{smallmatrix}00 &amp; 10 &amp; 01 &amp; 11\\end{smallmatrix} \\\\\\begin{smallmatrix}00\\\\10\\\\01\\\\11\\end{smallmatrix} &amp;   \\left(\\begin{smallmatrix}- &amp; \\mu_{1,2} &amp; \\mu_{1,3} &amp; 0 \\\\ \\mu_{2,1} &amp; - &amp; 0 &amp; \\mu_{2,4}\\\\ \\mu_{3,1} &amp; 0 &amp; - &amp; \\mu_{3,4} \\\\ 0 &amp; \\mu_{4,2} &amp; \\mu_{4,3} &amp; -  \\end{smallmatrix}\\right)\\\\\\end{smallmatrix}\\]Note that only single transitions, i.e., a transition of one character and not both, are allowed at a time.That is, we assume that not both characters can be gained or lost at the same infinitesimal small time interval, and instead that one usually precedes the other.In the first rate matrix, we only had 4 parameters.Now in the second rate matrix, we have 8 parameters.If we want to test for correlations, we can simply test if specific parameters are indeed distinguishable (Pagel and Meade 2004).These parameters are:  $\\mu_{1,2} \\neq \\mu_{3,4}$: a gain in character A does not depend on the state of character B  $\\mu_{2,1} \\neq \\mu_{4,3}$: a loss in character A does not depend on the state of character B  $\\mu_{1,3} \\neq \\mu_{2,4}$: a gain in character B does not depend on the state of character A  $\\mu_{3,1} \\neq \\mu_{4,2}$: a loss in character B does not depend on the state of character AWe will perform these tests here using reversible jump Markov chain Monte Carlo.  Let us start with a fresh Rev script.Create an empty text file and call it `mcmc_corr_RJ.Rev.Load the Data MatricesAs before, use the function readDiscreteCharacterData() to load a data matrix to the workspace from a formatted file.However, this time we need to load two data matrices, one for each character.morpho_A &lt;- readDiscreteCharacterData(\"data/primates_solitariness.nex\")morpho_B &lt;- readDiscreteCharacterData(\"data/primates_terrestrially.nex\")Next, we need to combine the 2 character into a single one.In RevBayes, there is a convenient way so you don’t have to do it manually.You can use the function combineCharacter() which will combine any two characters.morpho = combineCharacter( morpho_A, morpho_B )Again, to get a better understanding of how this happened just look into the newly created character data matrix (and the original ones if you want too).morpho   NaturalNumbers character matrix with 233 taxa and 1 characters   ==============================================================   Origination:                      Number of taxa:                233   Number of included taxa:       233   Number of characters:          1   Number of included characters: 1   Datatype:                      NaturalNumbersNotice that the Datatype for this character data matrix is NaturalNumbers.In RevBayes, we always default back to NaturalNumbers as the data type because we can list arbitrarily many states, but for standard states we wouldn’t know how to label them (except going through the alphabet, which is clearly shorter and doesn’t have any more meaning than numbers).Let us now also look at how the states are combined.Again, we look first at the original character data matrix.morpho.show()   Allenopithecus_nigroviridis   1   Allocebus_trichotis   2   Alouatta_belzebul   0   Alouatta_caraya   0   Alouatta_coibensis   0   Alouatta_fusca   0   Alouatta_palliata   0   Alouatta_pigra   0   Alouatta_sara   (0 2)   Alouatta_seniculus   0   Aotus_azarai   0   Aotus_brumbacki   (0 2)   Aotus_hershkovitzi   (0 2)   ...We see that Allenopithecus nigroviridis has state 1 and Allocebus trichotis has state 2.There are also ambiguous states if one or both character were ambiguous or missing.It is important to remember how the state space was expanded to set the rates up correctly.Create Helper VariablesAs before, we need to instantiate a couple “helper variables” that will be used by downstream parts of our model specification files.Create vectors of moves and monitorsmoves = VectorMoves()monitors = VectorMonitors()The PhylogenyAs usual for morphological analysis, we assume the phylogeny to be know.Thus, we read in the tree as a constant variable:phylogeny &lt;- readTrees(\"data/primates_tree.nex\")[1]The Correlated Rates ModelNow we need to specify the correlated rates model.Have a look again above at the rate matrix that we want to specify.In the current example, we assume two binary morphological character.This gives 4 states in total and therefore a 4x4 rate matrix.Start with creating a matrix called rates where all elements are 0.0.# we will fill the non-zero elements belowfor (i in 1:4) {  for (j in 1:4) {    rates[i][j] &lt;- 0.0  }}Next, we specify the global rate prior.As in all previous discrete morphological evolution excersises, we will use an exponential prior distribution with a mean of 10 events along the given phylogeny.rate_pr := phylogeny.treeLength() / 10We also specify a prior probability of 0.5 that each pair of rates is indeed correlated.mix_prob &lt;- 0.5Then, let us specify the rate of gain for character A.First, we do this for the case when character B is in state 0.We will assume an exponential prior distribution here.rate_gain_A_when_B0 ~ dnExponential( rate_pr )Second, we create the rate for a gain in character A when character B is in state 1.This can either be the same rate (i.e., independent of the state of B) or its own rate.Therefore we use the reversible jump mixture distribution.rate_gain_A_when_B1 ~ dnReversibleJumpMixture(rate_gain_A_when_B0, dnExponential( rate_pr ), mix_prob)Now proceed in the same way for the rate of loss in character A, the rate of gain in character B and the rate of loss in character B.rate_loss_A_when_B0 ~ dnExponential( rate_pr )rate_loss_A_when_B1 ~ dnReversibleJumpMixture(rate_loss_A_when_B0, dnExponential( rate_pr ), mix_prob)rate_gain_B_when_A0 ~ dnExponential( rate_pr )rate_gain_B_when_A1 ~ dnReversibleJumpMixture(rate_gain_B_when_A0, dnExponential( rate_pr ), mix_prob)rate_loss_B_when_A0 ~ dnExponential( rate_pr )rate_loss_B_when_A1 ~ dnReversibleJumpMixture(rate_loss_B_when_A0, dnExponential( rate_pr ), mix_prob)Next, since we use reversible jump MCMC, we also want to compute the probability that any of the pairs of rates is indeed equal.We use the ifelse function to test for equivalence and store the value 1 if the rates are identical (i.e., the charatacer are independent with respect to that rate) or a 0 otherwise.prob_gain_A_indep := ifelse( rate_gain_A_when_B0 == rate_gain_A_when_B1, 1.0, 0.0 )prob_loss_A_indep := ifelse( rate_loss_A_when_B0 == rate_loss_A_when_B1, 1.0, 0.0 )prob_gain_B_indep := ifelse( rate_gain_B_when_A0 == rate_gain_B_when_A1, 1.0, 0.0 )prob_loss_B_indep := ifelse( rate_loss_B_when_A0 == rate_loss_B_when_A1, 1.0, 0.0 )We need to specify moves on all our 8 rate variables.First, we specify a scaling move on each rate variable separately.moves.append( mvScale( rate_gain_A_when_B0, weight=2 ) )moves.append( mvScale( rate_gain_A_when_B1, weight=2 ) )moves.append( mvScale( rate_loss_A_when_B0, weight=2 ) )moves.append( mvScale( rate_loss_A_when_B1, weight=2 ) )moves.append( mvScale( rate_gain_B_when_A0, weight=2 ) )moves.append( mvScale( rate_gain_B_when_A1, weight=2 ) )moves.append( mvScale( rate_loss_B_when_A0, weight=2 ) )moves.append( mvScale( rate_loss_B_when_A1, weight=2 ) )On the 4 reversible jump variable, we also need to specify reversible jump moves to actually perform the reversible jump MCMC algorithm.moves.append( mvRJSwitch(rate_gain_A_when_B1, weight=2.0) )moves.append( mvRJSwitch(rate_loss_A_when_B1, weight=2.0) )moves.append( mvRJSwitch(rate_gain_B_when_A1, weight=2.0) )moves.append( mvRJSwitch(rate_loss_B_when_A1, weight=2.0) )Now that we have our rate parameters, we can fill in our rates matrix.If you are unsure about the indices, look again at the described rate matrix above.rates[1][2] := rate_gain_A_when_B0 # 00-&gt;10rates[1][3] := rate_gain_B_when_A0 # 00-&gt;01rates[2][1] := rate_loss_A_when_B0 # 10-&gt;00rates[2][4] := rate_gain_B_when_A1 # 10-&gt;11rates[3][1] := rate_loss_B_when_A0 # 01-&gt;00rates[3][4] := rate_gain_A_when_B1 # 01-&gt;11rates[4][2] := rate_loss_B_when_A1 # 11-&gt;10rates[4][3] := rate_loss_A_when_B1 # 11-&gt;01Finally, we can create our transition rate matrix Q using the rate matrix function fnFreeK.Q_morpho := fnFreeK(rates, rescaled=FALSE)For this model, we also want to specify parameters for the root frequencies $\\pi$, and thus also their prior distributions.We assume a flat Dirichlet distribution, which assigns each combination of root frequencies the exact same prior probability.Remember that we 2 binary characters and therefore 4 states.Thus we need a vector of 2*2 filled with ones.rf_prior &lt;- rep(1,2*2)We use this for our Dirichlet distribution.rf ~ dnDirichlet( rf_prior )We apply two different moves to the root frequencies, a mvBetaSimplex that changes a single frequencies and rescales the other frequencies, and a mvDirichletSimplex that redraws all root frequencies together.moves.append( mvBetaSimplex( rf, weight=2 ) )moves.append( mvDirichletSimplex( rf, weight=2 ) )Lastly, we set up the CTMC.Not that this time we need to specify the type=NaturalNumbers, as we saw this is used in the combined data matrix.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, Q=Q_morpho, rootFrequencies=rf, type=\"NaturalNumbers\")We conclude the model specification by attaching the combined data matrix to the CTMC object.phyMorpho.clamp( morpho )Complete MCMC AnalysisCreate Model ObjectWe can now create our workspace model variable with our fully specified model DAG.We will do this with the model() function and provide a single node in the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph and allows us to pass the model to various functions that are specific to our MCMC analysis.Specify Monitors and Output FilenamesAs in all our analyses, we will add the same monitors to plot the ancestral states and also the stochastic character maps.We will specify the same model monitor (mnModel), screen monitor (mnScreen), ancestral state monitor (mnJointConditionalAncestralState) as before (Discrete morphology - Ancestral State Estimation) and the stochastic character mapping monitor (mnStochasticCharacterMap) (Discrete morphology - Stochastic Character Mapping and Hidden Rates).# 1. for the full model #monitors.append( mnModel(filename=\"output/solitariness_terrestrially_corr_RJ.log\", printgen=1) )# 2. and a few select parameters to be printed to the screen #monitors.append( mnScreen(printgen=10) )# 3. add an ancestral state monitormonitors.append( mnJointConditionalAncestralState(tree=phylogeny,                                                  ctmc=phyMorpho,                                                  filename=\"output/solitariness_terrestrially_corr_RJ.states.txt\",                                                  type=\"NaturalNumbers\",                                                  printgen=1,                                                  withTips=true,                                                  withStartStates=false) )# 4. add an stochastic character map monitormonitors.append( mnStochasticCharacterMap(ctmc=phyMorpho,                                          filename=\"output/solitariness_terrestrially_corr_RJ_stoch_char_map.log\",                                          printgen=1,                                          include_simmap=true) )Set-Up the MCMCSetup the MCMC analysis as before (Discrete morphology - Ancestral State Estimation).This will run 2 replicated MCMC runs with 10,000 iterations and auto-tuning the moves every 200 iterations.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")mymcmc.run(generations=10000, tuningInterval=200)Summarizing the MCMC outputAfter the MCMC simulation, we can calculate the maximum a posteriorimarginal, joint, or conditional character history.As before (Discrete morphology - Ancestral State Estimation), we will compute the ancestral state estimates as well as the stochastic character mappings (Discrete morphology - Stochastic Character Mapping and Hidden Rates) which generates the SIMMAP (Bollback 2006)formatted files used for plotting in the phytools R package (Revell 2012).# Read in the tree trace and construct the ancestral states (ASE) #anc_states = readAncestralStateTrace(\"output/solitariness_terrestrially_corr_RJ.states.txt\")anc_tree = ancestralStateTree(tree=phylogeny, ancestral_state_trace_vector=anc_states, include_start_states=false, file=\"output/solitariness_terrestrially_ase_corr_RJ.tree\", burnin=0.25, summary_statistic=\"MAP\", site=1, nStates=2*2)anc_states_stoch_map = readAncestralStateTrace(\"output/solitariness_terrestrially_corr_RJ_stoch_char_map.log\")char_map_tree = characterMapTree(tree=phylogeny,                 ancestral_state_trace_vector=anc_states_stoch_map,                 character_file=\"output/solitariness_terrestrially_corr_RJ_marginal_character.tree\",                 posterior_file=\"output/solitariness_terrestrially_corr_RJ_marginal_posterior.tree\",                 burnin=0.25,                 num_time_slices=500)This is all you need for this analysis.Don’t forget to quit RevBayes at the end of the script.# Quit RevBayes #q()  This is all that you need to do for the rate variation analysis with hidden rate categories and stochastic character mapping. Save your script and give it a try!Evaluate and Summarize Your ResultsVisualizing Ancestral State EstimatesWe have previously plotted the ancestral states, both the maximum a posterior (MAP) states as well as the posterior probabilities of all states shown as pie chart Discrete morphology - Ancestral State Estimation.We also showed before how to plot the stochastic character mapping using phytools.My output is shown in .Stochastic character map (left) and ancestral state estimates (middle and right) of the combined character solitariness and terrestrially under the correlated model of morphological evolution. You might notice that there no joint state solitary and terrestrial, which means that if primates are solitary, then they live in trees.The more important output analysis here is the probability of the rates being dependent or not.Therefore, we will plot the probability that two rates were identical.You can do this nicely in RevGadgets (Tribble et al. 2022)library(RevGadgets)library(ggplot2)CHARACTER_A &lt;- \"solitariness\"CHARACTER_B &lt;- \"terrestrially\"# specify the input filefile &lt;- paste0(\"output/\",CHARACTER_A,\"_\",CHARACTER_B,\"_corr_RJ.log\")# read the trace and discard burnintrace_qual &lt;- readTrace(path = file, burnin = 0.25)BF &lt;- c(3.2, 10, 100)p = BF/(1+BF)# produce the plot object, showing the posterior distributions of the rates.p &lt;- plotTrace(trace = trace_qual,          vars = c(\"prob_gain_A_indep\", \"prob_gain_B_indep\", \"prob_loss_A_indep\", \"prob_loss_B_indep\"))[[1]] +          ylim(0,1) +          geom_hline(yintercept=0.5, linetype=\"solid\", color = \"black\") +          geom_hline(yintercept=p, linetype=c(\"longdash\",\"dashed\",\"dotted\"), color = \"red\") +          geom_hline(yintercept=1-p, linetype=c(\"longdash\",\"dashed\",\"dotted\"), color = \"red\") +     # modify legend location using ggplot2     theme(legend.position = c(0.40,0.825))ggsave(paste0(\"Primates_\",CHARACTER_A,\"_\",CHARACTER_B,\"_corr_RJ.pdf\"), p, width = 5, height = 5)Probability that a rate of gain or loss of a character was independent. If we see a high posterior probability for 1, then that means that the rate of gain or loss for the character is independent of the other character. We also show the prior (black solid line), weak support (BF $&lt;3.2$, long-dashed red line), substantial support ($3.2&lt;$ BF $&lt;10$, dashed red line), and strong support ($10&lt;$ BF $&lt;100$, dotted red line). Even though the support varies, for this specific analysis we don’t see any significant support of either correlated or independent rates.  How can you explain the observed allocation of clades to the slow and fast rate categories?Do ancestral state estimates match with the stochastic character maps?How certain are we in the ancestral state estimates?",
        "url": "/tutorials/morph_ase/corr.html",
        "index": "true"
      }
      ,
    
      "developer-docker-dev-in-container-html": {
        "title": "Developing in the Docker container",
        "content": "Developing in the Docker containerOk now we are ready to get the container running. Use:docker run --entrypoint /bin/bash revbayesdockerfiles:ubuntu-latestThe container is now running. We just need to start it. To figure out the name of the container (since we didn’t give it one), type docker container ls. It should have a name of two random words, mine is peaceful-hertz.You want to start this with     docker start Now we can go to vscode type: CTRL(CMD)+SHIFT+P. This opens the command prompt. Type in Remote it will populate a drop-down list go to the one that says: Remote-Container: Attach to running container. This will open a new window and in the bottom left corner it should say ‘Container: revbayesdockerfiles-latest’.The revbayes repository is in here. If we want to open it you can go to Open folder and navigate to that. It will be at /revbayes.If you want to debug you will need to install a debugger. For example, to install gdb, you can simply use the apt install manager (or yum/dnf on fedora/CentOS). You could also do this to install valgrind which does not work nicely on MacOS. Otherwise you can use them just as you would in terminal on your home machine. I am currently working on a full development environment that takes advantage of a more IDE-like environment for debugging so stay-tuned.",
        "url": "/developer/docker/dev_in_container.html",
        "index": ""
      }
      ,
    
      "developer": {
        "title": "Developer",
        "content": "Joining the RevBayes TeamRevBayes is a collaborative software project involving developers from Europe and North America. Since RevBayes is open source, everyone is free to clone the GitHub repository. If you would like to implement new methods or models in any of the RevBayes source code, you can contribute your work to the project by issuing a pull request on GitHub. Alternatively, if you are interested in joining the RevBayes development team, please contact one of the public members to request access.Also consider attending a RevBayes hackathon where the developers get together to work on the software.Before digging into the developer guide it may be useful to understand the user side of RevBayes. We have provided tutorials that walk through the basics of using directed acyclic graphs (DAGs) for conducting phylogenetic analyses in RevBayes.Developer’s GuideThe RevBayes Developer’s Guide will provide you with the information needed to implement new methods, models, functions, and algorithms in the RevBayes language and core libraries.RevBayes Git WorkflowHow to contribute code effectivelyAdding a RevBayes help fileHow to write and upload a help file.Setting up an IDE for RevBayesSetting up XCode, Eclipse, vimGetting familiar with the codeThe architecture of the RevBayes source codeImplementing functions, distributions, and movesGetting started with RevBayes developmentBest practices in RevBayesCoding and documentation guidelinesAutomated building and testing in RevBayesQuick automated feedback on code changesValidation Scripts in RevBayesEnsuring correct performance of models and methods using simulationsUsing Docker with RevBayesContainerized debugging and executeablesContributing a RevBayes tutorialHow to write and host a tutorial on this websiteIntegration Tests in RevBayesEnsuring that code changes did not unexpectedly alter the software",
        "url": "/developer/",
        "index": ""
      }
      ,
    
      "tutorials-divrate-div-rate-intro-html": {
        "title": "Introduction to Diversification Rate Estimation",
        "content": "Overview: Diversification Rate EstimationStochastic branching models allow for inference of speciation andextinction rates. In these tutorials we focus on the different types of macroevolutionarymodels to study diversification processes and thus thediversification-rate parameters themselves.Types of Hypotheses for Estimating Diversification RatesMacroevolutionary diversification rate estimation focuses on different key hypothesis,which may include:adaptive radiation, diversity-dependent and character diversification, key innovations, andmass extinction.We classify these hypotheses primarily into questions whether diversification rates vary through time,and if so, whether some external, global factor has driven diversification rate changes, or ifdiversification rates vary among lineages, and if so, whether some species specific factor is correlated withthe diversification rates.Below, we describe each of the fundamental questions regarding diversification rates.(1) Constant diversification-rate estimationWhat is the global rate of diversification in my phylogeny?The most basic models estimate parameters of the birth-death process(i.e., rates of speciation and extinction, or composite parameterssuch as net-diversification and relative-extinction rates)under the assumption that rates have remained constant across lineages and through time.This is the most basic example and should be treated as a primer and introduction into the topic.For more information, we recommend the Simple Diversification Rate Estimation.(2) Diversification rate variation through timeIs there diversification rate variation through time in my phylogeny?There are several reasons why diversification rates for the entire study group can vary through time, for example:adaptive radiation, diversity dependence and mass-extinction events.We can detect a signal any of these causes by detecting diversification rate variation through time.The different tutorials references below cover different scenarios for diversification rate variation through time.The common theme of these studies is that the diversification process is tree-wide, that is,all lineages of the study group have the exact same rates at a given time.(2a) Detecting diversification rate variation through timeIn RevBayes we use an episodic birth-death model to study diversification rate variation through time.That is, we assume that diversification rates are constant within an epoch but may shift between episodes (Stadler (2011), Höhna (2015)).Then, we are estimating the diversification rates for each episode, and thus diversification rate variation through time.You can find examples and more information in the Episodic Diversification Rate Estimation.(2b) Detecting the impact of mass-extinction events on diversificationAnother question in this category asks whether our study tree was impacted by a mass-extinction event(where a large fraction of the standing species diversity is suddenly lost, e.g., Höhna (2015), May et al. (2016), Magee and Höhna (2021)).That is, we infer and test for the impact of instantaneous mass extinction events where each species alive at the given time has a probability of survival of the event.You can find examples and more information in the Mass Extinction Estimation.(2c) Diversification-rate correlation to environmental (e.g., abiotic) factorsAre diversification rates correlated with some abiotic (e.g., environmental) variable in my phylogeny?If we have found evidence in the previous section that diversification rates vary through time,then we can start asking the question whether these changes in diversificationrates are driven by some abiotic (e.g., environmental) factors.For example, we can ask whether changes in diversification rates are correlated withenvironmental factors, such as environmental CO2 or temperature (Condamine et al. 2013; Condamine et al. 2018; Palazzesi et al. 2022).You can find examples and more information in the Environmental-dependent Speciation &amp; Extinction Rates.(3) Diversification-rate variation across branches estimationIs there diversification rate variation among lineages in my phylogeny?There are several reasons why diversification rates can vary among lineages primarily due to species specific factors (intrinsic and extrinsic), for example, key innovations.First, we can try to detect a signal of rate variation among lineages, and then we can test if their are variables that are associated with this among lineage rate variation.The different tutorials references below cover different scenarios for diversification rate variation among lineages.(3a) Detecting diversification-rate variation across branches estimationIs there evidence that diversification rates have varied across the branches of my phylogeny?Have there been significant diversification-rate shifts along branches in my phylogeny, and if so, how many shifts, what magnitude of rate-shifts and along which branches?Similarly, one may ask what are the branch-specific diversification rates?You can study diversification rate variation among lineages using our birth-death-shift process (Höhna et al. 2019).Examples and more information is provided in the Branch-Specific Diversification Rate Estimation.(3b) Character-dependent diversification-rate estimationIf we have found that there is rate variation among lineage, then we could ask if diversification rates correlated with some biotic (e.g., morphological) variable.This can be addressed by using character-dependent birth-death models(often also called state-dependent speciation and extinction models; SSE models).Character-dependent diversification-rate models aim to identifyoverall correlations between diversification rates and organismalfeatures (binary and multi-state discrete morphological traits,continuous morphological traits, geographic range, etc.). For example,one can hypothesize that a binary character, say if an organism isherbivorous/carnivorous or self-compatible/self-incompatible, impact thediversification rates. Then, if the organism is in state 0 (e.g., is herbivorous) it has a lower (or higher) diversification rate than if the organism is in state 1 (e.g., carnivorous) (Maddison et al. 2007).You can find examples and more information in  Background on state-dependent diversification rate estimation  State-dependent diversification with BiSSE and MuSSE tutorial  State-dependent diversification with HiSSE tutorial  State-dependent diversification with the ClaSSE model tutorial  Chromosome Evolution tutorial(4) General Extension to Diversification Rate EstimationThere exist some general considerations, assumptions and extensions that apply to most/all diversification rate models.We provide a few general topics.(4a) Incomplete taxon samplingFor most study groups, we do not have all extant taxa sampled.It is important that we properly model incomplete taxon sampling because otherwise our parameter estimates are biased (Höhna et al. 2011; Höhna 2014; Palazzesi et al. 2022).You can find examples and more information in the Diversification Rate Estimation with Missing Taxa.(4b) Conditions of the Birth-Death ProcessAs any statistical model, the birth-death process includes several assumptions/conditions.Primarily, we condition the process if we only consider study groups that (a) survived until the present, (b) left exactly $N$ extant taxa, or (c) no restrictions.The conditions become a bit more involved if phylogenies with fossils are considered.You can find more discussion and examples in the Conditions of the Birth-Death Process section below.Diversification Rate ModelsWe begin this section with a general introduction to the stochasticbirth-death branching process that underlies inference ofdiversification rates in RevBayes. This primer willprovide some details on the relevant theory of stochastic-branchingprocess models. We appreciate that some readers may want to skip thissomewhat technical primer; however, we believe that a betterunderstanding of the relevant theory provides a foundation forperforming better inferences. We then discuss a variety of specificbirth-death models, but emphasize that these examples represent only atiny fraction of the possible diversification-rate models that can bespecified in RevBayes.The birth-death branching processA realization of the birth-death process with mass extinction.Lineages that have no extant or sampled descendant are shown in gray andsurviving lineages are shown in a thicker black line.Our approach is based on the reconstructed evolutionary processdescribed by (Nee et al. 1994); a birth-death process in which only sampled,extant lineages are observed. Let $N(t)$ denote the number of species attime $t$. Assume the process starts at time $t_1$ (the ‘crown’ age ofthe most recent common ancestor of the study group, $t_\\text{MRCA}$)when there are two species. Thus, the process is initiated with twospecies, $N(t_1) = 2$. We condition the process on sampling at least onedescendant from each of these initial two lineages; otherwise $t_1$would not correspond to the $t_\\text{MRCA}$ of our study group. Eachlineage evolves independently of all other lineages, giving rise toexactly one new lineage with rate $b(t)$ and losing one existing lineagewith rate $d(t)$ ( and). Note that although each lineage evolvesindependently, all lineages share both a common (tree-wide) speciationrate $b(t)$ and a common extinction rate $d(t)$(Nee et al. 1994; Höhna 2015). Additionally, at certain times,$t_{\\mathbb{M}}$, a mass-extinction event occurs and each speciesexisting at that time has the same probability, $\\rho$, of survival.Finally, all extinct lineages are pruned and only the reconstructed treeremains ().Examples of trees produced under a birth-death process.The process is initiated at the first speciation event (the ‘crown-age’ of the MRCA)when there are two initial lineages. At each speciation event the ancestral lineage is replaced by twodescendant lineages. At an extinction event one lineage simplyterminates. (A) A complete tree including extinct lineages. (B) Thereconstructed tree of tree from A with extinct lineages pruned away. (C)A uniform subsample of the tree from B, where each species was sampledwith equal probability, $\\rho$. (D) A diversified subsample of thetree from B, where the species were selected so as to maximize diversity.To condition the probability of observing the branching times on thesurvival of both lineages that descend from the root, we divide by$P(N(T) &gt; 0 | N(0) = 1)^2$. Then, the probability density of thebranching times, $\\mathbb{T}$, becomes\\[\\begin{aligned}P(\\mathbb{T}) = \\frac{\\overbrace{P(N(T) = 1 \\mid N(0) = 1)^2}^{\\text{both initial lineages have one descendant}}}{ \\underbrace{P(N(T) &gt; 0 \\mid N(0) = 1)^2}_{\\text{both initial lineages survive}} } \\times \\prod_{i=2}^{n-1} \\overbrace{i \\times b(t_i)}^{\\text{speciation rate}} \\times \\overbrace{P(N(T) = 1 \\mid N(t_i) = 1)}^\\text{lineage has one descendant},\\end{aligned}\\]and the probability density of the reconstructed tree (topology and branching times) is then\\[\\begin{aligned}P(\\Psi) = \\; &amp; \\frac{2^{n-1}}{n!(n-1)!} \\times \\left( \\frac{P(N(T) = 1 \\mid N(0) = 1)}{P(N(T) &gt; 0 \\mid N(0) = 1)} \\right)^2 \\nonumber\\\\\t\t  \\; &amp; \\times \\prod_{i=2}^{n-1} i \\times b(t_i) \\times P(N(T) = 1 \\mid N(t_i) = 1)\t\t  \\label{eq:tree_probability}\\end{aligned}\\]We can expand Equation ([eq:tree_probability]) by substituting$P(N(T) &gt; 0 \\mid N(t) =1)^2 \\exp(r(t,T))$ for$P(N(T) = 1 \\mid N(t) = 1)$, where $r(u,v) = \\int^v_u d(t)-b(t)dt$; theabove equation becomes\\[\\begin{aligned}P(\\Psi) = \\; &amp; \\frac{2^{n-1}}{n!(n-1)!} \\times \\left( \\frac{P(N(T) &gt; 0 \\mid N(0) =1 )^2 \\exp(r(0,T))}{P(N(T) &gt; 0 \\mid N(0) = 1)} \\right)^2 \\nonumber\\\\\t\t  \\; &amp; \\times \\prod_{i=2}^{n-1} i \\times b(t_i) \\times P(N(T) &gt; 0 \\mid N(t_i) = 1)^2 \\exp(r(t_i,T)) \\nonumber\\\\\t\t= \\; &amp; \\frac{2^{n-1}}{n!} \\times \\Big(P(N(T) &gt; 0 \\mid N(0) =1 ) \\exp(r(0,T))\\Big)^2 \\nonumber\\\\\t\t  \\; &amp; \\times \\prod_{i=2}^{n-1} b(t_i) \\times P(N(T) &gt; 0 \\mid N(t_i) = 1)^2 \\exp(r(t_i,T)).\t\t\\label{eq:tree_probability_substitution}\\end{aligned}\\]For a detailed description of this substitution, see Höhna (2015). Additionalinformation regarding the underlying birth-death process can be found inThompson (1975) [Equation 3.4.6] and Nee et al. (1994) for constant rates andHöhna (2013), Höhna (2014), Höhna (2015) for arbitrary rate functions.To compute the equation above we need to know the rate function,$r(t,s) = \\int_t^s d(x)-b(x) dx$, and the probability of survival,$P(N(T)!&gt;!0|N(t)!=!1)$.Yule (1925) and later Kendall (1948) derived theprobability that a process survives ($N(T) &gt; 0$) and the probability ofobtaining exactly $n$ species at time $T$ ($N(T) = n$) when the processstarted at time $t$ with one species. Kendall’s results were summarizedin Equation (3) and Equation (24) in Nee et al. (1994)\\[\\begin{aligned}P(N(T)\\!&gt;\\!0|N(t)\\!=\\!1) &amp; = &amp; \\left(1+\\int\\limits_t^{T} \\bigg(\\mu(s) \\exp(r(t,s))\\bigg) ds\\right)^{-1} \\label{eq:survival} \\\\ \\nonumber \\\\P(N(T)\\!=\\!n|N(t)\\!=\\!1) &amp; = &amp; (1-P(N(T)\\!&gt;\\!0|N(t)\\!=\\!1)\\exp(r(t,T)))^{n-1} \\nonumber\\\\&amp; &amp; \\times P(N(T)\\!&gt;\\!0|N(t)\\!=\\!1)^2 \\exp(r(t,T)) \\label{eq:N} %\\\\%P(N(T)\\!=\\!1|N(t)\\!=\\!1) &amp; = &amp; P(N(T)\\!&gt;\\!0|N(t)\\!=\\!1)^2 \\exp(r(t,T)) \\label{eq:1}\\end{aligned}\\]An overview for different diversification models is given in Höhna (2015).Phylogenetic trees as observationsThe branching processes used here describe probability distributions onphylogenetic trees. This probability distribution can be used to inferdiversification rates given an “observed” phylogenetic tree. In realitywe never observe a phylogenetic tree itself. Instead, phylogenetic treesthemselves are estimated from actual observations, such as DNAsequences. These phylogenetic tree estimates, especially the divergencetimes, can have considerable uncertainty associated with them. Thus, thecorrect approach for estimating diversification rates is to include theuncertainty in the phylogeny by, for example, jointly estimating thephylogeny and diversification rates. For the simplicity of the followingtutorials, we take a shortcut and assume that we know the phylogenywithout error. For publication quality analysis you should alwaysestimate the diversification rates jointly with the phylogeny anddivergence times.Conditions of the Birth-Death ProcessFive different possible conditions for our generalized fossilized-birth-death process.    I) The process survives until the present.    II) The process starts at the root and both descendants of the root survive until the present.    III) Sampling at least one lineage.    IV) The process start at the root and both descendants have at least one lineage sampled.    V) The process starts at the root, both descendants have at least one lineage sampled, and the process survives until the present.Birth-death models are often conditioned on specific events, see (Stadler 2013), (Höhna 2015) and (Magee and Höhna 2021) for some discussion on the topic.However, when there are non-contemporaneous samples in the dataset which may be ancestral to other samples, conditioning becomes somewhat complex.The key issues for conditioning are whether it is assumed that the process starts at the root or the origin, and whether the descending lineage(s) is (are) assumed to leave any sampled descendant or specifically to have a descendant sampled at the present day.Consideration of these possibilities leads to five possible conditions, though conditioning is not strictly required.  Survival of the origin: We condition the process on survival of one lineage, i.e., at least on descendant of the lineage starting at the origin was sampled at the present. This condition represent a case when we have fossils and extant taxa and do not know if the fossils are stem fossils of the entire clade. The condition is obtained by computing $1-E(t_{or})$ with $\\phi(t)=0$.  Survival of the root: We condition the process on survival of both lineages, i.e., at least one descendant of each lineage starting at the root was sampled at the present. This is the case for most macroevolutionary analyses without any fossils or if the fossils are known to belong within the crown group of the extant taxa. The condition is obtained by computing $(1-E(t_{MRCA}))^2$ with $\\phi(t)=0$.  Sampling of origin: We condition the process to have at least one sample being a descendant of the origin. This is simply a minimal condition that at least something was observed/sampled. This condition represents the case if we would also consider complete extinct clades. The condition is obtained by computing $1-E(t_{or})$.  Sampling of the root: We condition the process to require that both lineages starting at the root are sampled. In this case, all taxa might be extinct but the root age is known or inferred as a parameter of the model. The condition is obtained by computing $(1-E(t_{MRCA}))^2$.  Sampling of the root and survival of the origin: We condition the process on sampling of both descendant lineages of the root and at least one sample at the present. In this case, we condition on this specific root age but one of the descendant lineages of the root might have gone extinct while the other descendant lineage from the root must have survived. The condition is obtained by computing $(1-E_{\\phi(t)=0}(t_{MRCA}))(1-E_{\\phi(t)\\neq 0}(t_{MRCA}))$.For macroevolutionary analyses of diversification rates, condition (I) is the most adequate if we have both extinct and extant taxa, condition (II) if we have only extant taxa, and condition (III) if we have only extinct taxa.For phylodynamic applications, if it can safely be assumed that there are no sampled ancestors prior to the first observed infection (which will always be true if $r(t) = 1$), condition (IV) may be used, otherwise only condition (III) is applicable.Conditioning on survival as in (I), (II), or (V) requires $\\Phi_0 &gt; 0$, and so is primarily of interest in macroevolutionary applications.Of these conditions, (II) is the strictest and requires prior knowledge that the MRCA of the extant samples is the MRCA of all samples.Condition (V) is less restrictive, requiring only that none of the fossils could be sampled ancestors prior to the first observed speciation event, which would apply if all fossils are within the crown group.We could additionally condition on the number of extant taxa $N$, as suggest by (Gernhard 2008), although there is, as of today and to our knowledge, no solution known to condition on the number of extinct taxa.",
        "url": "/tutorials/divrate/div_rate_intro.html",
        "index": "true"
      }
      ,
    
      "documentation": {
        "title": "Documentation",
        "content": "Welcome to the Rev Language Reference pages!These help pages are stored in Markdown files on the RevBayes GitHub repository, here.If you would like to edit a help page, click the  button at the bottom of the entry page.",
        "url": "/documentation/",
        "index": ""
      }
      ,
    
      "download": {
        "title": "Download",
        "content": "Current version: v1.2.5See the list of changes on GitHub.Mac OS XDownload Intel Executable (10.11+)Download Apple Silicon Executable (10.11+)or Compile from sourceWindowsDownload Executable (10)or Compile from sourceLinuxDownload ExecutableDownload Singularity Imageor Compile from sourceRevBayes Interface Tools (GUIs and notebooks)Compile with Spack",
        "url": "/download",
        "index": ""
      }
      ,
    
      "developer-best-practices-doxygen-guidelines-html": {
        "title": "Documentation guidelines",
        "content": "RevBayes uses Doxygen to compile documentation from the code. Documentation is critical to help other programmers understand the code, so here are some guidelines to get you started.Important notes  member functions which reimplement functions from a parent class will automatically inherit their documentation from the parent. Only document these if additional details are required. The @copydoc commands allows to copy documentation from any other class or member.  long documentation should always be placed just before the definition it applies to. Brief descriptions can be placed after members.Header files (.h)  document the class, with references (using the @cite command with a BibTEX reference) if applicable.  all members (functions and variables) including the private ones should be followed by a brief description (unless the parent description is adequate).  the @file annotation should be used only when the file contains no classes, otherwise classes should be documented rather than files.Implementation files (.cpp)  detailed descriptions of member functions go there (unless the parent description is adequate).  the commands @param, @return and @throw are used to document respectively the parameters, the return value and the exceptions thrown.  simple getters and setters do not need detailed documentation.Other useful doxygen commands  LateX-style formulas can be included using the @f$ annotation for inline text (example: @f$n^2@f$), and the @f[ and @f] annotations for equations (example: @f[ x = sum_{i=1}^n y^i @f])  @todo and @bug to document problematic behavior or missing features  @note and @warning to document tricky or unexpected behavior  @see to reference other classes/methodsDo  include information about memory side-effects of the code, in particular the use of new and delete.  document the default values of the parameters, if applicable.  document private members.  inherit documentation or use the @copydoc command instead of copy-pasting whenever possible.  run Doxygen and check the output before submitting your new documentation.Do not  replicate information given by git, such as author, license or modification dates of the files. Exception: code adapted or copied from outside the RevBayes project should be credited to the original author and include licensing information if applicable.  restate information easily available in the code, such as the parent class.  leave commented-out code unless accompanied by a comment stating what it does and why it was left in.",
        "url": "/developer/best_practices/doxygen_guidelines.html",
        "index": ""
      }
      ,
    
      "tutorials-divrate-ebd-html": {
        "title": "Episodic Diversification Rate Estimation",
        "content": "OverviewThis tutorial describes how to specify an episodic birth-death model in RevBayes (Höhna et al. 2016).The episodic birth-death model is a process where diversification rates vary episodically through timeand are modeled as piecewise-constant rates (Stadler 2011; Höhna 2015).The probabilistic graphical model is given once at the beginning of this tutorial.Your goal is to estimate speciation and extinction rates through-time usingMarkov chain Monte Carlo (MCMC).You should read first the Introduction to Diversification Rate Estimationwhich explains the theory and gives some general overview of diversification rate estimationand Simple Diversification Rate Estimation which goes through a very simple pure birth and birth-death modelfor estimating diversification rates.Episodic Diversification Rate ModelsThe basic idea behind the model is that speciation and extinction rates are constantwithin time intervals but can be different between time intervals.Thus, we will divide time into equally sized intervals.An overview of the underlying theory of the specific model and implementationis given in Höhna (2015), Magee and Höhna (2021).Two scenarios of birth-death models. On the left we show constant diversification.On the right we show an example of an episodic birth-death process where ratesare constant in each time interval (epoch). The top panel of this figure showsan example realization under the given rates. shows an example of a constant rate birth-death process andan episodic birth-death process.We assume that the log-transformed rates follow a Horseshoe Markov random field (HSMRF) prior distribution Magee et al. (2020).This prior assumes that rates are autocorrelated, that is, rates in the current time interval will be centered around the rates in the previous time interval.The assumption of autocorrelated rates does not only makes sense biologically but also improves our ability to estimate parameters.The HSMRF allows for periods of relatively constant rates interspersed with large jumps, and can be used with large numbers of intervals (hundreds) to provide fine-scale detail in estimated rates.A graphical model with the outline of the Rev code. On the left we see a graphical model describing the HSMRF model for rate-variation through time.On the right we show the corresponding Rev commands to instantiate this model.This figure gives a complete overview of the model that we use here in this analysis.There are many ways to represent the HSMRF model, this one is easy to read, butmore computationally burdensome than the version we will use in practice.We show an idealized graphical model of the episodic birth-death process with autocorrelated ratesin .This graphical model shows you which variables are included in the model,and the dependency between the variables.Thus, it makes the structure and assumption of the model clear andvisible instead of a black-box (Höhna et al. 2014).For example, you see how the speciation and extinction rates in each time intervaldepend on the rates of the previous interval, and that we use a hyperprior forthe standard deviation of rates between time intervals.Note that the actual set up of the plates in Rev code will appear to differ from what is shown here.This is because there are many ways to represent the HSMRF model, and with somesmall re-parameterization and a function to replace the plate(s), we can makeit run much more efficiently and easily in MCMC.Estimating Episodic Diversification RatesRead the treeBegin by reading in the “observed” tree.T &lt;- readTrees(\"data/primates_tree.nex\")[1]From this tree, we get some helpful variables, such as the taxon information which we need to instantiate the birth-death process.taxa &lt;- T.taxa()Additionally, we initialize a variable for our vector of moves and monitors.moves    = VectorMoves()monitors = VectorMonitors()Finally, we create a helper variable that specifies the number of intervals.NUM_INTERVALS = 10NUM_BREAKS := NUM_INTERVALS - 1Using this variable we can easily change our script to break-up time into many (e.g., NUM_INTERVALS = 100).Specifying the modelPriors on amount of rate variationWe start by specifying prior distributions on the rates.The overall model is a HSMRF birth-death model.In this model, the changes in log-scale rates between intervals follow a Horseshoe distribution (Carvalho et al. 2010), which allows for large jumps in rate while assuming most changes are very small.We need a parameter to control the overall variability from present to past in the diversification rates, the global scale parameter.We must also set the global scale hyperprior, which acts with the global scale parameter to set the prior on rate variability.In this example, we use 10 speciation and extinction rate intervals, but the HSMRF enables us to use much larger numbers.The global scale hyperprior should be set based on how many intervals are used, in the case of 100 intervals, use 0.0021.RevGadgets provides the function setMRFGlobalScaleHyperpriorNShifts() to compute this parameter for other numbers of intervals.speciation_global_scale_hyperprior &lt;- 0.044extinction_global_scale_hyperprior &lt;- 0.044speciation_global_scale ~ dnHalfCauchy(0,1)extinction_global_scale ~ dnHalfCauchy(0,1)Specifying episodic ratesAs we mentioned before, we will apply HSMRF distributions as prior for the log-transformed speciation and extinction rates.We begin with the rates at the present which is our initial rate parameter.The rates at the present will be specified slightly differentlybecause they are not correlated to any previous rates.This is because we are actually modeling rate-changes backwards in time andthere is no previous rate for the rate at the present.Modeling rates backwards in time makes it easier for us if we had some prior informationabout some event affected diversification sometime before the present,e.g., 25 million years ago.We use a uniform distribution between -10 and 10 because of our lack of prior knowledgeon the diversification rate.This actually means that we allow speciation and extinction ratesbetween $e^{-10}$ and $e^{10}$, so we should clearly cover the true values.(Note that for diversification rate estimates, $e^{-10}$ is virtually 0since the rate is so slow).log_speciation_at_present ~ dnUniform(-10.0,10.0)log_speciation_at_present.setValue(0.0)log_extinction_at_present ~ dnUniform(-10.0,10.0)log_extinction_at_present.setValue(-1.0)We apply efficient sliding moves to each parameter.moves.append( mvScaleBactrian(log_speciation_at_present,weight=5))moves.append( mvScaleBactrian(log_extinction_at_present,weight=5))To make MCMC possible for the HSMRF model, we use what is called a non-centered parameterization.This means that first we specify the log-scale changes in rate between intervals, and later we assemble these into the vector of rates, by adding them together and then exponentiating.The HSMRF also requires a vector of local scale parameters.These give the HSMRF a property called local adaptivity, which allow it to have rapidly varying rates in some intervals and nearly constant rates in others.This can be done efficiently using a for-loop.for (i in 1:NUM_BREAKS) {  sigma_speciation[i] ~ dnHalfCauchy(0,1)  sigma_extinction[i] ~ dnHalfCauchy(0,1)  # Make sure values initialize to something reasonable  sigma_speciation[i].setValue(runif(1,0.005,0.1)[1])  sigma_extinction[i].setValue(runif(1,0.005,0.1)[1])  # non-centralized parameterization of horseshoe  delta_log_speciation[i] ~ dnNormal( mean=0, sd=sigma_speciation[i]*speciation_global_scale*speciation_global_scale_hyperprior )  delta_log_extinction[i] ~ dnNormal( mean=0, sd=sigma_extinction[i]*extinction_global_scale*extinction_global_scale_hyperprior )}Now, we take the pieces we have for the rates (the global and local scales, the rate at the present, and the log-changes) and we assemble the overall rates.The delta parameters are differences in log-scale rates, and so we need to sum them then exponentiate.For example, speciation[2] := exp(log_speciation_at_present + delta_log_speciation[1]).Because the sd values for any of the log-scale differences is equivalent to what we wrote in the DAG above (simply written in expanded form), the model here is equivalent to the one in the DAG.The function fnassembleContinuousMRF() does all the summation and exponentiation efficiently for all speciation and extinction rates in all intervals and avoids making the DAG too big.speciation := fnassembleContinuousMRF(log_speciation_at_present,delta_log_speciation,initialValueIsLogScale=TRUE,order=1)extinction := fnassembleContinuousMRF(log_extinction_at_present,delta_log_extinction,initialValueIsLogScale=TRUE,order=1)The HSMRF requires a unique set of MCMC samplers to work.While you can place individual moves on the local and global scales and the log-changes, adequate MCMC sampling requires moves that exploit the structure of the model.These moves are the elliptical slice sampler that works on the log-changes, a Gibbs sampler for the global and local scales, and a recommended swap move (that works on both).But these moves require us to have the HSMRF model written out this way, and not the version we showed in the DAG.# Move all field parameters in one gomoves.append( mvEllipticalSliceSamplingSimple(delta_log_speciation,weight=5,tune=FALSE) )moves.append( mvEllipticalSliceSamplingSimple(delta_log_extinction,weight=5,tune=FALSE) )# Move all field hyperparameters in one gomoves.append( mvHSRFHyperpriorsGibbs(speciation_global_scale, sigma_speciation , delta_log_speciation , speciation_global_scale_hyperprior, propGlobalOnly=0.75, weight=10) )moves.append( mvHSRFHyperpriorsGibbs(extinction_global_scale, sigma_extinction , delta_log_extinction , extinction_global_scale_hyperprior, propGlobalOnly=0.75, weight=10) )# Swap moves to exchange adjacent delta,sigma pairsmoves.append( mvHSRFIntervalSwap(delta_log_speciation ,sigma_speciation ,weight=5) )moves.append( mvHSRFIntervalSwap(delta_log_extinction ,sigma_extinction ,weight=5) )Setting up the time intervalsThe model formulation we are using assumes that time points are evenly spaced.We must now create this vector of times to give to the birth death model.interval_times &lt;- abs(T.rootAge() * seq(1, NUM_BREAKS, 1)/NUM_INTERVALS)This vector of times will be used for both the speciation and extinction rates.Also, remember that the times of the intervals represent ages going backwards in time.Incomplete Taxon SamplingWe know that we have sampled 233 out of 367 living primate species.To account for this we can set the sampling parameter as a constant node with a value of 233/367.For simplicity, and since almost all species have been sampled,we assume uniform taxon sampling (Höhna et al. 2011; Höhna 2014),rho &lt;- T.ntips()/367Root ageThe birth-death process requires a parameter for the root age.In this exercise we use a fixed tree and thus we know the age of the tree.Hence, we can get the value for the root from the Magnuson-Ford and Otto (2012) tree.root_time &lt;- T.rootAge()The time treeNow we have all of the parameters we need to specify the full episodic birth-death model.We initialize the stochastic node representing the time tree.timetree ~ dnEpisodicBirthDeath(rootAge=T.rootAge(), lambdaRates=speciation, lambdaTimes=interval_times, muRates=extinction, muTimes=interval_times, rho=rho, samplingStrategy=\"uniform\", condition=\"survival\", taxa=taxa)You may notice that we explicitly specify that we want to condition on survival.It is possible to change this condition to the time of the process orthe number of sampled taxa too.Then we attach data to the timetree variable.timetree.clamp(T)Finally, we create a workspace object of our whole model using the model() function.mymodel = model(speciation)The model() function traversed all of the connections and found all of the nodes we specified.Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we need to set up a vector of monitors to record the states of our Markov chain.First, we will initialize the model monitor using the mnModel function.This creates a new monitor variable that will output the states for all model parameterswhen passed into a MCMC function.monitors.append( mnModel(filename=\"output/primates_EBD.log\",printgen=10, separator = TAB) )Additionally, we create four separate file monitors, one for each vector of speciation and extinction rates and for each speciation and extinction rate epoch (\\IE the times when the interval ends).We want to have the speciation and extinction rates stored separately so that we can plot them nicely afterwards.monitors.append( mnFile(filename=\"output/primates_EBD_speciation_rates.log\",printgen=10, separator = TAB, speciation) )monitors.append( mnFile(filename=\"output/primates_EBD_speciation_times.log\",printgen=10, separator = TAB, interval_times) )monitors.append( mnFile(filename=\"output/primates_EBD_extinction_rates.log\",printgen=10, separator = TAB, extinction) )monitors.append( mnFile(filename=\"output/primates_EBD_extinction_times.log\",printgen=10, separator = TAB, interval_times) )Finally, we create a screen monitor that will report the states of specified variablesto the screen with mnScreen:monitors.append( mnScreen(printgen=1000, extinction_global_scale, speciation_global_scale) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves,we can now set up the MCMC algorithm that will sample parameter values in proportionto their posterior probability.The mcmc() function will create our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=50000, tuningInterval=200)When the analysis is complete, you will have the monitored files in your output directory.You can then visualize the rates through time using R using our package RevGadgets (Tribble et al. 2022).If you don’t have the R-package RevGadgets installed, or if you have trouble with the package, then please read the separate tutorial about the package (Introduction to RevGadgets).Just start R in the main directory for this analysis and then type the following commands:library(RevGadgets)# specify the output filesspeciation_time_file &lt;- \"output/primates_EBD_speciation_times.log\"speciation_rate_file &lt;- \"output/primates_EBD_speciation_rates.log\"extinction_time_file &lt;- \"output/primates_EBD_extinction_times.log\"extinction_rate_file &lt;- \"output/primates_EBD_extinction_rates.log\"# read in and process ratesrates &lt;- processDivRates(speciation_time_log = speciation_time_file,                         speciation_rate_log = speciation_rate_file,                         extinction_time_log = extinction_time_file,                         extinction_rate_log = extinction_rate_file,                         burnin = 0.25,                         summary = \"median\")# plot rates through timep &lt;- plotDivRates(rates = rates) +        xlab(\"Millions of years ago\") +        ylab(\"Rate per million years\")ggsave(\"EBD.png\", p)(Note, you may want to add a nice geological timescale to the plot by setting use.geoscale=TRUE but then you can only plot one figure per page.)⇨ The Rev file for performing this analysis: mcmc_EBD.RevResulting diversification rate estimations when using 10 intervals.You should obtain similar results when you use 10 intervals.The estimated rates might change when you use a different resolution, i.e., a different number of intervals.Exercise 1  Run an MCMC simulation to estimate the posterior distribution of the speciation rate and extinction rate.  Visualize the rate through time using R.  Do you see evidence for rate decreases or increases? What is the general trend?  How much faster is the net-diversification rate at the present compared to the most ancient time interval?  Is there evidence for rate variation? Look at the estimates of speciation_global_scale and extinction_global_scale in Tracer: Is there information in the data to change the estimates from the prior?  Run the analysis using a different number of intervals, e.g., 100 or 200. How do the rates change?Exercise 2  In our results we see that the extinction rate is fairly constant. Modify the model by using a constant-rate for the extinction rate parameter but still let the speciation rate vary through time.          Remove all previous occurrences of the extinction rates (i.e., priors, parameters and moves).      Specify a lognormal prior distribution on the constant extinction rate (extinction ~ dnLognormal(-5,sd=2*0.587405))      Add a move for the new extinction rate parameter moves.append( mvScaleBactrian(extinction,weight=5.0) ).      Remove the argument muTimes=interval_times from the birth-death process.        How does this influence your estimated rates?  Click below to begin the next exercise!  Inferring the Impact of Mass Extinctions  Testing for Environmental Correlation",
        "url": "/tutorials/divrate/ebd.html",
        "index": "true"
      }
      ,
    
      "developer-setup-eclipse-html": {
        "title": "Setting up Eclipse for RevBayes development",
        "content": "Eclipse is a Java-based, cross-platform IDE with lots of nice features that make it convenient for RevBayes development. First, it’s cross-platform, and unlike Xcode, you can use any compiler you like (not just clang).Prerequisites  To compile RevBayes using Eclipse, you must install CMake and its command line tools. This is very easy if you have homebrew installed (brew install cmake)  You must also have a C++ compiler. If you are using a Mac, you can use clang (included with XCode), or you can install gcc (e.g. via homebrew brew install gcc).Installing Eclipse CDTTo start writing C++ in Eclipse, you must obtain the Eclipse CDT (C/C++ Development Tooling) package. You can download a distribution of Eclipse that comes prepackaged with CDT from the CDT Downloads page.If you already have Eclipse installed you can go to Help &gt; Install New Software… and enter the p2 repository URL for your version of eclipse found on the CDT downloads page. Then check the Main Features box, click Next twice, accept the User Agreement, and restart Eclipse.  Important! When you first open Eclipse, you will be asked to choose a location for your workspace. Make sure to choose a location that is not inside the source directory of your project!Create an Eclipse project for RevBayesAssuming you have cloned the RevBayes github repository into the directory &lt;revbayes-repo&gt;, you can create a new C++ Eclipse project as follows:  In Eclipse, go to File &gt; New &gt; Makefile Project with Existing Code  Set the Name of the project to revbayes  Select &lt;revbayes-repo&gt;/src as the Existing Code Location  Select Cross GCC in Toolchain for Indexer Settings (you can change this later if you want).  Click FinishConfigure the RevBayes projectYou will need to configure your Eclipse project so it correctly compiles the revbayes CMake project.Configure the PATH environment variableIf you installed the CMake command line tools in the default location :/usr/local/bin, you must add it to the PATH environment variable of your Eclipse project.      In the Project Explorer view, highlight your revbayes project directory    Go to Project &gt; Properties, or right click on the project name and select Properties.  Expand C/C++ Build and click on Environment  Click on the PATH entry, click Edit… and add /usr/local/bin to the end of the ValueConfigure the C/C++ build settingsThe RevBayes CMake project uses a special build script build.sh to build the RevBayes executable. You must tell your Eclipse project to use this script as a build command.  Click on C/C++ Build  Uncheck Use default build command and in Build command, enter sh build.sh  In Build directory, add ../projects/cmake to the directory path  Click on the Behavior tab  In Build (incremental build), enter -boost false  Again, click on C/C++ Build  Click Manage Configurations  Click on New… to create a new configuration, and name it Debug      Configure the Debug configuration by adding -debug true to the Build (incremental build) optionsNow, if you set the active configuration to Debug, RevBayes will be compiled with debugger symbols that can be loaded by lldb or gdb. You can also set the active build configuration by going to Project &gt; Build Configurations &gt; Set Active    Click Apply and CloseAt this point, C/C++ Indexer will get to work indexing the RevBayes code, during which time Eclipse might appear to be unresponsive.Configure the project to use spaces instead of tabsRevBayes code is indented using spaces. However, by default Eclipse uses tabs. Configure your project to automatically insert 4 spaces when you press the Tab key.  Open Eclipse &gt; Preferences  Expand C/C++ &gt; Code Style  Click on Formatter  Click the New… button to create a new profile, name the profile (e.g. “spaces”) then click OK to continue  Click the Indentation tab  Under General Settings set Tab Policy to Spaces only  Click OK and Apply and CloseBuild the RevBayes projectThe first time you build RevBayes, you will also need to build the included Boost libraries. You only need to do this once. To build the boost libraries, return to step 5 in the build configuration section and enter -boost true instead. Then after you’ve built the libraries, you can disable the Boost build flag by resetting -boost false.With all the build settings correctly configured, you can build RevBayes by highlighting the project in the Project Explorer view, and then going to Project &gt; Build Project. You can also right click on the project directory and select Build Project.The Console view should display the progress of the compilation process.You’re done! Now you can find the rb executable in the &lt;revbayes-repo&gt;/projects/cmake directory.Tips      Create a symlink of rb in your PATH, so it is automatically updated every time you build RevBayes. e.g.    sudo ln -s &lt;revbayes-repo&gt;/projects/cmake/rb /usr/local/bin/rb        If your Project Explorer or Console views disappear and you can’t find them, go to Window &gt; Show View to display various views.  ",
        "url": "/developer/setup/eclipse.html",
        "index": ""
      }
      ,
    
      "tutorials-divrate-efbdp-me-html": {
        "title": "Mass Extinction Estimation",
        "content": "OverviewThis tutorial describes how to infer mass extinctions using an episodic fossilized birth-death model in RevBayes.The episodic birth-death model is a process where diversification rates vary episodically through timeand are modeled as piecewise-constant rates (Stadler 2011; Höhna 2015).Your goal is to estimate whether or not there was a mass extinction using Markov chain Monte Carlo (MCMC).This tutorial builds on the time-varying birth-death model in the Episodic Diversification Rate Estimationtutorial, where the theory of the underlying time-varying diversification model is covered.This model allows the incorporation of fossils in the phylogeny, so theCombined-Evidence Analysis and the Fossilized Birth-Death Process for Analysis of Extant Taxa and Fossil Specimens tutorial may also be of interest.Episodic Diversification Rate Models with Mass ExtinctionsThe basic idea behind the model is that speciation, extinction, and fossilization rates are constant within time intervals but can be different between time intervals.At these time points, the model allows for the possibility of a mass extinction, where every lineage dies instantaneously with some probability $M_i$.This instantaneous model of mass extinctions allows us to codify the fact that mass extinctions should lead to very large proportions of lineages dying off relatively quickly.It is important to account for variation in the per-lineage rates of speciation, extinction, and fossilization so that diversificaton-rate shifts are not misinterpreted as mass extinctions.To do this, we will use a Horseshoe Markov random field model (see the Episodic Diversification Rate Estimation for more specifics).An overview of the underlying theory of the specific model and implementation is given in (Magee and Höhna 2021).Two trees simulated under two scenarios of birth-death models.Tree (A) on the left was simulated without a mass extinction, while tree (B) on theright experienced as mass extinction at the dashed line.Both trees contain sampled ancestors as well, but these have been omitted for clarity. shows an example of a tree without and a tree with a mass extinction, but with otherwise similar diversification rates.Note how the tree with a mass extinction contains a band of fossil tips shortlybefore the mass extinction time, and few lineages cross the boundary.This is to be expected, as mass extinctions kill off a majority of lineages alive at that time, thus few lineages survive and what might otherwise be sampled ancestors become tips.In our inference model, each mass extinction probability will have a reversible jump mixture modelprior with a probability $1 - p_M_i$ that there is no mass extinction.If there is a mass extinction, the probability that a lineage goes extinct atthat time, $M_i$, will be modeled with a Beta distribution.The probability $p_M_i$ that there is a mass extinction will be small (massextinctions are rare), but the probability of a lineage dying in a massextinction will be large (most lineages die in a mass extinction).To model background rate variation, we use the Horseshoe Markov random fieldbirth-death model as in the Episodic Diversification Rate Estimation tutorial.Estimating Mass ExtinctionsRead the dataBegin by reading in the ``observed’’ tree.T &lt;- readTrees(\"data/crocs_T1.tre\")[1]Here, we extract the fossil ages directly from the tree. If we were to simultaneouslyinfer the tree instead, these ages would have to be read in from a taxon data file.(You can find an example of such a file in the Episodic Diversification Rate Estimation tutorial.)taxa &lt;- T.taxa()Additionally, we initialize a variable for our vector of moves and monitors.moves    = VectorMoves()monitors = VectorMonitors()Finally, we create a helper variable that specifies the number of intervals.NUM_INTERVALS = 100NUM_BREAKS := NUM_INTERVALS - 1Using this variable we can easily change our script to break-up time into moreintervals (e.g., NUM_INTERVALS = 200) for finer resolution on the mass extinctions.Specifying the modelSetting up the time intervalsThe model formulation we are using assumes that time points are evenly spaced.We must now create this vector of times to give to the birth death model.interval_times &lt;- abs(T.rootAge() * seq(1, NUM_BREAKS, 1)/NUM_INTERVALS)This vector of times will be used for the speciation, extinction, and fossilizationrates, as well as the mass extinctions.Also, remember that the times of the intervals represent ages going backwards in time.Priors on the mass extinctionsRecall that the prior on mass extinctions is done with a reversible jump mixture model.This means our prior on mass extinctions comes in two parts: the probability thatthere is a mass extinction at any time interval, and the probability that alineage dies in that mass extinction.We set the reversible jump probability in terms of the prior expected number of mass extinctions experienced.For example, we might set the prior to 0.5 or 1.0 as a conservative prior estimate of the number of mass extinctions that shaped this tree.expected_number_of_mass_extinctions &lt;- 1.0Since there are NUM_BREAKS possible extinction locations, this makes the probabilitythat there is NOT a mass extinction at any one of them 1 - expected_number_of_mass_extinctions/NUM_BREAKS.mix_p &lt;- Probability(1.0 - expected_number_of_mass_extinctions/NUM_BREAKS)We must also choose a prior on the probability that a lineage dies at a mass extinction (when there is a non-zero mass extinction).Mass extinctions kill a large majority of lineages, so we use a Beta(18,2) prior,which correspondingly has a mean of 0.9 and a 95% CI of [0.74,0.987].Our full prior then says the following: mass extinctions are rare, but when they occur they kill most lineages.Now we must use these choices to set up the prior on all NUM_BREAKS possible mass extinctions.for (i in 1:NUM_BREAKS) {  mass_extinction_probabilities[i] ~ dnReversibleJumpMixture(0.0,dnBeta(18.0,2.0),mix_p)  moves.append( mvRJSwitch(mass_extinction_probabilities[i]) )  moves.append( mvSlideBactrian(mass_extinction_probabilities[i]) )}We must attach a reversible jump move to investigate whether there is or is not a mass extinction.We must also attach a move to explore the probability of death (when there is a mass extinction).Priors on amount of rate variationNow we must specify prior distributions on the rates of speciation, extinction, and fossilization through time.The overall model is a Horseshoe Markov random field (HSMRF) birth-death model Magee et al. (2020).In this model, the changes in log-scale rates between intervals follow a Horseshoe distribution Carvalho et al. (2010), which allows for large jumps in rate while assuming most changes are very small.We need a parameter to control the overall variability from present to past in the diversification rates, the global scale parameter.We must also set the global scale hyperprior, which acts with the global scale parameter to set the prior on rate variability.In this example, we use 100 rate intervals, but the HSMRF enables us to use larger numbers.The global scale hyperprior should be set based on how many intervals are used, in the case of 100 intervals, use 0.0021.RevGadgets provides the function setMRFGlobalScaleHyperpriorNShifts() to compute this parameter for other numbers of intervals.speciation_rate_global_scale_hyperprior &lt;- 0.0021extinction_rate_global_scale_hyperprior &lt;- 0.0021fossilization_rate_global_scale_hyperprior &lt;- 0.0021speciation_rate_global_scale ~ dnHalfCauchy(0,1)extinction_rate_global_scale ~ dnHalfCauchy(0,1)fossilization_rate_global_scale ~ dnHalfCauchy(0,1)Specifying episodic ratesAs we mentioned, we will apply HSMRF distributions as prior for the log-transformed speciation, extinction, and fossilization rates.We begin with the rates at the present which is our initial rate parameter.The rates at the present will be specified slightly differentlybecause they are not correlated to any previous rates.This is because we are actually modeling rate-changes backwards in time andthere is no previous rate for the rate at the present.Modeling rates backwards in time makes it easier for us if we had some prior informationabout some event affected diversification sometime before the present,e.g., 25 million years ago.Given that the full model contains many parameters, we set the prior on thefirst rate using an empirical Bayes strategy.In this approach, we first fit a constant-rate FBD model, then use the posteriordistribution on those rates to determine the prior on the rate at present.Here, this has already been done and a gamma prior has been fit to the samples.The script mcmc_CRFBD.Rev can be used to run the constant-rate analysis,and the RevGadgets function posteriorSamplesToParametricPrior() can be usedto fit a distribution to the posterior samples.The following three Gamma priors are suitable only for analyses of the Wilberg et al. (2019) datasets.New priors must be set for new datasets.The script fit_gamma_distributions.R shows you how to set the priors, using this Crocodylomorph dataset as an example, assuming you have run mcmc_CRFBD.Rev.speciation_rate_hyperprior_alpha &lt;- 8.333449speciation_rate_hyperprior_beta &lt;- 24.432402extinction_rate_hyperprior_alpha &lt;- 8.28311extinction_rate_hyperprior_beta &lt;- 24.34245fossilization_rate_hyperprior_alpha &lt;- 8.964942fossilization_rate_hyperprior_beta &lt;- 2717.621689speciation_rate_at_present ~ dnGamma(speciation_rate_hyperprior_alpha,speciation_rate_hyperprior_beta)extinction_rate_at_present ~ dnGamma(extinction_rate_hyperprior_alpha,extinction_rate_hyperprior_beta)fossilization_rate_at_present ~ dnGamma(fossilization_rate_hyperprior_alpha,fossilization_rate_hyperprior_beta)We apply a variety of moves to the rates at present individually.moves.append( mvScaleBactrian(speciation_rate_at_present,weight=5) )moves.append( mvScaleBactrian(extinction_rate_at_present,weight=5) )moves.append( mvScaleBactrian(fossilization_rate_at_present,weight=5) )moves.append( mvMirrorMultiplier(speciation_rate_at_present,weight=5) )moves.append( mvMirrorMultiplier(extinction_rate_at_present,weight=5) )moves.append( mvMirrorMultiplier(fossilization_rate_at_present,weight=5) )moves.append( mvRandomDive(speciation_rate_at_present,weight=5) )moves.append( mvRandomDive(extinction_rate_at_present,weight=5) )moves.append( mvRandomDive(fossilization_rate_at_present,weight=5) )We also apply joint moves to account for the correlation between the parameters.avmvn_rates_at_present = mvAVMVN(weight=50)avmvn_rates_at_present.addVariable(speciation_rate_at_present)avmvn_rates_at_present.addVariable(extinction_rate_at_present)avmvn_rates_at_present.addVariable(fossilization_rate_at_present)moves.append( avmvn_rates_at_present )up_down_move = mvUpDownScale(weight=5.0)up_down_move.addVariable(speciation_rate_at_present,TRUE)up_down_move.addVariable(extinction_rate_at_present,TRUE)moves.append( up_down_move )To make MCMC possible for the HSMRF model, we use what is called a non-centered parameterization.This means that first we specify the log-scale changes in rate between intervals, and later we assemble these into the vector of rates.The HSMRF also requires a vector of local scale parameters.These give the HSMRF a property called local adaptivity, which allow it to have rapidly varying rates in some intervals and nearly constant rates in others.This can be done efficiently using a for-loop.In this loop we also attach a variety of moves to improve MCMC mixing, but as with the Episodic Diversification Rate Estimation tutorial, we will also use HSMRF-specific moves.for (i in 1:NUM_BREAKS) {  # Variable-scaled variances for hierarchical horseshoe  sigma_speciation_rate[i] ~ dnHalfCauchy(0,1)  sigma_extinction_rate[i] ~ dnHalfCauchy(0,1)  sigma_fossilization_rate[i] ~ dnHalfCauchy(0,1)  # Make sure values initialize to something reasonable  sigma_speciation_rate[i].setValue(runif(1,0.005,0.1)[1])  sigma_extinction_rate[i].setValue(runif(1,0.005,0.1)[1])  sigma_fossilization_rate[i].setValue(runif(1,0.005,0.1)[1])  # moves on the single sigma values  moves.append( mvScaleBactrian(sigma_speciation_rate[i], weight=5) )  moves.append( mvScaleBactrian(sigma_extinction_rate[i], weight=5) )  moves.append( mvScaleBactrian(sigma_fossilization_rate[i], weight=5) )  # non-centralized parameterization of horseshoe  delta_log_speciation_rate[i] ~ dnNormal( mean=0, sd=sigma_speciation_rate[i]*speciation_rate_global_scale*speciation_rate_global_scale_hyperprior )  delta_log_extinction_rate[i] ~ dnNormal( mean=0, sd=sigma_extinction_rate[i]*extinction_rate_global_scale*extinction_rate_global_scale_hyperprior )  delta_log_fossilization_rate[i] ~ dnNormal( mean=0, sd=sigma_fossilization_rate[i]*fossilization_rate_global_scale*fossilization_rate_global_scale_hyperprior )  # Make sure values initialize to something reasonable  delta_log_speciation_rate[i].setValue(runif(1,-0.1,0.1)[1])  delta_log_extinction_rate[i].setValue(runif(1,-0.1,0.1)[1])  delta_log_fossilization_rate[i].setValue(runif(1,-0.1,0.1)[1])  moves.append( mvSlideBactrian(delta_log_speciation_rate[i], weight=5) )  moves.append( mvSlideBactrian(delta_log_extinction_rate[i], weight=5) )  moves.append( mvSlideBactrian(delta_log_fossilization_rate[i], weight=5) )  delta_up_down_move[i] = mvUpDownSlide(weight=5.0)  delta_up_down_move[i].addVariable(delta_log_speciation_rate[i],TRUE)  delta_up_down_move[i].addVariable(delta_log_extinction_rate[i],TRUE)  moves.append( delta_up_down_move[i] )}Now, we take the pieces we have for the rates (the global and local scales, the rate at the present, and the log-changes) and we assemble the overall rates.speciation_rate := fnassembleContinuousMRF(speciation_rate_at_present,delta_log_speciation_rate,initialValueIsLogScale=FALSE,order=1)extinction_rate := fnassembleContinuousMRF(extinction_rate_at_present,delta_log_extinction_rate,initialValueIsLogScale=FALSE,order=1)fossilization_rate := fnassembleContinuousMRF(fossilization_rate_at_present,delta_log_fossilization_rate,initialValueIsLogScale=FALSE,order=1)The HSMRF requires a unique set of MCMC samplers to work.While we can (and did) place individual moves on the local and global scales and the log-changes, adequate MCMC sampling requires moves that exploit the structure of the model.These moves are the elliptical slice sampler that works on the log-changes, a Gibbs sampler for the global and local scales, and a recommended swap move (that works on both).# Move all field parameters in one gomoves.append( mvEllipticalSliceSamplingSimple(delta_log_speciation,weight=5,tune=FALSE) )moves.append( mvEllipticalSliceSamplingSimple(delta_log_extinction,weight=5,tune=FALSE) )moves.append( mvEllipticalSliceSamplingSimple(delta_log_fossilization,weight=5,tune=FALSE) )# Move all field hyperparameters in one gomoves.append( mvHSRFHyperpriorsGibbs(speciation_rate_global_scale, sigma_speciation , delta_log_speciation , speciation_rate_global_scale_hyperprior, propGlobalOnly=0.75, weight=10) )moves.append( mvHSRFHyperpriorsGibbs(extinction_rate_global_scale, sigma_extinction , delta_log_extinction , extinction_rate_global_scale_hyperprior, propGlobalOnly=0.75, weight=10) )moves.append( mvHSRFHyperpriorsGibbs(fossilization_rate_global_scale, sigma_fossilization , delta_log_fossilization , fossilization_rate_global_scale_hyperprior, propGlobalOnly=0.75, weight=10) )# Swap moves to exchange adjacent delta,sigma pairsmoves.append( mvHSRFIntervalSwap(delta_log_speciation ,sigma_speciation ,weight=5) )moves.append( mvHSRFIntervalSwap(delta_log_extinction ,sigma_extinction ,weight=5) )moves.append( mvHSRFIntervalSwap(delta_log_fossilization ,sigma_fossilization ,weight=5) )Incomplete Taxon SamplingWe know that we have sampled 14 out of 23 living Crocodilian species.To account for this we can set the sampling parameter as a constant node with a value of 14/23.For simplicity we assume uniform taxon sampling (Höhna et al. 2011; Höhna 2014),sampling_at_present &lt;- 14/23Root ageThe birth-death process requires a parameter for the root age.In this exercise we use a fixed tree and thus we know the age of the tree.Hence, we can get the value for the root from the Wilberg et al. (2019) tree.root_time &lt;- T.rootAge()The time treeNow we have all of the parameters we need to specify the full episodic birth-death model.We initialize the stochastic node representing the time tree.timetree ~ dnFBDP(                  rootAge                                          = T.rootAge(),                  timeline                                         = interval_times,                  lambda                                           = speciation_rate,                  mu                                               = extinction_rate,                  phi                                              = fossilization_rate,                  Mu                                               = mass_extinction_probabilities,                  Phi                                              = sampling_at_present,                  condition                                        = \"time\",                  taxa                                             = taxa,                  initialTree                                      = T)You may notice that we explicitly specify that we want to condition on the time of the process.It is possible to change this condition to the the fact that we have sampledthe root (and at least one lineage has survived to the present).Then we attach data to the timetree variable.timetree.clamp(T)Finally, we create a workspace object of our whole model using the model() function.mymodel = model(speciation_rate)The model() function traversed all of the connections and found all of the nodes we specified.Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we need to set up a vector of monitors to record the states of our Markov chain.First, we will initialize the model monitor using the mnModel function.This creates a new monitor variable that will output the states for all model parameterswhen passed into a MCMC function.monitors.append( mnModel(filename=\"output/crocs_EFBDME.log\",printgen=10, separator = TAB) )Additionally, we create four separate file monitors, one for each vector of speciation and extinction rates and for each speciation and extinction rate epoch (\\IE the times when the interval ends).We want to have the speciation and extinction rates stored separately so that we can plot them nicely afterwards.monitors.append( mnModel(filename=\"output/crocs_EFBDME.log\",printgen=10, separator = TAB) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_speciation_rates.log\",printgen=10, separator = TAB, speciation_rate) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_speciation_rate_times.log\",printgen=10, separator = TAB, interval_times) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_extinction_rates.log\",printgen=10, separator = TAB, extinction_rate) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_extinction_rate_times.log\",printgen=10, separator = TAB, interval_times) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_fossilization_rates.log\",printgen=10, separator = TAB, fossilization_rate) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_fossilization_rate_times.log\",printgen=10, separator = TAB, interval_times) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_mass_extinction_probabilities.log\",printgen=10, separator = TAB, mass_extinction_probabilities) )monitors.append( mnFile(filename=\"output/crocs_EFBDME_mass_extinction_times.log\",printgen=10, separator = TAB, interval_times) )Finally, we create a screen monitor that will report the states of specified variablesto the screen with mnScreen:monitors.append( mnScreen(printgen=1000, speciation_rate_global_scale, extinction_rate_global_scale, fossilization_rate_global_scale) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves,we can now set up the MCMC algorithm that will sample parameter values in proportionto their posterior probability.The mcmc() function will create our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC.This will take some time, this model has roughly 700 parameters to infer which are generally correlated with each other.mymcmc.run(generations=300000, tuningInterval=200)Assessing support for mass extinctionsWhen the analysis is complete, you will have the monitored files in your output directory.The Episodic Diversification Rate Estimation tutorial covers how to summarize the continuous rates of speciation, extinction, and fossilization through time.The key summary for the mass extinctions is the posterior support for the presence of a mass extinction at any interval.Since we have used reversible jump mixture models, the posterior distribution for each has some samples where the mass extinction was disallowed (and the $M_i$ is exactly 0).If there is support for a mass extinction, there will also be samples with mass extinctions (where $M_i$ is not 0).We can use Bayes factors to summarize how significant support is for a mass extinction at any time, with a 2 log Bayes factor threshold of 10 indicating strong support.You can visualize the support at the allowed mass extinction times using R using our package RevGadgets.If you don’t have the R-package RevGadgets installed, or if you have trouble with the package, then please read the separate tutorial about the package.Note that you will have to give RevGadgets some information about the model setup from the Rev script for it to function.Just start R in the main directory for this analysis and then type the following commands:library(RevGadgets)mass_extinction_probabilities &lt;- readTrace(\"output/crocs_EFBDME_mass_extinction_probabilities.log\",burnin = 0.25)# prior probability of mass extinction at any timeprior_n_expected &lt;- 0.1n_intervals &lt;- 100prior_prob &lt;- prior_n_expected/(n_intervals-1)# times when mass extinctions were allowedtree_age &lt;- 243.5interval_times &lt;- tree_age * seq(1/n_intervals,(n_intervals-1)/n_intervals,1/n_intervals)# then plot results:p &lt;- plotMassExtinctions(mass.extinction.trace=mass_extinction_probabilities,mass.extinction.times=interval_times,mass.extinction.name=\"mass_extinction_probabilities\",prior_prob)pdf(\"mass_extinction_Bayes_factors.pdf\")pdev.off()⇨ The Rev file for performing this analysis: mcmc_EFBD_mass_extinctions.RevResulting Bayes Factor support when using 100 intervals.You should obtain similar results when you use 100 intervals.The support values might change when you use a different resolution, i.e., a different number of intervals.Exercise 1  Run an MCMC simulation to estimate the posterior support for mass extinctions.  Visualize the support using R.  Do you see evidence for mass extinctions? When? What known mass extinction event, if any, does this correspond to?  Plot the continuous rates of speciation, extinction, and fossilization (revisit the Episodic Diversification Rate Estimation tutorial for details). What patterns do you see?  Is there evidence for rate variation? Look at the estimates of speciation_global_scale, extinction_global_scale, and fossilization_global_scale in Tracer: Is there information in the data to change the estimates from the prior?Exercise 2  Assess prior sensitivity. Specifically, assess the sensitivity of the results to the prior expected number of mass extinctions.          Set expected_number_of_mass_extinctions to something other than 1.0, like 0.5 or 2.0 (or even something much smaller or bigger like 0.1 or 10).      Run the new analysis and plot the results.        How does this influence your support for mass extinctions?  Do the continuous rates of speciation, extintion, and/or fossilization change?",
        "url": "/tutorials/divrate/efbdp_me.html",
        "index": "true"
      }
      ,
    
      "tutorials-divrate-env-html": {
        "title": "Environmental-dependent Speciation &amp; Extinction Rates",
        "content": "OverviewThis tutorial describes how to specify a branching-process model withdiversification rate correlated with an environmental variable in RevBayes (Höhna et al. 2016).Diversification rates are assumed to be equal among all lineages but vary through time correlated with an environmental predictor variable (Condamine et al. 2013; Condamine et al. 2018; Palazzesi et al. 2022).Thus, this model can be used to test for correlations between diversification ratesand environmental variables, such as $\\text{CO}_2$ and temperature.However, these tests are only to establish a correlation, not to establish causality.As usual, we provide the probabilistic graphical model at the beginning of this tutorial.Hopefully this will help you to get a better idea of all the variables in the model and their dependencies.Our goal in this tutorial is to estimate the correlation coefficient between speciation and extinction rates to historical $\\text{CO}_2$ measurements using Markov chain Monte Carlo (MCMC).Environmental-dependent Diversification Rate ModelsThe fundamental idea of this model is the question if diversification rates are correlated with an environmental variable.Examples of environmental variables are $\\text{CO}_2$ and temperature.Estimates of historical $\\text{CO}_2$ values. These estimates are obtained from Beerling and Royer (2011).Have a look at  which shows the historical value $\\text{CO}_2$ in the last 50 million years.We can clearly see that the $\\text{CO}_2$ dropped drastically around 30 million years ago.Estimated diversification rates through time. These estimates are taken from the episodic birth-death model with autocorrelated (Brownian motion) rate as described in the Episodic Diversification Rate Estimation.In our previous Episodic Diversification Rate Estimation we estimated diversification as shown in .We clearly see that diversification rates were not constant through time.Now we wonder if perhaps the diversification rates are correlated with $\\text{CO}_2$.We want to build on our episodic birth-death model so that our environmental correlation model collapses to the episodic birth-death model if there is no correlation.Recall that we used a Brownian motion model on the log-transformed rates.Hence, we assumed that the rates in the next time interval (epoch) have the current value as their expectation:\\[E[\\log( \\lambda(t) )] = \\log( \\lambda(t-\\Delta t) )\\]For the environmental dependent birth-death model, we have additional observation from the environmental variable.Thus, we know how much the environmental variable changed between time intervals (epochs).We can compute this change by taking the ratio between two consecutive measurements:$\\frac{\\text{CO}_2(t)}{\\text{CO}_2(t-\\Delta t)}$.Hence, if the $\\text{CO}_2$ double from one epoch to the next we would compute a change of 2.This has the clear advantage that our computation is less sensitive to the unit and magnitude of the environmental variable.Now let us assume that our diversification rates shift synchronously with the environmental variable if they are actually correlated.Then we can express our expectation of the log-transformed diversification rate in the next time interval (epoch) as being equal the log-transform diversification rate in the current time interval plus the log-transformed change in the environmental variable:\\[E[\\log( \\lambda(t) )] = \\log( \\lambda(t-\\Delta t) ) + \\beta \\times \\log\\left( \\frac{\\text{CO}_2(t)}{\\text{CO}_2(t-\\Delta t)} \\right) \\mbox{ .}\\]Here we denote the correlation coefficient by $\\beta$.If $\\beta &gt; 0$ then there is a positive correlation between the speciation rate and $\\text{CO}_2$, that is, if the $\\text{CO}_2$ increases then the speciation increases also.If $\\beta &lt; 0$ then there is a negative correlation between the speciation rate and $\\text{CO}_2$, that is, if the $\\text{CO}_2$ increases then the speciation decreases.Finally, if $\\beta = 0$ then there is no correlation and our model collapses to the episodic birth-death model.In summary, we use a regression-like prior model for the speciation and extinction rate where the environmental variable (here $\\text{CO}_2$) is the predictor variable.Specifically, we use a Brownian motion model for the log-transformed speciation and extinction rates where the expectation depends on the shift in the environmental variable.Thus, our model can be considered as a Brownian motion model with drift where the drift parameter is the environmental variable.We will now walk you through setting up this analysis in RevBayes.Estimating Environment-dependent Diversification RatesRead the treeBegin by reading in the “observed” tree.T &lt;- readTrees(\"data/primates_tree.nex\")[1]From this tree, we get some helpful variables, such as the taxon information which we need to instantiate the birth-death process.taxa &lt;- T.taxa()Additionally, we can initialize a variable for our vector ofmoves and monitors:moves    = VectorMoves()monitors = VectorMonitors()Set up the environmental dataWe take the $\\text{CO}_2$ measurement from Beerling and Royer (2011) (see also Palazzesi et al. (2022)) and store the values on a vector; one measurement (value) per interval.var &lt;- v(297.6, 301.36, 304.84, 307.86, 310.36, 312.53, 314.48, 316.31, 317.42, 317.63, 317.74, 318.51, 318.29, 316.5, 315.49, 317.64, 318.61, 316.6, 317.77, 328.27, 351.12, 381.87, 415.47, 446.86, 478.31, 513.77, 550.74, 586.68, 631.48, 684.13, 725.83, 757.81, 789.39, 813.79, 824.25, 812.6, 784.79, 755.25, 738.41, 727.53, 710.48, 693.55, 683.04, 683.99, 690.93, 694.44, 701.62, 718.05, 731.95, 731.56, 717.76)Then we specify the maximum age of the measurements.This corresponds to the time of the last interval.MAX_VAR_AGE = 50We will later use this maximum age to compute the times for each interval by assuming that each interval is equal in time.Finally, we create a helper variable that specifies the number of intervals.NUM_INTERVALS = var.size()-1This variable will help us to create the episodic diversification rate using a for-loop.Setting up the time intervalsIn RevBayes you actually have the possibility to specify unequal time intervals or even different intervals for the speciation and extinction rate.This is achieved by providing a vector of times when each interval ends.However, here we assume for simplicity that each interval has the same length because this is how we obtained our environmental data.interval_times &lt;- MAX_VAR_AGE * (1:NUM_INTERVALS) / NUM_INTERVALSThis vector of times will be used for both the speciation and extinction rates.Also, remember that the times of the intervals represent ages going backwards in time.Specifying the modelPriors on amount of rate variationWe follow here exactly the prior specification as in the Episodic Diversification Rate Estimation tutorial because we want our model to collapse to the episodic birth-death if there is no correlation.We start by specifying prior distributions on the rates.Each interval-specific speciation and extinction rate will be drawn from a normal distribution.Thus, we need a parameter for the standard deviation of those normal distributions.We use an exponential hyperprior with rate SD = 0.587405 / NUM_INTERVALS to estimate the standard deviation, but assume that all speciation rates and all extinction rates share the same standard deviation.The motivation for an exponential hyperprior is that it has the highest probability density at 0 which would make the variance of rates between consecutive time intervals 0 and thus represent a constant rate process.The data will tell us if there should be much variation in rates through time.(You may want to experiment with this hyperprior if you are interested.)SD = abs(0.587405 / NUM_INTERVALS)speciation_sd ~ dnExponential( 1.0 / SD)extinction_sd ~ dnExponential( 1.0 / SD)We apply a simple scaling move on each prior parameter.moves.append( mvScale(speciation_sd, weight=5.0) )moves.append( mvScale(extinction_sd, weight=5.0) )Specifying the correlation coefficientsThen we specify normal prior distributions on the correlation coefficient $\\beta$ for the speciation and extinction rate.Again, out total lack of prior knowledge, we will assume that the standard deviation of $\\beta$ is $1.0$ and you may want to modify this value.Nevertheless, this normal prior distribution is motivated by being centered at 0.0 (no correlation) and gives equal weight to positive and negative correlations.beta_speciation ~ dnNormal(0,1.0)beta_extinction ~ dnNormal(0,1.0)We apply simple sliding-window moves for the two correlation coefficients because they are defined on the whole real line.moves.append( mvSlide(beta_speciation,delta=1.0,weight=10.0) )moves.append( mvSlide(beta_extinction,delta=1.0,weight=10.0) )Additionally, we might be interested in the posterior probability that there is a positive correlation, $\\mathbb{P}(\\beta&gt;0)$, or a negative correlation, $\\mathbb{P}(\\beta&lt;0)$, respectively.We achieve this using a deterministic variable that is 1 if $\\beta&lt;0$speciation_corr_neg_prob := ifelse(beta_speciation &lt; 0.0, 1, 0)extinction_corr_neg_prob := ifelse(beta_extinction &lt; 0.0, 1, 0)speciation_corr_pos_prob := ifelse(beta_speciation &gt; 0.0, 1, 0)extinction_corr_pos_prob := ifelse(beta_extinction &gt; 0.0, 1, 0)Note that in this model the probability of $\\beta$ being 0.0 ($\\mathbb{P}(\\beta=0)=0$)because we are working with a prior and posterior density on $\\beta$ and thus any specific value,e.g., 0.0, has a probability of 0.0.We will circumvent this issue in the next chapter when we use reversible-jump MCMC to set $\\beta$ specifically to 0.0.Here you can also check that the posterior probability of speciation_corr_pos_prob equals 1-speciation_corr_neg_prob.Specifying correlated ratesAs we mentioned before, we will apply normal distributions as priors for each log-transformed rate.We begin with the rate at the present which is our initial rate parameter.The rates at the present will be specified slightly differently because they are not correlated to any previous rates.This is because we are actually modeling rate-changes backwards in time and there is no previous rate for the rate at the present.We use a uniform distribution between -10 and 10 because of our lack of prior knowledge on the diversification rate.This actually means that we allow speciation and extinction rates between $e^{-10}$ and $e^{10}$ we should clearly cover the true values.(Note that for diversification rate estimates $e^{-10}$ is virtually 0 since the rate is so slow).log_speciation[1] ~ dnUniform(-10.0,10.0)log_speciation[1].setValue(0.0)log_extinction[1] ~ dnUniform(-10.0,10.0)log_extinction[1].setValue(-1.0)Notice that we store the diversification rate variables in vectors.Storing the rate parameters in vectors will be useful and important later when we pass the rates into the birth-death process.We apply simple sliding window moves for the rates.Normally we would use scaling moves but in this case we work on the log-transformed parameters and thus sliding moves perform better.(If you are keen you can test the differences.)moves.append( mvSlide(log_speciation[1], weight=2) )moves.append( mvSlide(log_extinction[1], weight=2) )Now we transform the diversification rate parameters into actual rates.speciation[1] := exp( log_speciation[1] )extinction[1] := exp( log_extinction[1] )Next, we specify the speciation and extinction rates for each time interval (i.e., epoch).This can be done efficiently using a for-loop.We will use a specific index variable so that we can easier refer to the rate at the previous interval.Remember that we want to model the rates as a Brownian motion,which we achieve by specify a normal distribution as the prior distribution on the rates centered around the previous rate plus the change in the environmental variable(i.e., the mean is equal to the previous rate plus the log-transformed ratio of the environmental variable divided by the previous value).for (i in 1:NUM_INTERVALS) {    index = i+1    expected_speciation[i] := log_speciation[i] + beta_speciation * ln( var[index] / var[i] )    expected_extinction[i] := log_extinction[i] + beta_extinction * ln( var[index] / var[i] )    log_speciation[index] ~ dnNormal( mean=expected_speciation[i], sd=speciation_sd )    log_extinction[index] ~ dnNormal( mean=expected_extinction[i], sd=extinction_sd )    moves.append( mvSlide(log_speciation[index], weight=2) )    moves.append( mvSlide(log_extinction[index], weight=2) )    speciation[index] := exp( log_speciation[index] )    extinction[index] := exp( log_extinction[index] )}Finally, we apply moves that slide all values in the rate vectors, i.e., all speciation or extinction rates.We will use an mvVectorSlide move.moves.append( mvVectorSlide(log_speciation, weight=10) )moves.append( mvVectorSlide(log_extinction, weight=10) )Additionally, we apply a mvShrinkExpand move which changes the spread of several variables around their mean.moves.append( mvShrinkExpand(log_speciation, sd=speciation_sd, weight=10) )moves.append( mvShrinkExpand(log_extinction, sd=extinction_sd, weight=10) )Both moves considerably improve the efficiency of our MCMC analysis.Incomplete Taxon SamplingWe know that we have sampled 233 out of 367 living primate species.To account for this we can set the sampling parameter as a constant node with a value of 233/367.For simplicity, and since almost all species have been sampled, we assume uniform taxon sampling (Höhna et al. 2011; Höhna 2014),rho &lt;- T.ntips()/367Root ageThe birth-death process requires a parameter for the root age.In this exercise we use a fix tree and thus we know the age of the tree.Hence, we can get the value for the root from the Magnuson-Ford and Otto (2012) tree.root_time &lt;- T.rootAge()The time treeNow we have all of the parameters we need to specify the full episodic birth-death model.We initialize the stochastic node representing the time tree.timetree ~ dnEpisodicBirthDeath(rootAge=T.rootAge(), lambdaRates=speciation, lambdaTimes=interval_times, muRates=extinction, muTimes=interval_times, rho=rho, samplingStrategy=\"uniform\", condition=\"survival\", taxa=taxa)You may notice that we explicitly specify that we want to condition on survival.It is possible to change this condition to the time of the process or the number of sampled taxa too.Then we attach data to the timetree variable.timetree.clamp(T)Finally, we create a workspace object of our whole model using the model() function.mymodel = model(speciation)The model() function traversed all of the connections and found all of the nodes we specified.Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we need to set up a vector of monitors to record the states of our Markov chain.First, we will initialize the model monitor using the mnModel function. This creates a new monitor variable that will output the states for all model parameters when passed into a MCMC function.monitors.append( mnModel(filename=\"output/primates_EBD_Corr.log\",printgen=10, separator = TAB) )Additionally, we create four separate file monitors, one for each vector of speciation and extinction rates and for each speciation and extinction rate epoch (i.e., the times when the interval ends).We want to have the speciation and extinction rates stored separately so that we can plot them nicely afterwards.monitors.append( mnFile(filename=\"output/primates_EBD_Corr_speciation_rates.log\",printgen=10, separator = TAB, speciation) )monitors.append( mnFile(filename=\"output/primates_EBD_Corr_speciation_times.log\",printgen=10, separator = TAB, interval_times) )monitors.append( mnFile(filename=\"output/primates_EBD_Corr_extinction_rates.log\",printgen=10, separator = TAB, extinction) )monitors.append( mnFile(filename=\"output/primates_EBD_Corr_extinction_times.log\",printgen=10, separator = TAB, interval_times) )Finally, create a screen monitor that will report the states of specified variables to the screen with mnScreen:monitors.append( mnScreen(printgen=1000, beta_speciation, beta_extinction) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, we can now set up the MCMC algorithm that will sample parameter values in proportion to their posterior probability.The mcmc() function will create our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=50000, tuningInterval=200)When the analysis is complete, you will have the monitored files in your output directory.You can then visualize the rates through time using R using our package RevGadgets.If you don’t have the R-package RevGadgets installed, or if you have trouble with the package, then please read the separate tutorial about the package.Just start R in the main directory for this analysis and then type the following commands:library(RevGadgets)library(ggplot2)# the CO2 values as a reference in our plotco2 &lt;- c(297.6, 301.36, 304.84, 307.86, 310.36, 312.53, 314.48, 316.31,         317.42, 317.63, 317.74, 318.51, 318.29, 316.5, 315.49, 317.64,          318.61, 316.6, 317.77, 328.27, 351.12, 381.87, 415.47, 446.86,          478.31, 513.77, 550.74, 586.68, 631.48, 684.13, 725.83, 757.81,          789.39, 813.79, 824.25, 812.6, 784.79, 755.25, 738.41, 727.53,         710.48, 693.55, 683.04, 683.99, 690.93, 694.44, 701.62, 718.05,          731.95, 731.56, 717.76)MAX_VAR_AGE = 50NUM_INTERVALS = length(co2)co2_age &lt;- MAX_VAR_AGE * (1:NUM_INTERVALS) / NUM_INTERVALSrates &lt;- processDivRates(  speciation_time_log = \"output/primates_EBD_Corr_speciation_times.log\",  speciation_rate_log = \"output/primates_EBD_Corr_speciation_rates.log\",  extinction_time_log = \"output/primates_EBD_Corr_extinction_times.log\",  extinction_rate_log = \"output/primates_EBD_Corr_extinction_rates.log\",  burnin=0.25)p &lt;- plotDivRates(rates, env_age = co2_age, env_var = co2, env_label = \"co2 (ppm)\", env_scaling = 1000)ggsave(\"figures/EBD_Corr.pdf\", p, width = 180, height = 130, units = \"mm\")⇨ The Rev file for performing this analysis: mcmc_EBD_Corr.RevA brief discussion on estimated diversification ratesResulting diversification rate estimations. shows the estimated diversification rates through time and the $\\text{CO}_2$.If you compare these estimates with  then you may notice that the diversification rate estimate are virtually identical.This is a good sign for the analysis because it shows that the information in the estimates comes from the data (the tree in this case) and not from the assumed model.Thus, we are not artificially forcing the diversification rates to follow our environmental variable but instead estimate if there is a correlation.Small deviation between the estimated rates under the different analyses are expected because there will be some interaction between the environmental variable and the diversification rate estimates.Additionally, the uncertainty in estimated diversification rates through time is large und minor changes are within this uncertainty.Testing for correlation using reversible-jump MCMCIn the previous exercise we wanted that our model collapses to the episodic birth-death process if there is no environmental correlation.We achieved this by setting up our prior model so that if $\\beta=0$ the model collapses.However, we also used a normal prior distribution with mean 0.0 and standard deviation 1.0 for $\\beta$.Thus, we implicitly specified that $\\beta$ being exactly 0.0 has probability 0.0 because every specific value of a continuous distribution has a 0.0 probability despite having a positive probability density.For example, you might notice that you will never sample in your MCMC run the value 0.0 exactly although we might sample values that are close to 0.0.Now we want to use reversible jump MCMC to test specifically if the hypothesis $\\beta=0$ is rejected.Remember that reversible jump MCMC can estimate the posterior probability for different models.The first model will be that $\\beta=0$ and the second model will be that $\\beta \\sim \\text{norm}(0,1)$.Then we can simply compute Bayes factors by computing the posterior ratio divided by the prior ratio to assess the support for either model.In RevBayes we have a very flexible way to specify a reversible-jump MCMC.We can provide any constant value and distribution to the distribution dnReversibleJumpMixture.This will mean that the value, beta_speciation and beta_extinction, will either take on the constant value or drawn from the base-distribution.beta_speciation ~ dnReversibleJumpMixture(constantValue=0.0, baseDistribution=dnNormal(0,1.0), p=0.5)beta_extinction ~ dnReversibleJumpMixture(constantValue=0.0, baseDistribution=dnNormal(0,1.0), p=0.5)Additionally we also need a specific move that switches if the value is equal to the constant value or drawn from the base-distribution.This is where we use the reversible-jump move mvRJSwitch.moves.append( mvRJSwitch(beta_speciation, weight=5) )moves.append( mvRJSwitch(beta_extinction, weight=5) )Now we can also monitor for convenience what the probability of beta_speciation and beta_extinction being 0.0 is.We will set this up by a deterministic variable that will be 1.0 if $\\beta \\neq 0$ and will be 0.0 if $\\beta = 0.0$.Thus the two variables speciation_corr_prob and extinction_corr_prob represent the probability that there is a correlation between the speciation rate or the extinction rate and $\\text{CO}_2$.speciation_corr_prob := ifelse(beta_speciation == 0.0, 0, 1)extinction_corr_prob := ifelse(beta_extinction == 0.0, 0, 1)These are the only necessary changes to the above analysis to run a reversible-jump MCMC.",
        "url": "/tutorials/divrate/env.html",
        "index": "true"
      }
      ,
    
      "developer-tutorial-external-file-yaml-html": {
        "title": "",
        "content": "This is an external Markdown filewith YAML front matter.This file will be formatted as plain text like the rest of the main text.You can even include $\\LaTeX$.",
        "url": "/developer/tutorial/external_file-yaml.html",
        "index": ""
      }
      ,
    
      "tutorials-fbd-fbd-specimen-html": {
        "title": "Combined-Evidence Analysis and the Fossilized Birth-Death Process for Analysis of Extant Taxa and Fossil Specimens",
        "content": "OverviewThis tutorial demonstrates how to specify the models used in a Bayesian“combined-evidence” phylogenetic analysis of extant and fossil species,combining morphological and molecular data as well as fossil occurrence data from the fossil record [e.g., Ronquist et al. (2012), Zhang et al. (2016), Gavryushkina et al. (2017)]. We begin with a concise to the models used in this analysis, followed by a detailed example analysis in demonstrating how to apply these models inRevBayes (Höhna et al. 2017) and use Markov chain Monte Carlo (MCMC) toestimate the posterior distribution of dated phylogenies for datacollected from living and fossil bears (family Ursidae).IntroductionThe “combined-evidence” analysis described in this tutorial uses aprobabilistic graphical model (Höhna et al. 2014) integrating three separatelikelihood components or data partitions (): onefor molecular data (), one formorphological data (), and one forfossil age data ().In addition, all likelihood components are conditioned on a treetopology with divergence times which is modeled according to a separateprior component ().Modular components of the graphical model used in the “combined-evidence” analysis described in this tutorial.In  we provide an example of the type of treeestimated from a total-evidence analysis. This example shows thecomplete tree (A) and the sampled orreconstructed tree (B). Importantly, we areinterested in estimating the topology, divergence times and fossilsample times of the reconstructed tree (B).We will describe the distinction between these two trees in . One possiblerealization of the fossilized birth-death (described in section) process starting at origin time $\\phi$, showingfossil sampling events (red circles), and 15 sampled extant taxa (blackcircles). (A) The complete tree shows all lineages both sampled (solidlines) and unsampled (dotted lines). (B) The reconstructed tree (alsocalled the sampled tree) shows only the sampled lineagesLineage Diversification and SamplingThe joint prior distribution on tree topologies and divergence times ofliving and extinct species used in this tutorial is described by thefossilized birth-death (FBD) process (Stadler 2010; Heath et al. 2014). Thismodel simply treats the fossil observations as part of the processgoverning the tree topology and branch times (the node in). The fossilized birth-death process provides amodel for the distribution of speciation times, tree topology, andlineage samples before the present(e.g., non-contemporaneous samples likefossils or viruses). This type of tree is shown in . Note that this model can be used with orwithout character data for the historical samples. Thus, it provides areasonable prior distribution for analyses combining morphological orDNA data for both extant and fossiltaxa—i.e., the so-called “total-evidence”approaches described by Ronquist et al. (2012) and extended by Zhang et al. (2016) andGavryushkina et al. (2017). When matrices of discrete morphological charactersfor both living and fossil species are unavailable, the fossilizedbirth-death model imposes a time structure on the tree bymarginalizingover all possible attachment points for the fossils on the extant tree(Heath et al. 2014), therefore, some prior knowledge of phylogeneticrelationships is important.The FBD model () describes the probability of thetree and fossils conditional on the birth-death parameters:$f[\\mathcal{T} \\mid \\lambda, \\mu, \\rho, \\psi, \\phi]$, where$\\mathcal{T}$ denotes the tree topology, divergence times fossiloccurrence times and the times at which the fossils attach to the tree.The birth-death parameters $\\lambda$ and $\\mu$ denote the speciation andextinction rates, respectively. The “fossilization rate” or “fossilrecovery rate” is denoted $\\psi$ and describes the rate at which fossilsare sampled along lineages of the complete tree. The samplingprobability parameter $\\rho$ represents the probability that an extantspecies is sampled, and $\\phi$ represents the time at which the processoriginated.A graphical model of the fossilizedbirth-death model describing the generation of the time tree (in) used in this tutorial. The parameters of thefossilized birth-death process are labeled in orange. The speciation,extinction and fossilization rates are stochastic nodes (circles) drawnfrom exponential distributions, while the origin time is uniformlydistributed. The sampling probability is constant node (square) andequal to one. The represents the phylogenetic continuous-time Markovchain that links the tree model to the other model components and theobserved sequence data.For more information on probabilistic graphicalmodels and their notation, please see (Höhna et al. 2014).In the example FBD tree shown in , thediversification process originates at time $\\phi$, giving rise to $n=20$species in the present, with both sampled fossils (red circles) andextant species (black circles). All of the lineages represented in A (both solid and dotted lines) show thecomplete tree. This is the tree of all extant and extinct lineagesgenerated by the process. The complete tree is distinct from thereconstructed tree (B) which is the treerepresenting only the lineages sampled as extant taxa or fossils. Fossilobservations (red circles in ) are recoveredover the lifetime of the process along the lineages of the completetree. If a lineage does not have any descendants sampled in the present,it is lost and cannot be observed, these are the dotted lines in A. The probability must be conditioned on the origintime of the process $\\phi$. The origin ($\\phi$) of a birth-death processis the starting time of the stem lineage, thus this conditions on asingle lineage giving rise to the tree.An important characteristic of the FBD model is that it accounts for theprobability of sampled ancestor-descendant pairs (Foote 1996). Giventhat fossils are sampled from lineages in the diversification process,the probability of sampling fossils that are ancestors to taxa sampledat a later date is correlated with the turnover rate ($r=\\mu/\\lambda$)and the fossil recovery rate ($\\psi$). This feature is important,particularly for datasets with many sampled fossils. In the example(), several of the fossils have sampleddescendants. These fossils have solid black lines leading to thepresent.Incorporating Fossil Occurrence Time UncertaintyIn order to account for uncertainty in the ages of our fossil species,we can incorporate intervals on the ages of our represented fossiltips. These should ideally represent specimen-level uncertainties(stemming from the imprecise dating of the geological unit from whicha given fossil was collected) rather than stratigraphic ranges (i.e.,the interval between the first occurrence of a given species and its lastoccurrence, assuming that at least two stratigraphically unique occurrencesdo in fact exist). We can account for such specimen-level uncertaintiesby assuming each fossil can occur with uniform probability anywhere withinits observed interval. This is somewhat different from the typical approachto node calibration. Here, instead of treating the calibration density asan additional prior distribution on the tree, we treat it as the likelihoodof our fossil data given the tree parameter. Specifically, we assume thelikelihood of a particular fossil’s age uncertainty range $F_i = (a_i, b_i)$is equal to one if the fossil’s inferred age on the tree $t_i$ falls withinthis interval, and zero otherwise:\\[f[F_i \\mid t_i] = \\begin{cases}1 &amp; \\text{if } a_i &lt; t_i &lt; b_i\\\\0 &amp; \\text{otherwise}\\end{cases}\\]In other words, we assume the likelihood is equal to oneif the inferred age is consistent with the age data. We canrepresent this likelihood in RevBayes using a distribution that isproportional to the likelihood,i.e. non-zero when the likelihood is equalto one (). This model component representsthe “Fossil Occurrence Time Data” in the modular graphical model shown in .A graphical model of thefossil age likelihood model used in this tutorial. The likelihood offossil observation $\\mathcal{F}_i$ is uniform and non-zero when theinferred fossil age $t_i$ falls within the age uncertainty interval$(a_i,b_i)$.Once again, it is worth noting that this is not the appropriate way tomodel fossil data that consist of stratigraphic ranges, which representmultiple fossil specimens (each of which can further be associated withits own specimen-level uncertainty) observed at different times along asingle lineage. An extension of the fossilized birth-death process thatis a distribution on stratigraphic ranges was described by Stadler et al. (2018).Nucleotide Sequence EvolutionThe model component for the molecular data uses a generaltime-reversible model of nucleotide evolution and gamma-distributed rateheterogeneity across sites (the Substitution Model and Sites Model in ). Thismodel of sequence evolution is covered thoroughly in theNucleotide substitution modelstutorial.Lineage-Specific Rates of Sequence EvolutionRates of nucleotide sequence evolution can vary widely among lineages,and so models that account for this variation by relaxing the assumptionof a strict molecular clock (Zuckerkandl and Pauling 1962) can allow for moreaccurate estimates of substitution rates and divergence times(Drummond et al. 2006). The simplest type of relaxed clock model assumes thatlineage-specific substitution rates are independent or “uncorrelated”.One example of such an uncorrelated relaxed model is the uncorrelatedexponential relaxed clock, in which the substitution rate for eachlineage is assumed to be independent and identically distributedaccording to an exponential density (). This is Branch Rates Model for the Molecular Data () that we will use in this tutorial. Another possible uncorrelated relaxedclock model is the uncorrelated lognormal model, described in theRelaxed Clocks &amp; Time Treestutorial [also see Thorne and Kishino (2002)].A graphical model of theuncorrelated exponential relaxed clock model. In this model, the clockrate on each branch is independent and identically distributed accordingto an exponential density with mean drawn from an exponential hyperpriordistribution.Morphological Character EvolutionFor the vast majority of extinct species, fossil morphology is theprimary source of phylogenetically informative characters. Therefore, anappropriate model of morphological character evolution is needed toreliably infer the positions of these species in a phylogeneticanalysis. The Mk model (Lewis 2001) uses a generalized Jukes-Cantormatrix to allow for the incorporation of morphology into likelihood andBayesian analyses. In its simplest form, this model assumes thatcharacters change states symmetrically—that a given character is aslikely to transition from a one state to another as it is to reverse. Inthis tutorial we will consider only binary morphological characters,i.e. characters that are observed in one oftwo states, 0 or 1. For example, the assumption of the single-rate Mkmodel applied to our binary character would mean that a change from a 0state to a 1 state is as likely as a change from a 1 state to a 0 state.This assumption is equivalent to assuming that the stationaryprobability of being in a 1 state is equal to $1/2$.In this tutorial, we will apply a single-rate Mk model as a prior onbinary morphological character change. If you are interested extensionsof the Mk model that relax the assumptions of symmetric state change,please see Discrete morphology - Tree Inference.Because of the way morphological data are collected, we need to exercisecaution in how we model the data. Traditionally, phylogenetic trees werebuilt from morphological data using parsimony. Therefore, only parsimonyinformative characters were collected—that is, characters that areuseful for discriminating between phylogenetic hypotheses under themaximum parsimony criterion. This means that many morphological datasetsdo not contain invariant characters orautapomorphies, as theseare not parsimony informative. However, by excluding these slow-evolvingcharacters, estimates of the branch lengths can be inflated(Felsenstein 1992; Lewis 2001). Therefore, it is important to use modelsthat can condition on this data-acquisition bias. RevBayes has twoways of doing this: one is used for datasets in which only parsimonyinformative characters are observed; the other is for datasets in whichparsimony informative characters and parsimony uninformative variablecharacters (such as autapomorphies) are observed.The Morphological ClockJust like with the molecular data ,our observations of discrete morphological characters are conditional onthe rate of change along each branch in the tree. This model componentdefines the of the in the generalized graphical model shown in . The relaxed clock model we described for themolecular data in  it allows thesubstitution rate to vary through time and among lineages. For themorphological data, we will instead use a “strict clock” model(Zuckerkandl and Pauling 1962), in which the rate of discrete character change isassumed to be constant throughout the tree. The strict clock is thesimplest morphological branch rate model we can construct (graphicalmodel shown in ).The graphical-modelrepresentation of the branch-rate model governing the evolution ofmorphological characters. This model is consistent with a strictmorphological clock, where every branch has the same rate of change($c$) and that rate is drawn from an exponential distribution with arate parameter of $\\delta_c$.Example: Estimating the Phylogeny and Divergence Times of Fossil and Extant BearsIn this exercise, we will combine different types of data from 22species of extant and extinct bears to estimate a posterior distributionof calibrated time trees for this group. We have molecular sequence datafor ten species, which represent all of the eight living bears and twoextinct species sequenced from sub-fossil specimens (Arctodus simus,Ursus spelaeus). The sequence alignment provided is a 1,000 bp fragmentof the cytochrome-b mitochondrial gene (Krause et al. 2008). The morphologicalcharacter matrix unites 18 taxa (both fossil and extant) with 62 binary(states 0 or 1) characters (Abella et al. 2012). For the fossil species,occurrence times are obtained from the literature or fossil databaseslike the Paleobiology Database or from your ownpaleontological expertise. The 14 fossil species used in this analysisare listed in  along with the age range for thespecies and relevant citation. Finally, there are two fossil species(Parictis montanus, Ursus abstrusus) for which we do not havemorphological character data (or molecular data) and we must use priorinformation about their phylogenetic relationships to incorporate thesetaxa in our analysis. This information will be applied using cladeconstraints.Age ranges of fossil bears.            Fossil Species      Age      Reference                  Parictis montanus      33.9–37.2      (Clark and Guensburg 1972; Krause et al. 2008)              Zaragocyon daamsi      20–22.8      (Ginsburg and Morales 1995; Abella et al. 2012)              Ballusia elmensis      13.7–16      (Ginsburg and Morales 1998; Abella et al. 2012)              Ursavus primaevus      13.65–15.97      (Andrews and Tobien 1977; Abella et al. 2012)              Ursavus brevihinus      15.97–16.9      (Heizmann et al. 1980; Abella et al. 2012)              Indarctos vireti      7.75–8.7      (Montoya et al. 2001; Abella et al. 2012)              Indarctos arctoides      8.7–9.7      (Geraads et al. 2005; Abella et al. 2012)              Indarctos punjabiensis      4.9–9.7      (Baryshnikov 2002; Abella et al. 2012)              Ailurarctos lufengensis      5.8–8.2      (Jin et al. 2007; Abella et al. 2012)              Agriarctos spp.      4.9–7.75      (Abella et al. 2011; Abella et al. 2012)              Kretzoiarctos beatrix      11.2–11.8      (Abella et al. 2011; Abella et al. 2012)              Arctodus simus      0.012–2.588      (Churcher et al. 1993; Krause et al. 2008)              Ursus abstrusus      1.8–5.3      (Bjork 1970; Krause et al. 2008)              Ursus spelaeus      0.027–0.25      (Loreille et al. 2001; Krause et al. 2008)      Data and Files  On your own computer or your remote machine, create a directory called RB_CombinedEvidence_Tutorial(or any name you like).  Then, navigate to the folder you created and make a new one called data.  Download the files listed below into the data folder. Click on the hyperlinked file names below (these files are also listed in the “Data files and scripts” box at the top of this page).In the data folder, add the following files:      bears_taxa.tsv: a tab-separated table listing every bear species(both fossil and extant) and their occurrence age ranges. For extanttaxa, the minimum age is 0.0(i.e. the present).        bears_cytb.nex: an alignment in NEXUS format of 1,000 bp ofcytochrome b sequences for 10 bear species. This alignment includes8 living bears and 2 extinct sub-fossil bears.        bears_morphology.nex: a matrix of 62 discrete, binary (coded 0or 1) morphological characters for 18 species of fossil andextant bears.  Getting Started  Create a new directory called scripts.When you execute RevBayes in this exercise, you will do so within themain directory you created (i.e., RB_CombinedEvidence_Tutorial). Thus, if you are using a Unix-based operating system, we recommend thatyou add the RevBayes binary to your path.Creating Rev FilesFor complex models and analyses, it is best to create Rev script filesthat will contain all of the model parameters, moves, and functions. Inthis exercise, you will work primarily in your text editor andcreate a set of modular files that will be easily managed andinterchanged. You will write the following files from scratch and savethem in the scripts directory:      mcmc_CEFBDP_Specimens.Rev: the master Rev file that loads the data, theseparate model file and specifies the monitors and MCMC sampler.        model_FBDP.Rev: specifies the model parameters and movesrequired for the fossilized birth-death prior on the tree topology,divergence times fossil occurrence times anddiversification dynamics.        model_UExp.Rev: specifies the components of theuncorrelated exponential model of lineage-specific substitutionrate variation.        model_GTRG.Rev: specifies the parameters and moves for thegeneral time-reversible model of sequence evolution withgamma-distributed rates across sites (GTR+$\\Gamma$).        model_Morph.Rev: specifies the model describing discretemorphological character change (binary characters) under a strictmorphological clock.  All of the files that you will create are also provided in theRevBayes tutorial at the top of the page to download. Please refer to these files toverify or troubleshoot your own scripts.Start the Master Rev File and Import DataIn this section you will begin the master file that you will load intoRevBayes when you’ve completed all of the components of the analysis.  Open your text editor and create the master Revfile called mcmc_CEFBDP_Specimens.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.The file you will begin in this section will be the one you load intoRevBayes when you’ve completed all of the components of the analysis.In this section you will begin the file and write the Rev commands forloading in the taxon list and managing the data matrices. Then, startingin section , you will move on to writingmodule files for each of the model components. Once the model files arecomplete, you will return to editing mcmc_CEFBDP_Specimens.Rev and complete theRev script with the instructions given in section, you will move on to writing the .Load Taxon ListBegin the Rev script by loading in the list of taxon names from thebears_taxa.tsv file using the readTaxonData function.taxa &lt;- readTaxonData(\"data/bears_taxa.tsv\")This function reads a tab-delimited file and creates a variable calledtaxa that is a list of all of the taxon names relevant to thisanalysis. This list includes all of the fossil and extant bear speciesnames in the first columns and a single age value in the second column.The ages provided are either 0.0 for extant species or the average ofthe age range for fossil species (see ).Load Data MatricesRevBayes uses the function readDiscreteCharacterData to load adata matrix to the workspace from a formatted file. This function can beused for both molecular sequences and discrete morphological characters.Load the cytochrome-b sequences from file and assign the data matrix toa variable called cytb.cytb &lt;- readDiscreteCharacterData(\"data/bears_cytb.nex\")Next, import the morphological character matrix and assign it to thevariable morpho.morpho &lt;- readDiscreteCharacterData(\"data/bears_morphology.nex\")Add Missing TaxaIn the descriptions of the files in section, we mentioned that the two data matriceshave different numbers of taxa. Thus, we must add any taxa that are notfound in the molecular (cytb) partition(i.e. are only found in the fossil data) tothat data matrix as missing data (with ? in place of all characters),and do the same with the morphological data partition (morpho). Inorder for all the taxa to appear on the same tree, they all need to bepart of the same dataset, as opposed to present in separate datasets.This ensures that there is a unified taxon set that contains all of ourtips.cytb.addMissingTaxa( taxa )morpho.addMissingTaxa( taxa )Create Helper VariablesBefore we begin writing the Rev scripts for each of the modelcomponents, we need to instantiate a couple “helper variables” that willbe used by downstream parts of our model specification files. Thesevariables will be used in more than one of the module files so it’s bestto initialize them in the master file.Create a new constant node called n_taxa that is equal to the numberof species in our analysis (22).n_taxa &lt;- taxa.size()Next, create a workspace variable called moves. This variable is a vector that will contain all of the MCMC moves usedto propose new states for every stochastic node in the model graph. Eachtime a new stochastic node is created in the model, we can append the move to this vector.moves = VectorMoves()One important distinction here is that moves is part of the RevBayesworkspace and not the hierarchical model. Thus, we use the workspaceassignment operator = instead of the constant node assignment &lt;-.  Save your current working version of mcmc_CEFBDP_Specimens.Rev in the scriptsdirectory.We will now move on to the next Rev file and will completemcmc_CEFBDP_Specimens.Rev in section .The Fossilized Birth-Death Process  Open your text editor and create the fossilized birth-death model filecalled model_FBDP.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.This file will define the models described in sections and  above. Ifnecessary, please review the graphical models depicted for thefossilized birth-death process () and the likelihoodof the tip sampling process ().Speciation and Extinction RatesTwo key parameters of the FBD process are the speciation rate (the rateat which lineages are added to the tree, denoted by $\\lambda$ in) and the extinction rate (the rate at whichlineages are removed from the tree, $\\mu$ in ).We’ll place exponential priors on both of these values. Each parameteris assumed to be drawn independently from a different exponentialdistribution with rates $\\delta_{\\lambda}$ and $\\delta_{\\mu}$respectively (see ). Here, we will assume that$\\delta_{\\lambda} = \\delta_{\\mu} = 10$. Note that an exponentialdistribution with $\\delta = 10$ has an expected value (mean) of $1/10$.Create the exponentially distributed stochastic nodes for thespeciation_rate and extinction_rate using the ~ operator.speciation_rate ~ dnExponential(10)extinction_rate ~ dnExponential(10)For every stochastic node we declare, we must also specify proposalalgorithms (called moves) to sample the value of the parameter inproportion to its posterior probability. If a move is not specified fora stochastic node, then it will not be estimated, but fixed to itsinitial value.The rate parameters for extinction and speciation are both positive,real numbers (i.e. non-negative floatingpoint variables). For both of these nodes, we will use a scaling move(mvScale), which proposes multiplicative changes to a parameter.Many moves also require us to set a tuning value, called lambda formvScale, which determine the size of the proposed change. Here, wewill use three scale moves for each parameter with different values oflambda. By using multiple moves for a single parameter, we will improvethe mixing of the Markov chain.moves.append( mvScale(speciation_rate, lambda=0.01, weight=1) )moves.append( mvScale(speciation_rate, lambda=0.1,  weight=1) )moves.append( mvScale(speciation_rate, lambda=1.0,  weight=1) )moves.append( mvScale(extinction_rate, lambda=0.01, weight=1) )moves.append( mvScale(extinction_rate, lambda=0.1,  weight=1) )moves.append( mvScale(extinction_rate, lambda=1,    weight=1) )You will also notice that each move has a specified weight. Thisoption allows you to indicate how many times you would like a given moveto be performed at each MCMC cycle. The way that we will run our MCMCfor this tutorial will be to execute a schedule of moves at each stepin our chain instead of just one move per step, as is done inMrBayes (Ronquist and Huelsenbeck 2003) or BEAST(Drummond et al. 2012; Bouckaert et al. 2014). Here, if we were to run our MCMC withour current vector of 6 moves, then our move schedule would perform 6moves at each cycle. Within a cycle, an individual move is chosen fromthe move list in proportion to its weight. Therefore, with all six movesassigned weight=1, each has an equal probability of being executed andwill be performed on average one time per MCMC cycle. For moreinformation on moves and how they are performed in RevBayes, pleaserefer to the Introduction to Markov chain Monte Carlo (MCMC) Sampling and Nucleotide substitution models tutorials.In addition to the speciation ($\\lambda$) and extinction ($\\mu$) rates,we may also be interested in inferring diversification ($\\lambda - \\mu$)and turnover ($\\mu/\\lambda$). Since these parameters can be expressed asa deterministic transformation of the speciation and extinction rates,we can monitor (that is, track the values of these parameters, and printthem to a file) their values by creating two deterministic nodes usingthe := operator.diversification := speciation_rate - extinction_rateturnover := extinction_rate/speciation_rateProbability of Sampling Extant TaxaAll extant bears are represented in this dataset. Therefore, we will fixthe probability of sampling an extant lineage ($\\rho$ in) to 1. The parameter rho will be specified as aconstant node using the &lt;- operator.rho &lt;- 1.0Because $\\rho$ is a constant node, we do not have to assign a move tothis parameter.The Fossil Sampling RateSince our data set includes serially sampled lineages, we must alsoaccount for the rate of sampling back in time. This is the fossilsampling (or recovery) rate ($\\psi$ in ), which wewill instantiate as a stochastic node (named psi). As with thespeciation and extinction rates(see ), we will use anexponential prior on this parameter and use scale moves to sample valuesfrom the posterior distribution.psi ~ dnExponential(10) moves.append( mvScale(psi, lambda=0.01, weight=1) )moves.append( mvScale(psi, lambda=0.1,  weight=1) )moves.append( mvScale(psi, lambda=1,    weight=1) )The Origin TimeWe will condition the FBD process on the origin time ($\\phi$ in) of bears, and we will specify a uniformdistribution on the origin age. For this parameter, we will use asliding window move (mvSlide). A sliding window samples a parameteruniformly within an interval (defined by the half-width delta).Sliding window moves can be tricky for small values, as the window mayoverlap zero. However, for parameters such as the origin age, there islittle risk of this being an issue.origin_time ~ dnUnif(37.2, 55.0)moves.append( mvSlide(origin_time, delta=0.01, weight=5.0) )moves.append( mvSlide(origin_time, delta=0.1,  weight=5.0) )moves.append( mvSlide(origin_time, delta=1,    weight=5.0) )Note that we specified a higher move weight for each of the proposalsoperating on origin_time than we did for the three previousstochastic nodes. This means that our move schedule will propose fivetimes as many updates to origin_time than it will tospeciation_rate, extinction_rate, or psi.The FBD Distribution ObjectAll the parameters of the FBD process have now been specified. The nextstep is to use these parameters to define the FBD tree priordistribution, which we will call fbd_dist.fbd_dist = dnBDSTP(r=0, origin=origin_time, lambda=speciation_rate, mu=extinction_rate, psi=psi, rho=rho, taxa=taxa)Clade ConstraintsNote that we created the distribution as a workspace variable using theworkspace assignment operator =. This is because we still need toinclude a topology constraint in our final specification of the treeprior. Specifically, we do not have any morphological or molecular datafor the fossil species Ursus abstrusus. Therefore, in order to use theage of this fossil as an observation, we need to specify to which cladeit belongs. In this case, Ursus abstrusus belongs to the subfamilyUrsinae, so we define a clade for the total group Ursinae includingUrsus abstrusus.clade_ursinae = clade(\"Melursus_ursinus\", \"Ursus_arctos\", \"Ursus_maritimus\",                       \"Helarctos_malayanus\", \"Ursus_americanus\", \"Ursus_thibetanus\",                       \"Ursus_abstrusus\", \"Ursus_spelaeus\")Then we can specify the final constrained tree prior distribution bycreating a vector of constraints, and providing it along with theworkspace FBD distribution to the constrained topology distribution.Here we use the stochastic assignment operator ~ to create astochastic node for our constrained FBD-tree variable (calledfbd_tree).constraints = v(clade_ursinae)fbd_tree ~ dnConstrainedTopology(fbd_dist, constraints=constraints)It is important to recognize that we do not know if Ursus abstrusus isa crown or stem Ursinae. Because of this, we defined this cladeconstraint so that it constrained the total group Ursinae and thisuncertainty is taken into account. As a result, our MCMC willmarginalize over both stem and crown positions of U. abstrusus andsample the phylogeny in proportion to its posterior probability,conditional on our model and data.Additionally, we do not have morphological data for the fossil speciesParictis montanus. However, we will not create a clade constraint forthis taxon because it is a very old, stem-fossil bear. Thus, the MCMCmay propose to place this taxon anywhere in the tree (except within theclade constraint we made above). This allows us to account for themaximum amount of uncertainty in the placement of P. montanus.Moves on the Tree Topology and Node AgesNext, in order to sample from the posterior distribution of trees, weneed to specify moves that propose changes to the topology (mvFNPR)and node times (mvNodeTimeSlideUniform). Included with these moves isa proposal that will collapse or expand a fossil branch(mvCollapseExpandFossilBranch). This will change a fossil that is asampled ancestor (see  and) so that it is on its own branch and viceversa. In addition, when conditioning on the origin time, we also needto explicitly sample the root age (mvRootTimeSlideUniform).moves.append( mvFNPR(fbd_tree, weight=15.0) )moves.append( mvCollapseExpandFossilBranch(fbd_tree, origin_time, weight=6.0) )moves.append( mvNodeTimeSlideUniform(fbd_tree, weight=40.0) )moves.append( mvRootTimeSlideUniform(fbd_tree, origin_time, weight=5.0) )Sampling Fossil Occurrence AgesNext, we need to account for uncertainty in the age estimates of ourfossils using the observed minimum and maximum stratigraphic ages.To do this, we get all the fossils from the tree and use a for loop to iterate over them.For each fossil observation, we create auniform random variable representing the likelihood. Remember, we canrepresent the fossil likelihood using any uniform distribution that isnon-zero when the likelihood is equal to one(see ).For example, if $t_i$ is the inferred fossil age and $(a_i,b_i)$ is theobserved stratigraphic interval, we know the likelihood is equal to onewhen $a_i &lt; t_i &lt; b_i$, or equivalently $t_i - b_i &lt; 0 &lt; t_i - a_i$. Solet’s represent the likelihood using a uniform random variable uniformlydistributed in $(t_i - b_i, t_i - a_i)$ and clamped at zero.fossils = fbd_tree.getFossils()for(i in 1:fossils.size()){    t[i] := tmrca(fbd_tree, clade(fossils[i]))    a_i = fossils[i].getMinAge()    b_i = fossils[i].getMaxAge()    F[i] ~ dnUniform(t[i] - b_i, t[i] - a_i)    F[i].clamp( 0 )}Finally, we add a move that samples the ages of the fossil nodes on thetree.moves.append( mvFossilTimeSlideUniform(fbd_tree, origin_time, weight=5.0) )Monitoring Parameters of Interest using Deterministic NodesThere are additional parameters that may be of particular interest to usthat are not directly inferred as part of this graphical model. As withthe diversification and turnover nodes specified in, we can createdeterministic nodes to sample the posterior distributions of theseparameters. Create a deterministic node called num_samp_anc thatwill compute the number of sampled ancestors in our fbd_tree.num_samp_anc := fbd_tree.numSampledAncestors();We are also interested in the age of the most-recent-common ancestor(MRCA) of all living bears. To monitor the age of this node in our MCMCsample, we must use the clade function to identify the node.Importantly, since we did not include this clade in our constraints thatdefined fbd_tree, this clade will not be constrained to bemonophyletic. Once this clade is defined we can instantiate adeterministic node called age_extant with the tmrca function thatwill record the age of the MRCA of all living bears.clade_extant = clade(\"Ailuropoda_melanoleuca\",\"Tremarctos_ornatus\",\"Melursus_ursinus\",                    \"Ursus_arctos\",\"Ursus_maritimus\",\"Helarctos_malayanus\",                    \"Ursus_americanus\",\"Ursus_thibetanus\")age_extant := tmrca(fbd_tree, clade_extant)In the same way we monitored the MRCA of the extant bears, we can alsomonitor the age of a fossil taxon that we may be interested inrecording. We will monitor the marginal distribution of the age ofKretzoiarctos beatrix, which is between 11.2–11.8 My.age_Kretzoiarctos_beatrix := tmrca(fbd_tree, clade(\"Kretzoiarctos_beatrix\"))Finally, we will monitor the tree after removing taxa for which we didnot have any molecular or morphological data. The phylogenetic placementof these taxa is based only on their occurrence times and any cladeconstraints we applied (see ).Because no data are available to resolve their relationships to otherlineages, we will treat their placement as nuisanceparameters andremove them from the output.We will remove two fossil taxa, Parictis montanus and Ursusabstrusus, from every tree in the trace file before summarizing thesamples. Use the fnPruneTree function to create a deterministic treevariable pruned_tree from which these taxa have been pruned. We willmonitor this tree instead of fbd_tree.pruned_tree := fnPruneTree(fbd_tree, prune=v(taxa[17],taxa[20]))  You have completed the FBD model file. Save model_FBDP.Rev in the scripts directory.We will now move on to the next model file.The Uncorrelated Exponential Relaxed Clock ModelWe will now define the molecular relaxed clock model.  Open your text editor and create the lineage-specific branch-rate modelfile called model_UExp.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.For our hierarchical, uncorrelated exponential relaxed clock model(described in section  and shown in), we first define the mean branch rate as anexponential random variable. Then, we specify scale proposal moves onthe mean rate parameter.branch_rates_mean ~ dnExponential(10.0)moves.append( mvScale(branch_rates_mean, lambda=0.01, weight=1.0) )moves.append( mvScale(branch_rates_mean, lambda=0.1,  weight=1.0) )moves.append( mvScale(branch_rates_mean, lambda=1.0,  weight=1.0) )Before creating a rate parameter for each branch, we need to get thenumber of branches in the tree. For rooted trees with $n$ taxa, thenumber of branches is $2n-2$.n_branches &lt;- 2 * n_taxa - 2Then, use a for loop to define a rate for each branch. The branch ratesare independent and identically exponentially distributed with meanequal to the mean branch rate parameter we specified above. For eachrate parameter we also create scale proposal moves.for(i in 1:n_branches){    branch_rates[i] ~ dnExp(1/branch_rates_mean)    moves.append( mvScale(branch_rates[i], lambda=1.0,  weight=1.0) )    moves.append( mvScale(branch_rates[i], lambda=0.1,  weight=1.0) )    moves.append( mvScale(branch_rates[i], lambda=0.01, weight=1.0) )}Lastly, we use a vector scale move to propose changes to all branchrates simultaneously. This way we can sample the total branch rateindependently of each individual rate, which can improve mixing.moves.append( mvVectorScale(branch_rates, lambda=0.01, weight=4.0) )moves.append( mvVectorScale(branch_rates, lambda=0.1,  weight=4.0) )moves.append( mvVectorScale(branch_rates, lambda=1.0,  weight=4.0) )You have completed the molecular relaxed clock model file. Save model_UExp.Rev inthe scripts directory.The General Time-Reversible + Gamma Model of Nucleotide Sequence EvolutionIn this section we will define our nucleotide sequence evolution model.  Open your text editor and create the molecular substitution model filecalled model_GTRG.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.For our nucleotide sequence evolution model, we need to define a generaltime-reversible (GTR) instantaneous-rate matrix(i.e. $Q$-matrix). A nucleotide GTR matrixis defined by a set of 4 stationary frequencies, and 6 exchangeabilityrates. We create stochastic nodes for these variables, each drawn from auniform Dirichlet prior distribution.sf_hp &lt;- v(1,1,1,1)sf ~ dnDirichlet(sf_hp)er_hp &lt;- v(1,1,1,1,1,1)er ~ dnDirichlet(er_hp)We need special moves to propose changes to a Dirichlet random variable,also known as a simplex (a vector constrained sum to one). Here, we usea mvSimplexElementScale move, which scales a single element of asimplex and then renormalizes the vector to sum to one. The tuningparameter alpha specifies how conservative the proposal should be,with larger values of alpha leading to proposals closer to the currentvalue.moves.append( mvSimplexElementScale(er, alpha=10.0, weight=5.0) )moves.append( mvSimplexElementScale(sf, alpha=10.0, weight=5.0) )Then we can define a deterministic node for our GTR $Q$-matrix using thespecial GTR matrix function (fnGTR).Q_cytb := fnGTR(er,sf)Next, in order to model gamma-distributed rates across, we create anexponential parameter $\\alpha$ for the shape of the gamma distribution,along with scale proposals.alpha_cytb ~ dnExponential( 1.0 )moves.append( mvScale(alpha_cytb, lambda=0.01, weight=1.0) )moves.append( mvScale(alpha_cytb, lambda=0.1,  weight=1.0) )moves.append( mvScale(alpha_cytb, lambda=1,    weight=1.0) )Then we create a Gamma$(\\alpha,\\alpha)$ distribution, discretized into 4rate categories using the fnDiscretizeGamma function. Here,rates_cytb is a deterministic vector of rates computed as the mean ofeach category.rates_cytb := fnDiscretizeGamma( alpha_cytb, alpha_cytb, 4 )Finally, we can create the phylogenetic continuous time Markov chain(PhyloCTMC) distribution for our sequence data, including thegamma-distributed site rate categories, as well as the branch ratesdefined as part of our exponential relaxed clock. We set the value ofthis distribution equal to our observed data and identify it as a staticpart of the likelihood using the clamp method.phySeq ~ dnPhyloCTMC(tree=fbd_tree, Q=Q_cytb, siteRates=rates_cytb, branchRates=branch_rates, type=\"DNA\")phySeq.clamp(cytb)  You have completed the GTR model file. Save model_GTRG.Rev inthe scripts directory.We will now move on to the next model file.Modeling the Evolution of Binary Morphological CharactersIn this section we will define the model of morphological character evolution.  Open your text editor and create the morphological character model filecalled model_Morph.Rev in the scripts directory.As stated in the introduction () we willuse Mk to model our data. Because the Mk model is a generalization ofthe Jukes-Cantor model, we will initialize our Q matrix from a Jukes-Cantormatrix.Q_morpho := fnJC(2)As in the molecular data partition, we will allow gamma-distributed rateheterogeneity among sites.alpha_morpho ~ dnExponential( 1.0 )rates_morpho := fnDiscretizeGamma( alpha_morpho, alpha_morpho, 4 )moves.append( mvScale(alpha_morpho, lambda=0.01, weight=5.0) )moves.append( mvScale(alpha_morpho, lambda=0.1,  weight=3.0) )moves.append( mvScale(alpha_morpho, lambda=1,    weight=1.0) )The phylogenetic model also assumes that each branch has a rate ofmorphological character change. For simplicity, we will assume a strictexponential clock—meaning that every branch has the same rate drawn froman exponential distribution (see ).clock_morpho ~ dnExponential(1.0)moves.append( mvScale(clock_morpho, lambda=0.01, weight=4.0) )moves.append( mvScale(clock_morpho, lambda=0.1,  weight=4.0) )moves.append( mvScale(clock_morpho, lambda=1,    weight=4.0) )As in our molecular data partition, we now combine our data and ourmodel in the phylogenetic CTMC distribution. There are some uniqueaspects to doing this for morphology.You will notice that we have an option called coding. This optionallows us to condition on biases in the way the morphological data werecollected (ascertainment bias). The option coding=variable specifiesthat we should correct for coding only variable characters (discussed in).phyMorpho ~ dnPhyloCTMC(tree=fbd_tree, siteRates=rates_morpho, branchRates=clock_morpho, Q=Q_morpho, type=\"Standard\", coding=\"variable\")phyMorpho.clamp(morpho)You have completed the morphology model file. Save model_Morph.Rev inthe scripts directory.We will now move on to the next model file.Complete Master Rev File  Return to the master Rev file you created in section called mcmc_CEFBDP_Specimens.Rev in the scripts directory.  Enter the Rev code provided in this section in this file.Source Model ScriptsRevBayes uses the source function to load commands from Revfiles into the workspace. Use this function to load in the model scriptswe have written in the text editor and saved in the scripts directory.source(\"scripts/model_FBDP.Rev\") # FBD tree priorsource(\"scripts/model_UExp.Rev\") # UExp relaxed clocksource(\"scripts/model_GTRG.Rev\") # Molecular substitution model (GTR+G)source(\"scripts/model_Morph.Rev\") # Morphological character change modelCreate Model ObjectWe can now create our workspace model variable with our fully specifiedmodel DAG. We will do this with the model function and provide asingle node in the graph (sf).mymodel = model(sf)The object mymodel is a wrapper around the entire model graph andallows us to pass the model to various functions that are specific toour MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our master Rev file is to specify themonitors and output file names. For this, we create a vector calledmonitors that will each sample and record or output our MCMC.monitors = VectorMonitors()The first monitor we will create will monitor every named randomvariable in our model graph. This will include every stochastic anddeterministic node using the mnModel monitor. The only parameter thatis not included in the mnModel is the tree topology. Therefore, theparameters in the file written by this monitor are all numericalparameters written to a tab-separated text file that can be opened byaccessory programs for evaluating such parameters. We will also name theoutput file for this monitor and indicate that we wish to sample ourMCMC every 10 cycles.monitors.append( mnModel(filename=\"output/bears.log\", printgen=10) )The mnFile monitor writes any parameter we specify to file. Thus, ifwe only cared about the speciation rate and nothing else (this is not atypical or recommended attitude for an analysis this complex) wewouldn’t use the mnModel monitor above and just use the mnFilemonitor to write a smaller and simpler output file. Since the treetopology is not included in the mnModel monitor (because it is notnumerical), we will use mnFile to write the tree to file by specifyingour pruned_tree variable in the arguments. Remember, we aremonitoring the tree with nuisance taxa pruned out (see).monitors.append( mnFile(filename=\"output/bears.trees\", printgen=10, pruned_tree) )The last monitor we will add to our analysis will print information tothe screen. Like with mnFile we must tell mnScreen which parameterswe’d like to see updated on the screen. We will choose the age of theMCRCA of living bears (age_extant), the number of sampled ancestors(num_samp_anc), and the origin time (origin_time).monitors.append( mnScreen(printgen=10, age_extant, num_samp_anc, origin_time) )Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now createthe workspace variable that defines our MCMC run. We do this using themcmc function that simply takes the three main analysis componentsas arguments.mymcmc = mcmc(mymodel, monitors, moves)The MCMC object that we named mymcmc has a member method calledrun. This will execute our analysis and we will set the chainlength to 10000 cycles using the generations option.mymcmc.run(generations=10000)Once our Markov chain has terminated, we will want RevBayes to close.Tell the program to quit using the q() function.q()  You made it! Save all of your files.Execute the MCMC AnalysisWith all the parameters specified and all analysis components in place,you are now ready to run your analysis. The Rev scripts you justcreated will all be used by RevBayes and loaded in the appropriateorder.Begin by running the RevBayes executable. In Unix systems, type thefollowing in your terminal (if the RevBayes binary is in your path):Provided that you started RevBayes from the correct directory, you can then use thesource function to feed RevBayes your master script file(mcmc_CEFBDP_Specimens.Rev).source(\"scripts/mcmc_CEFBDP_Specimens.Rev\")This will execute the analysis and you should see the various parameters you included when you created mnScreen printed to the screen every 10 generations.When the analysis is complete, RevBayes will quit and you will have anew directory called output that will contain all of the files youspecified with the monitors (see ).Evaluate and Summarize Your ResultsIn this section, we will evaluate the mixing and convergence of ourMCMC simulation using the program Tracer. We can alsosummarize the marginal distributions for particular parameters we’reinterested in. Tracer(Rambaut and Drummond 2011) is a tool for visualizing parameters sampled by MCMC.This program is limited to numerical parameters, however, and cannot beused to summarize or analyze MCMC samples of the tree topology (thiswill be discussed further in ).Open Tracer and import the bears.log file in theFile &gt; Import New Trace Files. Or click the button on theleft-hand side of the screen to add your log file (see ).The Tracerwindow. To add data, click on the “+” sign, highlighted in red aboveImmediately upon loading your file (see ),you will see the list of Trace Files on the left-handside (you can load multiple files). The bottom left section, calledTraces, provides a list of every parameter in the logfile, along with the mean and the effective sample size (ESS) for theposterior sample of that parameter. The ESS statistic provides a measureof the number of independent draws in our sample for a given parameter.This quantity will typically be much smaller than the number ofgenerations of the chain. In Tracer, poor to fair valuesfor the ESS will be colored red and yellow. You will likely see a lot ofred and yellow numbers because the MCMC runs in this exercise are tooshort to effectively sample the posterior distributions of mostparameters. For most MCMC analyses, it is recommended to run the chain for much longer so that you get an adequate sample from the target distribution.The Estimates window in Tracer showing thehistogram of the PosteriorThe inspection window for your selected parameter is theEstimates window, which shows a histogram and summarystatistics of the values sampled by the Markov chain.  shows the marginal distribution of thePosterior statistic for the bears.log file for an analysis run for 10,000 generations.  Look through the various parameters and statistics in the list ofTraces.  ⇨ Are there any parameters that have really low ESS? Why do you think that might be?Next, we can click over to the Trace window. Thiswindow shows us the samples for a given parameter at each iteration ofthe MCMC. The left side of the chain has a shaded portion that has beenexcluded as “burn-in”. Samples taken near the beginning of chain areoften discarded or “burned” because the MCMC may not immediately beginsampling from the target posterior distribution, particularly if thestarting condition of the chain is far from the region of highestposterior density.  shows thetrace for the extinction rate.The Trace window in Tracer. This windowshows a line plot of every sampled value for the extinction rate thatwas saved to file. The lighter shaded portion is the set of samplesdiscarded as “burn-in” and are not used to compute the summarystatistics found in the Estimates window.The Trace window allows us to evaluate how well ourchain is sampling the target distribution. For a fairly short analysis,the output in  shows reasonablemixing—there is no consistent pattern or trend in the samples, nor arethere long intervals where the statistic does not change. The presenceof a trend or large leaps in a parameter value might indicate that yourMCMC is not mixing well. You can read more about MCMC tuning andimproving mixing in the tutorials Introduction to Markov chain Monte Carlo (MCMC) Sampling.  Look through the traces for your parameters.  ⇨ Are there any parameters in your log files that show trends or large leaps?  ⇨ What steps might you take to solve these issues?In Tracer you can view the marginal probabilitydistributions of your parameters in the Marginal Prob Distribution window. Using this tool, you can compare thedistributions of several different parameters (by selecting them both).  Go to the diversification parameter in the Marginal Prob Distribution window.  ⇨ What is the mean value estimatedfor the net diversification rate ($d$)?  ⇨ What does the marginaldistribution tell you about the net diversification? (Hint:$d = \\lambda - \\mu$)While specifying the model, remember that we created severaldeterministic nodes that represent parameters that we would like toestimate, including the net diversification rate. Tracerallows us to view the summaries of these parameters since they appear inour log files.  Go to the age_extant parameter in the Estimateswindow.  ⇨ What is the mean and 95% highest posterior density of the age of the MRCA for all living bears?Since you have evaluated several of the parameters by viewing the tracefiles and the ESS values, you may be aware that the MCMC analysis youconducted for this tutorial did not sufficiently sample the jointposterior distribution of phylogenetic parameters. More explicitly,your run has not converged. It is not advisable to base yourconclusions on such a run and it will be critical to perform multiple,independent runs for many more MCMC cycles. For further discussion ofrecommended MCMC practices in RevBayes, please see the Introduction to Markov chain Monte Carlo (MCMC) Samplingtutorials.Summarize TreeIn addition to evaluating the performance and sampling of an MCMC runusing numerical parameters, it is also important to inspect the sampledtopology and tree parameters. This is a difficult endeavor, however. Onetool for evaluating convergence and mixing of the tree samples isRWTY (Warren et al. 2016). In thistutorial, we will only summarize the sampled trees, but we encourage youto consider approaches for assessing the performance of the MCMC withrespect to the tree topology.Ultimately, we are interested in summarizing the sampled trees andbranch times given that our MCMC has sampled all of the importantparameters in proportion to their posterior probabilities. RevBayesincludes some functions for summarizing the tree topology and other treeparameters.We will complete this part of the tutorial using RevBayesinteractively.  Begin by running the RevBayes executable. You should dothis from within the tutorial directory.Read in the MCMC sample of trees from file.trace = readTreeTrace(\"output/bears.trees\")By default, a burn-in of 25% is used when creating the tree trace (250trees in our case). You can specify a different burn-in fraction, say50%, by typing the command trace.setBurnin(500).Now we will use the mccTree function to return a maximum cladecredibility (MCC) tree. The MCC tree is the tree with the maximumproduct of the posterior clade probabilities. When considering treeswith sampled ancestors, we refer to the maximum sampled ancestor cladecredibility (MSACC) tree (Gavryushkina et al. 2017).mccTree(trace, file=\"output/bears.mcc.tre\" )When there are sampled ancestors present in the tree, visualizing thetree can be fairly difficult in traditional tree viewers. We will makeuse of a browser-based tree viewer calledIcyTree, created by TimVaughan. IcyTree has manyunique options for visualizing phylogenetic trees and can producepublication-quality vector image files(i.e. SVG). Additionally, it correctlyrepresents sampled ancestors on the tree as nodes, each with only onedescendant ().Maximum sampled ancestor cladecredibility (MSACC) tree of bear species used in this tutorial. Numbersabove fossil nodes indicate the posterior probability of being a sampledancestorNavigate to https://icytree.org and open the fileoutput/bears.mcc.tre in IcyTree.  Try to replicate the tree in  (Hint: Style &gt; Mark Singletons)  ⇨ Why might a node with a sampled ancestor bereferred to as a singleton?  ⇨ How can you see the names of the fossils that are putative sampled ancestors?  Try mousing over differentbranches (see ). What are the fieldstelling you?  ⇨ What is theposterior probability that Zaragocyon daamsi is a sampled ancestor?Highlighting a branch in IcyTree.Another newly available web-based tree viewer isPhylogeny.IO (Jovanovic and Mikheyev 2016). Try this site fora different way to view the tree.",
        "url": "/tutorials/fbd/fbd_specimen.html",
        "index": "true"
      }
      ,
    
      "tutorials-dating-fbdr-html": {
        "title": "Molecular dating",
        "content": "Exercise 4In exercise 3 we inferred the phylogeny of extant bears and a timeline for their evolution using node dating. Node dating has several caveats, one which is that we hardly used any of the available fossil information.In this exercise we will explictly incorporate information from the fossil record into the diversification process (i.e. the tree model). The fossilized birth-death (FBD) process is a model that assumes extant samples and fossils were generated by the same underlying evolutionary process (Stadler 2010; Heath et al. 2014). The model is an extension of the birth-death process that incorporates the fossil recovery process, meaning extinct samples are directly incorporated as part of the tree and may be sampled along internal branches. This is in contrast to node dating, where we only used information from the fossil record to constrain the node ages but the fossils were not assummed to be part of the tree.If a fossil falls directly along branch that has descendant samples in tree, it is referred to as a sampled ancestor.The data we have available for the fossil bear species are stratigraphic ranges. This means that for each species we have dates for the first and last appearances, instead of fossil occurrence data. We will therefore use the fossilized birth-death range process (Stadler et al. 2018), rather than the original FBD model, which is more suitable for specimen level datasets.A graphical model of the fossilized birth-death model describing the generation of the time tree (in) used in this tutorial. The parameters of thefossilized birth-death process are labeled in orange. The speciation,extinction and fossilization rates are stochastic nodes (circles) drawnfrom exponential distributions, while the origin time is uniformlydistributed. The sampling probability is constant node (square) andequal to one for the tree in  and for the analysis in the exercise given in this tutorial. This model represents the phylogenetic continuous-time Markovchain that links the tree model to the other model components and theobserved sequence data. For more information on probabilistic graphicalmodels and their notation, please see (Höhna et al. 2014).For a more comprehensive description of these models check out the tutorials written by Tracy Heath, April Wright and Walker Pett.  Combined-Evidence Analysis and the Fossilized Birth-Death Process for Stratigraphic Range Data  Combined-Evidence Analysis and the Fossilized Birth-Death Process for Analysis of Fossil and Extant SpecimensThe dataThe sequence data used in this exercise remains the same as the previous exercises (bears_cytb.nex). We will also use the same substitution and clock models (the GTR + $\\Gamma$ model and the uncorrelated exponential relaxed clock model).In addition, we’ll take advantage of all the stratigraphic age information available for 20 species in bears_taxa.tsv, where max records the age of the first appearance (i.e. the oldest) and min records the age of the last appearance (i.e. the youngest) of the species. Time 0.0 represents the present and means the species is extant. We will use the fossilized birth-death process as our tree model, which allows us to directly incorporate fossils into the tree. Note that in this exercise, we will not be including any character for the 12 fossil taxa. This means that we cannot resolve the relationships of these taxa, however, the fossil sampling times are still informative about the FBD model parameters and divergence times.There’s quite a few additional steps needed to set up this tree model.The master Rev scriptThis time we will start by making some changes to the master Rev script.  Copy the master script from the previous exercise and call it MCMC_dating_ex4.Rev.First, at the beginning of you script add the command to read the full list of taxon names from the bears_taxa.tsv file using the readTaxonData function.taxa &lt;- readTaxonData(\"data/bears_taxa.tsv\")This function reads a tab-delimited file and creates a variable called taxa that is a list of all of the taxon names relevant to this analysis, including the minimum/maximum ages.Delete the helper variable taxa that we created earlier from the alignment  using the command taxa &lt;- cytb.taxa().Add missing taxaRemember that we only have molecular sequence data for the living species. Thus, we must add any taxa that are not found in the molecular (cytb) partition (i.e. are only found in the fossil data) to that data matrix as missing data (with “?” in place of all characters). In order for all the taxa to appear on the same tree, they all need to be part of the same dataset, as opposed to present in separate datasets. This ensures that there is a unified taxon set that contains all of our tips. We can do this using the cytb.addMissingTaxa function.cytb.addMissingTaxa( taxa )The tree model  Start by creating a copy of your tree_BD.Rev script, call it tree_FBD.Rev and open it in your text editor.The fossilized birth-death processThe FBD model has some additional parameters we need to specify. In addition to speciation ($\\lambda$), extinction ($\\mu$) and extant species sampling ($\\rho$), we need to specify the fossil recovery rate ($\\psi$), plus the moves on this parameter. Include the following commands anywhere prior to specifying the timetree variable.psi ~ dnExponential(10) moves.append( mvScale(psi, lambda=0.5, tune=true, weight=3) )Next delete the command used to specify the root age extant_mrca.We will condition the FBD process on the origin time ($\\phi$), and specify a uniform distribution on this parameter, with a minimum equal to the oldest first appearance of bears (= 37.2 Ma) and a maximum equal to a prior estimate for the age of caniforms (= 49.0 Ma) that we used in the previous exercise.Create a stochastic node for the origin time parameter.origin_time ~ dnUnif(37.2, 49.0)For this parameter, we will use a sliding window move (mvSlide). A sliding window samples a parameter uniformly within an interval (defined by the half-width delta). Sliding window moves can be tricky for small values, as the window may overlap zero. However, for parameters such as the origin age, there is little risk of this being an issue.moves.append( mvSlide(origin_time, delta=1.0, tune=true, weight=5.0) )Note that the biological interpretation of this parameter is not that straightforward, so we can think of this as a nuisance parameter.Next, we need to change the tree model distribution from the birth-death process to the FBD range process using the dnFBDRP command. Note that this function takes a different set of arguments to dnBDP.tree_dist = dnFBDRP(lambda=speciation_rate, mu=extinction_rate, psi=psi, rho=rho, origin=origin_time, taxa=taxa)Clade constraintsAgain we’ll constrain the clade Ursinae to be monophyletic, but this time we have one fossil species to add this constraint: Ursus abstrusus. Add this taxon to your clade constraints before specifying the stochastic timetree node.clade_ursinae = clade(\"Melursus_ursinus\", \"Ursus_arctos\", \"Ursus_maritimus\",                   \"Helarctos_malayanus\", \"Ursus_americanus\", \"Ursus_thibetanus\", \"Ursus_abstrusus\") constraints = v(clade_ursinae)Moves on the treeIn addition to the move we applied previously to sample the tree topology, mvFNPR, we also need to add an extra move mvCollapseExpandFossilBranch that will change a fossil that is a sampled ancestor so that it is on its own branch and vice versa.moves.append( mvCollapseExpandFossilBranch(timetree, origin_time, weight=6.0) )In addition, when conditioning on the origin time, we also need to explicitly sample the root age (mvRootTimeSlideUniform).moves.append( mvRootTimeSlideUniform(timetree, origin_time, weight=5.0) )Monitoring parameters of interest using deterministic nodesWe are still interested in the age of the MRCA of all living bears (extant_mrca). However, since now we have extinct species in our tree, the root of the tree may not represent the MRCA of all extant species. To monitor the age of this node in our MCMC sample, we must use the clade function to define the node. Importantly, since we did not include this clade in our constraints that defined timetree, this clade will not be constrained to be monophyletic. Once this clade is defined we can instantiate a deterministic node called extant_mrca with the tmrca function that will record the age of the MRCA of all living bears.clade_extant = clade(\"Ailuropoda_melanoleuca\",\"Tremarctos_ornatus\",\"Melursus_ursinus\",                    \"Ursus_arctos\",\"Ursus_maritimus\",\"Helarctos_malayanus\",                    \"Ursus_americanus\",\"Ursus_thibetanus\")extant_mrca := tmrca(timetree, clade_extant)We can also create deterministic node to compute and keep track of the number of sampled ancestors (num_samp_anc) in our timetree.num_samp_anc := timetree.numSampledAncestors()Finally, we will monitor the tree after removing taxa for which we did not include any character data. The phylogenetic placement of these taxa is based only on their occurrence times and any clade constraints we applied. Because no data are available to resolve their relationships relative to other lineages, we will treat their placement as nuisance parameters and remove them from the output. Create a vector of extinct taxa and use the function fnPruneTree to create a deterministic node for the tree excluding extinct samples.extinct_sp = v(\"Agriarctos_spp\", \"Ailurarctos_lufengensis\", \"Ballusia_elmensis\",\t\t\t\t\"Indarctos_arctoides\", \"Indarctos_punjabiensis\", \"Indarctos_vireti\",\t\t\t\t\"Kretzoiarctos_beatrix\", \"Parictis_montanus\", \"Ursavus_brevirhinus\",\t\t\t\t\"Ursavus_primaevus\", \"Ursus_abstrusus\", \"Zaragocyon_daamsi\")pruned_tree := fnPruneTree(timetree, prune=extinct_sp)Back to the master Rev scriptAs before, change the file used to specify the tree model from tree_BD_nodedate.Rev to tree_FBD.Rev.source(\"scripts/tree_FBD.Rev\")Second, update the name of the output files. We also need to update the name of the tree we’re printing to file from the timetree variable that incorporates extinct taxa to the pruned_tree variable that includes our extant species only.monitors.append( mnModel(filename=\"output/bears_FBD.log\", printgen=10) )monitors.append( mnFile(filename=\"output/bears_FBD.trees\", printgen=10, pruned_tree) )Don’t forget to update the commands used to generate the summary tree.trace = readTreeTrace(\"output/bears_FBD.trees\")mccTree(trace, file=\"output/bears_FBD.mcc.tre\" )  Run your MCMC analysis!Examining the outputLet’s examine the output in Tracer and compare our results to those generated using node dating.  Open the program Tracer and load the log files bears_FBD.log and bears_nodedate.log.Compare the node age estimates for the MRCA of extant bears. What do you notice about the different marginal densities?The Marginal Density panel in Tracer showing the posterior estimates for the MRCA of extant bears using two different calibration appraoches (node dating versus the FBD model). To reproduce this figure select Colour by: Trace file and Legend: Top-Right.You may also notice that the age recovered for age_ursinae is also a bit different but we need to be careful - the estimates obtained for this variable between the node dating and FBD analyses are not directly comparable. Recall that we added an additional fossil taxon to the Ursinae constraint in this analysis. What steps would you have taken to recover an equivalent parameter, without eliminating the clade_ursinae constraint?The tree outputHave a quick look at your MCC tree in FigTree. How does it compare the tree your recovered using node dating?NextIt’ll be more fun to examine the tree output once we infer the phylognetic position of extinct taxa and sampled ancestors, which we’ll do in the next exercise by incorporating morphological character data.  Click below to begin the next exercise!  Estimating speciation times using total-evidence datingFor further options and information about the models used in this exercise see Tracy Heath, April Wright and Walker Pett’s tutorial Combined-Evidence Analysis and the Fossilized Birth-Death Process for Stratigraphic Range Data",
        "url": "/tutorials/dating/fbdr.html",
        "index": "false"
      }
      ,
    
      "workshops-fieldmuseum2018-html": {
        "title": "Bayesian Phylogenetics in RevBayes",
        "content": "",
        "url": "/workshops/fieldmuseum2018.html",
        "index": ""
      }
      ,
    
      "tutorials-format": {
        "title": "Tutorial Format",
        "content": "These tutorials will provide command line text and code in specific formats.All commands that are intended to be executed in your Unix terminal will be shown in gray boxes with the $ prompt. For example:cd my_directorypwdAll Rev code will be shown in light blue boxes with the &gt; prompt:n = 12;for(i in 1:n) {\t#this is a comment\ti++;}All output the RevBayes console will be given in light blue boxes with gray text:/home/my_directorySome tutorials will provide instructions for you that will be highlighted by a box. These instructions will guide you to create new files or other tasks that are not completed in the RevBayes console.  Make a new file called m_gtr.Rev.",
        "url": "/tutorials/format",
        "index": ""
      }
      ,
    
      "tutorials-intro-getting-started-html": {
        "title": "Getting Started with RevBayes",
        "content": "OverviewRevBayes has as its central idea that any statistical model, forexample a phylogenetic model, is composed of smaller parts that can bedecomposed and put back together in a modular fashion (Höhna et al. 2016).This comes from considering (phylogenetic) models as probabilisticgraphical models, which lends flexibility and enhances the capabilitiesof the program. Users interact with RevBayes via an interactive shell.Users communicate commands using a language specifically designed forRevBayes, called Rev; an R-like language (complete with controlstatements, user-defined functions, and loops) that enables the user tobuild up (phylogenetic) models from simple parts (random variables,variable/parameter transformations, models, and constants of differentsorts).Here we assume that you have successfully installed RevBayes. If thisisn’t the case, then please consult our website on how to installRevBayes.Getting StartedFor the examples outlined in each tutorial, we will use RevBayesinteractively by typing commands in the command-line console. For theexercises you can either use RevBayes interactively or run an entirescript. Execute the RevBayes binary. If this program is in your path,then you can simply type in your Unix terminal:rbWhen you execute the program, you will see a brief program information,including the current version number. Remember that more information canbe obtained from revbayes.github.io. When you executethe program with an additional filename,e.g.,rb my_analysis.Revthen RevBayes will run all commands specified in your file.You may want to run RevBayes in parallel using multiple processes.This can be done by starting RevBayes withmpirun -np 4 rb-mpiwhich starts 4 processes of RevBayes. You may want to change thenumber of processes depending on your available hardware.The format of the exercises uses to delineate code examples that youshould type into RevBayes. For example, after opening the RevBayesprogram, you can load your data file:data &lt;- readDiscreteCharacterData(\"data/primates_cytb.nex\")Examples can be copied and pasted directly as the RevBayes “&gt;” will not be copied. This is especially useful for larger command blocks, particularly loops, which will oftenbe displayed in boxesfor( i in 1:12 ){   x[i] ~ dnExponential(1.0)}The various RevBayes commands and syntax within the text are specifiedusing typewriter text.Most tutorials also includes hyperlinks: bibliographic citations such as Höhna et al. (2014) which link to the full citation in the references, external URLs such as lognormaldistribution, andinternal references to figures and equations such as .The various exercises in this tutorial take you through the stepsrequired to perform phylogenetic analyses of the example datasets. Inaddition, we have provided the Rev scripts and output files for someexercises so you can verify your results. (Note that since the MCMC runsyou perform will start from different random seeds, the output filesresulting from your analyses will not be identical to the ones weprovide you.)Probabilistic Graphical ModelsRevBayes uses probabilistic graphical models for modelspecification, visualization, and implementation (Höhna et al. 2014).Graphical models are frequently used in machine learning and statisticsto conceptually represent the conditional dependence structure ofcomplex statistical models with many parameters(missing reference). Thegraphical model framework allows for flexible model specification andimplementation and reduces redundant code. This framework provides a setof symbols for depicting a directed acyclicgraph (DAG).Höhna et al. (2014) described the use of probabilistic graphical models forphylogenetics. The different nodes and components of a phylogeneticgraphical model are shown in  [Fig. 1 fromHöhna et al. (2014)].The symbols for avisual representation of a graphical model. a) Solid squares representconstant nodes, which specify fixed-valued variables. b) Stochasticnodes are represented by solid circles. These variables correspond torandom variables and may depend on other variables. c) Deterministicnodes (dotted circles) indicate variables that are determined by aspecific function applied to another variable. They can be thought of asvariable transformations. d) Observed states are placed in clampedstochastic nodes, represented by gray-shaded circles. e) Replicationover a set of variables is indicated by enclosing the replicated nodesin a plate (dashed rectangle). [Partially reproduced from Fig. 1 inHöhna et al. (2014).]To represent the DAG, nodes are connected with arrows indicatingdependency. A simple, albeit abstract, graphical model is shown in. In this model, we observe a set of states forparameter $x$. We assume that the values of $x$ are samples from alognormal distribution with a location parameter (log mean) $\\mu$ and astandard deviation $\\sigma$. It is more straightforward to model ouruncertainty in the expectation of a lognormal distribution, rather than$\\mu$, thus we place a gamma distribution on the mean $M$. This gammahyperprior has two parameters that we specify with fixed values(constant nodes): the shape $\\alpha$ and rate $\\beta$. The variable $M$is a stochastic node with this prior density. The standard deviation,$\\sigma$, is also a stochastic node with an exponential prior densitywith rate parameter $\\lambda$. For any value of $M$ and any value of$\\sigma$ we can compute the deterministic variable $\\mu$ using theformula $\\mu = \\ln(M) - \\frac{\\sigma^2}{2}$. This formula is known fromusing simple algebra on the equation for the mean of any lognormaldistribution.With this model structure, we can then calculate the probability of thedata conditional on the model and parameter values (the likelihood):$\\mathbb{P}(\\boldsymbol{x} \\mid \\mu, \\sigma)$. Next we can get theposterior probability using Bayes’ theorem:\\(\\mathbb{P}(M,\\sigma \\mid \\boldsymbol{x}, \\alpha, \\beta, \\lambda) = \\frac{\\mathbb{P}(\\boldsymbol{x} \\mid \\mu, \\sigma) \\mathbb{P}(M \\mid \\alpha,\\beta) \\mathbb{P}(\\sigma \\mid \\lambda)}{\\mathbb{P}(\\boldsymbol{x})}.\\)Graphical modelrepresentation of a simple lognormal model. A total of $N$ observationsof variable $x$ are observed and occupy a clamped node. This parameteris log-normally distributed with parameters $\\mu$ and $\\sigma$ (log meanand standard deviation, respectively). The parameter $\\mu$ is adeterministic node that is calculated from the stochastic nodes $M$ (themean of the distribution) and $\\sigma$. Dotted arrows indicatedeterministic functions and are used to connect deterministic nodes totheir parent variables. A gamma distribution is applied as a hyperprioron $M$ with constant nodes for the shape $\\alpha$ and rate $\\beta$.Thestochastic variable $\\sigma$ is exponentially distributed with fixedvalue for the rate $\\lambda$.Rev: The RevBayes LanguageIn RevBayes models and analyses are specified using an interpretedlanguage called Rev. Rev bears similarities to the compiled languagein WinBUGS and the interpreted Rlanguage. Setting up andexecuting a statistical analysis in RevBayes requires the user tospecify all of the parameters of their model and the type of analysis(e.g.,an MCMC run). By using an interpretedlanguage, RevBayes enables the practitioner to build complex,hierarchical models and to check the current states of variables whilebuilding the model. This will be very useful in the beginning. Later onyou, when you run very complex analyses, you may want to writeRev-scripts.Differently to Rand BUGS, Rev is a strongly butimplicitly typed language. It is implicitly typed, and thus similar toPython, because you do not need to provide the type of a variable (whichyou need to in languages such as C++ and Java). We do implicit typing tohelp users who do not know about the actual types of the variables.However, strongly typed means that every variable has a type andarguments of functions need to match the required types. The strong typerequirements ensures that you build meaningful model graphs. Forexample, the variance parameter of a normal distribution needs to be apositive number, and thus you can only use variables that are positivereal numbers. RevBayes does automatic type conversion.Specifying ModelsRev assignment operators, clamp function, and plate/loop syntax.            Operator      Variable                  &lt;-      constant variable              ~      stochastic variable              :=      deterministic variable              node.clamp(data)      observed/fixed stochastic variable              =      inference (i.e.,non-model) variable              for(i in 1:N){...}      plate      The variables/parameters of a statistical model are created usingdifferent operators in Rev (). In Figure[revgmexample], the Rev syntax for creating the model in Figure[simpleGM] is provided. Because Rev is an interpreted language, itis important to consider the order in which you specify your variables(cf.BUGS where the order is not important).Thus, typically the first variables that are instantiated are constantvariables. Constant variables require you to assign a fixed value usingthe &lt;- operator. Stochastic variables are initialized using the ~operator followed by the constructor function for a distribution. InRev, the naming convention for distributions is dn*, where * isa wildcard representing the name of the distribution. Each distributionfunction requires hyperparameters passed in as arguments. This iseffectively linking nodes using arrows in the graphical model. Thefollowing code snippet creates a stochastic variable called M which isassigned a gamma-distributed hyperprior, with shape alpha and ratebeta:alpha &lt;- 2.0beta &lt;- 4.0M ~ dnGamma(alpha, beta)The flexibility gained from the graphical model framework and theinterpreted language allows you to easily change a model by swappingcomponents. For example, if you decide that a bimodal lognormaldistribution is a better representation of your uncertainty in M, thenyou can simply change the distribution associated with M (afterinitializing the bimodal lognormal hyperparameters):mean_1 &lt;- 0.5mean_2 &lt;- 2.0sd_1 &lt;- 1.0sd_2 &lt;- 1.0weight &lt;- 0.5M ~ dnBimodalLognormal(mean_1, mean_2, sd_1, sd_2, weight)Rev does allow you to specify constant-variable values in thedistribution constructor function; therefore this also works:M ~ dnBimodalLognormal(0.5, 2.0, 1.0, 1.0, 0.5)Both ways to specify priors are equivalent. The only difference is thatone code may be more readable than the other.Specifying a model withRev. The graphical model of the observed parameter $x$ is shown on theleft. In this example, $x$ is log-normally distributed with a locationparameter of $\\mu$ and a standard deviation of $\\sigma$, thus$x \\sim \\mbox{Lognormal}(\\mu, \\sigma)$. The expected value of $x$ (ormean) is equal to $M$: $\\mathbb{E}(x) = M$. In this model, $M$ and$\\sigma$ are random variables and each are assigned hyperpriors. Weassume that the mean is drawn from a gamma distribution with shapeparameter $\\alpha$ and rate parameter $\\beta$:$M \\sim \\mbox{Gamma}(\\alpha, \\beta)$. The standard deviation of thelognormal distribution is assigned an exponential hyperprior with rate$\\lambda$: $\\sigma \\sim \\mbox{Exponential}(\\lambda)$. Since we areconditioning our model on the expectation, we must compute thelocation parameter ($\\mu$) to calculate the probability of our model.Thus, $\\mu$ is a deterministic node that is the result of afunction executed on $M$ and $\\sigma$:\\(\\mu = \\ln(M) - \\frac{\\sigma^2}{2}\\). Since we observe values of $x$, weclamp this node.Deterministic variables are parameter transformations and initializedusing the := operator followed by the function or formula forcalculating the value. Previously we created a variable for theexpectation of the lognormal distribution. Now, if you have anexponentially distributed stochastic variable $\\sigma$, you can create adeterministic variable for the mean $\\mu$:lambda &lt;- 1.0sigma ~ dnExponential(lambda)mu := ln(M) - (sigma^2)/2.0Replication over lists of variables as a plate object is specified usingfor loops. A for-loop is an iterator statement that performs afunction a given number of times. In Rev you can use this syntax tocreate a vector of 7 stochastic variables, each drawn from a lognormaldistribution:for( i in 1:7 ) {   x[i] ~ dnLognormal(mu, sigma)}The for loop executes the statement x[i] ~ dnLognormal(mu, sigma) for different values of $i$ repeatedly, where $i$ takes thevalues 1 to 7. Thus, we created a vector $x$ of seven variables, eachbeing independent and identically distributed (i.i.d.).A clamped node/variable has observed data attached to it. Thus, you mustfirst read in or input the data, then clamp it to a stochastic variable.In  the observations are assigned and clamped tothe stochastic variables. If we observed 7 values for $x$ we wouldcreate 7 clamped variables:observations &lt;- [0.20, 0.21, 0.03, 0.40, 0.65, 0.87, 0.22]N &lt;- observations.size()for( i in 1:N ){   x[i].clamp(observations[i])}You may notice that the value of $x$ has now changed and is equal to theobservations.Getting help in RevBayesRevBayes provides an elaborate help system. Most of the help is foundonline on our website. Within RevBayes you can display the help fora function, distribution or any other type using the ? symbol followedby the command you want help for:?dnNorm?mcmc?mcmc.runAdditionally, RevBayes will print the correct usage of a function ifyou only type in its name and hit return:mcmc       MCMC function (Model model, Monitor[] monitors, Move[] moves, String moveschedule = \"sequential\" | \"random\" | \"single\", Natural nruns)RevBayes Users’ ForumAn email list has been created for users of RevBayes to discussRevBayes-related topics, including: RevBayes installation and use,scripting and programming, phylogenetics, population genetics, models ofevolution, graphical models, etc. The forum is hosted by Google Groups: revbayes-users",
        "url": "/tutorials/intro/getting_started.html",
        "index": "true"
      }
      ,
    
      "developer-git-flow-html": {
        "title": "RevBayes Git Workflow",
        "content": "  Overview  Contribution processWorking on the RevBayes repositoryThe main branches of the RevBayes repo exist are  master  developmentThe master branch should always reflect the state of the current release ofRevBayes. The development branch contains the working additions/changes to thecode that are to be included in the next release.You should not work on either of these branches directly. Rather, to make changes or work on a new feature, you should create a separate branch off of development. While working on your branch, frequently merge changes from development to stay up to date.Once your work is ready, and before you merge your branch into development,make sure to merge any changes from development and verify the code iscompiling and tests are passing.Once these checks have been done, create a pull request to merge your branch into development. You can request reviewers for your pull request directly via GitHub, or by asking on Slack. After your pull request is approved, or if it has not been reviewed within 30 days, it will be merged into development.The branch hotfix-development exists for small (one commit only) changes that are not worth creating a new branch for (for instance, small bugfixes, readme or help files edits, etc.). A pull request can then be created to merge those changes into development.New features should never be merged directly into master. Only hotfixes to the current release may be merged into master.For hotfixes, create a separate branch from master, make the fix and verifyit, and then merge the hotfix branch into master and development. Similarly to above, the hotfix-master branch exists for small (one commit only) bugfixes to the current release. A pull request can then be created to merge those changes into master and development.For more information, please follow this illustrated guide.Recommended readingThe RevBayes workflow is inspired by this guide: http://nvie.com/posts/a-successful-git-branching-model/Forking the RevBayes repoForking the RevBayes repository is not mandatory as long as the workflow outlined above is respected. However, occasional developers or people who are considering contributing may fork their own copy of the repository on GitHub in order to keep the total number of branches reasonable. They can then contribute their changes via pull request.Automated testsA suite of automated tests is run on all new pushes to the repository. The outcome of the tests will be notified by email to the author of the commits, and on Slack on the #github channel. More information on the tests and how to update them can be found here.Code review processAs mentioned above, all pull requests need to undergo a code review before being merged into the repository. All regular contributors to RevBayes may be asked to perform code reviews, so it is important to understand the process.Code review assignmentsA developer creating a pull request may decide to request reviews from one or more people. These people will appear under the Reviewers heading. This is only a request, and is not binding. If you do decide to review a pull request you have been requested on, please assign yourself under the Assignees heading. This helps both the pull request creators and the people managing the repository to easily see which pull requests are actively under review and which are still looking for reviewers.In case no reviewers were requested, or if all the requested reviewers are unable to do a review, a reviewer will be assigned from the list of available developers. If you are assigned, you will get a GitHub notification. If you are unable to review the pull request you have been assigned to, please contact the person who assigned you (whose name will appear in the notification as well as in the pull request’s Conversation tab) so you can be replaced.Reviewing guidelinesCode reviews are an essential part of the current process, for two main reasons. First, the current suite of automated tests in incomplete, and may not catch all issues. Second, some issues such as missing documentation, or the introduction of a new model without an accompanying validation test, can only be caught by human eyes.Some suggested checks for reviews are:  check that there is documentation and that it follows the RevBayes guidelines  if existing functions have been changed or extended, check that the documentation, the help files and/or the RevBayes website (if applicable) have also been updated  check that the added code is readable: proper indentation, good names for functions and variables (no bbb or thing or DoStuff)  if a new model or probability distribution is introduced, check that corresponding validation  and integration tests have also been added  check that added code or functionality does not duplicate existing parts of RevBayesNote that these are only suggestions and may not be applicable to all pull requests. Feel free to bring up any other issue you notice.Checking out codeChecking out the code in a pull request can be useful to run additional tests. If the pull request was created from a branch in the RevBayes repository, this can be done very simply by checking out the corresponding branch. Any changes committed and pushed to that branch will automatically be added to the pull request.The process is more complicated if the pull request was created from a fork of the repository. In that case, you can check out the pull request by using the command git fetch origin pull/ID/head:NAME, where ID is the pull request number and NAME any name you like, followed by git checkout NAME. Note that this will only allow you to check out the code, and not to push any edits. To edit the pull request, you will need to clone the forked repository and push your edits to the branch that was linked in the pull request.Sending your reviewSmall issues (e.g. typos) can be fixed directly by the reviewer, either by editing directly the files on the GitHub website, or by following the process outlined above to check out a pull request.Any other changes should be requested using the review option Request changes. The requests should state clearly what the issues are and where (file or function name) they appear.The review option Comment can be used to ask questions. If there are no requests for changes or questions, select the review option Accept.Follow-upThe creator of the pull request is responsible for making the requested changes or otherwise addressing the comments. They can then re-request a review from the assigned reviewer, which will notify them to check the added changes and approve or make new comments.Once the pull request is approved (directly or after changes) and the automated tests pass, the pull request can be merged into the repository. If some time has passed since the original request, it may be necessary to update the branch first (Update branch button). Updating and merging can be done by any developer.Once the pull request has been merged, the branch it was created from should be deleted by its creator, unless they intend to keep working on that branch.",
        "url": "/developer/git-flow.html",
        "index": ""
      }
      ,
    
      "tutorials-dating-global-html": {
        "title": "Molecular dating",
        "content": "Exercise 1In this exercise we will use molecular sequence data to estimate the relationships between extant species of bears and infer relative speciation times assuming a global (or strict) clock model. This model assumes that the rate of substitution is constant over time and across the tree.The graphical-model representation of the global molecular clock, where every branch has the same rate of change ($c$) and that rate is drawn from an exponential distribution with a rate parameter of $\\delta_c$.The dataFor this exercise we’ll use an alignment of 1,000 bp of cytochrome b sequences for 8 extant bear species.bears_cytb.nex contains the alignment in NEXUS format.The master Rev script  Create a script in you scripts directory called MCMC_dating_ex1.Rev and open it in your text editor.This is the master Rev file that loads the data, the files that specify different model component (e.g. the substitution model), the monitors that record the output and the MCMC sampler.In this exercise you will create separate files for the substitution model, clock model and tree model.Reading the dataFirst, we’ll begin the Rev script by importing the cytochrome b sequences and assign the data matrix to a variable called cytb.cytb &lt;- readDiscreteCharacterData(\"data/bears_cytb.nex\")At this stage we’ll also create some useful variables for later, including the number of taxa n_taxa and a vector of taxa that we can extract from the alignment data using cytb.taxa.n_taxa &lt;- cytb.size()taxa &lt;- cytb.taxa()We will also create a workspace variable called moves and monitors.This variable is a vector containing all of the MCMC moves and monitors respectively.moves    = VectorMoves()monitors = VectorMonitors()  Don’t forget to save your changes at the end of each section!The tree model  Create a script called tree_BD.Rev and open it in your text editor.The birth-death processThis script will contain all the parameters in our birth-death tree model, which is used to describe the process that generated our tree. Since all of the taxa included in the analysis in this exercise are living species we’ll use a birth-death model that doesn’t incorporate the fossil recovery process. Two key parameters in this model are the speciation rate (the rate at which lineages are added to the tree, denoted by $\\lambda$) and the extinction rate (the rate at which lineages are removed from the tree, $\\mu$). We will assume that these rates are constant over time and place exponential priors on each of these. Each parameter is assumed to be drawn independently from a different exponential distribution with rates $\\delta_\\lambda$ and $\\delta_\\mu$, with $\\delta_\\lambda$ = $\\delta_\\mu$ = 10. Note an exponential distribution with $\\delta = 10$ has an expected value (mean) of $1/\\delta$ = 0.1.Create the exponentially distributed stochastic nodes for speciation_rate and extinction_rate using the ~ operator.speciation_rate ~ dnExponential(10)extinction_rate ~ dnExponential(10)For every stochastic node we declare, we must also specify moves (or proposal algorithms) to sample the value of the parameter in proportion to its posterior probability. If a move is not specified for a stochastic node, then it will not be estimated, but fixed to its initial value.The rate parameters for extinction and speciation are both positive, real numbers (i.e. non-negative floating point variables). For both of these nodes, we will use a scaling move (mvScale), which proposes multiplicative changes to a parameter. Many moves also require us to set a tuning value, called lambda for mvScale, which determine the size of the proposed change. Here, we will also use tune=true, which will alter the magnitude of the proposed changes after an intital tuning phase. Note there are many different strategies available in RevBayes for improving mixing and convergence. See the tutorial Debugging your Markov chain Monte Carlo (MCMC) for more information on this topic.moves.append( mvScale(speciation_rate, lambda=0.5, tune=true, weight=3.0) )moves.append( mvScale(extinction_rate, lambda=0.5, tune=true, weight=3.0) )The weight option allows you to indicate how many times you would like a given move to be performed at each MCMC cycle. In this tutorial we will execute a schedule of moves at each step in our chain instead of just one move per step. Here, if we were to run our MCMC with our current vector of 6 moves, then our move schedule would perform 6 moves at each cycle. Within a cycle, an individual move is chosen from the move list in proportion to its weight. Therefore, with all six moves assigned weight=1, each has an equal probability of being executed and will be performed on average one time per MCMC cycle. For more information on moves and how they are performed in RevBayes, please refer to the Introduction to Markov chain Monte Carlo (MCMC) Sampling and Nucleotide substitution models tutorials.In addition to the speciation ($\\lambda$) and extinction ($\\mu$) rates, we may also be interested in inferring diversification ($\\lambda - \\mu$) and turnover ($\\mu/\\lambda$). Since these parameters can be expressed as a deterministic transformation of the speciation and extinction rates, we can monitor (that is, track the values of these parameters, and print them to a file) their values by creating two deterministic nodes using the := operator.diversification := speciation_rate - extinction_rateturnover := extinction_rate/speciation_rate$\\rho$ is the probability of extant species sampling. Since we sample all extant bears, we’ll specify this probability as a constant node = 1.0 using the &lt;- operator.rho &lt;- 1.0Because $\\rho$ is a constant node, we do not have to assign any moves to this parameter.Since in this exercise our aim is to infer relative times only, we’ll simply fix the root age to an arbitrary value = 1.0. In this exercise the root is the MRCA of all living bears.extant_mrca &lt;- 1.0Now that we’ve specified all of the parameters of the birth-death model, we can use these parameters to define the prior distribution on the tree topology and divergence times.tree_dist = dnBDP(lambda=speciation_rate, mu=extinction_rate, rho=rho, rootAge=extant_mrca, samplingStrategy=\"uniform\", condition=\"nTaxa\", taxa=taxa)Note that we created the distribution as a workspace variable using the workspace assignment operator =. This is because we still need to include a topology constraint in our final specification of the tree prior.Clade constraintsIn some cases we may want to constrain parts of the tree topology based on prior information. This is often necessary when we incorporate fossil calibration information, which we’ll do in subsequent tutorial exercises.Here, we will constrain the group Ursinae to be monophyletic by first creating a vector of constraints.clade_ursinae = clade(\"Melursus_ursinus\", \"Ursus_arctos\", \"Ursus_maritimus\",                   \"Helarctos_malayanus\", \"Ursus_americanus\", \"Ursus_thibetanus\") constraints = v(clade_ursinae)Next, we will specify the final constrained tree prior distribution, providing the constraints along with the workspace birth-death distribution to the constrained topology distribution. Here we use the stochastic assignment operator ~ to create a stochastic node for our constrained tree variable timetree.timetree ~ dnConstrainedTopology(tree_dist, constraints=constraints)Moves on the treeThe final step in our tree model script is to add the moves for the tree topology (mvFNPR) and node ages (mvNodeTimeSlideUniform).moves.append( mvNarrow(timetree, weight=n_taxa) )moves.append( mvFNPR(timetree, weight=n_taxa/4) )moves.append( mvNodeTimeSlideUniform(timetree, weight=n_taxa) )moves.append( mvSubtreeScale(timetree, weight=n_taxa/5.0) )Note there are lots of moves available for trees in RevBayes that you can use to improve the mixing, which you can learn about in other tutorials, including Divergence Time Calibration.Monitoring node ages of interestWe may be interested in monitoring the age of a given node in our MCMC sample. We can do this by first using the clade function to specify the node of interest, as we have done above for Ursinae. Once a clade is defined we can instantiate a deterministic node, in this case age_ursinae, with the tmrca function that will record the age of this node.age_ursinae := tmrca(timetree, clade_ursinae)Note that if we had not included this clade in the constraints that defined the timetree variable, this node would not be constrained to be monophyletic, but we could still monitor the age using the tmrca approach.The clock modelNext we’ll specify the clock (or branch-rate) model that describes how rates of substitution vary (or not) over the tree.  Create a script called clock_global.Rev and open it in your text editor.In this exercise we’ll use the global molecular clock model that assumes rates are constant over time and across the tree. Specifying this model in RevBayes is very simple.For the branch-rates parameter we will use an exponential prior, with rate parameter $\\delta_c$ = 10. Recall that the expected value (or mean) of this distribution is 0.1. The same branch-rate will apply to every branch in the tree.Create the exponentially distributed stochastic node for branch_rates and assign a move to this parameter.branch_rates ~ dnExponential(10.0)moves.append( mvScale(branch_rates, lambda=0.5, tune=true, weight=3.0) )The substitution modelThe next step is to specify the model that describes how sequences evolve along the tree and across sites.  Create a script called sub_GTRG.Rev and open it in your text editor.For this exercise we will use the general time-reversible (GTR) + $\\Gamma$ model.First, we need to define an instantaneous-rate matrix (i.e. a Q-matrix).A nucleotide GTR matrix is defined by a set of 4 stationary frequencies, and 6 exchangeability rates. Create stochastic nodes for these variables, each drawn from a uniform Dirichlet prior distribution.sf_hp &lt;- v(1,1,1,1)sf ~ dnDirichlet(sf_hp)er_hp &lt;- v(1,1,1,1,1,1)er ~ dnDirichlet(er_hp)We need special moves to propose changes to a Dirichlet random variable, also known as a simplex (a vector constrained to sum to one). Here, we use a mvSimplexElementScale move, which scales a single element of a simplex and then renormalises the vector to sum to one. The tuning parameter alpha specifies how conservative the proposal should be, with larger values of alpha leading to proposals closer to the current value.moves.append( mvBetaSimplex(er, alpha=10.0, weight=3.0) )moves.append( mvBetaSimplex(sf, alpha=10.0, weight=2.0) )Then we can define a deterministic node for our GTR Q-matrix using the special GTR matrix function (fnGTR).Q_cytb := fnGTR(er,sf)Next, in order to model gamma-distributed rates across sites, we will create an exponential parameter $\\alpha$ for the shape of the gamma distribution, along with scale proposals.alpha_cytb ~ dnUniform( 0.0, 1E6 )alpha_cytb.setValue( 1.0 )moves.append( mvScale(alpha_cytb, lambda=0.5,tune=true, weight=2.0) )Then we create a gamma distribution, discretized into 4 rate categories using the fnDiscretizeGamma function. Here, rates_cytb is a deterministic vector of rates computed as the mean of each category.rates_cytb := fnDiscretizeGamma( alpha_cytb, alpha_cytb, 4 )Finally, we can create the phylogenetic continuous time Markov chain (PhyloCTMC) distribution for our sequence data, including the gamma-distributed site rate categories, as well as the branch rates defined as part of our clock model. We set the value of this distribution equal to our observed data and identify it as a static part of the likelihood using the clamp method.phySeq ~ dnPhyloCTMC(tree=timetree, Q=Q_cytb, siteRates=rates_cytb, branchRates=branch_rates, type=\"DNA\")phySeq.clamp(cytb)Setting up the MCMC  Return to the master script MCMC_dating_ex1.Rev.RevBayes uses the source function to load commands from Rev files into the workspace. Use this function to load in the model scripts you have written in the text editor and saved in the scripts directory.source(\"scripts/tree_BD.Rev\") # BD tree priorsource(\"scripts/clock_global.Rev\") # Global clock modelsource(\"scripts/sub_GTRG.Rev\") # Molecular substitution model (GTR+G)We can now create our workspace model variable with our fully specified model DAG. We will do this with the model function and provide a single node in the graph (sf).mymodel = model(sf)\tThe object mymodel is a wrapper around the entire model graph and allows us to pass the model to various functions that are specific to our MCMC analysis.The next important step for our master Rev file is to specify the monitors and output file names. For this, we will create a vector called monitors that will each sample and record or output our MCMC.The first monitor we will create will monitor every named random variable in our model graph. This will include every stochastic and deterministic node using the mnModel monitor. The only parameter that is not included in the mnModel is the tree topology. Therefore, the parameters in the file written by this monitor are all numerical parameters and will be written to a tab-separated text file that can be opened by accessory programs for evaluating such parameters. We will also name the output file for this monitor and indicate that we wish to sample our MCMC every 10 cycles.monitors.append( mnModel(filename=\"output/bears_global.log\", printgen=10) )The mnFile monitor writes any parameter we specify to file. Thus, if we only cared about the speciation rate and nothing else (this is not a typical or recommended attitude for an analysis this complex) we wouldn’t use the mnModel monitor above and just use the mnFile monitor to write a smaller and simpler output file. Since the tree topology is not included in the mnModel monitor (because it is not numerical), we will use mnFile to write the tree to file by specifying our timetree variable in the arguments.monitors.append( mnFile(filename=\"output/bears_global.trees\", printgen=10, timetree) )The last monitor we will add to our analysis will print information to the screen. As with mnFile we must tell mnScreen which parameters we’d like to see updated on the screen. We will choose the age of the MRCA of living bears (extant_mrca) and the diversification rate (diversification) parameters.monitors.append( mnScreen(printgen=10, extant_mrca, diversification) )Once we have set up our model, moves, and monitors, we can now create the workspace variable that defines our MCMC run. We do this using the mcmc function that simply takes the three main analysis components as arguments.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")The MCMC object that we named mymcmc has a member method called run. This will execute our analysis and we will set the chain length to 20000 cycles using the generations option.mymcmc.run(generations=20000, tuningInterval=200)After the Markov chain has completed, we can create a summary tree. We’ll use the function readTreeTrace to read the MCMC sample of trees from file and the command mccTree to generate the maximum clade credibility (MCC) tree.trace = readTreeTrace(\"output/bears_global.trees\")mccTree(trace, file=\"output/bears_global.mcc.tre\" )Note by default, a burn-in of 25% is used when creating the tree trace (250 trees in our case). You can specify a different burn-in fraction, say 50%, by typing the command trace.setBurnin(500).Once all our analysis is complete, we will want RevBayes to close. Tell the program to quit using the q() function.q()  Execute your MCMC analysis in RevBayes!Examining the output  Open the program Tracer and load the log file bears_global.log.Evaluate the mixing and convergence of your MCMC analysis, and examine the marginal distributions of parameters of interest. For publication quality analysis you would probably want to run multiple independent MCMC chain and increase your chain length, but you should be able to see that the chain is mixing quite well.The Trace window in the program Tracer. Click on the “+” sign to load your trace file or drag and drop you file in the Trace files box.Note that the speciation and extinction rates are not especially meaningful because they’re not in absolute time.The tree outputIn addition to evaluating the performance and sampling of an MCMC run using numerical parameters, it is also important to inspect the sampled topology and tree parameters. This is a difficult endeavor, however. One tool for evaluating convergence and mixing of the tree samples is RWTY (Warren et al. 2016). In this tutorial, we will only summarize the sampled trees, but we encourage you to consider approaches for assessing the performance of the MCMC with respect to the tree topology.Ultimately, we are interested in summarizing the sampled trees and branch times given that our MCMC has sampled all of the important parameters in proportion to their posterior probabilities. RevBayes includes some functions for summarizing the tree topology and other tree parameters, including the mccTree that we used in this exercise.  Open the program FigTree and load MCC tree file bears_global.mcc.tre.The FigTree window. To open your tree you can use File &gt; Open. Select Node Labels to view the relative node ages.Note that because our analysis did not contain any temporal information from the fossil record and instead we fixed the age of the root to 1, the units of the scale bar are arbitrary and the node ages can only be interpreted as relative speciation times.In the following exercise we’ll relax the assumption of a global molecular clock.  Click below to begin the next exercise!  The uncorrelated exponential relaxed clock modelFurther readingFor further options and information about clock models see Tracy Heath’s tutorial Relaxed Clocks &amp; Time Trees.",
        "url": "/tutorials/dating/global.html",
        "index": "false"
      }
      ,
    
      "tutorials-intro-graph-models-html": {
        "title": "Introduction to Graphical Models",
        "content": "OverviewRevBayes uses a graphical model framework inwhich all probabilistic models, including phylogenetic models,are comprised of modular components that can be assembled in a myriad of ways.RevBayes provides a highly flexible language called Revthat users employ to specify their own custom graphical models.This tutorial is intended to be a gentle introduction on how to use Rev to specify graphical models.Additionally we’ll cover how to use Rev to specify the Markov chain Monte Carlo (MCMC) algorithms used to perform inference with the model.We will focus on a simple linear regression example,and use RevBayes to estimate the posterior distributions of our parameters.Why Graphical Models?RevBayes is a fundamental reconception of phylogenetic software.Most phylogenetic software have default settings that allow a user to run an analysiswithout truly understanding the assumptions of the model.RevBayes, on the other hand, has no defaults and is a completeblank slate when started. RevBayes requires users to fully specify the model they want to use for their analysis.This means the learning curve is steep, however there are a number of benefits:      Transparency: All the modeling assumptions are explicitly specified in RevBayes. The Rev script that runs an analysis makes these assumptions clear and can be easily shared. The assumptions can easily be modified in the Rev script and then the analysis can be rerun to see how changes affect the results. There is no reliance on “defaults” that may change with different versions of the software.        Flexibility: Users are not limited by a small set of models the programmers hard coded, instead users can specify their own custom models uniquely tailored to their hypotheses and datasets.        Modularity: Each model component can be combined with others in an endless number of new ways like a LEGO kit. Testing many complex evolutionary hypotheses require tying different models together. For example, suppose you wish to test how the effect of biographic range on trait evolution changes through time. In RevBayes you could simultaneously infer a time-calibrated phylogeny and estimate biogeography-dependent trait evolution using molecular data, biogeographic range data, and morphological data from both fossils and extant lineages.  What is a Graphical Model?Left: a graphical model in which the observed data points $X_i$ are conditionally independent given $\\theta$.Right: the same graphical model using plate notation to represent the $N$ repeated $X_i$.These graphical models represent the joint probability distribution\\(p(\\theta,X_1,\\dots,X_N)\\).See  for a description of the visual symbols.Image from (missing reference)A graphical model is a way to represent a joint multivariate probability distribution as a graph.Here we mean graph in the mathematical sense of a set of nodes (vertices) and edges.In a graphical model, the nodes represent variables and the edges represent conditional dependencies among the variables. There are three important typesof variables:  Constant nodes: represents a fixed value that will not change.  Stochastic nodes: represents a random variable with a value drawn from a probability distribution.  Deterministic nodes: represents a deterministic transformation of the values of other nodes.In the graphical modeling framework observed data is simply a variable with an observed value.To specify that a node has an observed value associated with it we say that thenode is clamped, or fixed, to the observed value. illustrates the graphical model that represents the joint probability distribution\\[\\begin{aligned}p(\\theta,\\mathcal{D}) = p(\\theta) \\Big[ \\displaystyle\\prod^N_{i=1} p(X_i|\\theta) \\Big],\\end{aligned}\\]where \\(\\mathcal{D}\\) is the vector of observed data points \\(X_1,\\dots,X_N\\).Nearly any probabilistic model can be represented as a graphical model: neural networks, classification models, time series models, and of course phylogenetic models!In some literature the terms Bayesian networks, belief networks, or causal networks are sometimes used to refer to graphical models.Visual RepresentationThe statistics literature has developed a rich visual representation for graphical models.Visually representing graphical models can be useful for communication and pedagogy.We explain the notation used in the visual representation of these models only briefly (see ),and enourage readers to see Höhna et al. (2014) for more details.As we will discuss below, representing graphical models in computer code (using the Rev language)will likely be the most useful aspect of graphical models to most readers.The symbols for a visual representation of a graphicalmodel. a) Solid squares represent constant nodes, which specify fixed-valued variables. b) Stochastic nodes are represented by solid circles.These variables correspond to random variables and may depend onother variables. c) Deterministic nodes (dotted circles) indicate variablesthat are determined by a specific function applied to another variable.They can be thought of as variable transformations. d) Observed statesare placed in clamped stochastic nodes, represented by gray-shadedcircles. e) Replication over a set of variables is indicated by enclosingthe replicated nodes in a plate (dashed rectangle). f) Tree plates represent the different classes of nodes in a phylogeny. The tree topology orders the nodes in the tree plate andmay be a constant node (as in this example) or a stochastic node (if thetopology node is a solid circle).Image and text modified from Höhna et al. (2014)Phylogenetic Graphical ModelsIn phylogenetics, observations about different species are not considered independent data pointsdue to their shared evolutionary history.So in a phylogenetic probabilistic model the topology of the tree determines the conditional dependencies among variables. This can be represented as a graphical model as in  (left).Phylogenetic models are often highly complex with hundreds of variables. Not only do we modelthe conditional dependencies due to shared evolutionary history (the tree topology), but we also commonly model character evolution (nucleotide substitution models, etc.),branching processes that determine the times between speciation events (birth-death processes),and many other aspects of the evolutionary process.With graphical models we can think of each part of these models as discrete components that canbe combined in a myriad of ways to assemble different phylogenetic models ( right).Left: In a phylogenetic probabilistic model the topology of the tree determines the conditional dependencies among variables.Right: A complex phylogenetic model that includes a clock model, a GTR+$\\Gamma$ nucleotide substitution model, and a uniform tree topology model. Here the repeated nodes within the tree are represented by a tree plate.Images from Höhna et al. (2014)Probabilistic ProgrammingTo describe complex probabilistic models and perform computational tasks with them,we need a way to formally specify the models in a computer. Probabilistic programming languages were designed exactly for this purpose.A probabilistic programming language is a tool for probabilistic inference that:  formally specifies graphical models, and  specifies the inference algorithms used with the model.Probabilistic programming languages are being actively developed withinthe statistics and machine learning communities.Some of the most common are Stan, JAGS, Edward, and PyMC3.While these are all excellent tools, they are all unsuitable for phylogenetic models since the tree topology itself must be treated as a random variable to be inferred.The Rev Probabilistic Programming LanguageRevBayes provides its own probabilistic programming language called Rev.While Rev focuses on phylogenetic models, nearly any type of probabilisticmodel can be programmed in Rev making it a highly flexible probabilistic computing environment.Most Rev scripts consist of two different parts:  Model specification. This part of the script defines the constant, stochastic, and determinstic nodes that make up the model.  Inference algorithm specification. This part of the script specifies what sort of inference algorithm we want to use with the model. Typically this is a Markov chain Monte Carlo algorithm, and we need to specify what sort of proposals (or moves) will operate on each variable.In more complex Rev scripts, these two different elements (model specification and infernence algorithm specification) will be woven together.In the example for this tutorial we will keep the two parts separate.Linear Regression ExampleTo demonstrate how to use the Rev language to specify a graphical model,we will start with a simple non-phylogenetic model.This tutorial will show both how to specify linear regressionas a graphical model, and how to perform Bayesian inference overthe model using MCMC.Tutorial FormatAll command-line text, including all Rev syntax, are given in monotype font. Furthermore, blocks of Rev code that are needed to build the model, specify the analysis, or execute the run are given in separate shaded boxes. For example, we will instruct you to create a new variable called n that is equal to 10 using the &lt;- operator like this:n &lt;- 10Setup Your FilesMake yourself familiar with the example script called linear_regression.Rev which shows the code for the following sections. Then, start a new and empty script in your text editor and follow each step provided as below. Name the script file my_linear_regression.Rev or anything you’d like.You’ll also want to download the x.csv and y.csv data files and place them in a data directory.Linear Regression as a Graphical ModelThe observed data used in the linear regression example.Suppose we observed the data shown in .We might hypothesize that $x$ and $y$ are related through the linearregression model\\[\\begin{aligned}y = \\beta x + \\alpha + \\epsilon.\\end{aligned}\\]In this model $\\beta$ and $\\alpha$ are the regression variables (slope and y-intercept, respectively) and $\\epsilon$ is an error or noise term.We can formulate this as the graphical model\\[\\begin{aligned}\\mu_y := \\beta x + \\alpha\\end{aligned}\\]\\[\\begin{aligned}y \\sim \\text{Normal}(\\mu_y, \\sigma_{\\epsilon}).\\end{aligned}\\]Here $\\mu_y$ is a deterministic variable, it is determined by whatever the valuesof $\\beta$ and $\\alpha$ are. We use the $:=$ assignment operator to designatethat $\\mu_y$ is deterministic. The error or noise term $\\epsilon$ is representedas a normal distribution where the mean equals $\\mu_y$ and the standard deviation is $\\sigma_{\\epsilon}$.$y$ is a stochastic variable, it has a value that is drawn from a probability distribution.This is designated by using the $\\sim$ assignment operator.Since we have observed values for $y$, we will clamp $y$ to those observed values.Bayesian Linear RegressionIn our linear regression model $\\beta$, $\\alpha$, and $\\sigma_{\\epsilon}$ are the free variables we wish to estimate.To perform Bayesian inference, we need some priors!\\[\\begin{aligned}\\beta \\sim \\text{Normal}(\\mu=0, \\sigma^2=1)\\end{aligned}\\]\\[\\begin{aligned}\\alpha \\sim \\text{Normal}(\\mu=0, \\sigma^2=1)\\end{aligned}\\]\\[\\begin{aligned}\\sigma_{\\epsilon} \\sim \\text{Exponential}(\\lambda=1)\\end{aligned}\\]Again, these are stochastic variables, so we use the $\\sim$ assignment operator.For now we will accept these as decent uninformative priors.Later in the tutorial we will discuss how the choice of a prior can affect the outcome of the analysis.  Exercise: Using the sticks-and-arrows visual symbols explained in , draw the linear regression graphical model. See the answer in the expandable box below.Answer: Visual Representation of the Linear Regression ModelVisual representation of the linear regression graphical model.The plate (dashed rectangle) around $x_i$, $\\mu_{yi}$ and $y_i$ representthe repeated variables for all the observed points.$y_i$ is a clamped (observed) stochastic node, so it is shaded.$\\mu_{yi}$ is a deterministic node, so it is dashed.Here we treat $x_i$ as a constant node, so it is square.$\\alpha$, $\\beta$, and $\\sigma$ are the stochastic variables we wishto estimate, and each of them are assigned priors distributions whichhave constant parameter values (the squares on the top row of the figure).Specifying the Model in RevRemember that graphical models are made up of three types of nodes: stochastic, constant, and deterministic nodes.In Rev we specify the type of node by using a specific assignment operator:  Stochastic node: n ~ dnNormal(0, 1)  Constant node: n &lt;- 5  Deterministic node: n := m + 5We will use each of these assignment operators to set up the linear regression model.First, we read in the observed data as constant nodes:x_obs &lt;- readDataDelimitedFile(file=\"data/x.csv\", header=FALSE, delimiter=\",\")[1]y_obs &lt;- readDataDelimitedFile(file=\"data/y.csv\", header=FALSE, delimiter=\",\")[1]Take a look at x_obs:x_obsThis is the vector of x-coordinates for the points plotted in .Now we will specify the prior distributions for the stochastic nodes.These are the variables that we will estimate:beta ~ dnNormal(0, 1)alpha ~ dnNormal(0, 1)sigma ~ dnExponential(1)Now, for each observed value in x_obswe will create a deterministic node for mu_y and a stochastic node for y:for (i in 1:x_obs.size()) {    mu_y[i] := (beta * x_obs[i]) + alpha    y[i] ~ dnNormal(mu_y[i], sigma)}Take a look at y:yThis produces a vector of simulated values of y!We have specified a model that describes the process that generates yconditioned on the observed values of x.We have not clamped, or fixed, the observed values y_obs to the stochastic nodes y.In Rev all models can be used to both simulate new values and, when clamped to observed values,perform parameter inference.In this case we are not interested in simulating new values of y, but insteadwe want to estimate our linear regression parameters. So let’s modify the above codeto clamp the observed values to y:for (i in 1:x_obs.size()) {    mu_y[i] := (beta * x_obs[i]) + alpha    y[i] ~ dnNormal(mu_y[i], sigma)    y[i].clamp(y_obs[i])}Note that we have now clamped each observed value y_obs to each stochastic node y.We have now fully specified the model, so we can begin specifying the inferencealgorithm.Setting up MCMC in RevHere we will use the Metropolis-Hastings MCMC algorithm (Metropolis et al. 1953; Hastings 1970) to perform parameter estimation.We focus here on providing a simple overview of how to set up and tweak MCMC in RevBayes,for a more in depth introduction to MCMC please see the Introduction to Markov chain Monte Carlo (MCMC) Sampling tutorial.The first step in setting up our MCMC algorithm is wrapping the entire modelinto a single variable:mymodel = model(beta)Since our model is a graph in which all the model nodes are connected,we can use any model variable and RevBayes will traverse the graph to copythe entire model into the variable mymodel.Note that we used the = assignment operator. This means that the variablemymodel is not part of the graphical model – it is not a stochastic, constant, or deterministicnode. We call this a Rev workspace variable. Workspace variablesare utility variables that we use for any programming task that is not specifically defining the model.Note, that unlike in R, in Rev the = and &lt;- assignment operators have very different functions!To sample different values of each variable, we must assignan MCMC move to each variable. Each MCMC move will propose new valuesof each parameter. We have three variables,so we will have three moves which we will save in avector called moves:moves[1] = mvSlide(beta, delta=0.001, weight=1)moves[2] = mvSlide(alpha, delta=0.001, weight=1)moves[3] = mvSlide(sigma, delta=0.001, weight=1)Here we used simple slide moves for each variable. The slide move proposes new values for the variable by “sliding” its value within a small windowdetermined by the delta argument.RevBayes provides many other types of moves that you will see in other tutorials.We set the weight of each move to 1, which means that eachmove will be performed on average once per MCMC iteration.Next, we need to set up some monitors that will sample values during the MCMC.We will use two monitors which we save into a vector called monitors.The first monitor mnScreen prints out values to the screen,and the second monitor mnModel prints a log file.monitors[1] = mnScreen()monitors[2] = mnModel(filename=\"output/linear_regression.log\")RevBayes provides many other monitors that can be useful for different types of analyses,but these are sufficient for this example.We can now pass the model, moves, and monitors into the mcmc functionto finalize our analysis.Then we use the run member method to run the MCMC for 10000 iterations.mymcmc = mcmc(mymodel, moves, monitors)mymcmc.run(10000)quit()Note that we included the quit() command so that RevBayes will automatically quitafter the MCMC has finished running.Improving MCMC Mixing  Exercise: Now open the file output/linear_regression.log in Tracer.You will notice that the MCMC analysis did not converge well:The MCMC trace for the beta parameter. This analysis never converged.We can fix this by modifying the MCMC moves we use.Let’s use a larger sliding window (the delta argument in mvSlide).We will also increase the weight of each move to 5.This means that each move will be now be performed on average 5 timesper MCMC iteration.moves[1] = mvSlide(beta, delta=1, weight=5)moves[2] = mvSlide(alpha, delta=1, weight=5)moves[3] = mvSlide(sigma, delta=1, weight=5)  Exercise: Rerun the MCMC analysis with these new moves and view the log file in Tracer.This analysis looks much better:Left: The MCMC trace for the beta parameter. This analysis has adequately converged;all parameter values have ESS values over 200.Right: Posterior parameter estimates from the converged MCMC analysis.The y-intercept ($\\alpha$) was estimated to be about -2 and the slope ($\\beta$) was estimated to be about 0.5. This closely matches what is observed in .Moreover, -2 and 0.5 were the true values used to simulate the data.Prior SensitivityNormal distributions with different values of the standard deviation.Prior distributions are a way to mathematically formalize ourprior knowledge.We used normal distributions as priors for $\\alpha$ and $\\beta$.How did we pick these distributions?  illustrates the normal distribution with differentvalues for the standard deviation.Using a smaller standard deviation (0.1) places most of the densityclose to 0. This sort of prior is appropriate only if we have prior informationthat the parameter’s true value is close to 0, so we can call thisan informative prior.Using a large standard deviation (10.0) is a highly uninformative prior.The density is diffuse and nearly uniform, allowing for a wide range of values.This is appropriate if we have very little idea what the true value ofthe parameter is.In RevBayes it is easy to modify the priors used in an analysis and rerun the analysis.  Exercise: Try rerunning the linear regression exercise using highly informative priors (standard deviation set to 0.1) on beta and alpha as shown below.beta ~ dnNormal(0, 0.1)alpha ~ dnNormal(0, 0.1) shows the posterior estimates when using these priors.Compare those results with those shown in .Using informative priors that are incorrect can badly bias the results.Biased posterior parameter estimates when using overly informative priors.The true values were $\\alpha = -2$, $\\beta = 0.5$, and $\\sigma = 0.25$.  Exercise: Try running the analysis again with highly uninformative priors (10.0).beta ~ dnNormal(0, 10.0)alpha ~ dnNormal(0, 10.0)These results are highly similar to our original estimatesshown in . Our original priors (that had a standard deviation of 1.0)did not introduce any bias.Typically the trade off is between informative priors that may introduce biasand uninformative priors that may increase the variance (uncertainty) of ourestimates.Generative vs Discriminative ModelsProbabilistic models can be understood as either discriminative or generative models.The distinction between the two can be useful in phylogeneticswhere different analyses often make use of these different types of models.RevBayes enables us to specify both types of models.Discriminative ModelsDiscriminative (or conditional) modelsinvolve a response variable conditioned on a predictor variable.The model represents the conditional distribution $p(y|x)$and so makes fewer assumptions about the data: it is not necessary to specify $p(x)$.The linear regression example we coded in Rev was a discriminative modelbecause it conditioned on the observed values of $x$. In other wordsthe model could simulate values of $y$ conditioned on the observed values of $x$,but it could not simulate values of $x$.In phylogenetics we often use discriminative models when we condition over a fixed tree (or set of trees):  estimating divergence times over a fixed topology  estimating ancestral states on a fixed tree  estimating shifts in diversification rates over a fixed treeWe can set up all these discriminative models in RevBayes.Generative ModelsGenerative modelsmodel the entire process used to generate the data.So these models represent the joint distribution $p(x, y)$,and therefore they must make more assumptions about the data: we need to define $p(x)$.This allows for a richer representation of the relations between variables.Additionally these models are more powerful; they allow us to compute $p(y|x)$ or $p(x|y)$and to simulate both $x$ and $y$.In phylogenetics we use fully generative models when we:  jointly estimate divergence times and the tree topology  jointly estimate ancestral states and the tree  jointly estimate shifting diversification rates and the treeRevBayes is unique because it allows us to specify both highly complex fully generative modelsas well as their more simple discriminative forms.A Generative Linear Regression ModelA fully generative linear regression model enables us to learn something about $x$, for example the meanand standard deviation, which we don’t get from the discriminative form.With the generative model:  we can simulate values of both $x$ and $y$,  both $x$ and $y$ will need to be clamped to the observed data,  and we will need to specify a prior distribution for $x$.  Exercise: Reformulate our linear regression example so that it is a fully generative model:      Draw the sticks-and-arrows diagram for a generative model and compare it to the discriminative form. See the expandable box for one solution.    Code up the model in Rev and run MCMC. A solution is provided in linear_regression_generative.Rev if you get stuck.  Answer: Visual Representation of the Generative Linear Regression ModelVisual representation of the generative linear regression graphical model.Compare this to .The major difference is we now treat $x_i$ as a clamped (observed)stochastic node.Additionally, we now estimate $\\mu_x$ and $\\sigma_x$ as stochastic variables.ConclusionRevBayes gives evolutionary biologists the tools to formalize their hypotheses as custom graphical models that represent the specific processthat generated their data.This enables many evolutionary hypothesesto now be tested in a rigorous and quantitative approach.Hopefully this tutorial will help readers develop their own custom modelsand not use defaults ever again!",
        "url": "/tutorials/intro/graph_models.html",
        "index": "true"
      }
      ,
    
      "workshops-hackathons-html": {
        "title": "RevBayes Hackathons",
        "content": "Members of the RevBayes development team meet periodicically to work on development objectives for the project. During these hackathons, we work on software design, new phylogenetic methods and models, documentation, etc. If you are interested in developing in RevBayes and joining a hackathon, please see the Developer page.Hackathon ScheduleDateLocationApril, 2023Germany, Iowa, North Carolina, Illinois, Missouri (virtual)October, 2021Germany and Iowa (in person and virtual)March, 2020Germany, Iowa, North Carolina, Missouri, Spain (virtual)March, 2018University of Minnesota, St. Paul, MN, USANovember, 2015LBBE, Lyon, FranceMarch, 2014LBBE, Lyon, FranceAugust, 2011University of California, Berkeley, USA",
        "url": "/workshops/hackathons.html",
        "index": ""
      }
      ,
    
      "tutorials-coalescent-heterochronous-html": {
        "title": "Coalescent Analyses with heterochronous data",
        "content": "OverviewThis tutorial describes how to run a coalescent analyses with heterochronous data in RevBayes.For the description of the whole script, we will focus on an analysis with a constant population size.At the bottom of the page, you can find links to RevBayes scripts performing more complex analyses.Theses scripts are similar to the ones described for isochronous data.If you have already done the tutorial for a constant coalescent model with isochronous data, most parts will be the same.Please specifically have a look at the section  and at the root calibration part of the section .Inference Example  For your info  The entire process of the estimation can be executed by using the mcmc_heterochronous_Constant.Rev script that you can download on the left side of the page.Save it in your scripts directory.You can type the following command into RevBayes:  source(\"scripts/mcmc_heterochronous_Constant.Rev\")    We will walk you through every single step in the following section.For every MCMC analysis, convergence assessment is an important step.In the tutorial Convergence assessment, you can find information and instructions on how to run the convergence assessment with the R package convenience.It is generally recommended to have at least two replicates per MCMC analysis to be able to compare convergence between runs.Thus, we first set the number of replicates:NUM_REPLICATES      = 2In the beginning, we also define a few variables for running the MCMC.These are the number of iterations, and the so-called “thinning” which we use to say that we want to sample every $10^{th}$ iteration.NUM_MCMC_ITERATIONS = 10000THINNING            = 10We also need to create vectors for the monitors and moves of the MCMC.Moves are functions that propose new parameter values in your MCMC, based on the current value.These newly proposed parameters can either be accepted or rejected.Depending on this acceptance / rejection, the posterior distribution of your parameters will be formed.See for example Introduction to MCMC for more information on the acceptance / rejection procedure.Monitors are later used to track the progress of your analysis, but are also needed to write output files.moves     = VectorMoves()monitors  = VectorMonitors()Read the dataStart by reading in the ages and the aligned sequences of the heterochronous horse data.taxa &lt;- readTaxonData(\"data/horses_heterochronous_ages.tsv\")sequences &lt;- readDiscreteCharacterData(\"data/horses_heterochronous_sequences.fasta\")You will also need the number of taxa.n_taxa &lt;- taxa.size()The Tree ModelFor the constant coalescent model, only one population size is estimated.For this population size, a prior needs to be set.Without knowing much about the population size of our horse sample, we set a uniform prior.pop_size ~ dnUniform(0,1E8)You may realize that in the full script, we initialize the population size to have a first value of $100000$.Later in the tutorial, we will constrain the root age of the tree to be inside the interval $[780 000, 1 200 000]$.In order for our first proposed tree to comply with this constraint, an initial value of $100000$ proved to lead to reasonable initial proposals.pop_size.setValue(100000)We also add a move for the population size.Here, we chose a scaling move which means that the current values is multiplied by a scaling factor to propose a new value.See for example Introduction to MCMC using RevBayes for information on moves.moves.append( mvScale(pop_size, lambda=0.1, tune=true, weight=2.0) )Now, we will instantiate the stochastic node for the tree.The dnCoalescent distribution should be used for a constant coalescent process.It takes a value for the population size (theta) and the taxa as input.psi ~ dnCoalescent(theta=pop_size, taxa=taxa)We calibrate the tree based on the root age.We chose a Normal distribution with a mean of $850 000$ and a standard deviation of $200 000$.As mentioned above, the root age will be constrained to the interval $[780 000, 1 200 000]$.As we have access to the original analysis from Vershinina et al. (2021), we could see that this should be the rough range of the root.root_age := psi.rootAge()diff &lt;- (1200000 - 780000)/2.0obs_root_age ~ dnNormal(mean = root_age, sd = 200000, min = root_age - diff, max = root_age + diff)obs_root_age.clamp(850000)We should also add moves for the tree.These include moves on a single branch, subtrees or the whole tree.Here, the weight of the different moves is based on the number of taxa.If a move changes a single branch (e.g. mvNNI), it will be applied more often and thus have a higher weight than a move which changes the whole tree (e.g. mvTreeScale).moves.append( mvNarrow(psi, weight=n_taxa) )moves.append( mvNNI(psi, weight=n_taxa) )moves.append( mvFNPR(psi, weight=n_taxa/4.0) )moves.append( mvSubtreeScale(psi, weight=n_taxa/5.0) )moves.append( mvNodeTimeSlideUniform(psi, weight=n_taxa) )moves.append( mvRootTimeScaleBactrian(psi, weight=n_taxa/5.0) )moves.append( mvTreeScale(psi, weight=n_taxa/5.0) )Substitution Model and other parametersFinally, sequence data should be added to the analysis. Here, we assume a GTR+$\\Gamma$+I substitution model, but you can of course use others. Have a look at the Nucleotide substitution models tutorial to see how you can define different substitution models.For the GTR model, we need to add exchangeability rates (er) and stationary frequences (pi).Of course, we also add moves for these.er_prior &lt;- v(1,1,1,1,1,1)pi_prior &lt;- v(1,1,1,1)er ~ dnDirichlet(er_prior)pi ~ dnDirichlet(pi_prior)moves.append( mvBetaSimplex(er, weight=3) )moves.append( mvDirichletSimplex(er, weight=1) )moves.append( mvBetaSimplex(pi, weight=2) )moves.append( mvDirichletSimplex(pi, weight=1) )This is everything needed for the Q matrix of the GTR model.Q := fnGTR(er,pi)For the $\\Gamma$ extension to the GTR model, we need to draw the site rates (sr) from a discretized Gamma function with two parameters.Here, we use alpha for both parameters.We also add a scaling move for alpha.alpha ~ dnUniform( 0.0, 1E6 )alpha.setValue( 1.0 )sr := fnDiscretizeGamma( alpha, alpha, 4 )moves.append( mvScale(alpha, weight=2.0) )We draw the proportion of invariant sites (p_inv) from a Beta distribution and add a sliding window move.p_inv ~ dnBeta(1,1)moves.append( mvSlide(p_inv) )The last step is to set the clock rate.We draw it from a log uniform distribution here.Again, we know from the original analysis (Vershinina et al. 2021) that the true value should be around $4.68*10^{-8}$ and thus set the lower bound of the distribution to $1*10^{-12}$ and the upper bound to $1*10^{-4}$.We also initialize the value to be equal to the original analysis.Then, we  add a scaling move for the clock rate.clock ~ dnLoguniform(1e-12,1e-4)clock.setValue(4.68e-8)moves.append( mvScale(clock, weight=2.0) )Additionally, we add a scaling move which makes sure to regulate clock rate and the root age.This needs to be done as root age and clock rate are intertwined and can not be clearly seperated.Here, whenever the clock rate will be increased, the root age will be decreased.Note that you could also calibrate the clock rate instead of the root age of the tree as we do it here.up_down_move = mvUpDownScale(weight=5.0)up_down_move.addVariable(clock,up=TRUE)up_down_move.addVariable(psi,up=FALSE)moves.append( up_down_move)The final dnPhyloCTMC function combines all of the previous defined parameters.We also need to clamp the sequence data.seq ~ dnPhyloCTMC(tree=psi, Q=Q, siteRates=sr, pInv=p_inv, type=\"DNA\", branchRates=clock)seq.clamp(sequences)Finalize and run the analysisIn the end, we need to wrap our model.mymodel = model(psi)Now, we add some monitors.The mnModel monitor keeps track of all model parameters and thus is written into our main .log file.With mnFile, you can keep track of the trees or parameters that you would like to keep in an extra file.mnScreen is responsible for having output printed directly to your screen.This output will not per se be saved in a file.monitors.append( mnModel(filename=\"output/horses_het_Constant.log\",printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_het_Constant.trees\",psi,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_het_Constant_NE.log\",pop_size,printgen=THINNING) )monitors.append( mnScreen(pop_size, root_age, printgen=100) )The final step is to run the mcmc.Make sure to set combine=\"mixed\" for the output of the two replicates to be combined in the end.mymcmc = mcmc(mymodel, monitors, moves, nruns=NUM_REPLICATES, combine=\"mixed\") mymcmc.burnin(NUM_MCMC_ITERATIONS*0.1,100)mymcmc.run(NUM_MCMC_ITERATIONS, tuning = 100)Check ConvergenceTo check whether your analysis has converged, you can use the R package convenience.Have a look at the Convergence assessment tutorial.ResultsAfter running your analysis, you can plot the results using the R package RevGadgets.See the RevGadgets Github repository for information on how to install the package.After installing the package, open R or RStudio and set the tutorial directory as your working directory.You can plot the RevBayes output as follows:library(RevGadgets)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"num_grid_points = 500max_age_het = 1.2e6min_age = 0spacing = \"equal\"population_size_log = \"output/horses_het_Constant_NE.log\"df &lt;- processPopSizes(population_size_log, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points, max_age = max_age_het, min_age = min_age, spacing = spacing)p &lt;- plotPopSizes(df) + ggplot2::coord_cartesian(ylim = c(1e3, 1e8))ggplot2::ggsave(\"figures/horses_het_constant.png\", p)Your output should look roughly like the following figure.Example output from plotting the constant coalescent analysis with heterochronous data run in this exercise. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals.Scripts for other analysesHere, you can find scripts for analyses similar to those performed in the previous tutorials for isochronous data.The main changing points are the data which is read in and the root age.The root age also has an influence on parameters like MAX_AGE, which is needed, e.g, for the Gaussian Markov Random Field (GMRF) scripts and the ESTIMATED_ROOT_AGE parameter, which you can find for example in the Skyride script.  Script for a Skyline analysis, have a look at the Skyline tutorial  Script for a Skyline analysis with a maximum a posteriori (MAP) tree as input  Script for a Skyline analysis with autocorrelated intervals (BSP model), have a look at the Alternative Priors section of the Skyline tutorial  Script for a Skyride analysis, have a look at the Alternative Priors section of the Skyline tutorial  Script for a Skyline analysis with estimation of the number of intervals (EBSP model), have a look at the Alternative Priors section of the Skyline tutorial  Script for a GMRF analysis, have a look at the GMRF tutorial  Script for a GMRF analysis with a sample of trees as input, have a look at the treebased GMRF tutorial  Script for a GMRF analysis with a MAP tree as input, have a look at the treebased GMRF tutorial  Scripts for a GMRF analysis with $100$ intervals, both sequence based, and MAP tree based  Script for an analysis with a Horseshoe Markov Random Field (HSMRF) prior, have a look at the HSMRF section of the GMRF tutorial  Script for a HSMRF analysis with a MAP tree as input  Scripts for a HSMRF analysis with $100$ intervals, both sequence based, and MAP tree based  Script for a Skygrid analysis, have a look at the Alternative Implementations section of the GMRF tutorial  Script for an analysis with the Skyfish model, have a look at the Skyfish tutorial  Script for a Skyfish analysis with a MAP tree as input",
        "url": "/tutorials/coalescent/heterochronous.html",
        "index": "false"
      }
      ,
    
      "tutorials-sse-hisse-html": {
        "title": "State-dependent diversification with HiSSE",
        "content": "IntroductionBiSSE and MuSSE are powerful approaches for testing the association of a character with diversificationrate heterogeneity. However, BiSSE has been shown to be prone to falsely identifying a positive associationwhen diversification rate shifts are correlated with a character not included in the model(Maddison and FitzJohn 2015; Rabosky and Goldberg 2015).The HiSSE (Hidden State-Dependent Speciation and Extinction) model reduces the possibility of falselyassociating a character with diversification rate heterogeneity (Beaulieu and O’Meara 2016).It does this by incorporating a second, unobserved character.The changes in the unobserved character’s state represent background diversification rate changesthat are not correlated with the observed character.See  for a schematic overview of the HiSSE model, and Table 2 for an explanation of the HiSSE model parameters.We will keep this tutorial brief and assume that you have worked through the State-dependent diversification with BiSSE and MuSSE.A schematic overview of the HiSSE model. Each lineage has an observed binary state associated to it:state 0 (blue) or state 1 (red). Furthermore, there is a second, unobserved (hidden), binary character with states Aor B. The HiSSE model describes jointly the evolution of both of these two characters; a lineage must be in one offour different states: 0A, 0B, 1A, or 1B. We estimate separate speciation and extinction rates for each of these fourstates. Note that just like BiSSE can easily be extended to MuSSE, RevBayes allows you to extend HiSSE modelsbeyond binary observed and unobserved characters.Estimating State-Dependent Speciation and Extinction under the HiSSE ModelFirst, we create some global variables to set-up our analysis.Using this variable we can easily change our script to use a different character with a different numberof states. We will also use this variable in our second example on hidden-state speciation and extinctionmodel.NUM_TOTAL_SPECIES     = 367NUM_STATES            = 2NUM_HIDDEN            = 4NUM_RATES             = NUM_STATES * NUM_HIDDENH                     = 0.587405DATASET               = \"activity_period\"Read in the DataBegin by reading in the observed tree and the character data.We have both stored in separate nexus files.observed_phylogeny &lt;- readTrees(\"data/primates_tree.nex\")[1]data &lt;- readCharacterData(\"data/primates_activity_period.nex\")It will be convenient to get some helper variables with information from the tree:taxa &lt;- observed_phylogeny.taxa()tree_length &lt;- observed_phylogeny.treeLength()For the HiSSE model, we need to expand our characters to the new state space.This means, that originally we had the states 0 and 1.Now, we want to have the states 0A, 0B, 0C, 0D and 1A, 1B, 1C, 1D.A character that was originally in state 0 will now be ambiguous for all states 0A, 0B, 0C and 0D.Instead of coding this up manually, RevBayes provides a simple function for you.data_exp &lt;- data.expandCharacters( NUM_HIDDEN )Finally, we initialize a variable for our vector of moves and monitors.moves    = VectorMoves()monitors = VectorMonitors()Specify the ModelPriors on the RatesWe start by specifying prior distributions on the diversification rates.Here, we will assume an identical prior distribution on each of thespeciation and extinction rates. Furthermore, we will use a log-uniformdistribution as the prior distribution on each speciation andextinction rate (i.e., a uniform distribution on the log of the rates).\\(\\lambda_{ij} = \\lambda_{\\text{observed},i} * \\lambda_{\\text{hidden},j}\\)For example, we have $\\lambda_{0A} = \\lambda_{\\text{observed},0} * \\lambda_{\\text{hidden},A}$Let’s code this up in RevBayes.First, we create the vector of hidden speciation rates.Following the idea of discretizing a continuous distribution of diversification rates(see Branch-Specific Diversification Rate Estimation), we will specify NUM_HIDDEN speciation ratesas the quantiles of a lognormal distribution.We need the average rate of the hidden speciation rates to be fixed, because otherwise the model is not identifiable.Therefore, we fix the median of the lognormal distribution to 1.0:ln_speciation_hidden_mean &lt;- ln(1.0)Next, we draw the standard deviation of the hidden speciation rates from an exponential distribution with mean H(so that we expect the 95% interval of the hidden speciation rate to span 1 order of magnitude).speciation_hidden_sd ~ dnExponential( 1.0 / H )moves.append( mvScale(speciation_hidden_sd, lambda=1, tune=true, weight=2.0) )With the mean and the standard deviation we can specify the distribution on the hidden speciation rates.We create a deterministic variable for the hidden speciation rate categories usinga discretized lognormal distribution (the N-quantiles of it).speciation_hidden_unormalized := fnDiscretizeDistribution( dnLognormal(ln_speciation_hidden_mean, speciation_hidden_sd), NUM_HIDDEN )However, we normalize the hidden speciation rates by dividing the rates with the main(so the mean of the normalized rates equals to 1.0):speciation_hidden := speciation_hidden_unormalized / mean(speciation_hidden_unormalized)Next, we repeat this same procedure for the hidden extinction rates.ln_extinction_hidden_mean &lt;- ln(1.0)extinction_hidden_sd ~ dnExponential( 1.0 / H )moves.append( mvScale(extinction_hidden_sd, lambda=1, tune=true, weight=2.0) )extinction_hidden_unormalized := fnDiscretizeDistribution( dnLognormal(ln_extinction_hidden_mean, extinction_hidden_sd), NUM_HIDDEN )extinction_hidden := extinction_hidden_unormalized / mean(extinction_hidden_unormalized)For the observed speciation and extinction rates, we will apply a different approach.We will draw the speciation and extinction rates for the observed characters from identical distribution,so that a priori we expect with probability 0.5 that $\\lambda_{\\text{observed},0} &gt; \\lambda_{\\text{observed},1}$,and with probability 0.5 we expect $\\lambda_{\\text{observed},1} &gt; \\lambda_{\\text{observed},0}$.For the lack of prior knowledge, we specify a log-uniform prior distribution on the speciation and extinction ratesfor the observed characters.Note that we also initialize the starting states to make the analysis run more efficiently.for (i in 1:NUM_STATES) {    ### Create a loguniform distributed variable for the speciation rate    speciation_observed[i] ~ dnLoguniform( 1E-6, 1E2)    speciation_observed[i].setValue( (NUM_TOTAL_SPECIES-2) / tree_length )    moves.append( mvScale(speciation_observed[i],lambda=1.0,tune=true,weight=3.0) )    ### Create a loguniform distributed variable for the speciation rate    extinction_observed[i] ~ dnLoguniform( 1E-6, 1E2)    extinction_observed[i].setValue( speciation_observed[i] / 10.0 )    moves.append( mvScale(extinction_observed[i],lambda=1.0,tune=true,weight=3.0) )}We have now specified the diversification rate variables for the observed and hidden states.That means, we can now put these two put together.for (j in 1:NUM_HIDDEN) {    for (i in 1:NUM_STATES) {        index = i+(j*NUM_STATES)-NUM_STATES        speciation[index] := speciation_observed[i] * speciation_hidden[j]        extinction[index] := extinction_observed[i] * extinction_hidden[j]    }}Now we can specify our character-specific speciation and extinction rateparameters. Because we will use the same prior for each rate, it’s easyto specify them all in a for-loop. We will use a log-uniform distribution as a prioron the speciation and extinction rates. The loop also allows us to apply moves to eachof the rates we are estimating and create a vector of deterministic nodesrepresenting the rate of diversification ($\\lambda - \\mu$) associated with eachcharacter state.The stochastic nodes representing the vector of speciation rates and vector ofextinction rates have been instantiated. The software assumes that the rate in position [1] of eachvector corresponds to the rate associated with diurnal 0 lineages and the rateat position [2] of each vector is the rate associated with nocturnal 1 lineages.Next we specify the transition rates between the states 0 and 1:$q_{01}$ and $q_{10}$. As a prior, we choose that each transition rateis drawn from an exponential distribution with a mean of 10 characterstate transitions over the entire tree. This is reasonable because weuse this kind of model for traits that transition not-infrequently, andit leaves a fair bit of uncertainty.Note that we will actually use a for-loop to instantiate the transition ratesso that our script will also work for non-binary characters.rate_pr := observed_phylogeny.treeLength() / 10for ( i in 1:(NUM_STATES*(NUM_STATES-1)) ) {    transition_rates[i] ~ dnExp(rate_pr)    moves.append( mvScale(transition_rates[i],lambda=0.50,tune=true,weight=3.0) )}Similarly to the rate of change between the observed character, we also add a rate of change between the unobserved character, the hidden rate.Thus, we will also assume the same exponential prior distribution.hidden_rate ~ dnExponential(rate_pr)moves.append( mvScale(hidden_rate,lambda=0.5,tune=true,weight=5) )Furthermore, since there is no additional information, we assume that the rate of change between all hidden rates is identical.for (i in 1:(NUM_HIDDEN * (NUM_HIDDEN - 1))) {    R[i] := hidden_rate}Finally, we can build our rate matrix.We do this using the specific fnHiddenStateRateMatrix function.rate_matrix := fnHiddenStateRateMatrix(transition_rates, R, rescaled=false)Note that we do not “rescale” the rate matrix. Rate matrices formolecular evolution are rescaled to have an average rate of 1.0, but forthis model we want estimates of the transition rates with the same timescale as the diversification rates.Prior on the Root StateCreate a variable for the root state frequencies. We are using a flat Dirichlet distribution as the prior oneach state. There has been some discussion about this in (FitzJohn et al. 2009).You could also fix the prior probabilities for the root states to be equal(generally not recommended), or use empirical state frequencies.rate_category_prior ~ dnDirichlet( rep(1,NUM_RATES) )Note that we use the rep() function which generates a vector of length NUM_STATESwith each position in the vector set to 1. Using this function and the NUM_STATESvariable allows us to easily use this Rev script as a template for a different analysisusing a character with more than two states.We will use a special move for objects that are drawn from a Dirichlet distribution:moves.append( mvBetaSimplex(rate_category_prior,tune=true,weight=2) )moves.append( mvDirichletSimplex(rate_category_prior,tune=true,weight=2) )The Probability of Sampling an Extant SpeciesAll birth-death processes are conditioned on the probability a taxon is sampled in the present.We can get an approximation for this parameter by calculating the proportion of sampledspecies in our analysis.We know that we have sampled 233 out of 367 living described primate species. Toaccount for this we can set the sampling probability as a constant nodewith a value of 233/367.rho &lt;- observed_phylogeny.ntips()/367Root AgeThe birth-death process also depends on time to the most-recent-common ancestor–i.e.,the root. In thisexercise we use a fixed tree and thus we know the age of the tree.root &lt;- observed_phylogeny.rootAge()The Time TreeNow we have all of the parameters we need to specify the full characterstate-dependent birth-death model. We initialize the stochastic noderepresenting the time tree and we create this node using the dnCDBDP() function.timetree ~ dnCDBDP( rootAge           = root,                    speciationRates   = speciation,                    extinctionRates   = extinction,                    Q                 = rate_matrix,                    delta             = 1.0,                    pi                = rate_category_prior,                    rho               = rho,                    condition         = \"survival\" )Now, we will fix the HiSSE time-tree to the observed values from our data files. We usethe standard .clamp() method to give the observed tree and branch times:timetree.clamp( observed_phylogeny )timetree.clampCharData( data_exp )And then we use the .clampCharData() to set the observed states at the tips of the tree:timetree.clampCharData( data )Finally, we create a workspace object of our whole model. The model()function traverses all of the connections and finds all of the nodes wespecified.mymodel = model(timetree)Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we set up a vector of monitors to record thestates of our Markov chain. The first monitor will model all numericalvariables; we are particularly interested in the rates of speciation,extinction, and transition.monitors.append( mnModel(filename=\"output/primates_HiSSE_activity_period.log\", printgen=1) )Optionally, we can sample ancestral states during the MCMC analysis.We need to add an additional monitor to record the state of each internal node in the tree.The file produced by this monitor can be summarized so that we can visualize the estimates of ancestral states.monitors.append( mnJointConditionalAncestralState(tree=timetree,                     cdbdp=timetree,                       type=\"Standard\",                     printgen=1,                     withTips=true,                     withStartStates=false,                     filename=\"output/primates_HiSSE_activity_period_anc_states.log\") )Similarly, you may want to add a stochastic character map.monitors.append( mnStochasticCharacterMap(cdbdp=timetree,                       filename=\"output/primates_HiSSE_activity_period_stoch_map.log\",                       printgen=1) )Then, we add a screen monitor showing some updates during the MCMCrun.monitors.append( mnScreen(printgen=10, speciation, extinction) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, wecan now set up the MCMC algorithm that will sample parameter values inproportion to their posterior probability. The mcmc() function willcreate our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=5000, tuningInterval=200)Summarize Sampled Ancestral StatesIf we sampled ancestral states during the MCMC analysis, we can use the RevGadgets (Tribble et al. 2022) R packageto plot the ancestral state reconstruction.First, though, we must summarize the sampled values in RevBayes.To do this, we first have to read in the ancestral state log file. This uses a specific function called readAncestralStateTrace().anc_states = readAncestralStateTrace(\"output/primates_HiSSE_activity_period_anc_states.log\")Now, we can write an annotated tree to a file. This function will write a tree with eachnode labeled with the maximum a posteriori (MAP) state and the posterior probabilities for eachstate.anc_tree = ancestralStateTree(tree=T,                  ancestral_state_trace_vector=anc_states,                  include_start_states=false,                  file=\"output/primates_HiSSE_anc_states_results.tree\",                  burnin=0,                  summary_statistic=\"MAP\",                  site=1)Similarly, we compute the maximum a posteriori (MAP) stochastic character map.anc_state_trace = readAncestralStateTrace(\"output/primates_HiSSE_activity_period_stoch_map.log\")characterMapTree(observed_phylogeny,                 anc_state_trace,                 character_file=\"output/primates_HiSSE_activity_period_stoch_map_character.tree\",                 posterior_file=\"output/primates_HiSSE_activity_period_stoch_map_posterior.tree\",                 burnin=0.1,                 reconstruction=\"marginal\")Visualize Estimated Ancestral StatesTo visualize the posterior probabilities of ancestral states, we will use the RevGadgets (Tribble et al. 2022) R package.  Open R.RevGadgets requires the ggtree package (Yu et al. 2017).First, install the ggtree and RevGadgets packages:install.packages(\"devtools\")library(devtools)install_github(\"GuangchuangYu/ggtree\")install_github(\"revbayes/RevGadgets\")Run this code (or use the script plot_anc_states_HiSSE.R):library(ggplot2)library(RevGadgets)# read in and process the ancestral statesHiSSE_file &lt;- paste0(\"output/primates_HiSSE_activity_period_anc_states_results.tree\")p_anc &lt;- processAncStates(HiSSE_file)# plot the ancestral statesplot &lt;- plotAncStatesMAP(p_anc,        tree_layout = \"rect\",        tip_labels_size = 1) +        # modify legend location using ggplot2        theme(legend.position = c(0.1,0.85),              legend.key.size = unit(0.3, 'cm'), #change legend key size              legend.title = element_text(size=6), #change legend title font size              legend.text = element_text(size=4))ggsave(paste0(\"HiSSE_anc_states_activity_period.png\"),plot, width=8, height=8)A visualization of the ancestral states estimated under the HiSSE model. We used the script plot_anc_states_HiSSE.R.Next, we also want to plot the stochastic character map.Use the script plot_simmap_HiSSE.R.A visualization of the stochastic character map estimated under the HiSSE model. We used the script plot_simmap_HiSSE.R.Summarizing Parameter EstimatesOur MCMC analysis generated a tab-delimited file called primates_HiSSE_activity_period.log that containsthe samples of all the numerical parameters in our model.Again, we will use the RevGadgets (Tribble et al. 2022) R package, which allow you to generate plots andvisually explore the posterior distributions of sampled parameters.  Open R.Run this code:library(RevGadgets)library(ggplot2)# read in and process the log fileHiSSE_file &lt;- paste0(\"output/primates_HiSSE_activity_period.log\")pdata &lt;- processSSE(HiSSE_file)# plot the ratesplot &lt;- plotMuSSE(pdata) +        theme(legend.position = c(0.875,0.915),              legend.key.size = unit(0.4, 'cm'), #change legend key size              legend.title = element_text(size=8), #change legend title font size              legend.text = element_text(size=6))ggsave(paste0(\"HiSSE_div_rates_activity_period.png\"),plot, width=5, height=5)Visualizing posterior samples of the speciation rates associated with daily activity time with the RevGadgets (Tribble et al. 2022) R package. We used the script plot_div_rates_HiSSE.R.",
        "url": "/tutorials/sse/hisse.html",
        "index": "true"
      }
      ,
    
      "tutorials-host-rep-host-rep-html": {
        "title": "Reconstructing Host Repertoire Evolution",
        "content": "IntroductionExtant ecological interactions are the result of a long history of coevolution between interacting lineages. In the case of host-parasite systems, species associations are continuously evolving via gains and losses of hosts. Given that parasites are usually specialized to their hosts, most methods developed to study coevolution focus on congruence between host and parasite phylogenies and use cospeciation as the null expectation (missing reference). Recent years have seen increasing evidence that parasites change what hosts they target much more often than previously thought. Thus, new methods are required to appropriately reconstruct coevolutionary histories. Below, we describe a model of host-parasite coevolution that was introduced in (missing reference), which is based on the Dispersal-Extinction-Cladogenesis approach Ree et al. (2005) used to model geographic range evolution (see Introduction to phylogenetic biogeography with the DEC model for an overview of the DEC model). However, in this first version, the model does not include evolutionary changes during parasite cladogenesis. This tutorial reviews the modeling concepts developed in (missing reference) then describes how to model host repertoire evolution and reproduce the results published in the paper.Model OverviewWe wish to model the co-evolution of ecological interactions between M extant parasite taxa and N host taxa in a phylogenetic context. Each parasite uses one or more hosts, which we encode as a character set called the host repertoire. During the course of evolution, any parasite lineage may gain or lose a host from its repertoire or modify whether a host is actually or potentially used by the parasite. Furthermore, it is likely that parasites have a phylogenetic preference when expanding their host repertoires, favoring species that are closely related to species they parasitize over distantly related species.This tutorial follows the modeling strategy developed in (missing reference), which identifies three central challenges to modeling host repertoire evolution: (1) defining the host repertoire character, (2) defining an event-based model for how host repertoires evolve in terms of gain and loss rates and phylogenetic distances among hosts in the repertoire, and (3) designing an inference method to fit our model to data. The next sections gives context to possible solutions to these problems.Hosts, parasites, and their interactionsRooted and time-calibrated phylogenetic trees describe the evolutionary relationships among the M parasite taxa and among the N host taxa. In this tutorial, the trees are considered to be known without error.Each parasite taxon has a host repertoire, which is represented by a vector of length N. At any given time, each host taxon can assume one of three states with respect to a parasite lineage: 0 (non host), 1 (potential host), or 2 (actual host).  We call the set of all potential and actual hosts fundamental repertoire, and the set of only actual hosts, realized repertoire (analogous to fundamental and realized niche). For example, say there are two hosts, A and B.            Repertoire(set)      Repertoire(vector)      Size of fundamentalrepertoire      Size of realizedrepertoire      State                  $\\emptyset$      00      0      0      1              a      10      1      0      2              A      20      1      1      3              b      01      1      0      4              B      02      1      1      5              ab      11      2      0      6              Ab      21      2      1      7              aB      12      2      1      8              AB      22      2      2      9      Example of host repertoire representations for an analysis with hosts A and B. Lowercase indicates potential hosts and uppercase, actual hosts.We only allow host repertoires in which the parasite has at least one actual host (states 3, 5, 7-9 in this example).Host gain and lossThe transition from state 0 to state 1 represents the gain of the ability to use the host, and the transition from state 1 to state 2 happens when the parasite actually starts to use the host in nature. If we assume that gains and losses of hosts occur according to a continuous-time Markov chain, the probability of a given coevolutionary history can be easily calculated based on a matrix, Q, containing the instantaneous rates of change between all pairs of host repertoires, and thus describing the Markov chain (see Introduction to phylogenetic biogeography with the DEC model for more details).Another new feature of our model is the ability to infer the effect of host phylogenetic relatedness on colonizations, i.e. the gain of a host depends on the phylogenetic distance between the given host and those currently used by a lineage. To formalize these dynamics, let qy,z($a$) be the rate of change from host repertoire y to repertoire z by changing the state of host a. Also, let $\\lambda_{ij}$ be the rate at which an individual host changes from state $i$ to state $j$, and $\\eta({\\bf y},a,\\beta)$ be a phylogenetic-distance rate modifier. We define the instantaneous rate of change asand the phylogenetic-distance rate modifier function as\\[\\begin{aligned}    \\eta({\\bf y},a,\\beta) = e^{-\\beta d/\\overline{d}},    \\end{aligned}\\]where $\\beta$ controls the effect of $d$, the average pairwise phylogenetic distance between the new host, $a$, and the hosts currently occupied in y; and $\\overline{d}$ is the average phylogenetic distance between all pairs of hosts. Pairwise phylogenetic distance is defined as the sum of branch lengths separating two leaf nodes.Cartoon of the model of host repertoire evolution within a parasite lineage. Parasites can have potential (lowercase) or actual (uppercase) interactions with host lineages A, B, and C, whose phylogeny is known (left). For example, the parasite begins with the host repertoire bC, which corresponds to the vector 012 (see Table 1). Time proceeds from left to right. Only anagenetic events are modeled, i.e. repertoires are identically inherited during cladogenesis. In this example, a repertoire of three hosts changes along a branch of the parasite tree. Gain rates are multiplied by the phylogenetic-distance rate modifier function, $\\eta$, using the host phylogeny.Let’s calculate the instantaneous rate for the second gain event in Figure 1 for two different values of $\\beta$. Say $\\mu = 0.5$, $\\lambda_{12} = 0.2$, $d_{\\bf AC}=10$ and $\\overline{d}=8$. In this case,\\[\\begin{aligned}  q_{\\bf aC,AC} = q_{102,202} = 0.5 \\times 0.2 \\times e^{-1.25\\beta}\\end{aligned}\\]If $\\beta = 0$,  $q_{102,202} = 0.5 \\times 0.2 \\times 1 = 0.1$.Whereas if $\\beta = 1$,  $q_{102,202} = 0.5 \\times 0.2 \\times 0.29 = 0.029$.What about the rate of gain of b? Calculate $q_{002,012}$ for $\\beta = 0$ and $\\beta =1$, given that $d_{\\bf BC}=4$ and $\\lambda_{01} = 0.1$.Dataset - Butterfly-plant interactionsIn this tutorial, we will go through the inference of host repertoire evolution in Nymphalini butterflies conducted in (missing reference). The data set includes 34 butterflies species and plants from 25 angiosperm families. For each butterfly species, host plants used in nature were coded as actual hosts, plants that are not used in nature, but there’s evidence that the larvae can feed on were coded as potential hosts, and those plants that the larvae do not eat were coded as non hosts.Interactions between Nymphalini butterflies (rows) and their Angiosperm host plants (columns). Black squares represent actual interactions and grey squares, potential interactions.  Download data  We have provided data and script files for this tutorial (see top of the page)  Once you have downloaded the zip file by clicking on the arrow above,unzip that and navigate inside of that directory.This directory should be called revbayes_host_rep  Once inside, you should see two directories: data and scripts  Once you have all of the files, open RevBayes and ensure that your working directory is the top directory above the data folder (e.g., revbayes_host_rep).In this tutorial we are going to perform the analysis with two different host trees: the time-calibrated tree (angio_25tips_time.phy), and a tree where all branch lengths were set to 1 (angio_25tips_bl1.phy). This is because we wanted to test if cladogenetic distances and anagenetic distances between host families have different effects on host colonization. Note that the columns in the nexus file containing the interaction data are ordered by theAnalysis - Butterfly-plant interactionsNow, let’s begin.First, create file management variables for inputphy_host_fn = \"data/angio_25tips_time.phy\"phy_parasite_fn = \"data/Nymphalini.phy\"dat_parasite_fn = \"data/interaction_matrix.nex\"then read in our character datadat_parasite &lt;- readDiscreteCharacterData(dat_parasite_fn)For this tutorial we’ll assume we know the host and parasite phylogenies without error. Note that our host repertoire inference method uses a root branch length to estimate the stationary probabilities at the root node. Our parasite tree file (Nymphalini.phy) is modified to have a branch length assigned to the root node. If you provide a tree without a root branch length, the software will consider it to be the same length as the tree height.phy_parasite &lt;- readTrees(phy_parasite_fn)[1]Here is where you can change the host tree to angio_25tips_bl1.phy when you repeat the analysisphy_host &lt;- readTrees(phy_host_fn)[1]Retrieve dataset dimensionsn_host_tips &lt;- phy_host.ntips()n_host_branches &lt;- 2 * n_host_tips - 2n_parasite_branches &lt;- 2 * phy_parasite.ntips() - 2n_sites &lt;- dat_parasite.nchar()Add more information to the name of output filesout_str = \"out.time\"out_fn = \"output/\" + out_str    We need to create vectors of moves and monitors to later inform how our Markov chain Monte Carlo (MCMC) analysis needs to propose and sample new model parameters and host repertoire histories. Also, we use monitors to record the information we want to usemoves = VectorMoves()monitors = VectorMonitors()Next, we’ll build the transition rate matrix for the model. In this example, the rate matrix requires four rates: two gain rates (0-&gt;1 and 1-&gt;2) and two loss rates (1-&gt;0 and 2-&gt;1).First, create a vector containing all transition rates and assign it a moveswitch_rates_pos ~ dnDirichlet( [1,1,1,1] )moves.append( mvSimplex(switch_rates_pos, alpha=10, weight=2, tune=false) )We’ll now create a set of deterministic nodes to help us map our simplex of transition rates onto specific host gain and loss eventsswitch_rate_0_to_1 := switch_rates_pos[1]switch_rate_0_to_2 := 0.switch_rate_1_to_0 := switch_rates_pos[2]switch_rate_1_to_2 := switch_rates_pos[3]switch_rate_2_to_0 := 0.switch_rate_2_to_1 := switch_rates_pos[4]Next, we assemble our named rate variables into a vectorswitch_rates := v( switch_rate_0_to_1, switch_rate_0_to_2, switch_rate_1_to_0, switch_rate_1_to_2, switch_rate_2_to_0, switch_rate_2_to_1 )We then construct a rate matrix for three states (0: non-host, 1: potential host, 2: actual host) using our vector of named rates. We found that the MCMC mixes better when the Q matrix is not rescaled such that the expected number of events per unit time per character is 1 (rescaled=false). This might not be true for every data set and you can always change it to rescaled=true.Q_char := fnFreeK( transition_rates=switch_rates, rescaled=false )Note that Q_char is a rate matrix for a single character, which would define the rates of host repertoire evolution if all hosts were gained and lost at rates independent of what other hosts were in the repertoire.To let host gain and loss rates be modified by other hosts in the repertoire, we must define how host usage evolves interdependently using a rate modifier function.In this analysis, we wish to model how the average phylogenetic distance between a new possible host and currently parasitized hosts might increase or decrease the rate of that host gain event.To this end, we now create the phylogenetic distance rate modifier function (effect of host phylogeny on gain rate). First, we define a parameter (beta) that defines how influential phylogenetic distances should be on host repertoire evolution. The beta parameter is a vector with two values: the first for the gain 0-&gt;1 and the second for 1-&gt;2. In this tutorial, we assume they are equalbeta[1] ~ dnExp(1)beta[2] := beta[1]Because the value of beta[2] is determined to be equal to beta[1], we only need to create a single move to update beta[1]moves.append( mvScale( beta[1], weight=1 ) )Next, we pass our beta parameter into a special function called fnHostSwitchRateModifier that rescales the host gain rate accordinglyrate_modifiers[1] := fnHostSwitchRateModifier(tree=phy_host, gainScale=beta)Now we can build the rate matrix for all characters (= number of hosts) based on the gain and loss rates and the phylogenetic distance rate modifierQ_host := fnRateGeneratorSequence(Q=Q_char,                                  numChars=n_host_tips,                                  rateModifiers=rate_modifiers)We then rescale all rates in $\\lambda$ with $\\mu$. We refer to $\\mu$ as the maximum rate (or rate, for short) of host repertoire evolution, since $0 \\leq \\mu \\lambda_{ij} \\leq \\mu$.mu ~ dnExp(10)moves.append( mvScale(mu, weight=2) )Next, set uniform subroot state priors over the states (0,1,2). The subroot is the start of the branch leading to the root node.rf_host &lt;- simplex(1,1,1)We have fully specified all of the parameters of our phylogenetic model. Our model components are then encapsulated in a distribution called the phylogenetic continuous-time Markov chain for data-augmented character sequences, and we use the dnPhyloCTMCDASequence constructor function to create this node. This distribution is similar to dnPhyloCTMC except specialized to use data-augmented histories under the non-iid model to integrate over coevolutionary histories. For more details on how this distribution computes the probability of a specific coevolutionary history, see (missing reference).m_host ~ dnPhyloCTMCDASequence(tree=phy_parasite,                               Q=Q_host,                               branchRates=mu,                               rootFrequencies=rf_host,                               nSites=n_host_tips,                               type=\"Standard\")Once the PhyloCTMCDASequence variable has been created, we can attach our observed host repertoires (interaction matrix) to the tip nodes in the parasite treem_host.clamp(dat_parasite)Let’s add the remaining moves to update the character histories. These moves share several arguments to configure how they update histories. ctmc and qmap_seq indicate which model variables are relevant to the move. The lambda argument assigns the probability of proposing a new history for any individual character, e.g. if lambda=0.2and n_host_tips equals 25, then we would expect the move to propose 5 new host interaction histories per update. The graph argument specifies whether the move performs on a single branch (graph=\"branch\") or on a single node and its three incident branches (graph=\"node\"). The proposal argument controls which MCMC proposal algorithm we use to sample and propose new branch histories; in our case, we use the rejection sampling algorithm of Nielsen (2002) (proposal=\"rejection\"). Finally, the weight argument controls the relative number of times that move is sampled per MCMC iteration. First we create a move to update character histories along a random branchmoves.append( mvCharacterHistory(ctmc=m_host,                                 qmap_seq=Q_host,                                 lambda=0.2,                                 graph=\"branch\",                                 proposal=\"rejection\",                                 weight=n_parasite_branches*2) )Then, we create a move to update the character histories for a random node and the three incident branchesmoves.append( mvCharacterHistory(ctmc=m_host,                                 qmap_seq=Q_host,                                 lambda=0.2,                                 graph=\"node\",                                 proposal=\"rejection\",                                 weight=n_parasite_branches) )Before we can start the analysis, we need to define the number of generations and the sampling frequency of the MCMCn_gen = 1e6sample_freq = 50We also need to set up a vector of monitors to record the states of our Markov chain. All the monitor functions start with mn. First, we will initialize the screen monitor that will report the states of specified variables to the screen. This monitor mostly helps us see the progress of the MCMC run.monitors.append( mnScreen(mu, beta, printgen=sample_freq) )Then we create a new monitor variable that will output the states for all model parameters when passed into a MCMC function.monitors.append( mnModel(printgen=sample_freq, filename=out_fn + \".log\") )The mnFile monitor will record the states for only the parameters passed in as arguments. We use this monitor to output the parasite tree, even though the parasite tree value remains constant. This is useful for mapping the indices of internal nodes onto the tree. These indices will also be used in the next monitor, which outputs all sampled coevolutionary historiesmonitors.append( mnFile( phy_parasite, filename=out_fn + \".tre\", printgen=sample_freq) )monitors.append( mnCharacterHistorySummary( filename=out_fn + \".history.txt\",                                            ctmc=m_host,                                            tree=phy_parasite,                                            printgen=sample_freq ) )Finally, we wrap the entire model in a single object. To do this, we only need to give the model() function a single node and it will find all of the other nodes in the modelmodel = model(m_host)Create MCMC object from the model, moves, and monitors variables.ch = mcmc(model, monitors, moves)Then run the MCMC analysis, which will sample parameter values and character histories in proportion to their posterior probabilitych.run(n_gen)Now you can repeat the analysis using the host tree angio_25tips_bl1.phy. Remember to change the output file name!ResultsYou can use the output files out.time.log and out.bl1.log to plot the posterior distributions for the four transition rates, the overall clock, and the phylogenetic distance rate modifier. We can examine the log files in the program Tracer. Once you open this program, you can open the log files using the “File &gt; Import Tracer File” option, navigate to the directory in which you ran the analysis and select the relevant log file. Or you can simply drag and drop the files into “Trace Files” (the empty white box on the upper left of the program).Take a look at the output obtained for the analysis with the time-calibrated host tree (out.time.log) and with the host tree with branch lengths set to 1 (out.bl1.log). Let’s focus on the posterior distributions of $\\beta$.Marginal density in Tracer. The left-hand window provides mean and ESS of the chain. The right-hand window shows the distribution of samples.  Discussion point  As we have seen above, we estimate different values of $\\beta$ when using different host trees. Which host tree has a stronger effect on host gains? What does that mean in terms of evolution of plant traits that are used by the butterflies?Now let’s move on to ancestral state reconstruction. To get the ancestral states, use the R script ancestral_states.R. This script produces a dataframe containing the posterior probability of potential and actual interactions between hosts and parasites (internal nodes in the parasite phylogeny). With that information, you can generate a figure like the one below.Ancestral state reconstruction of host repertoire across the Nymphalini phylogeny. Each square color represents one interaction with a given host family (only interactions with &gt;90% posterior probability). Some parasite nodes don’t display an ancestral state because no host interaction was estimated with probability higher than 90%. The x-axis shows time before present in millions of years.2-state modelIn cases where we don’t have information about potential hosts in our data set, the inference might not work well with the 3-state model. It is tricky to infer a state when you only have indirect evidence of it (actual hosts must have been potential hosts in the past). One option is to skip potential hosts altogether and describe a 2-state model where there are only non-hosts (coded as 0) and actual hosts (coded as 2).For that, we need to change the code in two places: the Q matrixswitch_rates_pos ~ dnDirichlet( [1,1] )switch_rate_0_to_1 := 1e-6               # MCMC gets stuck on first gen if rates = 0.switch_rate_0_to_2 := switch_rates_pos[1]switch_rate_1_to_0 := 1e-6switch_rate_1_to_2 := 1e-6switch_rate_2_to_0 := switch_rates_pos[2]switch_rate_2_to_1 := 1e-6and the root state frequenciesrf_host &lt;- simplex(1,0,1)And that’s it. Everything else should be the same.",
        "url": "/tutorials/host_rep/host_rep.html",
        "index": "true"
      }
      ,
    
      "developer-implementation-implement-distributions-html": {
        "title": "Implementing a distribution",
        "content": "General info before getting started      Within the RevBayes core directory, there are subdirectories for different categories of distributions. All mathematical distributions that have been implemented exist in core/distributions/math.        Note that when implementing a new distribution, you will need to create .cpp and .h files in both the revlanguage directory and the core directory. (For a refresher on the difference between these two directories, refer to the Getting familiar with the code section of this Developer’s guide).The overall naming format remains the same for every distribution in RevBayes. In the Beta Binomial Distribution example provided below, I specify what naming convention to use when creating each file.        It is often helpful to look at/borrow code from existing RevBayes distributions for general help on syntax and organization.  For the language side, one of the most important things is the create distribution function (it converts user-arguments into calculations). Also, the getParameterRules function is important (to get the degrees of freedom &amp; other things). It is often helpful to look at the code of existing distributions for general help on syntax &amp; organization.      Within every new distribution, you will need to include some functions. For example, each new distribution must have: the get class type, name, and help functions. You may not need to implement these from scratch (if they’re dictated by the parent class &amp; are already present), but you will need to implement other functions within your distribution (e.g. cdf, rv, quantile).        Distributions have a prefix DN (dag node), and all moves have a prefix MV. RevBayes takes the name within &amp; creates the DN automatically, so be aware of this. For a refresher on DAG nodes, refer to Getting familiar with the code.  In the following steps, we’ll implement the Beta Binomial Distribution as an example, for syntax purposes.Steps      Create new .cpp &amp; .h files in revlanguage/distributions/math/, named Dist_betabinomial.cpp and Dist_betaBinomial.h. Note: all files in this directory will follow this naming format: Dist_&lt;nameofdistribution&gt;.[cpp|h].    To populate these files, look at existing examples of similar distributions for specific info on what to include &amp; on proper syntax.  For example, for the Beta Binomial distribution, I checked the existing Binomial Distribution code for guidance.    //add code here        Test                  Create new .cpp &amp; .h files in core/distributions/math/, named BetaBinomialDistribution.cpp and BetaBinomialDistribution.h.        Note: This is the object oriented wrapper code that references the functions hard-coded in the next step. All files in this directory will follow this naming format: &lt;NameOfDistribution&gt;Distribution.[cpp|h].                    Create new .cpp and .h files in core/math/Distributions/, named DistributionBetaBinomial.cpp and DistributionBetaBinomial.h.        These are the raw procedural functions in the RevBayes namespace (e.g. pdf, cdf, quantile); they are not derived functions. RbStatistics is a namespace. To populate these files, look at existing examples of similar distributions to get an idea of what functions to include, what variables are needed, and the proper syntax.        Note: This is the most time-consuming step in the entire process of implementing a new distribution. All files in this directory will follow this naming format: Distribution&lt;NameOfDistribution&gt;.[cpp|h]                  Navigate to revlanguage/workspace/RbRegister_Dist.cpp    Every new implementation must be registered in RevBayes. All register files are located in the revlanguage/workspace directory, and there are different files for the different types of implementations (RbRegister_Func.cpp is for registering functions; RbRegister_Move is for registering moves; etc.).  We are implementing a distribution, so we will edit the RbRegister_Dist.cpp file.        You need to have an include statement at the top of the RbRegister script, to effectively add your distribution to the RevBayes language. You also need to add code to the bottom of this file, and give it a type and a new constructor. Generally, you can look within the file for an idea of proper syntax to use.    For the Beta Binomial distribution, we navigate to the section in the file with the header ‘Distributions’ and then look for the sub-header dealing with ‘math distributions’. Then, add the following line of code:    #include \"Dist_betaBinomial.h\"        This step registers the header file for the beta binomial distribution, effectively adding it to RevBayes.    Next, navigate to the section of the file that initializes the global workspace. This section defines the workspace class, which houses info on all distributions. Then, add the following line of code:    AddDistribution&lt; Natural\t\t&gt;( new Dist_betaBinomial());        This adds the distribution to the workspace. Without this step, the beta binomial distribution will not be added to the revlanguage.    Note: Depending on the kind of distribution, you may need to change Natural to a different type (e.g. Probability, Real, RealPos, etc.).    After registering your distribution, you are ready to test your code.        Before pushing your changes, you should ensure your code is working properly.    There are multiple ways to do this. As a best practice, you should first compile it to ensure there are no errors. Once it compiles with no problems, you can test in various ways (e.g. run each individual function within the new Beta Binomial distribution in R, then run the Binomial distribution with a Beta prior in Rev and see if the output matches). For more information, see the developer tutorials on validation and testing. (TODO)    After ensuring your code runs properly, you are ready to add it to the git repo. We recommend reading through the RevBayes Git Workflow section of the Developer’s guide before pushing.  ",
        "url": "/developer/implementation/implement_distributions.html",
        "index": ""
      }
      ,
    
      "developer-implementation-implement-functions-html": {
        "title": "Implementing a function",
        "content": "There are two main classes of functions in RevBayes: regular functions and member functions.  For our example, we will be implement a regular function. First we need to add two files to the RevBayes source code, Func_hyperbolicCosineFunction.cpp and Func_hyperbolicCosineFunction.h. These will go within revbayes/src/revlanguage/functions/math since we are adding a function and it is a mathematical function. Second, we need to register the function by modifying revbayes/src/revlanguage/workspace/RbRegister_Func.cpp.RevLanguage Header fileThe file Func_hyperbolicCosine.h should look like the following:#ifndef Func_hyperbolicCosine_h#define Func_hyperbolicCosine_h#include \"Real.h\"#include \"RlTypedFunction.h\"#include &lt;string&gt;namespace RevLanguage {        /**     * The RevLanguage wrapper of the hyperbolic Cosine function (sinh()).     *     * The RevLanguage wrapper of the hyperbolic function function connects     * the variables/parameters of the function and creates the internal HyperbolicCosineFunction object.     * Please read the HyperbolicCosineFunction.h for more info.     *     *     * @copyright Copyright 2024-     * @author The RevBayes Development Core Team (&lt;your-name&gt;)     * @since 2024-04-26, version 1.0     *     */    class Func_hyperbolicCosine :  public TypedFunction&lt;Real&gt; {            public:        // Basic utility functions        Func_hyperbolicCosine*                         clone(void) const;                                          //!&lt; Clone the object        static const std::string&amp;                       getClassType(void);                                         //!&lt; Get Rev type        static const TypeSpec&amp;                          getClassTypeSpec(void);                                     //!&lt; Get class type spec        std::string                                     getFunctionName(void) const;                                //!&lt; Get the primary name of the function in Rev        const TypeSpec&amp;                                 getTypeSpec(void) const;                                    //!&lt; Get the type spec of the instance                // Function functions you have to override        RevBayesCore::TypedFunction&lt;double&gt;*            createFunction(void) const;                                 //!&lt; Create internal function object        const ArgumentRules&amp;                            getArgumentRules(void) const;                               //!&lt; Get argument rules            };    }#endif /* Func_hyperbolicCosine_h */RevLanguage Implementation fileAnd the Func_hyperbolicCosine.cpp should look like this:#include \"Func_hyperbolicCosine.h\"#include \"Probability.h\"#include \"Real.h\"#include \"RlDeterministicNode.h\"#include \"TypedDagNode.h\"#include \"GenericFunction.h\"using namespace RevLanguage;double* hyperbolicCosine(double x){    double result = (exp(x) + exp(-x))/2;    return new double(result);}/** * The clone function is a convenience function to create proper copies of inherited objected. * E.g. a.clone() will create a clone of the correct type even if 'a' is of derived type 'b'. * * \\return A new copy of the function. */Func_hyperbolicCosine* Func_hyperbolicCosine::clone( void ) const{    return new Func_hyperbolicCosine( *this );}RevBayesCore::TypedFunction&lt;double&gt;* Func_hyperbolicCosine::createFunction( void ) const{    RevBayesCore::TypedDagNode&lt;double&gt;* x = static_cast&lt;const Real&amp;&gt;( this-&gt;args[0].getVariable()-&gt;getRevObject() ).getDagNode();        return RevBayesCore::generic_function_ptr&lt; double &gt;( hyperbolicCosine, x );}/* Get argument rules */const ArgumentRules&amp; Func_hyperbolicCosine::getArgumentRules( void ) const{    static ArgumentRules argumentRules = ArgumentRules();    static bool          rules_set = false;        if ( !rules_set )    {                argumentRules.push_back( new ArgumentRule( \"x\", Real::getClassTypeSpec(), \"The value.\", ArgumentRule::BY_CONSTANT_REFERENCE, ArgumentRule::ANY ) );                rules_set = true;    }    return argumentRules;}const std::string&amp; Func_hyperbolicCosine::getClassType(void){    static std::string rev_type = \"Func_hyperbolicCosine\";        return rev_type;}/* Get class type spec describing type of object */const TypeSpec&amp; Func_hyperbolicCosine::getClassTypeSpec(void){    static TypeSpec rev_type_spec = TypeSpec( getClassType(), new TypeSpec( Function::getClassTypeSpec() ) );        return rev_type_spec;}/** * Get the primary Rev name for this function. */std::string Func_hyperbolicCosine::getFunctionName( void ) const{    // create a name variable that is the same for all instance of this class    std::string f_name = \"cosh\";        return f_name;}const TypeSpec&amp; Func_hyperbolicCosine::getTypeSpec( void ) const{    static TypeSpec type_spec = getClassTypeSpec();        return type_spec;}The function RevBayesCore::generic_function_ptr&lt; ResultType &gt;(CppFunction, arg1, ...) in Func_hyperbolicCosine::createFunction( ) automatically constructs a RevBayesCore::Function from the C++ function so that we do not have to.If CppFunction is has multiple type signatures, as is the case with functions like sqrt( ), generic_function_ptr&lt; &gt; will not work.In such cases, we can solve this problem by writing a wrapper function with a single type signature.For example,// Specialize sqrt to only work on doublesdouble mysqrt(double x){   return sqrt(x);}Registering the new functionIn order to make the new function available for use within the Rev language, we first need to register it.To do this,  Go to the src/revlanguage/workspace/ directory.  Open the file RbRegister_Func.cpp file in your editor.  Scroll down until you find the #include commands for math functions.  Add the line #include \"Func_hyperbolicCosine.h\" in the correct alphabetical order for that group.  Scroll down in that file until you find the section of the code that adds math functions.  Add the line addFunction( new Func_hyperbolicCosine() );Functions with caching and optimized recalculationThe above method should be prefered for implementing new functions in RevBayes unless the function needs to save intermediate results and use them during recalculation.For example, if our function computes x*(y+z), then if only x has changed, we could save the old value of y+z and re-use it instead of computing y+z from scratch.  (In this case, y+z is not expensive enough to be worth saving, but is just a simple illustration of an intermediate result.)In such cases we will also need to write a RevBayesCore::Function.A RevBayesCore::Function serves a different role than a RevLanguage::Function.A RevLanguage::Function interprets the function arguments and connects them to the RevBayesCore::Function, which performs the actual calculation and holds the result.If we write our own RevBayesCore::Function, then we can use the update() method to intelligently recalculate the value,and we can add data memembers to save intermediate results;Here will illustrate how to write a simple RevBayesCore::Function using the hyperbolic cosine example.Our example does’t actually save any intermediate results, it just illustrates how a RevBayesCore::Function works.It is therefore equivalent to the RevBayesCore::Function that is automatically generated by generic_function_ptr.RevBayesCore Header fileFirst, we will write our new header file. Within our header file, we need to #include a few other RevBayes header files, including TypedDagNode.h since our typed function deals with nodes of DAGs. Note that the directory structure of core is similar to that of the revlanguage.  The file src/core/functions/math/HyperbolicCosineFunction.h will be:#ifndef HyperbolicCosineFunction_h #define HyperbolicCosineFunction_h#include \"TypedFunction.h\"#include \"TypedDagNode.h\"#include &lt;cmath&gt;namespace RevBayesCore {    /**     * \\brief Hyperbolic Cosine of a real number.     *     * Compute the hyperbolic cosine of a real number x. (cosh(x) = (exp(x) + exp(-x))/2).     *     * \\copyright (c) Copyright 2009-2018 (GPL version 3)     * \\author &lt;your-name&gt;     * \\since Version 1.0, 2015-01-31     *     */    class HyperbolicCosineFunction : public TypedFunction&lt;double&gt; {        public:                                          HyperbolicCosineFunction(const TypedDagNode&lt;double&gt; *a);             HyperbolicCosineFunction*     clone(void) const; //!&lt; creates a clone            void                          update(void);      //!&lt; recomputes the value        protected:            void                          swapParameterInternal(const DagNode *oldP, const DagNode *newP); //!&lt; Implementation of swapping parameters        private:            const TypedDagNode&lt;double&gt;* x;    };}#endifThe first part of this file should be the standard header that goes in all the files giving a brief description about what that file is as well as information about the copyright and the author of that file.RevBayesCore Implementation fileNext, after including the necessary header files, we have to ensure that our new function is included within the RevBayesCore namespace.Here we are implementing our hyperbolic cosine function as its own class that is derived from the typed function class. This class stores the hyperbolic cosine of a value that is held in a DAG node. We have also defined a clone method which can create a clone of our class, and an update method which will update the value of our Hyperbolic Cosine class whenever the value of the DAG node changes.The file src/core/functions/math/HyperbolicCosineFunction.cpp will look like:#include \"HyperbolicCosineFunction.h\"using namespace RevBayesCore;HyperbolicCosineFunction::HyperbolicCosineFunction(const TypedDagNode&lt;double&gt; *a) : TypedFunction&lt;double&gt;( new double(0.0) ),x( a ){    addParameter( a );}HyperbolicCosineFunction* HyperbolicCosineFunction::clone( void ) const{    return new HyperbolicCosineFunction(*this);}void HyperbolicCosineFunction::swapParameterInternal(const DagNode *oldP, const DagNode *newP){    if (oldP == x)    {        x = static_cast&lt;const TypedDagNode&lt;double&gt;* &gt;( newP );    }    }void HyperbolicCosineFunction::update( void ){    // get the current value of x    double xValue = x -&gt; getValue();    // compute the function result    double result = (exp(xValue) + exp(-xValue))/2;    // update the stored value    *value = result;        }Alternate implemention for createFunctionIn order to use the new RevBayesCore::HyperbolicCosineFunction class, we need to modify the RevLanguage::Func_hyperbolicCosine class to use it instead of generic_function_ptr( ).  To do this,  Open src/revlanguage/functions/math/Func_hyperbolicCosine.cpp in your editor.  Add an #include statement to make the RevBayesCore::HyperbolicCosineFunction class visible  Modify the definition of Func_hyperbolicCosine::createFunction() as follows:...#include \"HyperbolicCosineFunction.h\"...         RevBayesCore::TypedFunction&lt;double&gt;* Func_hyperbolicCosine::createFunction( void ) const{        RevBayesCore::TypedDagNode&lt;double&gt;* x = static_cast&lt;const Real&amp;&gt;( this-&gt;args[0].getVariable()-&gt;getRevObject() ).getDagNode();        return new RevBayesCore::HyperbolicCosineFunction( x );}",
        "url": "/developer/implementation/implement_functions.html",
        "index": ""
      }
      ,
    
      "developer-implementation-implement-moves-html": {
        "title": "Implementing a Metropolis-Hastings Move",
        "content": "General info before getting startedThe steps to implementing a new move vary slightly, depending on the move’s type (e.g., Metropolis-Hastings versus Gibbs). For the purpose of this guide, we will focus on a Metropolis-Hastings move.In general, the fastest and easiest way to get help is to find the most similar move already implemented in RevBayes and use it as a guide. Remember that, as with implementing a new distribution or function, you’ll need to add relevant code to both the core of RevBayes and the language. Also remember that you’ll need to work out the math appropriate for your move (e.g., the Hastings ratio) ahead of time.Steps      Orienting within the repository - For the core, navigate in the repository to src/core/moves. For a Metropolis-Hastings move, we’ll then go into the proposal directory. In this directory, you can find several templates for generic proposal classes, as well as subdirectories containing moves for specific parameter types. To keep things easy, we’ll focus on a single scalar parameter, so we’ll navigate one step further into the scalar directory. For the language, navigate to src/revlanguage/moves. For this example, as we did in the core, we’ll focus on a move for a scalar parameter, so we’ll then open scalar. To register our new move after it’s implemented, we’ll also need to update the file src/revlanguage/workspace/RbRegister_Move.cpp.        Creating new files for the core - As an example, we’ll implement a new move that draws a random value from a Gamma distribution and proposes a new scalar by multiplying the current value by the draw from the Gamma. This move will be called a “Gamma Scaling move”. Since this move is similar to an existing scaling move, we can start by copying the file ScaleProposal.h and naming the new copy GammaScaleProposal.h. As a reminder, we’re working in the directory src/core/moves/proposal/scalar/.    Once the new header file is created and named, we can update the content to match our new move. The simplest changes involve renaming things to match the new move (e.g., updating the preprocessor macro from ScaleProposal_H to GammaScaleProposal_H, or changing the name of object references and constructor from ScaleProposal to GammaScaleProposal). The comments at the top of the header file that describe how the move works should also be updated, but these changes will obviously be specific to the move being implemented.    Next, we’ll need to create a new .cpp file containing the implementation of our new move. As with the header, it’s easiest to copy and rename an existing file, so we’ll use ScaleProposal.cpp as our template, copy it, and rename to GammaScaleProposal.cpp. As with the header file, most of the necessary changes involve updating the names of variables and function names. If the move requires access to other math functions, additional header files may need to be included at the top. Explore src/core/math as needed to find the necessary functions or distributions. In this case, we will need access to methods associated with the Gamma distribution, so we will add #include \"DistributionGamma.h\". For our example, the number and type of variables used by our move are the same as our template based on the Scale move, so we don’t need to modify the constructor or variable initialization, other than updating the constructor name. Similarly for this example, we don’t need to alter the code in the ::cleanProposal, ::clone, ::prepareProposal, ::printParameterSummary, ::undoProposal, ::swapNodeInternal, and ::tune methods as these are common to our template and new moves (and will be identical to many of the scalar moves), but we do need to update the class names associated with the methods (i.e., ScaleMove:: -&gt; GammaScaleMove::). For the ::getProposalName method, we need to update the string in the method that provides a descriptive name for the move - name = \"Gamma Scaling\". The bulk of the necessary changes for the new move will come in the ::propose method and the help description above the method. For this example, the new ::propose method looks like this:    /* * Perform the proposal. * * A gamma scaling proposal draws a random number from a gamma distribution u ~ Gamma(lambda,1) and scales the current vale by u * lambda is the tuning parameter of the proposal which influences the size of the proposals by changing the shape of the Gamma. * * \\return The hastings ratio. */double GammaScaleProposal::propose( double &amp;val ){        // Get random number generator    RandomNumberGenerator* rng     = GLOBAL_RNG;        // copy value    storedValue = val;        // Generate new value (no reflection, so we simply abort later if we propose value here outside of support)    double u = RbStatistics::Gamma::rv(lambda,1,*rng);    val *= u;        // compute the Hastings ratio    double ln_hastings_ratio = 0;    try    {        // compute the Hastings ratio        double forward = RbStatistics::Gamma::lnPdf(lambda, 1, u);        double backward = RbStatistics::Gamma::lnPdf(lambda, 1, (1.0/u));                ln_hastings_ratio = backward - forward - log(u);    // The -log(u) term is the Jacobian    }    catch (RbException e)    {        ln_hastings_ratio = RbConstants::Double::neginf;    }    return ln_hastings_ratio;}        In this case, the Hastings ratio involves the probability density of the forward move (the scaling factor u), the density of the corresponding backward move (the scaling factor $\\frac{1}{u}$), and the solution to the Jacobian ($-\\frac{1}{u}$).        Creating new files for the rev language - After implementing the detailed machinery to perform the new move in the RevBayes core, you need to modify a few files associated with the Rev language to make it available to users. As a reminder, the first set of these files is found in src/revlanguage/moves. For our example, we will be focusing specifically on a move for a scalar value, so navigate to the scalar subdirectory. As with the files in the core, we will copy and rename a header (.h) and implementation (.cpp) file from the Scale move. In this case, our new header file will be called Move_GammaScale.h and our new implementation file will be called Move_GammaScale.cpp. In the header file, simply update the names of the preprocessor macro, the class, and the objects. Also remember to update the help comments near the top of the file.    In the new .cpp file, begin by updating the names of the header files included at the top. Note that we include, and will need to update, the names of both the header for the core GammaScaleProposal.h and the workspace Move_GammaScale.h. Most of the rest of the changes in this file involve updating the names of classes and objects associated with this move, but we also need to update the string specifying the type in the ::getClassType method and specifying the constructor name in the ::getMoveName method. Pay special attention to the rules specified in ::getParameterRules and make sure they satisfy the constraints required by the new move. Update these rules as needed, using existing rules from other moves as templates.    For our particular example, we also need to make one additional change. Because we’ve only updated the ScaleProposal, and not the ScaleProposalContinuous, we need to remove the part of the code in this move that could call ScaleProposalContinuous. The ::constructInternalObject method now looks like this (compare to the corresponding method from Move_Scale.cpp):    void Move_GammaScale::constructInternalObject( void ){    // we free the memory first    delete value;        RevBayesCore::Proposal *p = NULL;        // now allocate a new sliding move    double d = static_cast&lt;const RealPos &amp;&gt;( lambda-&gt;getRevObject() ).getValue();    double w = static_cast&lt;const RealPos &amp;&gt;( weight-&gt;getRevObject() ).getValue();    double r = static_cast&lt;const Probability &amp;&gt;( tuneTarget-&gt;getRevObject() ).getValue();    RevBayesCore::TypedDagNode&lt;double&gt;* tmp = static_cast&lt;const RealPos &amp;&gt;( x-&gt;getRevObject() ).getDagNode();    RevBayesCore::StochasticNode&lt;double&gt; *n = dynamic_cast&lt;RevBayesCore::StochasticNode&lt;double&gt; *&gt;( tmp );    p = new RevBayesCore::GammaScaleProposal(n, d, r);    bool t = static_cast&lt;const RlBoolean &amp;&gt;( tune-&gt;getRevObject() ).getValue();        value = new RevBayesCore::MetropolisHastingsMove(p, w, t);    }            Updating the Rev language register - The last step in implementing our new move is to make sure that it’s registered in the Rev language. To do this, we will need to update the file RbRegister_Move.cpp in src/revlanguage/workspace. In this file, we’ll need to include the header for our new move #include \"Move_GammaScale.h\" in the section corresponding to moves on real values. We’ll also need to add the constructor to the workspace by updating the ::initializeMoveGlobalWorkspace method to include addTypeWithConstructor( new Move_GammaScale() );, again in the section corresponding to moves on real values.        Testing the performance of the new move - If properly implemented, a new move can be validated by running an MCMC analysis where the clamped data are ignored and one tries to sample only the prior. This can be done in RevBayes by calling model.ignoreAllData() to mark the data as ignored. The recommended strategy is to implement the simplest possible model that uses a variable of the type appropriate for the new move, and assigning that variable an easily validated prior (e.g., a uniform). Run the analysis with only the new move operating on that variable and then plot the variable’s marginal distribution to make sure it matches the prior.  ",
        "url": "/developer/implementation/implement_moves.html",
        "index": ""
      }
      ,
    
      "tutorials-concatenation": {
        "title": "Alignment concatenation",
        "content": "Overview: Gene tree-species tree modelsEver since (missing reference), researchers have acknowledged thatphylogenies reconstructed from homologous gene sequences could differfrom species phylogenies. As molecular sequences accumulated, the linkbetween gene trees and species trees started to be modeled. The firstmodels were based on parsimony, and aimed for instance at reconciling agene tree with a species tree by minimizing the number of events of geneduplication and gene loss. In the past dozen years, probabilistic modelshave been proposed to reconstruct gene trees and species trees in arigorous statistical framework. Models and algorithms have quickly grownin complexity, to model biological processes with increasing realism, toaccommodate several processes at the same time, or to handlegenome-scale data sets. In this overview we will not detail thesemodels, and we invite the interested reader to take a look at recentreviews (e.g., (missing reference)).Processes of discordThere are several reasons why a gene tree may differ from a speciestree. Of course, a gene tree may differ from the species tree justbecause a mistake was made during the analysis of the gene sequences, atany point in a pipeline going from the sequencing itself to the genetree reconstruction. Such a mistake would produce an incorrect genetree. Here we do not mean this kind of discord, but rather discord thatcomes from a real biological process that generates true gene historiesthat differ from true species histories. These processes include geneduplication, gene loss, gene transfer (used loosely here to also includereticulation, hybridization between species), and incomplete lineagesorting (Fig. [fig1]). In this tutorial we focus on Incomplete lineagesorting, which will be discussed in more details in the followingsubsection.Fig. [fig1] suggests that for all processes the gene tree can be seenas the product of a branching process operating inside the species tree.Indeed, all processes are modeled as some type of birth-death processrunning along the species tree. For duplication/loss models, birthcorrespond to gene duplication events, and death to gene loss events.Transfers can be added to the model by introducing another type ofbirth, with a child lineage appearing in another branch of the speciestree. Incomplete lineage sorting is also modeled with a birth-death typeof model, the coalescent. All these models can be made heterogeneous,for instance by allowing different sets of parameters for differentbranches of the species tree. This is useful to model differences inrates of duplication, loss or transfer among species, or to modeldifferent effective population sizes in a species tree. InRevBayes so far only models of incomplete lineage sortinghave been implemented (models of duplication and loss and transfer willsoon be added). Thanks to RevBayes’ modular design, thereis quite a lot of flexibility in specifying the model, for instance byassociating different parameters to different branches of the speciestree, or by combining the gene tree-species tree model to other types ofmodels, for instance models of trait evolution, or models of relaxedmolecular clock.Gene tree discordance is a problem for species tree reconstructionThere have been several approaches to species tree reconstruction:concatenation and supertree approaches, which have been used for quitesome time now, and more recently methods that rely on gene tree-speciestree models.      Concatenation simply consists in taking all gene alignments,concatenating them into one super alignment, and then analyzing itas if it were a single gene sequence. More sophisticated approachesallow different partitions for different genes, but the mainassumption at the heart of this approach is that all sites of allgenes have evolved according to the same species tree. Thisassumption is often not correct because all the processes of discordpresented above conspire to make gene trees different from thespecies tree. In practice, this matters: simulation studies havefound that in the presence of incomplete lineage sorting, in someparticular areas of the parameter space, concatenation will oftenreturn an incorrect species tree (Leaché and Rannala 2011). Concatenation mayalso be a questionable approach in prokaryotic phylogenetics, wherethe quest for a tree of life has been difficult, to the point thatsome doubted that one could find a meaningful species treerepresenting vertical descent. Nonetheless, the concatenationapproach may be fairly robust to lateral gene transfers, as itreturns good species trees (arguably better than small subunit orlarge subunit rRNA trees) in a range of prokaryotic groups(missing reference).        Supertree approaches differ from concatenation notably by discardingsequence information once individual gene trees have been built.Contrary to concatenation approaches that combine individual genealignments, supertree approaches combine individual gene trees toobtain a species tree. Most supertree methods are not based on anexplicit model of the processes causing discordance between genetrees and species tree (although there are exceptions, notablymodelling incomplete lineage sorting, see below). Instead, they aimat finding a tree that would best describe the distribution of genetrees, according to some fairly arbitrary criterion. In practice,these methods have been found to provide reasonable results in manycases, but in simulations they are usually less accuratethan concatenation.        Methods that rely on gene tree-species tree models appear verypromising as they explicitly model the processes of discord. Theadvantage of these models is that we account for processes that weknow have taken a part in generating the data, thus possiblyimproving the accuracy and robustness of our inferences. Further,these models can be combined withe.g., models of sequence evolution,models of co-evolution between gene trees, or models of traitevolution. However, these models are computationally challenging touse, because they require estimating jointly gene trees, speciestrees, and other parameters that entertain strong correlations. As aconsequence, in many gene tree-species tree models, devising awell-mixing MCMC strategy can be problematic.  Concatenating genes to model species evolutionThe simplest model to estimate a species tree when you have severalgenes is to concatenate your genes and to assume that all gene trees areexactly equal to the species tree. You thus assume that there is nodiscordance between gene trees in the hope that the combined informationin all genes will provide a reliable signal for the species tree.The exercises assume you have a working installation of RevBayes. Inthis introductory tutorial, we will apply the concatenated model to 10gene alignments from 23 primate species. We will assume that:      The species tree is drawn from a constant birth-death process.        All genes share the same tree, that is, both the topology andbranch lengths.        Each gene has its own set of substitution model and clock parameterswhich are drawn from a shared prior distribution.        Gene sequences are evolved according to an HKY model with gammadistributed rate variation among sites and a strict global clock.        Here, we run an MCMC on this model, using data from 10 genes in 23mammalian species.  Scripts are all placed in$tutorials/RB_GeneConcatenation_Tutorial/RevBayes_scripts/$.      Open RevBayes        Let’s load all 10 gene alignments.    # read in each data matrix together which will create a vector of objectsdata = readDiscreteCharacterData(\"data/merged.nex\")# Now we get some useful variables from the data. We need these later on.num_loci = data.size()# get the number of speciesn_species &lt;- data[1].ntaxa()# get the taxon information (e.g. the taxon names)taxa &lt;- data[1].taxa()n_branches &lt;- 2 * n_species - 1 # number of branches in a rooted tree# We set our move indexmi = 0            We specified a constant-rate birth-death process as our prior on thespecies tree. The birth-death process has a speciation andextinction rate as its parameters. We will use here a transformationand specify priors on the speciation rate and relative extinctionrate. Additionally, we calibrate the tree by assuming that the crownage of primates is around 75 MYA. Thus, we specify a normaldistribution with mean 75 and standard deviation 2.5 as the prior onthe root age. Since the root age can only be a positive real numberwe truncate the normal distribution at 0.    # Specify a prior on the diversification and turnover ratespeciation ~ dnGamma(2,2)relativeExtinction ~ dnBeta(1,1)# Now transform the diversification and turnover rates into speciation and extinction ratesextinction := speciation * relativeExtinction# Specify a prior on the root age (our informed guess is about ~75 mya)# Note that we use a truncated normal distribution because the root age must be positiveroot ~ dnNormal(mean=75,sd=2.5,min=0.0, max=Inf)sampling_fraction &lt;- 23 / 450 # we sampled 23 out of the ~ 450 primate species# create some moves that change the stochastic variables# Moves are sliding and scaling proposalsmoves[++mvi] = mvSlide(diversification,delta=1,tune=true,weight=2)moves[++mvi] = mvSlide(relativeExtinction,delta=1,tune=true,weight=2)moves[++mvi] = mvScale(diversification,lambda=1,tune=true,weight=2)moves[++mvi] = mvScale(relativeExtinction,lambda=1,tune=true,weight=2)moves[++mvi] = mvSlide(root,delta=1,tune=true,weight=0.2)# construct a variable for the tree drawn from a birth-death processpsi ~ dnBDP(lambda=speciation, mu=extinction, rootAge=root, rho=sampling_fraction, taxa=taxa )moves[++mvi] = mvNarrow(psi, weight=5.0)moves[++mvi] = mvNNI(psi, weight=1.0)moves[++mvi] = mvFNPR(psi, weight=3.0)moves[++mvi] = mvGPR(psi, weight=3.0)moves[++mvi] = mvSubtreeScale(psi, weight=3.0)moves[++mvi] = mvNodeTimeSlideUniform(psi, weight=15.0)            Now that we have a species tree, which we assume is shared exactlyfor all genes. That is, we assume each gene evolves under exactlythe same tree, and thus each gene tree is equivalent to the speciestree. Nevertheless, we assume that each gene evolves at a differentrate and with its own substitution model parameters. Here we willassume for simplicity that every gene evolves under a global strictclock but has its own independent clock rate. We assume that thelogarithm of the clock rate is uniformly distribution, thus wespecify in effect a log-uniform prior distribution. This priorassumption means that we put the same prior probability on values ofeach magnitude, e.g., values between 0.0001 and 0.001 have the sameprior probability as values between 10 and 100.    for ( i in 1:num_loci ) {    log_clock_rate[i] ~ dnUniform(-8,4)   clock_rate[i] := 10^log_clock_rate[i]          moves[++mvi] = mvSlide(log_clock_rate[i], weight=1.0)}            Next we need our model for the substitution process. Hence, we justneed to define the substitution matrix. We use a single HKY matrixthat will apply to all sites per gene. Additionally, we assume thatsites evolve according to one of four possible rates, where eachrate corresponds to a quantile from a gamma distribution.    for ( i in 1:num_loci ) {    #### specify the HKY substitution model applied uniformly to all sites of a gene    kappa[i] ~ dnLognormal(0,1)    moves[++mvi] = mvScale(kappa[i],weight=1)    pi_prior[i] &lt;- v(1,1,1,1)     pi[i] ~ dnDirichlet(pi_prior[i])    moves[++mvi] = mvSimplexElementScale(pi[i],weight=2)    #### create a deterministic variable for the rate matrix    Q[i] := fnHKY(kappa[i],pi[i])     #### create the rates to model the gamma distributed rate variation among sites.    alpha_prior[i] &lt;- 0.05    alpha[i] ~ dnExponential( alpha_prior[i] )    gamma_rates[i] := fnDiscretizeGamma( alpha[i], alpha[i], 4, false )    # add moves for the stationary frequencies, exchangeability rates and the shape parameter    moves[++mvi] = mvScale(alpha[i],weight=2)}            Finally, we can create our distribution for the character evolution.We will use the common ‘PhyloCTMC‘ distribution, which is acontinuous time Markov process along a phylogenetic tree. We createa ‘seq‘ variable and attach/clamp each gene to one of the‘seq‘ variables.    for ( i in 1:num_loci ) {     # the sequence evolution model    seq[i] ~ dnPhyloCTMC(tree=psi, Q=Q[i], branchRates=clock_rate[i], siteRates=gamma_rates[i], type=\"DNA\")    # attach the data    seq[i].clamp(data[i])}            Now we have defined all the bricks of the model, and create ourmodel object from it.    # We get a handle on our model.# We can use any node of our model as a handle, here we choose to use the topology.mymodel = model(psi)            Finally, we need to perform inference under the model, usingthe data.    # Monitors to check the progression of the programmonitors[1] = mnScreen(printgen=100, root)monitors[2] = mnModel(filename=\"output/primates_concatenation_root_calibration\",printgen=10, separator = TAB)monitors[3] = mnFile(filename=\"output/primates_concatenation_root_calibration\",printgen=10, separator = TAB, psi)# Here we use a plain MCMC. You could also set nruns=2 for a replicated analysis# or use mcmcmc with heated chains.mymcmc = mcmc(mymodel, monitors, moves)# This should be sufficient to obtain enough MCMC samplesmymcmc.burnin(generations=3000,tuningInterval=100)mymcmc.run(generations=10000)            Now we can perform some post-run analyses.    # Now, we will analyze the tree output.# Let us start by reading in the tree tracetreetrace = readTreeTrace(\"output/primates_concatenation_root_calibration\", treetype=\"clock\")# and get the summary of the tree tracetreetrace.summarize()mapTree(treetrace,\"output/primates_concatenation_root_calibration\")      ",
        "url": "/tutorials/concatenation/",
        "index": "false"
      }
      ,
    
      "developer-tutorial": {
        "title": "Contributing a RevBayes tutorial",
        "content": "\tContributing a RevBayes tutorial\t\t  Last modified on May 30, 2019  Overview  Getting startedThis tutorial will take you through the steps of contributing a new RevBayes tutorial to this website.In order to build and test your tutorial, you will need to download and host an instance of the RevBayes website on your local machine. The site was built using Jekyll and is hosted by GitHub Pages. Please refer to the README.md in the site’s GitHub repository for instructions on setting up and hosting the site on your computer.This website is generated from simple plain-text files in Markdown format with YAML front matter, with additional support for accessing/modifying page attribute data using Liquid templates. This tutorial presumes the reader is already at least somewhat familiar with these markup tools.Tutorial directory organizationEach tutorial follows the same general file organization:revbayes.github.io└── tutorials    └── new_tutorial        ├── data/        ├── scripts/        ├── figures/        └── index.md &lt;-- main text of tutorialSpecifically, your new RevBayes tutorial should be placed in its own directory (e.g. new_tutorial) under the tutorials directory of the root website repository. The main text of the tutorial should be written in a Markdown file named index.md (although any unique name will work). If the tutorial uses additional data, scripts or image files they should be stored in the subdirectories data, scripts and figures respectively. Your tutorial can include additional pages or other files, but the above basic components are used in integrating your tutorial with the rest of the website.Writing tutorial front matterIn order for Jekyll to process your tutorial’s page, it must include YAML front matter, indicated by------You will populate the front matter with YAML attributes that are used to organize your tutorial on the RevBayes website. The most basic attributes are title, subtitle, authors which are displayed in the tutorial’s title header section.---title: How to sciencesubtitle: A postmodern experienceauthors:- Alice- Boblevel: 0---The level attribute is used to group tutorials with similar levels of complexity in the tutorial index.At the moment, only these levels are recognized:level: 0 is for Introduction to RevBayeslevel: 1 is for Introduction to MCMClevel: 2 is for Model Selection and Testinglevel: 3 is for Standard tree inferencelevel: 4 is for Complex hierarchical models for phylogenetic inferencelevel: 5 is for Discrete Morphologylevel: 6 is for Comparative methodslevel: 7 is for Diversification Rate Estimationlevel: 8 is for Biogeographylevel: 9 is for Demographic InferenceAny other level is for Miscellaneous tutorialsFront matter attributesHere is a glossary of all the tutorial attributes used by this site.            Attribute      Description                  title      Tutorial title displayed in the title header and the tutorial index              subtitle      Italicized description displayed below the title.              authors      Title authors. Displayed in the title header.              level      Numeric level 0-2 indicating basic, intermediate or advanced subject matter.              prerequisites      List of prerequisite tutorials.              include_files      List of file name patterns that should be included in the tutorial file download.              exclude_files      List of file name patterns that should not be included in the download (overrides matching include_files).              include_all      Automatically include files in the data and scripts tutorial subdirectories?              include_example_output      Automatically include example output files from the example_output directory in tutorial?              index      Boolean indicating whether the tutorial should be included in the main tutorial index              order      Number indicating the order in which this tutorial should appear in its level in the tutorial index.              keywords      List of keywords for grouping tutorials with related subject matter.              title-old      Name of the corresponding tutorial in the deprecated Latex repository              redirect      Automatically redirect to the deprecated Latex tutorial? Used for tutorials not yet updated for this site.              permalink      Address relative to {{ baseurl }} where you want the tutorial to be accessible on the internet. You may identify the primary version of your tutorial by setting this to the relative path of your tutorial’s home directory.      Filling in the overview boxAt the top of each tutorial, an Overview box is displayed with a list of prerequisite tutorials and a table of contents. Take another look at the overview for this tutorial.Including prerequisitesIf you would like to refer users to prequisite tutorials in the tutorial’s overview section, list the names of the prequisite tutorials in the prerequisites YAML attribute. Tutorials can be referred to by any unique path identifier, ignoring index.md and .md suffixes.prerequisites:- intro- ctmc- clocks/simpleFilling in the table of contentsTo create a heading that will be included in the table of contents, you can use the section and subsection Liquid tags, which take the heading text and anchor id as arguments, separated by a pipe |. Sections are referenced using the ref Liquid tag.{% section This is a section header | section1 %}This is a reference to {% ref section1 %}This is a section headerThis is a reference to {% subsection This is a subsection header | subsection1 %}This is a reference to {% ref subsection1 %}This is a subsection headerThis is a reference to Including data files and scriptsAt the top of each tutorial, the Data files and scripts box contains a list of files that can be downloaded by clicking the  icon.    Data files and scripts                    Data Files                                          example.nex                                    Scripts                                          example.Rev                    Files in the data and scripts directories of your tutorial are added to the Data files and scripts box automatically. You can include files from another tutorial by listing their relative paths in the include_files YAML front matter attribute. You can exclude files using the exclude_files attribute.include_files:    - ctmc/data/primates_and_galeopterus_cytb.nexexclude_files:    - data/useless_file.nexIncluding script snippetsYou can dynamically include snippets from your script files using the snippet filter. You can extract a range of line numbers using the lines option, or you can extract a block of text using the block option. You can refer to the script file by name, or you can use a Liquid variable, for instance, obtained from the page.files array, or using the match_file filter.Here are lines 1-7 of `example.Rev`{{ \"example.Rev\" | snippet:\"line\",\"1-7\" }}Here is the third block of text from `{{ page.files[1].name }}`{{ page.files[1] | snippet:\"block\",\"3\" }}{% assign example_script = \"example.Rev\" | match_file %}Here are the second and third blocks of text from `{{ example_script.name }}`{{ example_script | snippet:\"block#\",\"2-3\" }}Here are lines 1-7 of example.Rev# this is an example Rev script# this is line 2a ~ dnExp(1)# this is the second block# this is line 6b ~ dnBeta(1,1)Here is the third block of text from example.Rev# this is the third block# this is line 10c := a + bHere are the second and third blocks of text from example.Revb ~ dnBeta(1,1)c := a + bFormatting codeYou can format code using the ``` or ~~~ fenced code delimiters. By default, tutorial code is formatted for Rev```for (i in 1:n_branches) {   br_lens[i] ~ dnExponential(10.0)   moves[mvi++] = mvScale(br_lens[i]) }```for (i in 1:n_branches) {   br_lens[i] ~ dnExponential(10.0)   moves[mvi++] = mvScale(br_lens[i]) }Code representing output from the Rev console can be formatted using the {:.Rev-output} class```Lorem ipsum dolor sit amet, consectetur adipiscing elit,sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.```{:.Rev-output}Lorem ipsum dolor sit amet, consectetur adipiscing elit,sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.Other formatting optionsOther code formats can be assigned using the {:.} syntax.{:.bash} - bash console input```ls tutorialsgrep phylogenetics tutorial.md```{:.bash}ls tutorialsgrep phylogenetics tutorial.md{:.instruction} - Instruction box&gt; Lorem ipsum dolor sit amet, consectetur adipiscing elit,&gt; sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.{:.instruction}  Lorem ipsum dolor sit amet, consectetur adipiscing elit,sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.{% aside %} - Aside box{% aside Aside header %}Lorem ipsum dolor sit amet, consectetur adipiscing elit,sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.{% endaside %}Aside headerLorem ipsum dolor sit amet, consectetur adipiscing elit,sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.{:.info} - Optional information box&gt; ## Info header&gt; Lorem ipsum dolor sit amet, consectetur adipiscing elit,&gt; sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.{:.info}  Info header  Lorem ipsum dolor sit amet, consectetur adipiscing elit,sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.Blockquote&gt; Lorem ipsum dolor sit amet, consectetur adipiscing elit,&gt; sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.  Lorem ipsum dolor sit amet, consectetur adipiscing elit,sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.Math notationMathematical notation using $\\LaTeX$ symbols are rendered with MathJax. You can write inline math using the $...$, $$...$$ or \\\\(...\\\\) delimiters, while display math uses \\\\[...\\\\].You can alternatively write display math using $$...$$ if it is separated from the previous text by a line break.Inline math$S = \\mathbb{R}$$$f(x) = x^2$$\\\\(\\implies \\min_{x\\in S}f(x) = 0\\\\)is coolDisplay math \\\\[f(a) = \\frac{1}{2\\pi i}\\int_{\\gamma}\\frac{f(z)}{z-a}dz\\\\]Alternative display math$$\\int(1-e^{-x})^n\\,dx=x-\\sum_{p=1}^n\\tfrac{1}{p}(1-e^{-x})^p$$Inline math$S = \\mathbb{R}$\\(f(x) = x^2\\)\\(\\implies \\min_{x\\in S}f(x) = 0\\)is coolDisplay math \\[f(a) = \\frac{1}{2\\pi i}\\int_{\\gamma}\\frac{f(z)}{z-a}dz\\]Alternative display math\\[\\int(1-e^{-x})^n\\,dx=x-\\sum_{p=1}^n\\tfrac{1}{p}(1-e^{-x})^p\\]Referencing equationsYou can number and reference equations using the \\label, \\tag and \\ref Latex commands.$$\\begin{align}\\frac{(s-1)^b}{(s x + y)^{b+p}} &amp;= \\frac{1}{(s x +y)^p}\\left(\\frac{s-1}{s x + y}\\right)^b \\label{equation1}\\tag{1} \\\\&amp;= \\sum_{i=0}^b \\binom{b}{i}(-1)^i \\frac{(x+y)^i}{x^b(sx+y)^{i+p}} \\label{equation2}\\tag{2}\\end{align}$$This is a reference to Equation $\\ref{equation1}$. This is a reference to Equation $\\ref{equation2}$.\\[\\begin{align}\\frac{(s-1)^b}{(s x + y)^{b+p}} &amp;= \\frac{1}{(s x +y)^p}\\left(\\frac{s-1}{s x + y}\\right)^b \\label{equation1}\\tag{1} \\\\&amp;= \\sum_{i=0}^b \\binom{b}{i}(-1)^i \\frac{(x+y)^i}{x^b(sx+y)^{i+p}} \\label{equation2}\\tag{2}\\end{align}\\]This is a reference to Equation $\\ref{equation1}$. This is a reference to Equation $\\ref{equation2}$.Modularizing your tutorialYou can modularize your tutorial by putting major sections in separate markdown files in your tutorial directory. These can be included in the main text using the include_relative Liquid tag. Make sure that only the main text Markdown file contains YAML front matter.For example, if you have an external file external_file.md with the following contentsThis is an external *Markdown* filewithout [YAML](http://camel.readthedocs.io/en/latest/yamlref.html) front matter.This *file* will be __formatted__ ~~as plain text~~ like the rest of the main text.You can even include $\\LaTeX$.You can include it in the main text as follows{% include_relative external_file.md %}This is an external Markdown filewithout YAML front matter.This file will be formatted as plain text like the rest of the main text.You can even include $\\LaTeX$.Making alternative versions of your tutorialAny Markdown file in your tutorial directory will be rendered as a separate tutorial as long as it has YAML front matter. For example, if we make a copy of the same external file (say external_file-yaml.md) and simply add an empty front matter section, then Jekyll will automatically generate this page when building the website.This makes it easy for you to create alternative versions of your tutorial. Each alternative versions is built from a Markdown file with YAML front matter, each of which then includes one or more external Markdown module text files that do not include front matter. Consider the following example modular tutorial structure.new_tutorial├── data/├── scripts/├── figures/├── modules/ &lt;-- module files (no front matter)│   ├── intro.md│   ├── exercise1.md│   ├── exercise2.md│   └── exercise3.md├── new_tutorial.md &lt;-- tutorial page (with front matter)└── v2.md           &lt;-- tutorial page (with front matter)In this example, there are two versions of the tutorial, both of which will get listed on the main Tutorials page. If you do not want your tutorial listed in the Tutorials index, use the index: false attribute in the YAML front matter.Linking to other tutorialsYou can reference another tutorial using the page_ref and page_url Liquid tags, or with the match_page filter. As with Prerequisites, tutorials can be referred to by any unique path identifier, ignoring index.md and .md suffixes.This is a reference to the {% page_ref intro %} tutorial.{% assign var = \"ctmc\" %}This how to link to the {% page_ref {{ var }} %} tutorial using a liquid variable,{% assign tut = \"clocks/simple\" | match_page %}This is how to retrieve the *{{ tut.title }}* tutorial page by name.The relative URL of the `{{ var }}` tutorial is `{% page_url {{ var }} %}`.This is a reference to the Getting Started with RevBayes and Rev Language Syntax tutorial.This how to link to the Nucleotide substitution models tutorial using a liquid variable,This is how to retrieve the Simple Diversification Models &amp; Estimating Time Trees tutorial page by name.The relative URL of the ctmc tutorial is /tutorials/ctmc/.FiguresFigures can be included from the figures subdirectory (or elsewhere) using the figure and figcaption Liquid tags. Figures are referenced using the ref Liquid tag.{% figure example %}&lt;img src=\"figures/example.png\" width=\"200\"&gt;{% figcaption %}This is an example figure caption. You can include *Markdown* and $\\LaTeX$.{% endfigcaption %}{% endfigure %}  This is a reference to {% ref example %}This is an example figure caption. You can include Markdown and $\\LaTeX$.This is a reference to TablesTable can be included in Markdown using the table and tabcaption Liquid tags. Tables are referenced using the ref Liquid tag.{% table exampletable %}  |    Range    | Bits  | Size | State |  |-------------|-------|------|-------|  |$\\emptyset$  |  000  |  0   |   0   |  |A            |  100  |  1   |   1   |  |B            |  010  |  1   |   2   |  |C            |  001  |  1   |   3   |  |AB           |  110  |  2   |   4   |  |AC           |  101  |  2   |   5   |  |BC           |  011  |  2   |   6   |  |ABC          |  111  |  3   |   7   |   {% tabcaption %}    Example table caption   {% endtabcaption %}{% endtable %}  This is a reference to {% ref exampletable %}            Range      Bits      Size      State                  $\\emptyset$      000      0      0              A      100      1      1              B      010      1      2              C      001      1      3              AB      110      2      4              AC      101      2      5              BC      011      2      6              ABC      111      3      7      Example table captionThis is a reference to Printing to PDFYou can prevent any element from being printed when the tutorial is sent to PDF or a printer by tagging it with {:.no-print}.Citations and bibliographyYou can include a citation using the cite and citet Liquid tags.This is a citation {% cite Felsenstein1981 %}.  This is an in-text citation of {% citet Felsenstein1981 %}.  This is a citation with multiple sources {% cite Felsenstein1981 Hoehna2016b %}.This is a citation (Felsenstein 1981).This is an in-text citation of Felsenstein (1981).This is a citation with multiple sources (Felsenstein 1981; Höhna et al. 2016).Citations are included in the References section at the end of each tutorial. References are formatted according to the CSL style for Systematic Biology.Felsenstein J. 1981. Evolutionary Trees from DNA Sequences: a Maximum Likelihood Approach. Journal of Molecular Evolution. 17:368–376.10.1007/BF01734359Höhna S., Landis M.J., Heath T.A., Boussau B., Lartillot N., Moore B.R., Huelsenbeck J.P., Ronquist F. 2016. RevBayes: Bayesian Phylogenetic Inference Using Graphical Models and an Interactive Model-Specification Language. Systematic Biology. 65:726–736.10.1093/sysbio/syw021",
        "url": "/developer/tutorial/",
        "index": ""
      }
      ,
    
      "developer-docker": {
        "title": "Using Docker with RevBayes",
        "content": "  Overview  Setting up DockerSetting up DockerThe first step to this is installing Docker. For Windows or MacOS this means installing Docker Desktop. For Linux this means installing just regular old Docker. For this tutorial I will also be using vscode which is an open-source editor made by Microsoft (strange as that sentence is for me to type). The tutorial could easily be done in the terminal also (e.g. PowerShell, bash). If you want to use vscode you will also need the Remote Explorer extension. If you search remote in the extensions tab its the one called Remote - Containers. This lets us connect our vscode window to the docker container.OK now we at least have Docker. We can run the Docker pull (currently this is not hosted on our organization page). Run this command in your terminal:    docker pull docker.pkg.github.com/wadedismukes/revbayes-dockerfiles/revbayesdockerfiles:ubuntu-latestThis will pull the image down which will take a bit. After this completes you will be able to run a Rev terminal using:    docker run –rm -it docker.pkg.github.com/wadedismukes/revbayes-dockerfiles/revbayesdockerfiles:ubuntu-latestIf you want a terminal inside of the container you can use:    docker run -it –entrypoint /bin/bash revbayesdockerfiles:ubuntu-latestWithout any errors you opened either revbayes or a terminal inside the container.Developing in the Docker containerDeveloping in the Docker containerOk now we are ready to get the container running. Use:docker run --entrypoint /bin/bash revbayesdockerfiles:ubuntu-latestThe container is now running. We just need to start it. To figure out the name of the container (since we didn’t give it one), type docker container ls. It should have a name of two random words, mine is peaceful-hertz.You want to start this with     docker start Now we can go to vscode type: CTRL(CMD)+SHIFT+P. This opens the command prompt. Type in Remote it will populate a drop-down list go to the one that says: Remote-Container: Attach to running container. This will open a new window and in the bottom left corner it should say ‘Container: revbayesdockerfiles-latest’.The revbayes repository is in here. If we want to open it you can go to Open folder and navigate to that. It will be at /revbayes.If you want to debug you will need to install a debugger. For example, to install gdb, you can simply use the apt install manager (or yum/dnf on fedora/CentOS). You could also do this to install valgrind which does not work nicely on MacOS. Otherwise you can use them just as you would in terminal on your home machine. I am currently working on a full development environment that takes advantage of a more IDE-like environment for debugging so stay-tuned.",
        "url": "/developer/docker/",
        "index": ""
      }
      ,
    
      "developer-setup": {
        "title": "Setting up an IDE for RevBayes",
        "content": "The easiest way to get started developing in RevBayes is to use an IDE such as XCode or Eclipse.You can also use your favorite text editor (e.g. vim or emacs). This page will provide some helpful tips on how to set up these development enviroments.See the Download page for instructions on how to obtain the source code.  Overview  Setting up Visual Studio Code for RevBayes developmentVisual Studio Code (or VSCode) is an open-source text editor by Microsoft. You can download and install it here.PrerequisitesYou will first need to install RevBayes from source. Once RevBayes is installed (i.e. once you have successfully ran ./build.sh), open VSCode. Once VSCode is open you will need to select the RevBayes folder from wherever you have stored it on your computer.Now you will need to install three extensions to get RevBayes to work nicely. To do this click on the button shown below in VSCode.Getting RevBayes workingOnce here you need to search for the  “C/C++ extension” , the  “CMake extension” , and the  “CMake tools extension” .Now that these are installed, CMake tools will prompt in the lower right asking you to locate the file named CMakeLists.txt. If this prompt does not appear, configure CMake manually by typing Ctrl+Shift+P (or CMD+Shift+P) to bring up the command palette, typing “configure”, and running CMake: Configure or CMake: Delete cache and Reconfigure. This should prompt the choice to locate CMakeLists.txt. Choose the file located in revbayes/src/CMakeLists.txt. This will prompt a choice of compiler kit. You can choose clang or gcc, either will work. This should begin building RevBayes using CMake and a new folder should appear in revbayes called build. This will contain the compiled files and will eventually be added to .gitignore. If it is not currently in it, you will need to add this to the .gitignore file. Specifically, add build/** and vscode/** to .gitignore. If the build does not start automatically click the build button (the gear icon) on the blue bar on the bottom of the screen. Note that VScode will build RevBayes to a different location than running ./build.sh on the command line. The rb executable built by VSCode will be in revbayes/build.Once the CMake build finishes, you can setup debugging by clicking the triangle with the small beatle in the lower left. Now click on the gear shown below. This will open a new file called launch.json. You need to change the values for the \"program:\" object to the path where your rb executeable is. For example, \"${workspaceFolder}/build/rb\". In VSCode ${workspaceFolder} refers to the root folder of the project you opened (unless you have changed the folder’s name, this is most likely revbayes). To test a specific rev script, change the value of the \"args\" object to a location of a Rev script. With that you can use debugging in VSCode by pressing the play button on the debug panel, or using the command palette (Ctrl+Shift+P or CMD+Shift+P) and using the “CMake: Debug” command. VSCode will then run the file you added to \"args\", but stop the process whenever it reaches a line you have set with a breakpoint (i.e. where you have clicked to the left of the line number, adding a small red dot). When stopped at a breakpoint, you can use the debugging panels to investigate variables and functions to debug your program.Setting up Eclipse for RevBayes developmentEclipse is a Java-based, cross-platform IDE with lots of nice features that make it convenient for RevBayes development. First, it’s cross-platform, and unlike Xcode, you can use any compiler you like (not just clang).Prerequisites  To compile RevBayes using Eclipse, you must install CMake and its command line tools. This is very easy if you have homebrew installed (brew install cmake)  You must also have a C++ compiler. If you are using a Mac, you can use clang (included with XCode), or you can install gcc (e.g. via homebrew brew install gcc).Installing Eclipse CDTTo start writing C++ in Eclipse, you must obtain the Eclipse CDT (C/C++ Development Tooling) package. You can download a distribution of Eclipse that comes prepackaged with CDT from the CDT Downloads page.If you already have Eclipse installed you can go to Help &gt; Install New Software… and enter the p2 repository URL for your version of eclipse found on the CDT downloads page. Then check the Main Features box, click Next twice, accept the User Agreement, and restart Eclipse.  Important! When you first open Eclipse, you will be asked to choose a location for your workspace. Make sure to choose a location that is not inside the source directory of your project!Create an Eclipse project for RevBayesAssuming you have cloned the RevBayes github repository into the directory &lt;revbayes-repo&gt;, you can create a new C++ Eclipse project as follows:  In Eclipse, go to File &gt; New &gt; Makefile Project with Existing Code  Set the Name of the project to revbayes  Select &lt;revbayes-repo&gt;/src as the Existing Code Location  Select Cross GCC in Toolchain for Indexer Settings (you can change this later if you want).  Click FinishConfigure the RevBayes projectYou will need to configure your Eclipse project so it correctly compiles the revbayes CMake project.Configure the PATH environment variableIf you installed the CMake command line tools in the default location :/usr/local/bin, you must add it to the PATH environment variable of your Eclipse project.      In the Project Explorer view, highlight your revbayes project directory    Go to Project &gt; Properties, or right click on the project name and select Properties.  Expand C/C++ Build and click on Environment  Click on the PATH entry, click Edit… and add /usr/local/bin to the end of the ValueConfigure the C/C++ build settingsThe RevBayes CMake project uses a special build script build.sh to build the RevBayes executable. You must tell your Eclipse project to use this script as a build command.  Click on C/C++ Build  Uncheck Use default build command and in Build command, enter sh build.sh  In Build directory, add ../projects/cmake to the directory path  Click on the Behavior tab  In Build (incremental build), enter -boost false  Again, click on C/C++ Build  Click Manage Configurations  Click on New… to create a new configuration, and name it Debug      Configure the Debug configuration by adding -debug true to the Build (incremental build) optionsNow, if you set the active configuration to Debug, RevBayes will be compiled with debugger symbols that can be loaded by lldb or gdb. You can also set the active build configuration by going to Project &gt; Build Configurations &gt; Set Active    Click Apply and CloseAt this point, C/C++ Indexer will get to work indexing the RevBayes code, during which time Eclipse might appear to be unresponsive.Configure the project to use spaces instead of tabsRevBayes code is indented using spaces. However, by default Eclipse uses tabs. Configure your project to automatically insert 4 spaces when you press the Tab key.  Open Eclipse &gt; Preferences  Expand C/C++ &gt; Code Style  Click on Formatter  Click the New… button to create a new profile, name the profile (e.g. “spaces”) then click OK to continue  Click the Indentation tab  Under General Settings set Tab Policy to Spaces only  Click OK and Apply and CloseBuild the RevBayes projectThe first time you build RevBayes, you will also need to build the included Boost libraries. You only need to do this once. To build the boost libraries, return to step 5 in the build configuration section and enter -boost true instead. Then after you’ve built the libraries, you can disable the Boost build flag by resetting -boost false.With all the build settings correctly configured, you can build RevBayes by highlighting the project in the Project Explorer view, and then going to Project &gt; Build Project. You can also right click on the project directory and select Build Project.The Console view should display the progress of the compilation process.You’re done! Now you can find the rb executable in the &lt;revbayes-repo&gt;/projects/cmake directory.Tips      Create a symlink of rb in your PATH, so it is automatically updated every time you build RevBayes. e.g.    sudo ln -s &lt;revbayes-repo&gt;/projects/cmake/rb /usr/local/bin/rb        If your Project Explorer or Console views disappear and you can’t find them, go to Window &gt; Show View to display various views.  Setting up XCode for RevBayes developmentXCode is an IDE for Mac OSX. XCode does not keep track of files, so each time you open your RevBayes project in Xcode you must pull the RevBayes master branch from git &amp; remove reference to all of the source. Eclipse Oxygen does a cleaner job of managing the files; you do not need to pull from git each time you work in it.Set up the XCode Project with an internal build system  Open Xcode and in the Welcome to Xcode window, choose Create a new Xcode project.  Select Command Line Tool and name it rb and click Next.   Click New Folder  to create an empty directory and name it whatever you’d like.  Click Create.  Delete all of the files and folders in each of the directories including main.cpp so that RevBayes is empty. You can do this by selecting the folders and files, right clicking and selecting delete. When asked, choose Move to Trash.  Add the source files by selecting the appropriate directory and going to the File pull-down menu and selecting Add Files to rb.  Click on Options at the button of the window, and under the Added Folders heading, select the Create Groups radio button.          Select the                  revbayes/src/revlanguage          revbayes/src/core          revbayes/src/libs                    directories and click Add.      Note: On some versions of XCode, you may need to click on the “Options” Tab, and choose “Create Groups” for the import to work properly. This is the default behavior on most XCode installs.        Add the boost library to your Xcode project:          You have to install boost from another source and point your Xcode project to that version. If you use Homebrew as a package management system on your Mac, you can install boost using brew install boost. This will install the boost libraries in your /usr/local directory. To enable this version in your Xcode project, scroll to the Search Paths section in the Build Settings and add the following paths:                  Header Search Paths: /usr/local/include          Library Search Paths: /usr/local/lib                      Add boost linker flags          Scroll to the Linking section in the Build Settings      Go to Other Linker Flags and double click on the space. This will bring up a box where you can add the following flags: -lboost_program_options, -lboost_filesystem, -lboost_system.      Add the RB_XCODE Preprocessor Macro to your Xcode project  Select the RevBayes project and go to the Build Settings.  Search for or Scroll down to the Apple Clang - Preprocessing heading and find the sub-heading named Preprocessor Macros.  Double click on right hand column, click on the + and enter RB_XCODE. Do not replace the debug flag that is already present.Check C++ language options  Select the RevBayes project and go to the Build Settings.  Search or scroll to: Apple Clang - Language - C++.  Make sure C++ Language Dialect is set to GNU++17 [-std=gnu++17].At this point, if everything has been setup correctly, you should be able to build the project. You can try by clicking on Product - Build or by using ⌘+B.Set up the XCode Project with an external build systemPrerequisites  To compile RevBayes using Xcode, first install https://cmake.org/ and its https://stackoverflow.com/questions/30668601/installing-cmake-command-line-tools-on-a-mac. This is easily done using https://brew.sh/ by running brew install cmake in terminal.Create an Xcode project for RevBayes  Open Xcode and in the Welcome to Xcode window, choose Create a new Xcode project.  Click on the Cross-Platform tab at the top  Select External Build System and name it rb (or whatever you’d like)  Under Build Tool type the following directory /&lt;path-to-revbayes&gt;/revbayes/projects/cmake/build.sh  Click Next.Configure the build  After clicking Next as directed above, you should see a screen that looks like this:  On this screen, under Arguments type $(ACTION) -boost true -debug true, or if you haven’t built the boost libraries in RevBayes already. If you have then type $(ACTION) -boost false -debug true.  Under Directory put /&lt;path-to-revbayes&gt;/revbayes/projects/cmake/.  Add the source files by selecting the appropriate directory and going to the File pull-down menu and selecting **Add Files to **.  This should open a screen that looks like this:  Click on Options at the button of the window, and under the Added Folders heading, select Create as Folder References, and add to the target that was created in Step 2.          Select the following directories from the revbayes directory:                  revbayes/src/revlanguage          revbayes/src/core          revbayes/src/libs                    Click Add.        At this point, if everything has been setup correctly, you should be able to build the project. You can try by clicking on Product - Build or by using ⌘+B.  Once revbayes has built go to Product-&gt;Scheme-&gt;Edit Scheme, it should bring a window like this:  Click on info, then go to executable and locate the newly built revbayes executable.  Now you should be able to click the play button or ⌘+R, and you should see the revbayes command line prompt in the loading screen.Setting up vim for RevBayes developmentVim is a text editor that some people love.It’s not an IDE, but it can provide useful IDE-like behaviors.If you are reading this, you are probably a vim-lover and already have a set of customizations that you like.Here are some more to consider.YouCompleteMeYouCompleteMe is an extremely useful plugin that provides suggestions as you type for function names, prompts about their arguments, etc.There are several steps to get it working.Install dependenciesThis seems to be sufficient on Ubuntu 16.04:sudo apt-get install build-essential cmake python-dev python3-dev clangGet the vim code itselfGrab these two plugins:  YouCompleteMe  YCM-GeneratorPut them wherever you put your plugins, e.g., .vim/bundle/ if you’re using Pathogen.For YCM, you also need to get its submodules:cd YouCompleteMe/git submodule update --init --recursive(If you manage your plugins as git subtrees, note that you probably can’t for YouCompleteMe because it contains submodules itself.)Compile the YCM pluginYCM has a compiled component as well as vim code.  This may take a few minutes to run.cd YouCompleteMe/./install.py --clang-completerProvide the compilation flags to YCMThe above was to install YCM in general.To use it specifically with RevBayes (or any other project), you need to give it information about the codebase.YCM-Generator is one way to do this.cd revbayes/  # or wherever you keep revbayescd projects/cmake/build/~/.vim/bundle/YCM-Generator/config_gen.py . # adjust the vim path if necessaryThat should take a few seconds to run.Then move the result to the top-level directory:mv .ycm_extra_conf.py ../../../Try it outThat should be it.If YCM is working, when you open a revbayes .cpp or .h file, vim will ask Found revbayes/.ycm_extra_conf.py. Load?If you find that you don’t want YCM operating on all your other filetypes, you can put something like this in your .vimrc.let g:ycm_filetype_whitelist = { 'cpp': 1, 'c': 1, 'python': 1 }Debugging with GDBIf you use vim, we recommend debugging in GDB. You’ll need to compile RevBayes with the -debug true flag:./build.sh -debug trueThe you can debug RevBayes with GDB:gdb rbSee here for more on using GDB.",
        "url": "/developer/setup/",
        "index": ""
      }
      ,
    
      "developer-tests": {
        "title": "Integration Tests in RevBayes",
        "content": "IntroductionRevBayes is an extremely large and function rich software.It is not possible to foresee if any code changes that you do might affect other functions.This especially scares new developer who do not want to break existing code.For this reason we have design a suite of so called integration tests.In this document we will explain how these integration tests tests work in RevBayes.The basic idea behind our integration tests is to used something closer to high level testing instead of unit testing.That is, an integration test in RevBayes consists of a standard analysis, or even only reading in data and extracting information, and printing some output.The output is then compared to the expected output using the common tool diff.An integration test passes if the output exactly matches the expected output.Hence, integration tests ensure that the software does not change (at least from a user perspective).However, the integration test does not guarantee that the software works correctly.You should always make sure that the implementation is correct, for example, by running some validation using simulation-based calibration.Once you have established that the implementation is correct, you should use the integration tests to freeze the functionality which enforce that you can always rely on the code producing the output you expected at this time.(Note, no test if perfect and sometimes changes to the functionality could be missed.)Since MCMC analyses in RevBayes and other software are intrinsically stochastic, a naive approach would expect different output every singly time you run an analysis.For example, if you run again an MCMC analysis in RevBayes, you should never get exactly the same trace of parameter samples.Thus, to enable exactly predictable output, you have to specify the random number seed at the very beginning of your analysis.In the end, all of our algorithms are deterministic given a sequence of random numbers ;)Creating and Executing an Integration TestDirectory StructureIn the RevBayes GitHub repository, the validation scripts are all located within the folder called tests.The directory structure within RevBayes highlighting where the integration tests can be found. Note specifically the bash script run_integration_tests.sh.Within the tests directory, there is one bash script call run_integration_tests.sh.This is the bash script that runs all integration tests.Additionally, there is a data directory where all the data needs to be stored.You must not store data for your integration elsewhere.Then, each integration test has its own directory which must start with test_.If an integration test should excluded from the automatic testing, then simply prepend the directory with a **, for example, **_test_DEC_DA**.Our bash script run_integration_tests.sh traverses the **tests** directory and searches for all subdirectories starting with **test.Hence, if you add your new integration test by creating a new directory called, for example, test_my_first_test, then your new integration test if automatically included in our test suite.So far not too difficult ;)The directory structure within a specific integration tests. There should always be at least the two subdirectories called output_expected and scripts.Next, within a specific integration test, you need to have at least two subdirectories:  scripts: containing the Rev scripts one for each integration test analysis. There could be multiple files as long as each is its own analysis because we assume that each script can be run by itself.  output_expected: containing the expected output which basically is a copy of the output from your successful benchmark.Writing an integration testLet us look at a simple integration test as an example.Look at the file test/scripts/test_BM/scripts/mcmc_BM.Rev.This script will run a standard Brownian motion analysis for a single continuous trait evolving along a phylogeny.The only estimated parameter here is the rate of evolution sigma.The details of the model are not important because they will differ for your analysis.In the end, an integration test a almost a standard analysis.The main differences and important aspects to consider/follow are:  seed(12345): We specify the seed of the random number generator at the very beginning of the script.  data: We always assume that your data is stored in the common data directory, so we can share the data among tests.  output: The results of the analysis will be stored in one or multiple files within the directory output. The is absolutely necessary because we run a diff on the directories output and output_expected.  printgen: often it makes sense to print every iteration. Here we are not interested in convergence but instead in the correct sequence of MCMC steps which only occurs if the likelihoods and MCMC moves are still working exactly the same way.  runtime Your integration test should not run longer than 5 seconds on your lousy laptop so that we can run hundreds of integration tests in less than 5 minutes. Therefore:          use a small toy dataset      run the MCMC only for very few iterations      use moveschedule=\"single\" within the MCMC command to only use one move per generation.      Running the integration tests locallyYou can run the integration tests locally using (assuming your RevBayes GitHub directory is called revbayes)cd revbayes/tests./run_integration_tests.sh \"$PWD/../projects/cmake/rb\"This will run all the integration tests which can take a few minutes, but currently it should be less than 2 minutes.At the end, you will receive the results as the output:#### Checking output from tests...#### Test passed: BM#### Test passed: BM_MVN#### Test passed: BP#### Test passed: CBDSP#### Test passed: ChromoSSE#### Test passed: Coal_const#### Test passed: Coal_skyline&gt;&gt;&gt;&gt; Test failed: DEC (expected)\tmismatch: mcmc_DEC.out#### Test passed: EBD&gt;&gt;&gt;&gt; Test failed: FBDP (expected)\tmismatch: mcmc_FBDP.out\tmismatch: mcmc_FBDP.trees\tmismatch: mcmc_FBDP_mcc.tre#### Test passed: HSMRF_BDP#### Test passed: JC_BDP#### Test passed: JC_EBD#### Test passed: JC_unrooted#### Test passed: MVN_move#### Test passed: OU#### Test passed: Partition#### Test passed: UCLD_noncentered#### Test passed: basics#### Test passed: multiple_mcmc#### Test passed: mvBM#### Test passed: skygrid#### Test passed: steppingstone#### Success! unexpected failures: 0   expected failures: 2   total tests: 23#### All tests passed.Yeah, all tests passed!Try now write your own integration test!Frequently Asked QuestionsThe MPI version fails the testYes, the MPI version uses a different sequence of random numbers, and thus the expected output will look different.We are currently working on a solution for this problem.",
        "url": "/developer/tests/",
        "index": ""
      }
      ,
    
      "developer-architecture": {
        "title": "Getting familiar with the code",
        "content": "To develop in RevBayes, it is important to be familiar with the overall file structure.Below is a visual represenation of the directory/file structure of a RevBayes project.Notice the repetition in naming within the core and revlanguage directories. This may seem confusing at first, so keep the following explanation in mind for clarity:      The core directory contains all hard-coded functionality for RevBayes.        The revlanguage directory is essentially a wrapper for all code within the core directory. The code within revlanguage is written to match the scripting Rev language syntax.  For more specific information on functions and distributions, and for general information on implementing in RevBayes, navigate to the Implementing functions, distributions, and moves section of this Developer’s guide.",
        "url": "/developer/architecture/",
        "index": ""
      }
      ,
    
      "developer-implementation": {
        "title": "Implementing functions, distributions, and moves",
        "content": "  Overview  Implementing a functionThere are two main classes of functions in RevBayes: regular functions and member functions.  For our example, we will be implement a regular function. First we need to add two files to the RevBayes source code, Func_hyperbolicCosineFunction.cpp and Func_hyperbolicCosineFunction.h. These will go within revbayes/src/revlanguage/functions/math since we are adding a function and it is a mathematical function. Second, we need to register the function by modifying revbayes/src/revlanguage/workspace/RbRegister_Func.cpp.RevLanguage Header fileThe file Func_hyperbolicCosine.h should look like the following:#ifndef Func_hyperbolicCosine_h#define Func_hyperbolicCosine_h#include \"Real.h\"#include \"RlTypedFunction.h\"#include &lt;string&gt;namespace RevLanguage {        /**     * The RevLanguage wrapper of the hyperbolic Cosine function (sinh()).     *     * The RevLanguage wrapper of the hyperbolic function function connects     * the variables/parameters of the function and creates the internal HyperbolicCosineFunction object.     * Please read the HyperbolicCosineFunction.h for more info.     *     *     * @copyright Copyright 2024-     * @author The RevBayes Development Core Team (&lt;your-name&gt;)     * @since 2024-04-26, version 1.0     *     */    class Func_hyperbolicCosine :  public TypedFunction&lt;Real&gt; {            public:        // Basic utility functions        Func_hyperbolicCosine*                         clone(void) const;                                          //!&lt; Clone the object        static const std::string&amp;                       getClassType(void);                                         //!&lt; Get Rev type        static const TypeSpec&amp;                          getClassTypeSpec(void);                                     //!&lt; Get class type spec        std::string                                     getFunctionName(void) const;                                //!&lt; Get the primary name of the function in Rev        const TypeSpec&amp;                                 getTypeSpec(void) const;                                    //!&lt; Get the type spec of the instance                // Function functions you have to override        RevBayesCore::TypedFunction&lt;double&gt;*            createFunction(void) const;                                 //!&lt; Create internal function object        const ArgumentRules&amp;                            getArgumentRules(void) const;                               //!&lt; Get argument rules            };    }#endif /* Func_hyperbolicCosine_h */RevLanguage Implementation fileAnd the Func_hyperbolicCosine.cpp should look like this:#include \"Func_hyperbolicCosine.h\"#include \"Probability.h\"#include \"Real.h\"#include \"RlDeterministicNode.h\"#include \"TypedDagNode.h\"#include \"GenericFunction.h\"using namespace RevLanguage;double* hyperbolicCosine(double x){    double result = (exp(x) + exp(-x))/2;    return new double(result);}/** * The clone function is a convenience function to create proper copies of inherited objected. * E.g. a.clone() will create a clone of the correct type even if 'a' is of derived type 'b'. * * \\return A new copy of the function. */Func_hyperbolicCosine* Func_hyperbolicCosine::clone( void ) const{    return new Func_hyperbolicCosine( *this );}RevBayesCore::TypedFunction&lt;double&gt;* Func_hyperbolicCosine::createFunction( void ) const{    RevBayesCore::TypedDagNode&lt;double&gt;* x = static_cast&lt;const Real&amp;&gt;( this-&gt;args[0].getVariable()-&gt;getRevObject() ).getDagNode();        return RevBayesCore::generic_function_ptr&lt; double &gt;( hyperbolicCosine, x );}/* Get argument rules */const ArgumentRules&amp; Func_hyperbolicCosine::getArgumentRules( void ) const{    static ArgumentRules argumentRules = ArgumentRules();    static bool          rules_set = false;        if ( !rules_set )    {                argumentRules.push_back( new ArgumentRule( \"x\", Real::getClassTypeSpec(), \"The value.\", ArgumentRule::BY_CONSTANT_REFERENCE, ArgumentRule::ANY ) );                rules_set = true;    }    return argumentRules;}const std::string&amp; Func_hyperbolicCosine::getClassType(void){    static std::string rev_type = \"Func_hyperbolicCosine\";        return rev_type;}/* Get class type spec describing type of object */const TypeSpec&amp; Func_hyperbolicCosine::getClassTypeSpec(void){    static TypeSpec rev_type_spec = TypeSpec( getClassType(), new TypeSpec( Function::getClassTypeSpec() ) );        return rev_type_spec;}/** * Get the primary Rev name for this function. */std::string Func_hyperbolicCosine::getFunctionName( void ) const{    // create a name variable that is the same for all instance of this class    std::string f_name = \"cosh\";        return f_name;}const TypeSpec&amp; Func_hyperbolicCosine::getTypeSpec( void ) const{    static TypeSpec type_spec = getClassTypeSpec();        return type_spec;}The function RevBayesCore::generic_function_ptr&lt; ResultType &gt;(CppFunction, arg1, ...) in Func_hyperbolicCosine::createFunction( ) automatically constructs a RevBayesCore::Function from the C++ function so that we do not have to.If CppFunction is has multiple type signatures, as is the case with functions like sqrt( ), generic_function_ptr&lt; &gt; will not work.In such cases, we can solve this problem by writing a wrapper function with a single type signature.For example,// Specialize sqrt to only work on doublesdouble mysqrt(double x){   return sqrt(x);}Registering the new functionIn order to make the new function available for use within the Rev language, we first need to register it.To do this,  Go to the src/revlanguage/workspace/ directory.  Open the file RbRegister_Func.cpp file in your editor.  Scroll down until you find the #include commands for math functions.  Add the line #include \"Func_hyperbolicCosine.h\" in the correct alphabetical order for that group.  Scroll down in that file until you find the section of the code that adds math functions.  Add the line addFunction( new Func_hyperbolicCosine() );Functions with caching and optimized recalculationThe above method should be prefered for implementing new functions in RevBayes unless the function needs to save intermediate results and use them during recalculation.For example, if our function computes x*(y+z), then if only x has changed, we could save the old value of y+z and re-use it instead of computing y+z from scratch.  (In this case, y+z is not expensive enough to be worth saving, but is just a simple illustration of an intermediate result.)In such cases we will also need to write a RevBayesCore::Function.A RevBayesCore::Function serves a different role than a RevLanguage::Function.A RevLanguage::Function interprets the function arguments and connects them to the RevBayesCore::Function, which performs the actual calculation and holds the result.If we write our own RevBayesCore::Function, then we can use the update() method to intelligently recalculate the value,and we can add data memembers to save intermediate results;Here will illustrate how to write a simple RevBayesCore::Function using the hyperbolic cosine example.Our example does’t actually save any intermediate results, it just illustrates how a RevBayesCore::Function works.It is therefore equivalent to the RevBayesCore::Function that is automatically generated by generic_function_ptr.RevBayesCore Header fileFirst, we will write our new header file. Within our header file, we need to #include a few other RevBayes header files, including TypedDagNode.h since our typed function deals with nodes of DAGs. Note that the directory structure of core is similar to that of the revlanguage.  The file src/core/functions/math/HyperbolicCosineFunction.h will be:#ifndef HyperbolicCosineFunction_h #define HyperbolicCosineFunction_h#include \"TypedFunction.h\"#include \"TypedDagNode.h\"#include &lt;cmath&gt;namespace RevBayesCore {    /**     * \\brief Hyperbolic Cosine of a real number.     *     * Compute the hyperbolic cosine of a real number x. (cosh(x) = (exp(x) + exp(-x))/2).     *     * \\copyright (c) Copyright 2009-2018 (GPL version 3)     * \\author &lt;your-name&gt;     * \\since Version 1.0, 2015-01-31     *     */    class HyperbolicCosineFunction : public TypedFunction&lt;double&gt; {        public:                                          HyperbolicCosineFunction(const TypedDagNode&lt;double&gt; *a);             HyperbolicCosineFunction*     clone(void) const; //!&lt; creates a clone            void                          update(void);      //!&lt; recomputes the value        protected:            void                          swapParameterInternal(const DagNode *oldP, const DagNode *newP); //!&lt; Implementation of swapping parameters        private:            const TypedDagNode&lt;double&gt;* x;    };}#endifThe first part of this file should be the standard header that goes in all the files giving a brief description about what that file is as well as information about the copyright and the author of that file.RevBayesCore Implementation fileNext, after including the necessary header files, we have to ensure that our new function is included within the RevBayesCore namespace.Here we are implementing our hyperbolic cosine function as its own class that is derived from the typed function class. This class stores the hyperbolic cosine of a value that is held in a DAG node. We have also defined a clone method which can create a clone of our class, and an update method which will update the value of our Hyperbolic Cosine class whenever the value of the DAG node changes.The file src/core/functions/math/HyperbolicCosineFunction.cpp will look like:#include \"HyperbolicCosineFunction.h\"using namespace RevBayesCore;HyperbolicCosineFunction::HyperbolicCosineFunction(const TypedDagNode&lt;double&gt; *a) : TypedFunction&lt;double&gt;( new double(0.0) ),x( a ){    addParameter( a );}HyperbolicCosineFunction* HyperbolicCosineFunction::clone( void ) const{    return new HyperbolicCosineFunction(*this);}void HyperbolicCosineFunction::swapParameterInternal(const DagNode *oldP, const DagNode *newP){    if (oldP == x)    {        x = static_cast&lt;const TypedDagNode&lt;double&gt;* &gt;( newP );    }    }void HyperbolicCosineFunction::update( void ){    // get the current value of x    double xValue = x -&gt; getValue();    // compute the function result    double result = (exp(xValue) + exp(-xValue))/2;    // update the stored value    *value = result;        }Alternate implemention for createFunctionIn order to use the new RevBayesCore::HyperbolicCosineFunction class, we need to modify the RevLanguage::Func_hyperbolicCosine class to use it instead of generic_function_ptr( ).  To do this,  Open src/revlanguage/functions/math/Func_hyperbolicCosine.cpp in your editor.  Add an #include statement to make the RevBayesCore::HyperbolicCosineFunction class visible  Modify the definition of Func_hyperbolicCosine::createFunction() as follows:...#include \"HyperbolicCosineFunction.h\"...         RevBayesCore::TypedFunction&lt;double&gt;* Func_hyperbolicCosine::createFunction( void ) const{        RevBayesCore::TypedDagNode&lt;double&gt;* x = static_cast&lt;const Real&amp;&gt;( this-&gt;args[0].getVariable()-&gt;getRevObject() ).getDagNode();        return new RevBayesCore::HyperbolicCosineFunction( x );}Implementing a distributionGeneral info before getting started      Within the RevBayes core directory, there are subdirectories for different categories of distributions. All mathematical distributions that have been implemented exist in core/distributions/math.        Note that when implementing a new distribution, you will need to create .cpp and .h files in both the revlanguage directory and the core directory. (For a refresher on the difference between these two directories, refer to the Getting familiar with the code section of this Developer’s guide).The overall naming format remains the same for every distribution in RevBayes. In the Beta Binomial Distribution example provided below, I specify what naming convention to use when creating each file.        It is often helpful to look at/borrow code from existing RevBayes distributions for general help on syntax and organization.  For the language side, one of the most important things is the create distribution function (it converts user-arguments into calculations). Also, the getParameterRules function is important (to get the degrees of freedom &amp; other things). It is often helpful to look at the code of existing distributions for general help on syntax &amp; organization.      Within every new distribution, you will need to include some functions. For example, each new distribution must have: the get class type, name, and help functions. You may not need to implement these from scratch (if they’re dictated by the parent class &amp; are already present), but you will need to implement other functions within your distribution (e.g. cdf, rv, quantile).        Distributions have a prefix DN (dag node), and all moves have a prefix MV. RevBayes takes the name within &amp; creates the DN automatically, so be aware of this. For a refresher on DAG nodes, refer to Getting familiar with the code.  In the following steps, we’ll implement the Beta Binomial Distribution as an example, for syntax purposes.Steps      Create new .cpp &amp; .h files in revlanguage/distributions/math/, named Dist_betabinomial.cpp and Dist_betaBinomial.h. Note: all files in this directory will follow this naming format: Dist_&lt;nameofdistribution&gt;.[cpp|h].    To populate these files, look at existing examples of similar distributions for specific info on what to include &amp; on proper syntax.  For example, for the Beta Binomial distribution, I checked the existing Binomial Distribution code for guidance.    //add code here        Test                  Create new .cpp &amp; .h files in core/distributions/math/, named BetaBinomialDistribution.cpp and BetaBinomialDistribution.h.        Note: This is the object oriented wrapper code that references the functions hard-coded in the next step. All files in this directory will follow this naming format: &lt;NameOfDistribution&gt;Distribution.[cpp|h].                    Create new .cpp and .h files in core/math/Distributions/, named DistributionBetaBinomial.cpp and DistributionBetaBinomial.h.        These are the raw procedural functions in the RevBayes namespace (e.g. pdf, cdf, quantile); they are not derived functions. RbStatistics is a namespace. To populate these files, look at existing examples of similar distributions to get an idea of what functions to include, what variables are needed, and the proper syntax.        Note: This is the most time-consuming step in the entire process of implementing a new distribution. All files in this directory will follow this naming format: Distribution&lt;NameOfDistribution&gt;.[cpp|h]                  Navigate to revlanguage/workspace/RbRegister_Dist.cpp    Every new implementation must be registered in RevBayes. All register files are located in the revlanguage/workspace directory, and there are different files for the different types of implementations (RbRegister_Func.cpp is for registering functions; RbRegister_Move is for registering moves; etc.).  We are implementing a distribution, so we will edit the RbRegister_Dist.cpp file.        You need to have an include statement at the top of the RbRegister script, to effectively add your distribution to the RevBayes language. You also need to add code to the bottom of this file, and give it a type and a new constructor. Generally, you can look within the file for an idea of proper syntax to use.    For the Beta Binomial distribution, we navigate to the section in the file with the header ‘Distributions’ and then look for the sub-header dealing with ‘math distributions’. Then, add the following line of code:    #include \"Dist_betaBinomial.h\"        This step registers the header file for the beta binomial distribution, effectively adding it to RevBayes.    Next, navigate to the section of the file that initializes the global workspace. This section defines the workspace class, which houses info on all distributions. Then, add the following line of code:    AddDistribution&lt; Natural\t\t&gt;( new Dist_betaBinomial());        This adds the distribution to the workspace. Without this step, the beta binomial distribution will not be added to the revlanguage.    Note: Depending on the kind of distribution, you may need to change Natural to a different type (e.g. Probability, Real, RealPos, etc.).    After registering your distribution, you are ready to test your code.        Before pushing your changes, you should ensure your code is working properly.    There are multiple ways to do this. As a best practice, you should first compile it to ensure there are no errors. Once it compiles with no problems, you can test in various ways (e.g. run each individual function within the new Beta Binomial distribution in R, then run the Binomial distribution with a Beta prior in Rev and see if the output matches). For more information, see the developer tutorials on validation and testing. (TODO)    After ensuring your code runs properly, you are ready to add it to the git repo. We recommend reading through the RevBayes Git Workflow section of the Developer’s guide before pushing.  Implementing a Metropolis-Hastings MoveGeneral info before getting startedThe steps to implementing a new move vary slightly, depending on the move’s type (e.g., Metropolis-Hastings versus Gibbs). For the purpose of this guide, we will focus on a Metropolis-Hastings move.In general, the fastest and easiest way to get help is to find the most similar move already implemented in RevBayes and use it as a guide. Remember that, as with implementing a new distribution or function, you’ll need to add relevant code to both the core of RevBayes and the language. Also remember that you’ll need to work out the math appropriate for your move (e.g., the Hastings ratio) ahead of time.Steps      Orienting within the repository - For the core, navigate in the repository to src/core/moves. For a Metropolis-Hastings move, we’ll then go into the proposal directory. In this directory, you can find several templates for generic proposal classes, as well as subdirectories containing moves for specific parameter types. To keep things easy, we’ll focus on a single scalar parameter, so we’ll navigate one step further into the scalar directory. For the language, navigate to src/revlanguage/moves. For this example, as we did in the core, we’ll focus on a move for a scalar parameter, so we’ll then open scalar. To register our new move after it’s implemented, we’ll also need to update the file src/revlanguage/workspace/RbRegister_Move.cpp.        Creating new files for the core - As an example, we’ll implement a new move that draws a random value from a Gamma distribution and proposes a new scalar by multiplying the current value by the draw from the Gamma. This move will be called a “Gamma Scaling move”. Since this move is similar to an existing scaling move, we can start by copying the file ScaleProposal.h and naming the new copy GammaScaleProposal.h. As a reminder, we’re working in the directory src/core/moves/proposal/scalar/.    Once the new header file is created and named, we can update the content to match our new move. The simplest changes involve renaming things to match the new move (e.g., updating the preprocessor macro from ScaleProposal_H to GammaScaleProposal_H, or changing the name of object references and constructor from ScaleProposal to GammaScaleProposal). The comments at the top of the header file that describe how the move works should also be updated, but these changes will obviously be specific to the move being implemented.    Next, we’ll need to create a new .cpp file containing the implementation of our new move. As with the header, it’s easiest to copy and rename an existing file, so we’ll use ScaleProposal.cpp as our template, copy it, and rename to GammaScaleProposal.cpp. As with the header file, most of the necessary changes involve updating the names of variables and function names. If the move requires access to other math functions, additional header files may need to be included at the top. Explore src/core/math as needed to find the necessary functions or distributions. In this case, we will need access to methods associated with the Gamma distribution, so we will add #include \"DistributionGamma.h\". For our example, the number and type of variables used by our move are the same as our template based on the Scale move, so we don’t need to modify the constructor or variable initialization, other than updating the constructor name. Similarly for this example, we don’t need to alter the code in the ::cleanProposal, ::clone, ::prepareProposal, ::printParameterSummary, ::undoProposal, ::swapNodeInternal, and ::tune methods as these are common to our template and new moves (and will be identical to many of the scalar moves), but we do need to update the class names associated with the methods (i.e., ScaleMove:: -&gt; GammaScaleMove::). For the ::getProposalName method, we need to update the string in the method that provides a descriptive name for the move - name = \"Gamma Scaling\". The bulk of the necessary changes for the new move will come in the ::propose method and the help description above the method. For this example, the new ::propose method looks like this:    /* * Perform the proposal. * * A gamma scaling proposal draws a random number from a gamma distribution u ~ Gamma(lambda,1) and scales the current vale by u * lambda is the tuning parameter of the proposal which influences the size of the proposals by changing the shape of the Gamma. * * \\return The hastings ratio. */double GammaScaleProposal::propose( double &amp;val ){        // Get random number generator    RandomNumberGenerator* rng     = GLOBAL_RNG;        // copy value    storedValue = val;        // Generate new value (no reflection, so we simply abort later if we propose value here outside of support)    double u = RbStatistics::Gamma::rv(lambda,1,*rng);    val *= u;        // compute the Hastings ratio    double ln_hastings_ratio = 0;    try    {        // compute the Hastings ratio        double forward = RbStatistics::Gamma::lnPdf(lambda, 1, u);        double backward = RbStatistics::Gamma::lnPdf(lambda, 1, (1.0/u));                ln_hastings_ratio = backward - forward - log(u);    // The -log(u) term is the Jacobian    }    catch (RbException e)    {        ln_hastings_ratio = RbConstants::Double::neginf;    }    return ln_hastings_ratio;}        In this case, the Hastings ratio involves the probability density of the forward move (the scaling factor u), the density of the corresponding backward move (the scaling factor $\\frac{1}{u}$), and the solution to the Jacobian ($-\\frac{1}{u}$).        Creating new files for the rev language - After implementing the detailed machinery to perform the new move in the RevBayes core, you need to modify a few files associated with the Rev language to make it available to users. As a reminder, the first set of these files is found in src/revlanguage/moves. For our example, we will be focusing specifically on a move for a scalar value, so navigate to the scalar subdirectory. As with the files in the core, we will copy and rename a header (.h) and implementation (.cpp) file from the Scale move. In this case, our new header file will be called Move_GammaScale.h and our new implementation file will be called Move_GammaScale.cpp. In the header file, simply update the names of the preprocessor macro, the class, and the objects. Also remember to update the help comments near the top of the file.    In the new .cpp file, begin by updating the names of the header files included at the top. Note that we include, and will need to update, the names of both the header for the core GammaScaleProposal.h and the workspace Move_GammaScale.h. Most of the rest of the changes in this file involve updating the names of classes and objects associated with this move, but we also need to update the string specifying the type in the ::getClassType method and specifying the constructor name in the ::getMoveName method. Pay special attention to the rules specified in ::getParameterRules and make sure they satisfy the constraints required by the new move. Update these rules as needed, using existing rules from other moves as templates.    For our particular example, we also need to make one additional change. Because we’ve only updated the ScaleProposal, and not the ScaleProposalContinuous, we need to remove the part of the code in this move that could call ScaleProposalContinuous. The ::constructInternalObject method now looks like this (compare to the corresponding method from Move_Scale.cpp):    void Move_GammaScale::constructInternalObject( void ){    // we free the memory first    delete value;        RevBayesCore::Proposal *p = NULL;        // now allocate a new sliding move    double d = static_cast&lt;const RealPos &amp;&gt;( lambda-&gt;getRevObject() ).getValue();    double w = static_cast&lt;const RealPos &amp;&gt;( weight-&gt;getRevObject() ).getValue();    double r = static_cast&lt;const Probability &amp;&gt;( tuneTarget-&gt;getRevObject() ).getValue();    RevBayesCore::TypedDagNode&lt;double&gt;* tmp = static_cast&lt;const RealPos &amp;&gt;( x-&gt;getRevObject() ).getDagNode();    RevBayesCore::StochasticNode&lt;double&gt; *n = dynamic_cast&lt;RevBayesCore::StochasticNode&lt;double&gt; *&gt;( tmp );    p = new RevBayesCore::GammaScaleProposal(n, d, r);    bool t = static_cast&lt;const RlBoolean &amp;&gt;( tune-&gt;getRevObject() ).getValue();        value = new RevBayesCore::MetropolisHastingsMove(p, w, t);    }            Updating the Rev language register - The last step in implementing our new move is to make sure that it’s registered in the Rev language. To do this, we will need to update the file RbRegister_Move.cpp in src/revlanguage/workspace. In this file, we’ll need to include the header for our new move #include \"Move_GammaScale.h\" in the section corresponding to moves on real values. We’ll also need to add the constructor to the workspace by updating the ::initializeMoveGlobalWorkspace method to include addTypeWithConstructor( new Move_GammaScale() );, again in the section corresponding to moves on real values.        Testing the performance of the new move - If properly implemented, a new move can be validated by running an MCMC analysis where the clamped data are ignored and one tries to sample only the prior. This can be done in RevBayes by calling model.ignoreAllData() to mark the data as ignored. The recommended strategy is to implement the simplest possible model that uses a variable of the type appropriate for the new move, and assigning that variable an easily validated prior (e.g., a uniform). Run the analysis with only the new move operating on that variable and then plot the variable’s marginal distribution to make sure it matches the prior.  ",
        "url": "/developer/implementation/",
        "index": ""
      }
      ,
    
      "developer-best-practices": {
        "title": "Best practices in RevBayes",
        "content": "  Overview  Documentation guidelinesRevBayes uses Doxygen to compile documentation from the code. Documentation is critical to help other programmers understand the code, so here are some guidelines to get you started.Important notes  member functions which reimplement functions from a parent class will automatically inherit their documentation from the parent. Only document these if additional details are required. The @copydoc commands allows to copy documentation from any other class or member.  long documentation should always be placed just before the definition it applies to. Brief descriptions can be placed after members.Header files (.h)  document the class, with references (using the @cite command with a BibTEX reference) if applicable.  all members (functions and variables) including the private ones should be followed by a brief description (unless the parent description is adequate).  the @file annotation should be used only when the file contains no classes, otherwise classes should be documented rather than files.Implementation files (.cpp)  detailed descriptions of member functions go there (unless the parent description is adequate).  the commands @param, @return and @throw are used to document respectively the parameters, the return value and the exceptions thrown.  simple getters and setters do not need detailed documentation.Other useful doxygen commands  LateX-style formulas can be included using the @f$ annotation for inline text (example: @f$n^2@f$), and the @f[ and @f] annotations for equations (example: @f[ x = sum_{i=1}^n y^i @f])  @todo and @bug to document problematic behavior or missing features  @note and @warning to document tricky or unexpected behavior  @see to reference other classes/methodsDo  include information about memory side-effects of the code, in particular the use of new and delete.  document the default values of the parameters, if applicable.  document private members.  inherit documentation or use the @copydoc command instead of copy-pasting whenever possible.  run Doxygen and check the output before submitting your new documentation.Do not  replicate information given by git, such as author, license or modification dates of the files. Exception: code adapted or copied from outside the RevBayes project should be credited to the original author and include licensing information if applicable.  restate information easily available in the code, such as the parent class.  leave commented-out code unless accompanied by a comment stating what it does and why it was left in.",
        "url": "/developer/best_practices/",
        "index": ""
      }
      ,
    
      "developer-automated-builds": {
        "title": "Automated building and testing in RevBayes",
        "content": "Automated buildsA series of automated builds and tests is run every time code is pushed to the repository in any branch through Github Actions. The status of current workflows can be checked on the repository page, and a notification is sent to Slack (channel #github) once the full workflow is complete.The Github Actions workflow is configured in the .github/workflows/build.yml file. Linux and Mac builds use the CMake build configured in projects/cmake/build.sh, while the Windows build uses the Meson build in projects/meson/.The automated workflow runs the following tests:      Tutorial tests        Integration tests (in the tests/ submodule). To learn more about integration tests or add one, see this page.        Likelihood calculations checks using testiphy.  The tests are stored in a separate repository. Instructions on how to update this repository can be found in the README.TroubleshootingKnown issues:  the Windows build fails at the Configure and build step with the message ERROR: Dependency \"boost\" not found. The Install BOOST step shows ERROR 404 when attempting to download BOOST.Resolution: BOOST and other necessary libraries for the Windows build are downloaded from a repository which periodically removes old versions. This error means that the versions used by RevBayes need to be updated in the file projects/meson/make_winroot.sh, l90. Available packages are listed here.  the Windows build fails at the Test step with an error message about a missing dll file.Resolution: the missing DLL can be downloaded from the repository here. It needs to be added to the download list in file projects/meson/make_winroot.sh (l90) and moved to the build folder by adding the instruction cp /home/runner/win_root/mingw64/bin/your_missing_file.dll ${GITHUB_WORKSPACE}/wincrossbuild/bin to the file .github/workflows/build.yml in step Configure and build (Windows meson cross compile). For an example already present in the build, see libwinpthread-1.dll.  the Mac build fails at the Install step with Homebrew-related errors.Resolution: this is not a RevBayes issue, but an issue with the Mac virtual environment set up by Github. A list of current issues (and possible workarounds) can be found here.  some tests pass on my local machine but then fail on Github Actions. The output looks identical at the start but then diverges after a number of generations.Resolution: this is not a RevBayes issue, but comes from changes to the RNG in different Boost versions. Workarounds are to truncate the problem tests to a shorter run, or run the tests with the same Boost version as Github Actions.",
        "url": "/developer/automated_builds/",
        "index": ""
      }
      ,
    
      "developer-validation": {
        "title": "Validation Scripts in RevBayes",
        "content": "IntroductionWe use a procedure that is called Simulation-Based Calibration (SBC) (Talts et al. 2018) to test the implementation of our models (i.e., likelihood functions and parameter transformations) and MCMC algorithm (i.e., moves).The idea behind SBC is using the frequentists properties of Bayesian inference.That is, if we draw parameter values from their prior distributions and subsequently simulate data using these parameter values, the inferred XX % credible intervals (e.g., 90% CI) will contain the true (i.e., the simulated) parameter value in XX % of the repetitions (coverage probabilities; e.g., 90% of the simulations).The last sentence was quite a mouthful.Read it over and over again and try to fully understand it.With some more words, the SBC algorithm/procedure works as follows:  Specify a full Bayesian model with at least one variable representing the parameter (or multiple parameter and hyperprior distributions) and at least one variable for the data.  Specify the full MCMC procedure which would surely converge for such a tiny test scenario.  Now run the SBC validation which:  a Randomly draws parameter values (hierarchically) from the prior distribution.  b Randomly draws observations (the data) given the parameter values.  c Runs an MCMC analysis to estimate the posterior distribution of the parameters given the simulated data.  d Checks if the true (i.e., simulated) parameters fall into the XX % credible interval (e.g., 90% CI).  Repeat Step 3 many times, e.g., 1,000 or 10,000 times.  Compute the frequency how often the true (i.e., simulated) parameter was covered in the XX % credible interval (e.g., 90% of the simulations).Now let’s work through an example to understand how to implement SBC in RevBayes.Creating and Executing a Validation ScriptDirectory StructureOverview of an SBC analysis in RevBayes:  Create a Rev script for the validation analysis.  Run the Rev script, e.g., rb scripts/Validation_normal.Rev or mpirun -np 20 rb-mpi scripts/Validation_normal.Rev  Check that you obtained acceptable coverage probabilities.Directory StructureIn the RevBayes GitHub repository, the validation scripts are all located within the folder called validation.That directory structure within RevBayes. You should see the validation directory with the two sub-directories scripts and data.In this directory there are two subdirectories:  scripts: containing the Rev scripts defining different models and analyses that use the RevBayes validation functions to evaluate the coverage of relevant parameters.  data: containing the data files used to determine the dimensions of the simulations for someof the validation analyses (e.g., taxon names, number of taxa, alignment length). Data filesare not necessary for validation of all models.Writing a Validation Script for the Normal DistributionWe can write a simple validation script that checks the performance of the normal distribution.This script will check the coverage probabilities of estimators for parameters of the normal distribution.The model we will use is shown in .A graphical model of describing the generation of $N$ samples, where $x_i \\sim Normal(\\mu,\\sigma)$.Additionally, priors are specified for the parameters of the normal distribution such that$\\mu \\sim Uniform(-10,10)$ and $\\sigma \\sim Exponential(1)$. The validation tools willassess the coverage probabilities for estimates of $\\mu$ and $\\sigma$.The outcome of this validation will allow us to determine if the following things arebehaving correctly in RevBayes:  the functions for drawing random variates under the uniform, exponential, and normal distributions  the calculation of the probability densities under each distribution  the scale and slide moves  and all of the machinery that puts these components together.The validation methods in RevBayes will take a model and set of moves, simulate true values for stochastic nodes and an observed data, estimate the values for the stochastic nodes conditioned on the simulated data, and compare whether the true values fall within a given credible interval (the default value is 90%) for each stochastic random variable.This routine is performed for many simulation replicates (set in the validation script) and the coverage probability across replicates is summarized and reported to the screen.The complete validation script for the Normal distribution can be found in the RevBayes GitHub repository:Validation_normal.Rev.If you view this script, you will see that it begins with a header in comments that provide details about the script:  # RevBayes Validation Test: Normal Distribution – the name of the validation test.  # Model: 10 random variables from a normal distribution with a uniform prior on the mean and exponential prior on the standard deviation. – a brief description of what the validation script is testing.  # Inference: MCMC algorithm with the following moves: mvSlide and mvScale – a list of moves that are tested in this validation  authors: Sebastian Hoehna – the author of the script and the person you should contact if you implement any new functionality in RevBayes that break their validation test.Specify the modelThe first part of the script specifies the model DAG (i.e., random variables and conditional dependency) and its dimensions (i.e. the number of samples).n_samples = 10mu ~ dnUniform( -10, 10 )sigma ~ dnExponential( 1.0 )moves = VectorMoves()moves.append( mvSlide(mu) )moves.append( mvScale(sigma) )for (i in 1:n_samples ) {   x[i] ~ dnNormal(mu,sigma)   x[i].clamp( 0.0 ) # clamping is actually not necessary but good practice}Note that in this script, the data node is clamped: x[i].clamp( 0.0 ).This is not necessary since the validation methods will change the clamped value to the simulated data value for each simulation replicate.It also doesn’t hurt to clamp the nodes in this case if you prefer to do so.Clamping will make it more obvious which variables represent the data.Once the model DAG is set up, you can create a model workspace variable:mymodel = model(mu)Specify the MCMC objectBefore using the validation functions, you need to first have an MCMC workspace variable.Because the mcmc() function requires a list of monitors, we create an empty list to satisfy the required arguments of that function.Ultimately, the validation functions will not actually use this monitor because a new oneswill be created for each simulation replicate.monitors = VectorMonitors()The MCMC object is initialized just like in any normal MCMC analysis.mymcmc = mcmc(mymodel, monitors, moves)Specify the validation analysisThe first step in specifying the validation analysis is to create a workspace object usingthe validationAnalysis() function.This function takes 2 arguments: (1) and MCMC object and (2) the number ofsimulation replicates you want it to perform.validation = validationAnalysis( mymcmc, 1000 )For this validation, note that we chose 1000 replicates.For very large numbers of replicates, you will have a better approximation of the coverage probability.However, there is a substantial trade-off when it comes to run-times.Some of the validation scripts can take quite a long time to run.Importantly, this can be alleviated if you compile the MPI version of RevBayes, which will run the individual replicates in parallel.Next, you can specify a burn-in period, which will run the MCMC for each replicate for a specified number of MCMC cycles, while re-tuning the moves.validation.burnin(generations=10000,tuningInterval=100)Now you can run the validation MCMC.(As previously mentioned, it is best to compile the MPI version of RevBayes.)The number of generations should be chosen at your discretion, but this should be a sufficient number of samplesso that you can ensure your MCMC has effectively sampled the target distribution.validation.run(generations=30000)Validation summaryOnce the validation run is complete, you can use the .summarize() method of the validation object.validation.summarize( coverageProbability=0.9 )Summarizing analysis ...The validation analysis ran 1000 simulations to validate the implementation.This analysis used a 0.9 credible interval.Coverage frequencies should be between 0.881 and 0.918 in 95% of the simulations.Coverage frequencies of parameters in validation analysis:==========================================================mu                  \t\t0.917sigma               \t\t0.882At this point, you should conduct a visual inspection of the coverage frequencies for the parameters of your model.Do the coverage frequencies fall within the interval calculated by the validation analysis to a reasonable degree?In the example output above, the interval is 0.881 to 0.918.Problems and Frequently Asked QuestionsMy coverage probabilities are outside the expected range, what should I do?SBC is a stochastic procedure, so it can happen by chance that your coverage probabilities are outside the expected range.You should run the SBC with more replicates and check if your coverage probabilities is now closer to the expected value.If more replicates are not helping, then you should check your Rev script if everything is properly set-up, that you didn’t forget any MCMC moves by chance.If you still get unexpected coverage probabilities, then you need to check:  the simulation function used in the distribution,  the probability density function implemented for each distribution of your model, and  the MCMC moves for your analysis (you could try to use different moves that are established).I got a segmentation fault when summarizing the resultsThis can happen when you run multiple SBC’s at the same time because they have hard-coded output files.That means, your output might have been overwritten and does not match the analysis/model.Make sure that the",
        "url": "/developer/validation/",
        "index": ""
      }
      ,
    
      "tutorials-ctmc": {
        "title": "Nucleotide substitution models",
        "content": "This tutorial comes with a recorded video walkthrough. The video corresponding to each section of the exercise is linked next to the section title. The full playlist is available here: OverviewThis tutorial covers the first protocol from Höhna et al. (2017),which demonstrates how to set up and perform analysesusing common nucleotide substitution models. The substitution modelsused in molecular evolution are continuous time Markov models, which arefully characterized by their instantaneous-rate matrix:\\[Q = \\begin{pmatrix}-\\mu_A &amp; \\mu_{AC} &amp; \\mu_{AG} &amp; \\mu_{AT} \\\\\\mu_{CA} &amp; -\\mu_C  &amp; \\mu_{CG} &amp; \\mu_{CT} \\\\\\mu_{GA} &amp; \\mu_{GC} &amp; -\\mu_C  &amp; \\mu_{GT} \\\\\\mu_{TA} &amp; \\mu_{TC} &amp; \\mu_{TG} &amp; -\\mu_T\\end{pmatrix} \\mbox{  ,}\\]where $\\mu_{ij}$ represents the instantaneous rate of substitution fromstate $i$ to state $j$. The diagonal elements $\\mu_i$ are the rates ofnot changing out of state $i$, equal to the sum of the elements in thecorresponding row. Given the instantaneous-rate matrix, $Q$, we cancompute the corresponding transition probabilities for a branch oflength $t$, $P(t)$, by exponentiating the rate matrix:\\[P(t) = \\begin{pmatrix}p_{AA}(t) &amp; p_{AC}(t) &amp; p_{AG}(t) &amp; p_{AT}(t) \\\\p_{CA}(t) &amp; p_{CC}(t) &amp; p_{CG}(t) &amp; p_{CT}(t) \\\\p_{GA}(t) &amp; p_{GC}(t) &amp; p_{GG}(t) &amp; p_{GT}(t) \\\\p_{TA}(t) &amp; p_{TC}(t) &amp; p_{TG}(t) &amp; p_{TT}(t)\\end{pmatrix} = e^{Qt} = \\sum_{j=0}^\\infty\\frac{(Qt)^j}{j!} \\mbox{  .}\\]Each of the named substitution models (e.g., HKY or GTR) has a uniquely definedinstantaneous-rate matrix, $Q$.In this tutorial you will perform phylogeny inference under commonmodels of DNA sequence evolution: JC, F81, HKY85, GTR, GTR+Gamma andGTR+Gamma+I. For all of these substitution models, you will perform aMarkov chain Monte Carlo (MCMC) analysis to estimate phylogeny and other model parameters. Theestimated trees will be unrooted trees with independent branch-lengthparameters. We will provide comments on how to modify the tutorial ifyou wish to estimate rooted, clock-like trees. All the assumptions willbe covered in more detail later in this tutorial.Specific functions for substitution models available in RevBayes.            Model      Reference      Function      Parameters                  Jukes-Cantor      (Jukes and Cantor 1969)      fnJC      -              K80 (a.k.a. K2P)      (Kimura 1980)      fnK80      $\\kappa$              Felsenstein-81      (Felsenstein 1981)      fnF81      $\\pi$              T92      (Tamura 1992)      fnT92      $\\pi_{GC}$, $\\kappa$              HKY      (Hasegawa et al. 1985)      fnHKY      $\\pi$, $\\kappa$              GTR      (Tavaré 1986)      fnGTR      $\\pi$, $\\epsilon$      Example: Character Evolution under the Jukes-Cantor Substitution ModelGetting StartedThe first section of this exercise involves:  setting up a Jukes-Cantor (JC) substitution model for an alignment of the cytochrome b subunit;  approximating the posterior probability of the tree topology and node ages (and all other parameters) using MCMC, and;  summarizing the MCMC output by computing the maximum a posteriori tree.Graphical model representation of a simple phylogenetic model. The graphical model shows the dependencies among parameters (Höhna et al. 2014). Here, the rate matrix $Q$ is a constant variable because it is fixed and does not depend on any parameter. The only free parameters of this model, the Jukes-Cantor model, are the tree $\\Psi$ including the branch lengths.We first consider the simplest substitution model described byJukes and Cantor (1969). The instantaneous-rate matrix for the JC substitutionmodel is defined as\\[Q_{JC69} = \\begin{pmatrix}{*} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} \\\\\\frac{1}{3} &amp; {*} &amp; \\frac{1}{3} &amp; \\frac{1}{3} \\\\\\frac{1}{3} &amp; \\frac{1}{3} &amp; {*} &amp; \\frac{1}{3} \\\\\\frac{1}{3} &amp; \\frac{1}{3} &amp; \\frac{1}{3} &amp; {*}\\end{pmatrix} \\mbox{  ,}\\]which has the advantage that the transition probability matrix can becomputed analytically\\[P_{JC69} = \\begin{pmatrix} {\\frac{1}{4} + \\frac{3}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} \\\\\\\\ {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} + \\frac{3}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} \\\\\\\\ {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} + \\frac{3}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} \\\\\\\\ {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} - \\frac{1}{4}e^{-rt}} &amp; {\\frac{1}{4} + \\frac{3}{4}e^{-rt}}\\end{pmatrix} \\mbox{  ,}\\]where $t$ is the branch length in units of time, and $r$ is the rate (clock) for the process. In the later exercises you will be asked to specify more complex substitution models. Don’t worry, you won’t have to calculate all of the transition probabilities, because RevBayes will take care of all the computations for you. Here we only provide some of the equations for the models in case you might be interested in the details. You will be able to complete the exercises without understanding the underlying math.The files for this example analysis are provided for you (mcmc_JC.Rev).If you download this file and place it in a directory called scripts inside your main tutorial directory,you caneasily execute this analysis using the source() function in the RevBayes console:source(\"scripts/mcmc_JC.Rev\")If everything loaded properly, then you should see the program initiatethe Markov chain Monte Carlo analysis that estimates the posteriordistribution. If you continue to let this run, then you will see itoutput the states of the Markov chain once the MCMC analysis begins.Ultimately, this is how you will execute most analyses in RevBayes, with the full specification of the model and analyses contained in the sourced files. You could easily run this entire analysis on your own data by substituting your data file name for that in the model-specification file. However, it is important to understand the components of the model to be able to take full advantage of the flexibility and richness of RevBayes. Furthermore, without inspecting the Rev scripts sourced in mcmc_JC.Rev, you may end up inadvertently performing inappropriate analyses on your dataset, which would be a waste of your time and CPU cycles. The next steps will walk you through the full specification of the model and MCMC analyses.Loading the Data  First create a directory for this tutorial and name it RB_CTMC_Tutorial, or any nameyou like.  Navigate to this new directory and create a new folder called data inside of it.  Download the data file called primates_and_galeopterus_cytb.nexand save it to the data directory.  Now start RevBayes from your working directory (RB_CTMC_Tutorial).Checking and Changing Your Working DirectoryFor this tutorial and much of the work you will do in RevBayes, you will need to access files.It is important that you are aware of your current working directory if you use relative file pathsin your Rev scripts or in the RevBayes console.To check your current working directory, use the function getwd().getwd()/Users/tombayes/WorkIf you want to change the directory, enter the path to your directory in the arguments of the function setwd().setwd(\"Tutorials/RB_CTMC_Tutorial\")Now check your directory again to make sure you are where you want to be:getwd()/Users/tombayes/Work/Tutorials/RB_CTMC_TutorialFirst load in the sequences using the readDiscreteCharacterData()function.data = readDiscreteCharacterData(\"data/primates_and_galeopterus_cytb.nex\")Executing these lines initializes the data matrix as the respectiveRev variables. To report the current value of any variable, simplytype the variable name and press enter. For the data matrix, thisprovides information about the alignment:data   DNA character matrix with 23 taxa and 1102 characters   =====================================================   Origination:                   \"primates_and_galeopterus_cytb.nex\"   Number of taxa:                23   Number of included taxa:       23   Number of characters:          1102   Number of included characters: 1102   Datatype:                      DNANext we will specify some useful variables based on our dataset. The variable data has member functionsthat we can use to retrieve information about the dataset. These include, for example,the number of species and the taxa.To check all available member functions:data.methods()We will need taxon information for setting up different parts of our model.num_taxa &lt;- data.ntaxa()num_branches &lt;- 2 * num_taxa - 3taxa &lt;- data.taxa()Additionally, we set up a (vector) variable that holds all the moves for our analysis.Recall that moves are algorithms used to propose new parameter values during the MCMC simulation.Similarly, we set up a variable for the monitors.Monitors print the values of model parameters to the screen and/or log files during the MCMC analysis.moves    = VectorMoves()monitors = VectorMonitors()You may have noticed that we used the = operator to create the move index.This simply means that the variable is not part of the model.You will later see that we use this operator more often, e.g., when we create moves and monitors.With the data loaded, we can now proceed to specify our specifying the model.Setting up the Graphical Model and MCMCEstimating an unrooted tree under the JC model requires specification of two main components:(1) the  and (2) the .Jukes-Cantor Substitution ModelA given substitution model is defined by its correspondinginstantaneous-rate matrix, $Q$. The Jukes-Cantor substitution model doesnot have any free parameters (as the substitution rates are all assumedto be equal, and there is a separate parameter that scales their overallmagnitude), so we can define it as a constant variable. The functionfnJC(n) will create an instantaneous-rate matrix for a character with$n$ states. Since we use DNA data here, we create a 4x4instantaneous-rate matrix:Q &lt;- fnJC(4)You can see the rates of the $Q$ matrix by typingQ   [ [ -1.0000, 0.3333, 0.3333, 0.3333 ] ,     [ 0.3333, -1.0000, 0.3333, 0.3333 ] ,     [ 0.3333, 0.3333, -1.0000, 0.3333 ] ,     [ 0.3333, 0.3333, 0.3333, -1.0000 ] ]As you can see, all substitution rates are equal.Tree Topology and Branch LengthsThe tree topology and branch lengths are stochastic nodes in our phylogenetic model.In , the tree topology is denoted $\\Psi$ and thelength of the branch leading to node $i$ is $bl_i$.We will assume that all possible labeled, unrooted tree topologies have equal probability.This is the dnUniformTopology() distribution in RevBayes.Note that in RevBayes it is advisable to specify the outgroup for your study systemif you use an unrooted tree prior, whereas other software, e.g.,MrBayes uses the first taxon in the data matrix file as the outgroup.Providing RevBayes with an outgroup clade will enable the monitor writing the treesto file to orient the topologies with the outgroup clade at the base,thus making the trees easier to visualize.Specify the topology stochastic node by passing in the list of taxato the dnUniformTopology() distribution:out_group = clade(\"Galeopterus_variegatus\")topology ~ dnUniformTopology(taxa, outgroup=out_group)Some types of stochastic nodes can be updated by a number of alternative moves.Different moves may explore parameter space in different ways,and it is possible to use multiple different moves for a given parameter to improve mixing(the efficiency of the MCMC simulation).In the case of our unrooted tree topology, for example, we can use both a nearest-neighbor interchange move (mvNNI) and a subtree-prune and regrafting move (mvSPR). These moves do not have tuning parameters associated with them, thus you only need to pass in the topology node and proposal weight.moves.append( mvNNI(topology, weight=num_taxa) )moves.append( mvSPR(topology, weight=num_taxa/10) )The weight specifies how often the move will be applied either on average per iteration or relative to all other moves. Have a look at the MCMC Diagnosis tutorial for more details about moves and MCMC strategies (found in Tutorials).Next we have to create a stochastic node representing the length of each of the $2N - 3$ branches in our tree (where $N=$ n_species). We can do this using a for loop — this is a plate in our graphical model. In this loop, we can create each of the branch-length nodes and assign each move. Copy this entire block of Rev code into the console:for (i in 1:num_branches) {    bl[i] ~ dnExponential(10.0)    moves.append( mvScale(bl[i]) )}It is convenient for monitoring purposes to add the tree length as deterministic variable. The tree length is simply the sum of all branch lengths. Accordingly, the tree length can be computed using the sum() function, which calculates the sum of any vector of values.TL := sum(bl)Finally, we can create a phylogram (a phylogeny in which the branch lengths are proportional to the expected number of substitutions/site) by combining the tree topology and branch lengths. We do this using the treeAssembly() function, which applies the value of the $i^{th}$ member of the br_lens vector to the branch leading to the $i^{th}$ node in topology. Thus, the psi variable is a deterministic node:psi := treeAssembly(topology, bl)Alternative tree priorsFor large phylogenetic trees, i.e., with more than 200 taxa, it might be easier to specify a combined topology and branch length prior distribution.We can achieve this by simple using the distribution dnUniformTopologyBranchLength().br_len_lambda &lt;- 10.0psi ~ dnUniformTopologyBranchLength(taxa, branchLengthDistribution=dnExponential(br_len_lambda))moves.append( mvNNI(psi, weight=num_taxa) )moves.append( mvSPR(psi, weight=num_taxa/10.0) )moves.append( mvBranchLengthScale(psi, weight=num_branches) )You might think that this approach is in fact simpler than the for loop that we explained above.We still think that it is pedagogical to specify the prior on each branch length separately in this tutorial to emphasize all components of the model.Alternative branch-length priorsSome studies, e.g. Brown et al. (2010), Rannala et al. (2012),have criticized the exponential prior distribution for branch lengthsbecause it induces a gamma-distributed tree-length and the mean of this gamma distributiongrows with the number of taxa. As an alternative, we can instead use a specific gamma prior distribution(or any other distribution defined on a positive real variable) for the tree length,and then use a Dirichlet prior distribution to break the tree length intothe corresponding branch lengths (Zhang et al. 2012).First, specify a prior distribution on the tree length with your desired mean.For example, we use a gamma distribution as our prior on the tree length.TL ~ dnGamma(2,4)moves.append( mvScale(TL) )Now we create a random variable for the relative branch lengths.rel_branch_lengths ~ dnDirichlet( rep(1.0,num_branches) )moves.append( mvBetaSimplex(rel_branch_lengths, weight=num_branches) )moves.append( mvDirichletSimplex(rel_branch_lengths, weight=num_branches/10.0) )Finally, transform the relative branch lengths into actual branch lengthsbr_lens := rel_branch_lengths * TLAlternative Prior on Time-Trees: Tree Topology and Node AgesAlternatively, you may want to specify a prior on time-trees.Here we will briefly indicate how to specify such an prior which will lead to inference of time trees.The tree (the topology and node ages) is a stochastic node in our phylogenetic model.For simplicity, we will assume a uniform prior on both topologies and node ages.The distribution in RevBayes is dnUniformTimeTree().Fore more information on tree priors, such as birth-death processes, please read the Relaxed Clocks &amp; Time Trees.First, we need to specify the age of the tree:root_age &lt;- 10.0Here we simply assumed that the tree is 10.0 time units old. We could also specify a prior on the root age if we have fossil calibrations (see Relaxed Clocks &amp; Time Trees). Next, we specify the tree stochastic variable by passing in the taxon information taxa to the dnUniformTimeTree() distribution:psi ~ dnUniformTimeTree(rootAge=root_age, taxa=taxa)Some types of stochastic nodes can be updated by a number of alternative moves.Different moves may explore parameter space in different ways,and it is possible to usemultiple different moves for a given parameter to improve mixing(the efficiency of the MCMC simulation). In the case of our rooted tree,for example, we can use both a nearest-neighbor interchange move without and with changingthe node ages (mvNarrow and mvNNI) and a fixed-node-height subtree-prune and regraftingmove (mvFNPR) and its Metropolized-Gibbs variant (mvGPR) (Höhna et al. 2008; Höhna and Drummond 2012).We also need moves that change the ages of the internal nodes, for example, mvSubtreeScaleand mvNodeTimeSlideUniform. These moves do not have tuning parameters associated withthem, thus you only need to pass in the psi node and proposal weight.moves.append( mvNarrow(psi, weight=num_taxa) )moves.append( mvNNI(psi, weight=num_taxa/5.0) )moves.append( mvFNPR(psi, weight=num_taxa/5.0) )moves.append( mvGPR(psi, weight=num_taxa/30.0) )moves.append( mvSubtreeScale(psi, weight=num_taxa/3.0) )moves.append( mvNodeTimeSlideUniform(psi, weight=num_taxa) )The weight specifies how often the move will be applied either on average per iteration or relative to all other moves. Have a look at the Introduction to Markov chain Monte Carlo (MCMC) Sampling for more details about moves and MCMC strategies.Molecular ClockAdditionally, in the case of time-calibrated trees, we need to add a molecular clock rate parameter. For example, we know from empirical estimates that the molecular clock rate is about 0.01 (=1%) per million years per site. Nevertheless, we can estimate it here because we fixed the root age. We use a uniform prior on the log-transform clock rate. This specifies our lack of prior knowledge on the magnitude of the clock rate.log_clock_rate ~ dnUniform(-6,1)moves.append( mvSlide(log_clock_rate, weight=2.0) )clock_rate := 10^log_clock_rateInstead, you could also fix the clock rate and estimate the root age.For more information on molecular clocks please read the Relaxed Clocks &amp; Time Trees tutorial.Putting it All TogetherWe have fully specified all of the parameters of our phylogeneticmodel—the tree topology with branch lengths, and the substitution modelthat describes how the sequence data evolved over the tree with branchlengths. Collectively, these parameters comprise a distribution calledthe phylogenetic continuous-time Markov chain, and we use thednPhyloCTMC constructor function to create this node. Thisdistribution requires several input arguments:  the tree with branch lengths;  the instantaneous-rate matrix Q;  the type of character data.Build the random variable for the character data (sequence alignment).seq ~ dnPhyloCTMC(tree=psi, Q=Q, type=\"DNA\")Once the PhyloCTMC model has been created, we can attach our sequencedata to the tip nodes in the tree.seq.clamp(data)Note that although we assume that our sequence data are randomvariables—they are realizations of our phylogenetic model—for thepurposes of inference, we assume that the sequence data are “clamped” to their observed values.When this function is called, RevBayes sets each of the stochasticnodes representing the tips of the tree to the corresponding nucleotidesequence in the alignment. This essentially tells the program that wehave observed data for the sequences at the tips.Finally, we wrap the entire model in a single object to provide convenient access to theDAG. To do this, we only need to give the model() function a singlenode. With this node, the model() function can find all of the othernodes by following the arrows in the graphical model:mymodel = model(Q)Now we have specified a simple phylogenetic analysis—each parameter ofthe model will be estimated from every site in our alignment. If weinspect the contents of mymodel we can review all of the nodes in theDAG:mymodelSpecifying Monitors and Output FilesFor our MCMC analysis, we need to set up a vector of monitors torecord the states of our Markov chain. The monitor functions are allcalled mn\\*, where \\* is the wild-card representing the monitor type.First, we will initialize the model monitor using the mnModelfunction. This creates a new monitor variable that will output thestates for all model parameters when passed into a MCMC function.monitors.append( mnFile(filename=\"output/primates_cytb_JC.trees\", printgen=10, psi) )The mnFile monitor will record the states for only the parameterspassed in as arguments. We use this monitor to specify the output forour sampled trees and branch lengths.monitors.append( mnScreen(printgen=100, TL) )Finally, create a screen monitor that will report the states ofspecified variables to the screen with mnScreen:This monitor mostly helps us to see the progress of the MCMC run.Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, wecan now set up the MCMC algorithm that will sample parameter values inproportion to their posterior probability. The mcmc() function willcreate our MCMC object:mymcmc.run(generations=20000)Now, run the MCMC:When the analysis is complete, you will have the monitored files in your output directory.Saving and restarting analysesMCMC analyses can take a long time to converge, and it is usually difficult to predict how many generations will be needed to obtain results. In addition, many analyses are run on computer clusters with time limits, and so may be stopped by the cluster partway through. For all of these reasons, it is useful to save the state of the chain regularly through the analysis.mymcmc.run(generations=100000000, checkpointInterval=100, checkpointFile=\"output/primates_cytb_JC.state\")The checkpointInterval and checkpointFile inputs specify respectively how often, and to which file, the chain should be saved. Three different files will be used for storing the state, with no extension and with extensions _mcmc and _moves.When multiple independent runs are specified, they will automatically be saved in separate files (with extensions _run_1, _run_2, etc.).Restarting the chain from a previous run is done by adding this line:mymcmc.initializeFromCheckpoint(\"output/primates_cytb_JC.state\")before calling the function mcmc.run(). The file name should match what was given as checkpointFile when running the previous analysis. NB: Note that this line will create an error if the state file does not exist yet, and so should be commented out in the first run.The full MCMC block thus becomes:mymcmc = mcmc(mymodel, monitors, moves)mymcmc.initializeFromCheckpoint(\"output/primates_cytb_JC.state\")mymcmc.run(generations=100000000, checkpointInterval=100, checkpointFile=\"output/primates_cytb_JC.state\")Summarizing MCMC SamplesMethods for visualizing the marginal densities of parameter values are not currently available in RevBayes itself.Thus, it is important to use programs like Tracer (Rambaut and Drummond 2011) to evaluate mixing and non-convergence.Look at the file called output/primates_cytb_JC.log in Tracer.There you see the posterior distribution of the continuous parameters, e.g., the tree length variable TL. Left: Trace of tree-length samples for one MCMC run. The caterpillar-like look is a good sign.You will also see that the effective sample size is comparably large, i.e., much larger than 200. Right: Posterior distribution of the tree length of the primate phylogeny under a Jukes-Cantor substitution model.It is always important to carefully assess the MCMC samples for the various parameters in your analysis.You can read more about MCMC tuning andevaluating and improving mixing in the tutorials Introduction to Markov chain Monte Carlo (MCMC) Sampling.Exercise 1We are interested in the phylogenetic relationship of the Tarsiers.Therefore, we need to summarize the trees sampled from the posteriordistribution. RevBayes can summarize the sampled trees by reading inthe tree-trace file:# and then get the MAP treeThe mapTree() function will summarize the tree samples and write themaximum a posteriori tree to file:Maximum a posteriori estimate of the primate phylogeny under a Jukes-Cantor substitution model. The numbers at the nodes show the posterior probabilities for the clades. We have rooted the tree at the outgroup Galeopterus_variegatusLook at the file called output/primates_cytb_JC_MAP.tree inFigTree. We show it in .Fill in the following table as you go through the tutorial.Posterior probabilities under different analyses            Model      Lemuroidea      Lorisoidea      Platyrrhini      Catarrhini                  Jukes-Cantor                                          HKY85                                          F81                                          GTR                                          GTR+Gamma                                          GTR+Gamma+I                                  Note, you can query the posterior probability of a clade beingmonophyletic using the following command:map_tree = mapTree(treetrace,\"output/primates_cytb_JC_MAP.tree\")Lemuroidea &lt;- clade(\"Cheirogaleus_major\",                    \"Daubentonia_madagascariensis\",                    \"Lemur_catta\",                    \"Lepilemur_hubbardorum\",                    \"Microcebus_murinus\",                    \"Propithecus_coquereli\",                    \"Varecia_variegata_variegata\")Primate and species relationships.            Species      Family      Parvorder      Suborder                  Aotus trivirgatus      Aotidae      Platyrrhini (NWM)      Haplorrhini              Callicebus donacophilus      Pitheciidae      Platyrrhini (NWM)      Haplorrhini              Cebus albifrons      Cebidae      Platyrrhini (NWM)      Haplorrhini              Cheirogaleus major      Cheirogaleidae      Lemuroidea      Strepsirrhini              Chlorocebus aethiops      Cercopithecoidea      Catarrhini      Haplorrhini              Colobus guereza      Cercopithecoidea      Catarrhini      Haplorrhini              Daubentonia madagascariensis      Daubentoniidae      Lemuroidea      Strepsirrhini              Galago senegalensis      Galagidae      Lorisidae      Strepsirrhini              Hylobates lar      Hylobatidea      Catarrhini      Haplorrhini              Lemur catta      Lemuridae      Lemuroidea      Strepsirrhini              Lepilemur hubbardorum      Lepilemuridae      Lemuroidea      Strepsirrhini              Loris tardigradus      Lorisidae      Lorisidae      Strepsirrhini              Macaca mulatta      Cercopithecoidea      Catarrhini      Haplorrhini              Microcebus murinus      Cheirogaleidae      Lemuroidea      Strepsirrhini              Nycticebus coucang      Lorisidae      Lorisidae      Strepsirrhini              Otolemur crassicaudatus      Galagidae      Lorisidae      Strepsirrhini              Pan paniscus      Hominoidea      Catarrhini      Haplorrhini              Perodicticus potto      Lorisidae      Lorisidae      Strepsirrhini              Propithecus coquereli      Indriidae      Lemuroidea      Strepsirrhini              Saimiri sciureus      Cebidae      Platyrrhini (NWM)      Haplorrhini              Tarsius syrichta      Tarsiidae      Tarsiiformes      Haplorrhini              Varecia variegata variegata      Lemuridae      Lemuroidea      Strepsirrhini      Setting up the Kimura 1980 (K80 or K2P) substitution modelThe K80 model (AKA the K2P model) allows the rates of transition and transversion substitutions to be unequal (Kimura 1980).The parameter $\\kappa$ describes the relative rate of transition to transversion substitutions (if $\\kappa &gt; 1$, transitions occur at a higher rate than transversions). The instantaneous-rate matrix for the K80 model is defined as:\\[Q_{K80} = \\begin{pmatrix}                  -                 &amp; \\frac{1}{4} &amp; \\frac{\\kappa}{4} &amp; \\frac{1}{4} \\\\                  \\frac{1}{4} &amp; -               &amp; \\frac{1}{4} &amp; \\frac{\\kappa}{4} \\\\                  \\frac{\\kappa}{4} &amp; \\frac{1}{4} &amp; -                &amp; \\frac{1}{4} \\\\                  \\frac{1}{4} &amp; \\frac{\\kappa}{4}\t&amp; \\frac{1}{4} &amp; -\\end{pmatrix} \\mbox{  .}\\]Now, add the parameter $\\kappa$ to the substitution model, and create a K80 rate matrix:kappa ~ dnExp(1)moves.append( mvScale(kappa, weight=1.0) )Q := fnK80(kappa)The Hasegawa-Kishino-Yano (HKY) 1985 Substitution ModelThe Jukes-Cantor model assumes that all substitution rates are equal,which also implies that the stationary frequencies of the fournucleotide bases are equal. These assumptions are not very biologicallyreasonable, so we might wish to consider a more realistic substitutionmodel that relaxes some of these assumptions. For example, we mightallow stationary frequencies, $\\pi$, to be unequal, and allow rates oftransition and transversion substitutions to differ, $\\kappa$. Thiscorresponds to the substitution model proposed by Hasegawa et al. (1985),which is specified with the following instantaneous-rate matrix:\\[Q_{HKY} = \\begin{pmatrix}{\\cdot}             &amp; {\\pi_C}           &amp; {\\kappa\\pi_G}     &amp; {\\pi_T} \\\\{\\pi_A}             &amp; {\\cdot}           &amp; {\\pi_G}           &amp; {\\kappa\\pi_T} \\\\{\\kappa\\pi_A}       &amp; {\\pi_C}           &amp; {\\cdot}           &amp; {\\pi_T} \\\\{\\pi_A}             &amp; {\\kappa\\pi_C}     &amp; {\\pi_G}           &amp; {\\cdot}\\end{pmatrix} \\mbox{  ,}\\]where the diagonal ${\\cdot}$ entries are equal to the negative sum of theelements in the corresponding row.Use the file mcmc_JC.Rev as a starting point for the HKY analysis.Note that we are adding two new variables to our model. We can define avariable pi for the stationary frequencies that are drawn from a flatDirichlet distribution bypi_prior &lt;- v(1,1,1,1)pi ~ dnDirichlet(pi_prior)Since pi is a stochastic variable, we need to specify a move topropose updates to it. A good move on variables drawn from a Dirichletdistribution is the mvBetaSimplex. This move randomly takes an elementfrom the simplex, proposes a new value for it drawn from a Betadistribution, and then rescales all values of the simplex to sum to 1again.moves.append( mvBetaSimplex(pi, weight=2) )moves.append( mvDirichletSimplex(pi, weight=1) )The second new variable is $\\kappa$, which specifies the ratio oftransition-transversion rates. The $\\kappa$ parameter must be apositive-real number and a natural choice as the prior distribution isthe lognormal distribution:kappa ~ dnLognormal(0.0, 1.0)Again, we need to specify a move for this new stochastic variable. Asimple scaling move should do the job.moves.append( mvScale(kappa) )Finally, we need to create the HKY instantaneous-rate matrix using thefnHKY function:Q := fnHKY(kappa,pi)This should be all for the HKY model. Don’t forget to change the output file names, otherwise your old analyses files will be overwritten.Exercise 2      With  as your guide, draw the probabilisticgraphical model of the HKY model.        Download the file called mcmc_JC.Rev and rename itmcmc_HKY.Rev. Save this file in a directory called scripts located in the samedirectory as your data folder.        Modify mcmc_HKY.Rev by including thenecessary parameters to specify the HKY substitution model. Be sure to change the output file names given to the monitors.        Run a MCMC analysis to estimate the posterior distribution underthe HKY substitution model.        Are the resulting estimates of the base frequencies equal? If not,how much do they differ? Are the estimated base frequencies similarto the empirical base frequencies? The empirical base frequenciesare the frequencies of the characters in the alignment, which can becomputed with RevBayes by data.getEmpiricalBaseFrequencies().        Is the inferred rate of transition substitutions higher than therate of transversion substitutions? If so, by how much?        Like the HKY model, the Felsenstein 1981 (F81) substitution modelhas unequal stationary frequencies, but it assumes equaltransition-transversion rates (Felsenstein 1981).Can you set up the F81 model and run an analysis?        Complete the  by reporting the posteriorprobabilities of phylogenetic relationships.  The General Time-Reversible (GTR) Substitution ModelThe HKY substitution model can accommodate unequal base frequencies anddifferent rates of transition and transversion substitutions. Despitethese extensions, the HKY model may still be too simplistic for manyreal datasets. Here, we extend the HKY model to specify the general timereversible (GTR) substitution model (Tavaré 1986), which allows all sixexchangeability rates to differ ().The instantaneous-rate matrix for the GTR substitution model is:\\[Q_{GTR} = \\begin{pmatrix}{\\cdot}\t   &amp; {r_{AC}\\pi_C} &amp; {r_{AG}\\pi_G} &amp; {r_{AT}\\pi_T} \\\\{r_{AC}\\pi_A} &amp; {\\cdot}       &amp; {r_{CG}\\pi_G} &amp; {r_{CT}\\pi_T} \\\\{r_{AG}\\pi_A} &amp; {r_{CG}\\pi_C} &amp; {\\cdot}       &amp; {r_{GT}\\pi_T} \\\\{r_{AT}\\pi_A} &amp; {r_{CT}\\pi_C} &amp; {r_{GT}\\pi_G} &amp; {\\cdot}       \\\\\\end{pmatrix} \\mbox{  ,}\\]where the six exchangeability parameters, $r_{ij}$, specify the relativerates of change between states $i$ and $j$.Graphical model representation of the general-time reversible (GTR) phylogenetic model.The GTR model requires that we define and specify a prior on the sixexchangeability rates, which we will describe using a flat Dirichletdistribution. As we did previously for the Dirichlet prior on basefrequencies, we first define a constant node specifying the vector ofconcentration-parameter values using the v() function:er_prior &lt;- v(1,1,1,1,1,1)This node defines the concentration-parameter values of the Dirichletprior distribution on the exchangeability rates. Now, we can create astochastic node for the exchangeability rates using the dnDirichlet()function, which takes the vector of concentration-parameter values as anargument and the ~ operator. Together, these create a stochastic nodenamed er ($\\theta$ in ):er ~ dnDirichlet(er_prior)The Dirichlet distribution assigns probability densities to a group of parameters: e.g.,  those that measure proportions and must sum to 1. Here, we have specified a six-parameter Dirichlet prior, where each value describes one of the six relative rates of the GTR model: (1) $A\\leftrightarrows C$; (2) $A\\leftrightarrows G$; (3) $A\\leftrightarrows T$; (4) $C\\leftrightarrows G$; (5) $C\\leftrightarrows T$; (6) $G\\leftrightarrows T$. The input parameters of a Dirichlet distribution are called shape (or concentration) parameters. The expectation and variance for each variable are related to the sum of the shape parameters. The prior we specified above is a ‘flat’ or symmetric Dirichlet distribution; all of the shape parameters are equal (1,1,1,1,1,1). This describes a model that allows for equal rates of change between nucleotides, such that the expected rate for each is equal to $\\frac{1}{6}$ (a).We might also parameterize the Dirichlet distribution such that all of the shape parameters were equal to 100, which would also specify a prior with an expectation of equal exchangeability rates (b). However, by increasing the values of the shape parameters, er_prior &lt;- v(100,100,100,100,100,100), the Dirichlet distribution will more strongly favor equal exchangeability rates; (i.e., a relatively informative prior).Alternatively, we might consider an asymmetric Dirichlet parameterization that could reflect a strong prior belief that transition and transversion substitutions occur at different rates. For example, we might specify the prior density er_prior &lt;- v(4,8,4,4,8,4). Under this model, the expected rate for transversions would be $\\frac{4}{32}$ and that for transitions would be $\\frac{8}{32}$, and there would be greater prior probability on sets of GTR rates that matched this configuration (c).Yet another asymmetric prior could specify that each of the six GTR rates had a different value conforming to a Dirichlet(2,4,6,8,10,12). This would lead to a different prior probability density for each rate parameter (d). Without strong prior knowledge about the pattern of relative rates, however, we can better reflect our uncertainty by using a vague prior on the GTR rates. Notably, all patterns of relative rates have the same probability density under er_prior &lt;- v(1,1,1,1,1,1).Four different examples of Dirichlet priors on exchangeability rates.For each stochastic node in our model, we must also specify a proposal mechanism if we wish to estimate that parameter. The Dirichlet prior on our parameter er creates a simplex of values that sum to 1.moves.append( mvBetaSimplex(er, weight=3) )moves.append( mvDirichletSimplex(er, weight=1) )We can use the same type of distribution as a prior on the 4 stationaryfrequencies ($\\pi_A, \\pi_C, \\pi_G, \\pi_T$) since these parameters alsorepresent proportions. Specify a flat Dirichlet prior density on thebase frequencies:pi_prior &lt;- v(1,1,1,1) pi ~ dnDirichlet(pi_prior)The node pi represents the $\\pi$ node in . Now add the simplex scale move on the stationary frequencies to the moves vector:moves.append( mvBetaSimplex(pi, weight=2) )moves.append( mvDirichletSimplex(pi, weight=1) )We can finish setting up this part of the model by creating a deterministic node for the GTR instantaneous-rate matrix Q. The fnGTR() function takes a set of exchangeability rates and a set of base frequencies to compute the instantaneous-rate matrix used when calculating the likelihood of our model.Q := fnGTR(er,pi) Exercise 3      Use one of your previous analysis files—either the mcmc_JC.Rev ormcmc_HKY.Rev—to specify a GTR analysis in a new file calledmcmc_GTR.Rev. Adapt the old analysis to be performed under theGTR substitution model.        Run an MCMC analysis to estimate the posterior distribution.        Complete the table of the phylogenetic relationship of primates.  The Discrete Gamma Model of Among Site Rate VariationMembers of the GTR family of substitution models assume that rates are homogeneous across sites, an assumption that is often violated by real data. We can accommodate variation in substitution rate among sites (ASRV) by adopting the discrete-gamma model (Yang 1994). This model assumes that the substitution rate at each site is a random variable that is described by a discretized gamma distribution, which has two parameters: the shape parameter, $\\alpha$, and the rate parameter, $\\beta$. In order that we can interpret the branch lengths as the expected number of substitutions per site, this model assumes that the mean site rate is equal to 1. The mean of the gamma is equal to $\\alpha/\\beta$, so a mean-one gamma is specified by setting the two parameters to be equal, $\\alpha=\\beta$. This means that we can fully describe the gamma distribution with the single shape parameter, $\\alpha$. The degree of among-site substitution rate variation is inversely proportional to the value of the $\\alpha$-shape parameter. As the value of the $\\alpha$-shape increases, the gamma distribution increasingly resembles a normal distribution with decreasing variance, which therefore corresponds to decreasing levels of ASRV (). By contrast, when the value of the $\\alpha$-shape parameter is $&lt; 1$, the gamma distribution assumes a concave distribution that concentrates most of the prior density on low rates, but retains some prior mass on sites with very high rates, which therefore corresponds to high levels of ASRV (). Note that, when $\\alpha = 1$, the gamma distribution collapses to an exponential distribution with a rate parameter equal to $\\beta$.The probability density of mean-one gamma-distributed rates for different values of the $\\alpha$-shape parameter.We typically lack prior knowledge regarding the degree of ASRV for a given alignment.Accordingly, rather than specifying a precise value of $\\alpha$, we can instead estimate the value of the $\\alpha$-shape parameter from the data. This requires that we specify a diffuse (relatively ‘uninformative’) prior on the $\\alpha$-shape parameter. For this analysis, we will use a uniform distribution between 0 and 10.This approach for accommodating ASRV is another example of a hierarchical model ().That is, variation in substitution rates across sites is addressed by applying a site-specific rate multiplier to each of the $j$ sites, $r_j$.These rate-multipliers are drawn from a discrete, mean-one gamma distribution; the shape of this prior distribution (and the corresponding degree of ASRV) is governed by the $\\alpha$-shape parameter. The $\\alpha$-shape parameter, in turn, is treated as a lognormal distributed random variable. Finally, the shape of the lognormal prior is governed by the mean and standard deviation parameters, which are set to fixed values.Graphical model representation of the General Time Reversible (GTR) + Gamma phylogenetic model with invariable sites.Setting up the Gamma Model in RevBayesThen create a stochastic node called alpha with a uniform prior distribution between 0.0 and $10$(this represents the stochastic node for the $\\alpha$-shape parameter in):alpha ~ dnUniform( 0.0, 10 )The way the ASRV model is implemented involves discretizing the mean-one gamma distribution into a set number of rate categories, $k$. Thus, we can analytically marginalize over the uncertainty in the rate at each site. The likelihood of each site is averaged over the $k$ rate categories, where the rate multiplier is the mean (or median) of each of the discrete $k$ categories. To specify this, we need a deterministic node that is a vector that will hold the set of $k$ rates drawn from the gamma distribution with $k$ rate categories. The fnDiscretizeGamma() function returns this deterministic node and takes three arguments: the shape and rate of the gamma distribution and the number of categories. Since we want to discretize a mean-one gamma distribution, we can pass in alpha for both the shape and rate.Initialize the sr deterministic node vector using the fnDiscretizeGamma() function with 4 bins:sr := fnDiscretizeGamma( alpha, alpha, 4 )Note that here, by convention, we set $k = 4$. The random variable that controls the rate variation is the stochastic node alpha. We will apply a simple scale move to this parameter.moves.append( mvScale(alpha, weight=2.0) )Remember that you need to call the PhyloCTMC constructor to include the new site-rate parameter:seq ~ dnPhyloCTMC(tree=psi, Q=Q, siteRates=sr, type = \"DNA\")Exercise 4      Modify the previous GTR analysis to specify the GTR+Gamma model.Run an MCMC simulation to estimate the posterior distribution.        Is there an impact on the estimated phylogeny compared with theprevious analyses? Look at the MAP tree and the posteriorprobabilities of the clades.        Complete the table of the phylogenetic relationship of primates.  Modeling Invariable SitesAll of the substitution models described so far assume that the sequence data are potentially variable. That is, we assume that the sequence data are random variables; specifically, we assume that they are realizations of the specified PhyloCTMC distribution. However, some sites may not be free to vary—when the substitution rate of a site is zero, it is said to be invariable. Invariable sites are often confused with invariant sites—when each species exhibits the same state, it is said to be invariant. The concepts are related but distinct. If a site is truly invariable, it will necessarily give rise to an invariant site pattern, as such sites will always have a zero substitution rate. However, an invariant site pattern may be achieved via multiple substitutions that happen to end in the same state for every species.Here we describe an extension to our phylogenetic model to accommodate invariable sites. Under the invariable-sites model (Hasegawa et al. 1985), each site is invariable with probability p_inv, and variable with probability $1-$p_inv.First, let’s have a look at the data and see how many invariant sites we have:data.getNumInvariantSites()There seem to be a substantial number of invariant sites.Now let’s specify the invariable-sites model in RevBayes. We need to specify the prior probability that a site is invariable. A Beta distribution is a common choice for parameters representing probabilities.p_inv ~ dnBeta(1,1)The Beta(1,1) distribution is a flat prior distribution that specifies equal probability for all values between 0 and 1.Then, as usual, we add a move to change this stochastic variable; we’ll use a simple sliding window move.moves.append( mvSlide(p_inv) )Finally, you need to call the PhyloCTMC constructor to include thenew p_inv parameter:seq ~ dnPhyloCTMC(tree=psi, Q=Q, siteRates=sr, pInv=p_inv, type=\"DNA\")Exercise 5      Extend the GTR model to account for invariable sites and runan analysis.        What is the estimated probability of invariable sites and how doesit relate to the ratio of invariant sites to the total number ofsites?        Extend the GTR+$\\Gamma$ model to account for invariable sites andrun an analysis.        What is the estimated probability of invariable sites now?        Complete the table of the phylogenetic relationship of primates.  ",
        "url": "/tutorials/ctmc/",
        "index": "true"
      }
      ,
    
      "tutorials-fig-intro": {
        "title": "Introduction to phylogenetic biogeography with the FIG model",
        "content": "IntroductionModels of phylogenetic biogeography are used to estimate how historical processes have shaped the distribution of species in space over time. This page provides an overview for using the FIG model to study phylogenetic biogeography in RevBayes. Briefly, the FIG model builds upon the geographic state-dependent speciation-extinction (GeoSSE) model (Goldberg et al. 2011), a member of the larger SSE model family (Maddison et al. 2007). The FIG model extends the GeoSSE model by allowing regional features to shape biogeographic rates (Landis et al. 2022; Swiston and Landis 2023). The purpose of FIG is to allow biologists to test new hypotheses that involve the timing and location of phylogenetic, biogeographic, and paleogeographic events.TutorialsThe FIG tutorial series targets biologists who are familiar with phylogenetic models and are interested in the following topics in phylogenetic biogeography: phylogenetic and biogeographic model design, ancestral range estimation, testing associations between regional features with biogeographic rates, estimating regional biogeographic rates through time, and biogeographic dating of speciation times.Tutorials in this series are:  Molecular phylogenetics  GeoSSE model  MultiFIG model  TimeFIG model  Biogeographic dating with node age priors  Biogeographic dating with TimeFIGSoftware setupThe FIG tutorials are designed to be run with RevBayes using the TensorPhylo plugin. Developed by Mike May and Xavier Meyer, TensorPhylo is a high-performance library for rapidly computing state-dependent diversification model likelihoods.  Important version info!  Note: Tutorials in this series currently require specific versions of RevBayes and TensorPhylo to run properly (see linked branches and commits).  We recommend that you complete the tutorials using a PhyloDocker container, which is pre-configured with the above versions of RevBayes and TensorPhylo. Instructions to install and use PhyloDocker are here: link.Empirical system: Hawaiian KaduaIsland archipelagos are ideal microcosms for studying biogeographic patterns of dispersal, speciation, and extinction. Among islands, the Hawaiian archipelago holds particular value for biogeographers in part because of its unique paleogeography. Each island in the Hawaiian “chain” is produced through volcanic eruption from a hotspot in the mid pacific and then moves northwest along a tectonic assembly line during which subsidence and erosion cause gradual decay. Thus, the Hawaiian chain acts as a geological time-capsule, with hundreds of progressively older, more eroded islands stretching northwest towards the arctic. The vast majority of Hawaiian biodiversity is concentrated within four larger, younger high island systems of varying age, Kauai (~6.1 Ma), Oahu (~4.1 Ma), Maui Nui(~2.5 Ma), and Hawaii(~1.2 Ma).The age and origin of the many independent radiation of plants, animals, and fungi that have occurred in Hawaii has been a perennial topic of evolutionary studies. One hypothesis that has been difficult to test concerns the age of onset of endemic hawaiian evolutionary radiations. The extreme isolation of the Hawaiian islands makes colonization from distant sources highly improbably, but various biogeographers have hypothesized that the now eroded northwest islands could have provided a landing pad for such lineages that is much older, making dispersal more likely, followed by dispersal to and subsequent radiation in the modern high islands.Hawaiian Kadua (Rubiaceae) is a clade of 29 flowering plant taxa. They inhabit all the major High Island complexes, from Kauai to Hawaii, across a variety of habitat types, including forests, bogs, coastal regions, near waterfalls, and on basaltic remnants of lava flows. Previous work has shown Hawaiian Kadua forms a clade, possibly as the sister lineage to another clade of Kadua found in Polynesian islands in the South Pacific (Rapa Iti, Ua Huku, and Nuku Hiva).Kadua cookiana basking by a waterfall. Photo by Ken Wood (NTBG).Based in initial phylogenetic studies of Kadua, it is likely the Hawaiian Archipelago was colonized once by a progenitor lineage with multiple dispersal events between Hawaiian islands. However, there are no known fossils that could be used to date the age of Hawaiian Kadua lineages, making it difficult to estimate when, where, and how the clade dispersed, speciated, and went extinct as different Hawaiian islands formed, rose, eroded, and subsided.These tutorials will analyze an unpublished Kadua dataset that contains 26 Hawaiian taxa and 3 non-Hawaiian taxa. The geographic dataset includes 8 quantitative and 8 categorical paleogeographical features for 7 regions across 7 timeslices. The biogeographic dataset presence-absence data that records species ranges against the taxon set. In addition, we include a 2-region dataset to illustrate how GeoSSE models behave before moving to the 7-region dataset. Lastly, the molecular dataset includes 10 loci from a larger Angiosperms353 analysis.AcknowledgementsThis tutorial series was written for the 2024 Phylogenetic Biogeography Workshop in St. Louis. The workshop was made possible through an NSF-funded project entitled Modeling the Origin and Evolution of Hawaiian Plants. The botanical and biogeographical context of these tutorials was enriched through ongoing interactions with our project collaborators, Warren Wagner, Nina Rønsted, Bruce Baldwin, and Ken Wood. We also thank Mike May for his indispensable help adapting TensorPhylo for use with TimeFIG analyses for biogeographic dating.",
        "url": "/tutorials/fig_intro/",
        "index": "true"
      }
      ,
    
      "tutorials-sequential-bayes": {
        "title": "Sequential Bayesian inference of phylogeny",
        "content": "OverviewThis tutorial aims to guide you through our sequential (or stepwise) Bayesian inference pipeline using RevBayes (Höhna and Hsiang 2024). The exercises are based on a dataset of North American fireflies (genus Photinus) for which we have molecular sequence data for extant species (Catalan et al. 2022). Our goal is to estimate a time-calibrated phylogeny using a relaxed clock model. The two steps in our sequential Bayesian analysis are first to estimate the posterior distribution of phylogenies with branch lengths, and second to use these samples to infer a time-calibrated phylogeny.In exercises 1 we’ll use the molecular sequence data to infer the relationships among living species.In exercise 2 we transform the branch lengths from units of substitution into units of time assuming a relaxed clock model.The data  Create a directory on your computer for this tutorial.In this directory, create a subdirectory called data, and download the data files that you can find on the left of this page.In the data folder, you should now have seven files.Each file is a fasta formatted alignment of a different protein coding gene.In the tutorial we are only going to use a single gene, COI, but as an exercise you should repeat the analysis with another gene and also with all genes concatenated together as a partitioned analysis.ScriptsFor more complex models and analyses, it’s useful to create separate Rev scripts that contain all the model parameters, moves, and functions for different model components (e.g., the substitution model and the clock model).  Create another subdirectory called scripts.In this tutorial, you will work primarily in your text editor and create a set of modular files that can be easily managed and interchanged. Examples of all the commands used to perform each analysis are also provided at the top of this page under scripts but try to write the complete scripts yourself from the beginning to ensure you understand all the steps involved and the differences between setting up each analysis.Exercises  Click on the first exercise to begin!  Estimating unrooted gene tree(s)  Rooting and time calibrating the gene tree(s)",
        "url": "/tutorials/sequential_bayes/",
        "index": "true"
      }
      ,
    
      "tutorials-multifig": {
        "title": "Feature-informed geographic state-dependent speciation-extinction (FIG) model",
        "content": "OverviewIn the previous examples, we used a GeoSSE model (Goldberg et al. 2011) to investigate the evolution of the Hawaiian Kadua. The GeoSSE model allows us to estimate rates of within-region speciation, extinction, between-region speciation, and dispersal that differ among regions. Biologically, we expect that these different rates are informed by features of the regions where the species are evolving. For example, we might expect that species disperse at a lower rate between more distant islands, or go extinct at a higher rate on smaller islands.The FIG model (Landis et al. 2022) and the Multiple Feature-Informed GeoSSE (MultiFIG) model (Swiston and Landis 2023) attempt to model this expectation. Rather than giving each region its own evolutionary rate parameters, FIG models use functions to link features of those regions to evolutionary rates. This allows us to test hypotheses about the importance of certain environmental features on evolutionary processes. It also has the benefit of reducing the number of parameters that need to be estimated. The number of parameters in the MultiFIG model is constant with respect to the number of regions, so we can investigate systems with more regions without suffering an explosion in the number of model parameters. In this tutorial, we will model the evolution and biogeography of Kadua using seven regions and eight regional features. Later tutorials will explore how to adapt FIG to allow for regional features, and their linked biogeographic rates, to change over time.The MultiFIG modelMuch like the GeoSSE model, MultiFIG uses four core processes: within-region speciation, extinction, between-region speciation, and dispersal. However, instead of assigning each region or region pair its own rate for each process, MultiFIG uses regional feature data and a series of strength parameters and functions to construct rates.Graphical model of MultiFIG. Square nodes represent constant values (data). Circle nodes with solid lines represent stochastic variables (model parameters, and the phylogeny, which is fixed in this analysis). Circle nodes with dotted lines represent deterministic variables (functions). Large rectangles indicate iterative plates.The FIG model incorporates geographical features with two value types as model variables: quantitative features and categorical features. Quantitative features have continuous real values while categorical features have discrete values. MultiFIG also separates data by dimensionality type, incorporating one-dimensional within-region data and two-dimensional between-region data. We use four containers to store this data: $w_c$, $w_q$, $b_c$, and $b_q$.Each regional feature is assigned a “feature effect” parameter that measures the strength and direction of the effect of a particular feature on a particular process. Note that “effect” refers to a mathematical relationship here, but does not indicate causality. These strength parameters are referred to as $\\sigma$ and $\\phi$, representing the effects of categorical and quantitative features respectively. There is one $\\sigma$ or $\\phi$ parameter per feature per process. For example, $\\phi_w^{Altitude}$ would represent the relationship between region altitude and within-region speciation.For each process, the categorical and quantitative feature effects (with feature data modified by strength parameters) are gathered into $c$ and $q$ vectors, then ultimately combined into an $m$ vector. The $m$ vector represents the total effects of all regional features on a particular process, with entries representing each region (or region pair for between-region processes). The $m$ vector represents relative rates among regions, but to obtain absolute rates, the $m$ vector for each process is multiplied by a process-specific base rate parameter $\\rho$. This constructs the $r$ vectors that are analogous to GeoSSE rates: $r_w$ for within-region speciation rates, $r_e$ for extinction rates, and $r_d$ for dispersal rates. Calculating $r_b$ for between-region speciation rates also requires the use of a range split score, as in (Landis et al. 2022).For example, the absolute rates for within-region speciation at region $i$ equals\\[r_w(i) = \\rho_w \\times m_w(i)\\]where $\\rho_w$ is the base rate and $m_w(i)$, the relative rate factor for this process at region $i$. The relative rate factor is defined as\\[m_w(i) = \\underbrace{ \\prod_{k} q^{k}_w(i) }_{\\text{quantitative effects}} \\times \\underbrace{\\prod_{\\ell} c^{\\ell}_w(i)}_{\\text{categorical effects}}\\]where you’ll notice that $m_w(i)$ equals the product of large number of terms. Each term is either the relative rate contribution from a quantitative feature from layer $k$ for region $i$, as represented by $q^{k}_w(i)$, or a categorical feature from layer $\\ell$ for region $i$, as represented by $\\sigma_w^{\\ell}$. These terms can be broken down into even simpler parts. Each quantitative effect variable is parameterized as\\[q^{k}_w(i) = \\text{exp} \\left\\{ \\phi_w^{k} \\times w_q^{k}(i) \\right\\}\\]where $\\phi_w^{k}$ is the quantitative feature effect parameter controlling within-region speciation (subscript $w$) for feature layer $k$. Notice that if the feature effect parameter equals zero, then $q^{k}_w(i) = e^0 = 1$, which means that layer has no influence on the rate for the process.Similarly, each categorical effect variable is parameterized as\\[c^{\\ell}_w(i) = \\text{exp} \\left\\{ \\sigma_w^{\\ell} \\times w_c^{\\ell}(i) \\right\\}\\]and behaves similarly to the quantitative effect variable. (In this example, we assume categorical variables take values 0 or 1 and treat them numerically to simplify notation. More complex relationships are written with more complex notation.)Each individual effect variable can be $&lt;1$, $&gt;1$, or $=1$, and so can the product of all effect variables. All relative rates are eventually rescaled by the same base rate (e.g. $\\rho_w$). If the relative rates for regions $i$ and $j$ have the relationship $m_w(i) &gt; m_w(j)$ then the absolute rates also follow $r_w(i) &gt; r_w(j)$. Other $m$ functions behave in a similar manner. More details on the design of the $m$ functions are provided in (Swiston and Landis 2023).In this analysis, we are examining eight regional features. The first 4 are quantitative: maximum altitude (m), log maximum altitude (m), distance (km), and log distance (km). We include the log features because they will allow us to better understand the shape of the relationship between features and processes. For example, it may be that intermediate values of a particular feature are related to the highest rates of a particular process, so we would expect the feature strength parameter to be positive and the log-feature strength parameter to be negative. The other 4 features are categorical: age class (old/young), growth class (decay/growth), dispersal class (short/long), and relative age class (older/younger).Examples of a within-region quantitative feature (max. altitude), a between-region quantitative feature (distance), and a categorical quantitative feature (relative island age). These features (and others) may shape core biogeographic rates depending on which regions are involved in an event.Because each within-region feature acts on 2 processes and each between-region feature acts on 2 processes, this creates a total of 16 parameters. Adding one $\\rho$ parameter for each process results in a total of 20 model parameters to be estimated. We will use a time-calibrated phylogeny and present-day ranges for Kadua to estimate these parameters, and use those estimates to determine which regional features are most strongly related to particular processes.The 8 regional features investigated in this analysis and the 16 associated parameters relating these features to core biogeographic processes.Setup  Important version info!  Note: This tutorial currently requires specific versions of RevBayes and TensorPhylo to run properly (see linked branches and commits).  We recommend that you complete the tutorial using a PhyloDocker container, which is pre-configured with the above versions of RevBayes and TensorPhylo. Instructions to install and use PhyloDocker are here: link.Running a MultiFIG analysis in RevBayes requires several important data files, including a file representing the time-calibrated phylogeny and a biogeographic data matrix describing the ranges for each species. kadua.tre is a time-calibrated phylogeny of Kadua. kadua_range_n7.nex assigns ranges to each species for a seven-region system: G (Gardner), N (Necker), K (Kauaii), O (Oahu), M (Maui Nui Complex), H (Hawaii), and Z (mainland). For each species (row) and region (column), the file reports if the species is present (1) or absent (0) in that region. There are also feature files that contain regional feature data, and a feature_summary.csv file that describes all the regional feature files (where they are found and what kind of data they contain).If you prefer to run a single script instead of entering each command manually, the RevBayes script called multifig.Rev contains all of the commands that are used in the tutorial. The data and script can be found in the Data files and scripts box in the left sidebar of the tutorial page. Somewhere on your computer, you should create a directory (folder) for this tutorial. This is the main directory for the tutorial, and you will run all of your commands from here. Inside the tutorial directory, you should create a scripts directory. This is the directory where you will put the multifig.Rev script. Then, you should create a data directory inside the tutorial directory. Within data, create two more directories: hawaii, and kadua. The data files related to Kadua (kadua.tre and kadua_range_n7.nex) will go in the kadua directory. The data related to Hawaii, including the feature_summary.csv file, the feature_description.csv file, and all feature-related *_feature*.csv files will go in the hawaii directory. However, you can always modify the filepaths to locate the data wherever you choose to download it.MultiFIG in RevBayesGetting startedAfter starting up RevBayes from within your main tutorial directory, you can load the TensorPhylo plugin. You will need to know where you downloaded the plugin. For example, if you cloned the TensorPhylo directory into your home directory at ~/tensorphylo, you would use the following command to load the plugin:loadPlugin(\"TensorPhylo\", \"~/tensorphylo/build/installer/lib\")Note that if you’re using the PhyloDocker image, then the Tensorphylo plugin is installed in /.plugins, where RevBayes is able to find it without including a filepath:loadPlugin(\"TensorPhylo\")Next, we want to tell RevBayes where to find our data (and where to save our output later). If you have set up your tutorial directory in a different way than suggested, you will need to modify the filepaths.# filesystemanalysis      = \"multifig\"dat_fp        = \"./data/kadua/\"phy_fn        = dat_fp + \"kadua.tre\"bg_fn         = dat_fp + \"kadua_range_n7.nex\"label_fn      = dat_fp + \"kadua_range_label.csv\"geo_fp        = \"./data/hawaii/\"feature_fn    = geo_fp + \"feature_summary.csv\"out_fn        = \"./output/\" + analysisEventually, we will be running an MCMC analysis at the end of the tutorial. This means that any time we create a new model parameter, we will want RevBayes to update that parameter in MCMC, so we need to add a ‘move’ for it. After we have set up our model, we will add some ‘monitors’ so RevBayes records the MCMC output in certain locations. Therefore, we will set up containers moves and monitors at the beginning of the script. We can also choose some MCMC settings for later: the number of computer processors to use, the number of generations we want to run the analysis for, and how often we want RevBayes to record output.# MCMC variablesnum_proc  = 6num_gen   = 500             # set num_gen = 5000 for full analysisprint_gen = 20moves     = VectorMoves()monitors  = VectorMonitors()DataNext, we will read in the data. Let’s start with the phylogenetic tree.# tree inputphy &lt;- readTrees(phy_fn)[1]In order to set up our analysis, we will want to know some information about this tree: the taxa, the number of taxa, and the number of branches.taxa         = phy.taxa()num_taxa     = taxa.size()num_branches = 2 * num_taxa - 2We also want to read in the range data. Much like in the GeoSSE example, this range data is in a binary format, with rows representing each species and columns representing regions. A 1 is used to indicate that a species is present in a particular region, and a 0 indicates that it is not present in that region.dat_01 = readDiscreteCharacterData(bg_fn)We want to get some information about this range data: how many regions there are, and how many ranges can be constructed from these regions. But before we can calculate the number of ranges, we have to decide what our maximum range size will be – that is, the maximum number of regions a species is allowed to exist in at the same time. In this example, we will set the maximum range size to 4. This has the potential to speed up the analysis dramatically in systems with many regions because it decreases the total state space (the number of ranges in the model).num_regions    = dat_01.nchar()max_range_size = 4num_ranges     = 0for (k in 1:max_range_size) {    num_ranges += choose(num_regions, k)}We want to format the range data to be used in a GeoSSE-type analysis. This will take the binary range data and output integer states. We can look at what binary ranges correspond to which integer states by printing out the state descriptions. Actually, we can save this information to a file for easy access later!dat_nn         = formatDiscreteCharacterData(dat_01, format=\"GeoSSE\", numStates=num_ranges)desc           = dat_nn.getStateDescriptions()write(\"index,range\\n\", filename=label_fn)for (i in 1:desc.size()) {    write((i-1) + \",\" + desc[i] + \"\\n\", filename=label_fn, append=true)}We also want to get our feature data. Using the RevBayes function readRegionalFeatures, we can look at the feature_summary.csv file and automatically look for feature data. The feature_summary.csv file is specially formated to be read by RevBayes, consisting of 5 columns. The first column is time_index. More advanced analyses, like TimeFIG, may involve time-heterogenous region features, in which case we would need to index our feature data by time slices. However, in this analysis, we are only using present-day data, so all of our features will have a time_index of 1. The second column is feature_index. Each feature type (within-region categorical, within-region quantitative, between-region categorical, and between-region quantitative) has a container that can contain several features, so we want to index the features within those containers. For example, in this analysis, each container has two features, indexed 1 and 2. Keep in mind that Feature 1 in one container does not have to be related to Feature 1 in another container. It’s important to keep track of these indices so you know which output corresponds to which feature. For this analysis, we’ve listed the features in order on the feature table. The third column is feature_relationship. This column is for indicating whether the feature is a within-region feature or a between-region feature, with options ‘within’ or ‘between’. The fourth column is feature_type, for indicating whether the feature is quantitative of categorical. Finally, the fifth column is feature_path, which gives a filepath for the actual file containing the data for that feature.geo_features &lt;- readRegionalFeatures(feature_fn, delimiter=\",\", nonexistent_region_token=\"nan\")Next, we transform the feature data into feature layers, a RevBayes object that we will use later for informing our biogeographic rates. First, we normalize the features (important for scaling reasons). Then we pull each feature type out of our geo_features object using .get(). The 1 in this function is for time slice 1, because we are only using one time slice in this analysis (features do not change over time). Finally, we create the layers.# normalize/re-center values, then provide methods to get feature-sets# for CW/QW/CB/QB associated with D/E/W/Bgeo_features.normalize(\"within\")geo_features.normalize(\"between\")# get feature-sets for each measurement-type, process-type, and timeslicefeature_CW &lt;- geo_features.get(\"within\",\"categorical\",1)feature_QW &lt;- geo_features.get(\"within\",\"quantitative\",1)feature_CB &lt;- geo_features.get(\"between\",\"categorical\",1)feature_QB &lt;- geo_features.get(\"between\",\"quantitative\",1)for (i in 1:feature_CW.size()) {layer_CW[i] &lt;- feature_CW[i].get()}for (i in 1:feature_QW.size()) {layer_QW[i] &lt;- feature_QW[i].get()}for (i in 1:feature_CB.size()) {layer_CB[i] &lt;- feature_CB[i].get()}for (i in 1:feature_QB.size()) {layer_QB[i] &lt;- feature_QB[i].get()}Model setupIn the MultiFIG model, there are four processes: within-region speciation, extinction, between-region speciation, and dispersal. Rates per region or region pair are calculated using feature data, feature effect parameters, and base rate parameters. First, we will set priors for the feature effect parameters. Then we will use the RevBayes function fnFeatureInformedRates to combine the feature data and feature effect parameters to create $m$ vectors/matrices, representing relative rates of a particular process per region or region pair. Finally, we will multiply the $m$ for each process by base rate parameters to get model rates $r_w$, $r_e$, $r_b$, and $r_d$.Let’s start by creating distributions that we will use for all $\\phi$ and $\\sigma$ parameters. We will use reversible jump distributions here. These distributions are used for MCMC analyses that use reversible jump moves. In a reversible jump MCMC move, we can draw proposals from a traditional distribution (like the uniform distributions used here), but we can also propose that a certain parameter is equal to a particular fixed value (in this case, rj_null_value=0). Setting a feature effect to 0 essentially lets our MCMC turn certain feature effects ‘off’ entirely. This allows us to do model selection based on the results of the MCMC analysis – it will actually tell us which combination of features is the best model! We also have to assign an rj_prob to the reversible jump distribution, which is the prior probability of RJMCMC using the fixed value instead of the continuous distribution.The continuous distributions we will use in this analysis are various uniform distributions with different bounds. If we have strong reasoning about how a particular feature might affect a process, we can apply an informed prior to that feature effect parameter. For example, if we believe that distance should always have a negative effect on dispersal, we can apply the rj_base_neg_dist to the parameter that associates distance with dispersal, telling the analysis that the parameter should either be ‘off’ or some negative value.# set up priors for feature effectsrj_null_value &lt;- 0.0          # fixed \"off-value\" for RJMCMCrj_prob       &lt;- 0.5          # prob. of RJMCMC taking \"off-value\"# prior of \"on-value\" for RJMCMCbound &lt;- 2rj_base_sym_dist = dnUniform(-bound, bound)rj_base_neg_dist = dnUniform(-bound, 0)     # negative only (e.g. distance on dispersal)rj_base_pos_dist = dnUniform(0, bound)      # positive only (e.g. distance on betw.-reg. speciation)rj_sym_dist = dnRJMixture(rj_null_value, rj_base_sym_dist, p=rj_prob)rj_neg_dist = dnRJMixture(rj_null_value, rj_base_neg_dist, p=rj_prob)rj_pos_dist = dnRJMixture(rj_null_value, rj_base_pos_dist, p=rj_prob)We will start by assigning symmetric distributions to all of the parameters, and then change a selection of parameters for which we have strong reasoning.# categorical feature effectsfor (i in 1:feature_CW.size()) sigma_w[i] ~ rj_sym_distfor (i in 1:feature_CW.size()) sigma_e[i] ~ rj_sym_distfor (i in 1:feature_CB.size()) sigma_d[i] ~ rj_sym_distfor (i in 1:feature_CB.size()) sigma_b[i] ~ rj_sym_dist# quantitative feature effectsfor (i in 1:feature_QW.size()) phi_w[i] ~ rj_sym_distfor (i in 1:feature_QW.size()) phi_e[i] ~ rj_sym_distfor (i in 1:feature_QB.size()) phi_d[i] ~ rj_sym_distfor (i in 1:feature_QB.size()) phi_b[i] ~ rj_sym_dist# force signed relationships between region features and rates# (overrides existing distribution assignments)phi_b[1]   ~ rj_pos_dist   # Distance (km) results in faster speciationphi_b[2]   ~ rj_pos_dist   # Log-distance (km) results in faster speciationsigma_b[1] ~ rj_pos_dist   # LDD (1) results in faster speciationsigma_w[1] ~ rj_pos_dist   # High Islands (1) drives faster speciation phi_d[1]   ~ rj_neg_dist   # Distance (km) results in slower dispersalphi_d[2]   ~ rj_neg_dist   # Log-distance (km) results in slower dispersalsigma_d[1] ~ rj_neg_dist   # LDD (1) results in slower dispersalsigma_e[1] ~ rj_neg_dist   # High Islands (1) drives slower extinctionNow we can create the relative rates of each process. These $m$ containers hold the per-region or per-region-pair relative rates. We will turn these into actual rates (incorporating a base rate parameter) later. Each of these relative rate containers also has a null_rate argument, which tells RevBayes what to do with missing regions. This is not important for an analysis that only uses present-day information (like this one), but in time-heterogeneous analyses, it is possible that some regions did not exist during some times. For instance, the Hawaiian archipelago formed over time, and not all islands always existed.m_w := fnFeatureInformedRates(layer_CW, layer_QW, sigma_w, phi_w, null_rate=0)m_e := fnFeatureInformedRates(layer_CW, layer_QW, sigma_e, phi_e, null_rate=1e3)m_d := fnFeatureInformedRates(layer_CB, layer_QB, sigma_d, phi_d, null_rate=0)m_b := fnFeatureInformedRates(layer_CB, layer_QB, sigma_b, phi_b, null_rate=1)Because we are going to do an MCMC analysis later in the tutorial, we want MCMC to update all of the $\\sigma$ and $\\phi$ parameters. Each move is assigned a ‘weight’ that tells RevBayes how often to do this move. We will assign a different list of moves depending on if we are using reversible jump (adding mvRJSwitch moves) or ignoring features (adding no moves on feature effects and fixing their values to 0). We may also want to initialize the MCMC to reasonable values for these feature effect parameters. We will set the values of our distributions to be (temporarily) equal to those initial values to start the MCMC.First, we will address the categorical feature effects for each process (w, e, d, and b). These are our $\\sigma$ parameters. The logic is the same for each process. First, we find the container of features which impact that process (within-region features for within-region speciation and extinction, between-region features for between-region speciation and dispersal). Then we loop over the different features inside that container. For each feature, we initialize the value of the parameter, and add appropriate moves for the MCMC. We also include a use_ line that allows us to turn off certain features if we want to perform analyses without them.# initialize categorical feature effects, create moves, add monitor variablesfor (i in 1:feature_CW.size()) {    sigma_w[i].setValue(0)    moves.append( mvScale(sigma_w[i], weight=2) )    moves.append( mvSlide(sigma_w[i], weight=2) )    moves.append( mvRJSwitch(sigma_w[i], weight=3) )    use_sigma_w[i] := ifelse(sigma_w[i] == 0.0, 0, 1)}for (i in 1:feature_CW.size()) {    sigma_e[i].setValue(0)    moves.append( mvScale(sigma_e[i], weight=2) )    moves.append( mvSlide(sigma_e[i], weight=2) )    moves.append( mvRJSwitch(sigma_e[i], weight=3) )    use_sigma_e[i] := ifelse(sigma_e[i] == 0.0, 0, 1)}for (i in 1:feature_CB.size()) {    sigma_d[i].setValue(0)    moves.append( mvScale(sigma_d[i], weight=2) )    moves.append( mvSlide(sigma_d[i], weight=2) )    moves.append( mvRJSwitch(sigma_d[i], weight=3) )    use_sigma_d[i] := ifelse(sigma_d[i] == 0.0, 0, 1)}for (i in 1:feature_CB.size()) {    sigma_b[i].setValue(0)    moves.append( mvScale(sigma_b[i], weight=2) )    moves.append( mvSlide(sigma_b[i], weight=2) )    moves.append( mvRJSwitch(sigma_b[i], weight=3) )    use_sigma_b[i] := ifelse(sigma_b[i] == 0.0, 0, 1)}Similarly, we will address the quantitative features for each process. These are our \\phi parameters.# initialize quantitative feature effects, create moves, add monitor variablesfor (i in 1:feature_QW.size()) {    phi_w[i].setValue(0)    moves.append( mvScale(phi_w[i], weight=2) )    moves.append( mvSlide(phi_w[i], weight=2) )    moves.append( mvRJSwitch(phi_w[i], weight=3) )    use_phi_w[i] := ifelse(phi_w[i] == 0.0, 0, 1)}for (i in 1:feature_QW.size()) {    phi_e[i].setValue(0)    moves.append( mvScale(phi_e[i], weight=2) )    moves.append( mvSlide(phi_e[i], weight=2) )    moves.append( mvRJSwitch(phi_e[i], weight=3) )    use_phi_e[i] := ifelse(phi_e[i] == 0.0, 0, 1)}for (i in 1:feature_QB.size()) {    phi_d[i].setValue(0)    moves.append( mvScale(phi_d[i], weight=2) )    moves.append( mvSlide(phi_d[i], weight=2) )    moves.append( mvRJSwitch(phi_d[i], weight=3) )    use_phi_d[i] := ifelse(phi_d[i] == 0.0, 0, 1)}for (i in 1:feature_QB.size()) {    phi_b[i].setValue(0)    moves.append( mvScale(phi_b[i], weight=2) )    moves.append( mvSlide(phi_b[i], weight=2) )    moves.append( mvRJSwitch(phi_b[i], weight=3) )    use_phi_b[i] := ifelse(phi_b[i] == 0.0, 0, 1)}Now we will set up our rates for the four core processes, and put together our tree object. First, we will assign distributions to our base process rates, $\\rho$. These rates are shared amongst all regions, and are combined with relative rates $m$ to get true process rates in each region or pair, $r$. We will use exponential distributions with rate 30 for each base rate parameter. Once again, we will initialize these values so MCMC will start in a reasonable place, and append the appropriate moves. We can also calculate the total speciation rate from the base rates of each type of speciation event.# base rate parametersrho_d ~ dnExp(40)rho_e ~ dnExp(40)rho_w ~ dnExp(40)rho_b ~ dnExp(40)rho_d.setValue(0.1)rho_e.setValue(0.1)rho_w.setValue(0.1)rho_b.setValue(0.1)moves.append( mvScale(rho_d, weight=5) )moves.append( mvScale(rho_e, weight=5) )moves.append( mvScale(rho_w, weight=5) )moves.append( mvScale(rho_b, weight=5) )# summarize base ratesspeciation_rates := [ rho_w, rho_b ]total_speciation := sum( speciation_rates )Next, we will construct the total rates for each process. We will set up dispersal rates first. Because relative dispersal rates are stored in a two-dimensional matrix, we will loop over the rows in that matrix and multiply each row by the base rate.for (i in 1:num_regions) {r_d[i] := rho_d * m_d[i]}Extinction rates are set up similarly. In the MultiFIG model, lineage-level extincion events occur when a species becomes extirpated from its last region, rendering it globally extinct. Therefore, extinction occurs only through extirpation, and we will use extirpation rates to create our anagenetic rate matrix.# extirpation rate (region loss)r_e := rho_e * m_e[1]From these rates, we can use a RevBayes function to construct the anagenetic rate matrix, which gives rates of anagenetic processes. The setting maxRangeSize may be used to reduce the number of range patterns in the model by not allowing very widespread ranges (with more regions than the maximum range size). This is particularly useful when the number of regions is large. In this analysis, we restrict the maximum range size to 4 (we did this earlier in the tutorial).# dispersal-extirpation rate matrix# - states are discrete ranges# - elements are rates of range expansion/contractionQ_bg := fnBiogeographyRateMatrix(dispersalRates=r_d,                                 extirpationRates=r_e,                                 maxRangeSize=max_range_size)We also construct a cladogenetic event matrix, describing the absolute rates of different cladogenetic events. From this matrix, we can obtain the total speciation rates per state, as well as a cladogenetic probability matrix.# speciation rate matrixclado_map := fnBiogeographyCladoEventsBD(speciation_rates=speciation_rates,                                         within_region_features=m_w[1],                                         between_region_features=m_b,                                         max_range_size=max_range_size,                                         normalize_split_score=false)# speciation rates for each rangelambda := clado_map.getSpeciationRateSumPerState()# probabilities of speciation outcomes for each rangeomega := clado_map.getCladogeneticProbabilityMatrix()Because we want to monitor the absolute rates of different speciation types, we will construct $r$ rates just like we did for dispersal and extinction.# monitor variables for absolute speciation ratesr_w := rho_w * m_w[1]# NOTE: this rate only represents species with range size 2#       i.e., the inverse sum of inverse edge weights#       (relative rates in m_b[i][j]) is equal to the edge weight#       of a 2-region rangefor (i in 1:num_regions) {    r_b[i] := rho_b * m_b[i]}We may also want to monitor the absolute extinction rates. Because only lineages with a range of size 1 can go extinct, we will assign larger ranges an absolute extinction rate of 0.# extinction rates (lineage death)for (i in 1:num_ranges) {    if (i &lt;= num_regions) {        # species with range-size 1 can go extinct        mu[i] := r_e[i]    } else {        # widespread species cannot        mu[i] &lt;- abs(0)    }}Next, we need to assign a probability distribution to range of the most recent common ancestor of all species, prior to the first speciation event. This will be a distribution (simplex) of possible range states that the ancestor might have had. In this analysis, we assume all ranges were equally probable for the ancestor.# base frequenciespi_bg_base &lt;- rep(1, num_ranges)pi_bg &lt;- simplex(pi_bg_base)We also need to set up the tip sampling probabilities based on state. In this analysis, the Hawaiian (ingroup) Kadua have been thoroughly sampled. However, we have only included 3 mainland (outgroup) samples, so we have to account for the low sampling here. Also, we will assign rho_times the value of 0, because we only sampled at the present (age =n_total           &lt;- 29 + 2 + 1n_total_ingroup   &lt;- 22 + 2n_total_outgroup  &lt;- n_total - n_total_ingroupn_sample_ingroup  &lt;- 24n_sample_outgroup &lt;- 3rho_ingroup       &lt;- Probability(n_sample_ingroup/n_total_ingroup) rho_outgroup      &lt;- Probability(n_sample_outgroup/n_total_outgroup)rho_poorly_sampled_ranges &lt;- [ 7 ]for (i in 1:num_ranges) {    rho_sample[1][i] &lt;- rho_ingroup}for (i in rho_poorly_sampled_ranges) {    rho_sample[1][i] &lt;- rho_outgroup}rho_times &lt;- [ 0.0 ]Before getting to the tree object, we want to make the root age of the tree object equal to the height of the input phylogeny. When we run future analyses that do not use a fixed tree, we can actually estimate this instead.# fixed root ageroot_age &lt;- phy.rootAge()With all of the rates constructed, we can create a stochastic variable drawn from this MultiFIG model with state-dependent birth, death, and speciation processes. This establishes how the various processes interact to generate a tree with a topology, divergence times, and terminal taxon states (ranges).# use Time/Multi FIG setuptimetree ~ dnGLHBDSP( rootAge      = root_age,                      lambda       = lambda,                      mu           = mu,                      eta          = Q_bg,                      omega        = omega,                      rhoTimes     = rho_times,                      pi           = pi_bg,                      rho          = rho_sample,                      condition    = \"time\",                      taxa         = taxa,                      nStates      = num_ranges,                      nProc        = num_proc)Then we can clamp the variable with the fixed tree and present-day range states, allowing us to infer model parameters based on our observed data.timetree.clamp(phy)timetree.clampCharData(dat_nn)MCMCFor this analysis, we will perform an MCMC of 10000 generations. This may seem like a low number of generations (compared to other programs), but this is because RevBayes performs multiple moves per iteration under the random move scheduler (a setting from the start of the tutorial). You can alter this MCMC by changing the number of iterations, the move schedule, or how frequently the MCMC prints output. You can even add a period of burnin that tunes hyperparameters for moves. We have already created all of our moves for this MCMC, so we can move on to monitors!Monitors are instructions for RevBayes to record MCMC output. Since we want RevBayes to record every iteration, we have set the printgen argument to 1 (one of those settings from the beginning of the tutorial). We want it to print some output to the screen so we can see how it is running (mnScreen). We also want it to save model parameters to a file (mnModel and both mnFile). Finally, if we want to use the output for ancestral state reconstruction, we want to save states (mnJointConditionalAncestralStates) and stochastic mappings (mnStochasticCharacterMap). All of the output files will be saved in the output directory so that it can be accessed later.# screen monitor, so you don't get boredmonitors.append( mnScreen(rho_d, rho_e, rho_w, rho_b, printgen=print_gen) )# file monitor for all simple model variablesmonitors.append( mnModel(printgen=print_gen, file=out_fn+\".model.txt\") )# file monitor for treemonitors.append( mnFile(timetree, printgen=print_gen, file=out_fn + \".tre\") )# monitor ancestral ranges at internal nodesmonitors.append( mnJointConditionalAncestralState(    tree=timetree, glhbdsp=timetree, printgen=print_gen,    filename=out_fn+\".states.txt\",    withTips=true, withStartStates=true, type=\"NaturalNumbers\") )# file monitor for biogeographic ratesbg_mon_fn = out_fn + \".bg.txt\"monitors.append( mnFile( filename = bg_mon_fn, printgen=print_gen,                         rho_e, rho_w, rho_d, rho_b,                         r_e, r_w,                         r_d[1], r_d[2], r_d[3], r_d[4],                         r_d[5], r_d[6], r_d[7],                         r_b[1], r_b[2], r_b[3], r_b[4],                         r_b[5], r_b[6], r_b[7],                         m_e[1], m_w[1],                         m_d[1], m_d[2], m_d[3], m_d[4],                         m_d[5], m_d[6], m_d[7],                         m_b[1], m_b[2], m_b[3], m_b[4],                         m_b[5], m_b[6], m_b[7] ) )# monitor stochastic mappings along branches of tree# NOTE: can cause performance issues, comment out if necessary# monitors.append( mnStochasticCharacterMap(#    glhbdsp=timetree, printgen=print_gen*10,#    filename=out_fn+\".stoch.txt\",#    use_simmap_default=false) )Then we can start up the MCMC. It doesn’t matter which model parameter you use to initialize the model, so we will use the timetree. RevBayes will find all the other parameters that are connected to the timetree and include them in the model as well. Then we create an MCMC object with the moves, monitors, and model. Finally, we can run that MCMC! Note, we suggest that you set moveschedule=\"single\" for your first time running this code. Full analyses should instead use moveschedule=\"random\".# create model objectmymodel = model(timetree)# create MCMC objectmymcmc = mcmc(mymodel, moves, monitors, moveschedule=\"single\")     # set moveschedule=\"random\" for full analysis# run MCMCmymcmc.run(num_gen)After the MCMC analysis has concluded, we can summarize the ancestral states we obtained, creating an ancestral state tree. This tree will be written to the file ase.tre . It may take a little while.f_burn = 0.2x_stoch = readAncestralStateTrace(file=\"output/\" + analysis + \".stoch.txt\")x_states = readAncestralStateTrace(file=\"output/\" + analysis + \".states.txt\")summarizeCharacterMaps(x_stoch,timetree,file=\"output/\" + analysis + \".events.txt\",burnin=f_burn)state_tree = ancestralStateTree(    tree=timetree,    ancestral_state_trace_vector=x_states,    include_start_states=true,    file=\"output/\" + analysis + \".ase.tre\",    summary_statistic=\"MAP\",    reconstruction=\"marginal\",    burnin=f_burn,    nStates=num_ranges,    site=1)writeNexus(state_tree,filename=\"output/\" + analysis + \".ase.tre\")Output  Example output  Note: Complete FIG analyses can take several hours to run. To exploreFIG analysis output as part of a workshop, we recommend that youdownload precomputed “example output” from the top left menu on thispage. Save these files into your local output directory and view resultsand/or run the following plotting code.Example output files are provided with this tutorial (see panel on top left). This section shows how generate plots for FIG analysis results using the FIG Tools repository, which primarily uses R, RevGadgets, ggplot, and igraph for visualization.NOTE: Your output may look slightly different than the output shown below. If you want to exactly replicate the results of the tutorial, you must set a seed at the beginning of the kadua_geosse.Rev script by adding the RevBayes command seed(1).To proceed, we’ll exit RevBayes and work from the command line prompt in shell. We assume that ./multifig is a subdirectory from your current location. To generate the images below, first save a copy of FIG tools to your filesystem:# Download .zip file (open in browser our save in command line)wget https://github.com/hawaiian-plant-biogeography/fig_tools/archive/refs/heads/main.zip# Unzip file as \"fig_tools-main\"unzip main.zip# Rename directorymv fig_tools-main fig_toolsNext, copy the files in ./fig_tools/scripts into your MultiFIG project directory as ./multifig/plot:# copycp -R ./fig_tools/scripts ./multifig/plotThese scripts assume you are in the base of your analysis directory:cd ~/multifigNow we can generate plots using FIG tools. First, we generate a tree with ancestral range estimates using these commands:# prepare tree and state output for plottingrb --args ./output/multifig.tre ./output/multifig.states.txt --file ./plot/make_tree.Rev# make ancestral tree plotRscript ./plot/plot_states_tree.R ./output/out.states.tre ./output/out.mcc.tre ./data/kadua/kadua_range_label.csv GNKOMHZHere, we show an ancestral tree built using RevGadgets, with a few other stylistic changes.Ancestral state reconstruction of Kadua.Next, we generate figures for the marginal posterior densities associated with the core GeoSSE processes:# make model posterior plotsRscript ./plot/plot_model_posterior.R ./output/multifig.model.txt ./data/hawaii/feature_summary.csv ./data/hawaii/feature_description.csvPosterior estimates for parameters related to the dispersal process.From top to bottom, the first figure shows the base dispersal rate, $\\rho_d$, that would apply if all regions were completely identical. The second figure shows quantitative feature effect parameters, $\\phi^{(k)}_d$, wherein the parameter for Distance is negative, meaning dispersal rates decrease with distance. The third figure shows that models using distance as an explanatory factor to shape dispersal rates have higher reversible jump probabilities, whereas including or excluding log-distance has no major impact on model fit. The fourth figure shows categorical feature effect parameters, $\\sigma^{(k)}_d$, for which dispersal into younger regions tend to have higher dispersal rates (positive) and dispersal into or out of the Hawaiian islands is penalized (negative). In the last figure, models with or without these categorical features tend to have similar fit, though models that favor dispersal into younger islands are roughly three times as probable as those that do not.Lastly, we generate figures for region-specific biogeographic rates. Note, this code is intended for the time-heterogeneous FIG models that are introduced in the next tutorial. This tutorial can be thought as a special case of the time-heterogeneous FIG model where time is constant. Because of this, the current scripts require we that we pass a “dummy” age summary file showing to generate the figure. The code for this is:# make region rate plotsRscript ./plot/plot_rates_vs_time_grid.R ./output/multifig ./data/hawaii/feature_summary.csv ./data/hawaii/age_summary.csv ./data/hawaii/feature_description.csv GNKOMHZMean posterior estimates for dispersal rates, $r_d(i,j)$.A plot of regional dispersal rates shows how geographic distance and youngwards dispersal shape variation in $r_d(i,j)$ ().Mean posterior estimates for within-region speciation rates, $r_w(i)$.Within-region specation rates, $r_w(i)$, are estimated as highest in the middle-aged islands, Kauai and Oahu ().Lastly, this script will plot a network that summarizes relationships between regional features, feature effect parameters, and core biogeographic processes:# make feature vs. rate network plotRscript ./plot/plot_feature_rate_network.R ./output/multifig.model.txt ./data/hawaii/feature_description.csvNetwork diagram displaying the relationships between regional features (gold), feature effect parameters ($\\phi$ and $\\sigma$ in green), and rate modifier functions ($m$ in cyan). Edges colors indicate positive (blue) versus negative (red) relationships and widths indicate weak (thin) versus (strong) interactions.",
        "url": "/tutorials/multifig/",
        "index": "true"
      }
      ,
    
      "tutorials-docker": {
        "title": "Using RevBayes with Docker",
        "content": "OverviewDocker is a way to share pre-built, pre-configured, and platform-independent sets of programs and files. The PhyloDocker image described below comes pre-configured with RevBayes, the TensorPhylo library, and other useful software.As a user, you download and a build a Docker image. Once built, you can create a Docker container from that image. Each Docker image lives on your computer and uses its resources for work. In this way, you can think of Docker as a program that runs a miniature “virtual” computer through your actual computer. A container built from PhyloDocker can be used to run RevBayes, just like you would through the terminal, but without needing to install and configure RevBayes yourself. For example:~ $ ### from terminal on my Mac laptop (host)~ $ docker run --name phylodocker_demo --volume /Users/mlandis/projects/docker_test:/docker_test -it sswiston/phylo_docker:slim_amd64~ # ### from terminal inside Docker container~ # cd docker_test//docker_test # lshistory.txt  test.Rev/docker_test # rb test.RevRevBayes version (1.2.2)Build from remotes/origin/stochmap_tp_dirty_merge (rapture-2396-g55c817) on Mon May 20 16:11:27 UTC 2024Visit the website www.RevBayes.com for more information about RevBayes.RevBayes is free software released under the GPL license, version 3. Type 'license()' for details.To quit RevBayes type 'quit()' or 'q()'.&gt; source(\"test.Rev\")   Processing file \"test.Rev\"Congrats! RevBayes is working!/docker_test # ### new file in container will be visible to laptop/docker_test # touch new_file.txt/docker_test # lsnew_file.txt  test.Rev/docker_test # exit~ $ ### back on my laptop (host)~ $ ls /Users/mlandis/projects/docker_testnew_file.txt  test.Rev~ $ ### reconnect to Docker container~ $ docker start -ai phylodocker_demo~ # On the development side, a Dockerfile (a list of commands for building and configuring programs/files) is used to build an image, which can then be shared with others. On the user side, the image can be downloaded and run to create a container. Inside the container, you have access to all of the programs contained in the image, as well as any files you mount to the container when you run the image. It works a bit like a virtual machine, except it runs on top of the host operating system. A Docker image containing RevBayes, TensorPhylo, Python, R, Julia, and several other programs can be found on Docker Hub.This tutorial explains how to install Docker on your computer, how to obtain a RevBayes Docker image, and how to use the image to run scripts on your computer or a computing cluster. This will require a basic familiarity with command line (navigating directories and entering basic commands), but does not require much programming.Setting up the tutorialDownloading a test scriptThis tutorial includes a test script called test.Rev so that you can assess whether the RevBayes Docker container is functioning properly after being installed. The test script can be found in the Data files and scripts box in the left sidebar of the tutorial page. Somewhere on your computer, you should create a directory (folder) for this tutorial, download the test script, and put the test script inside the directory. You can put this directory anywhere you want on your computer, but you will need to know the filepath to the directory in order to access it from the Docker container.Setting up DockerJoining Docker HubOn Docker Hub, there are many Docker images available that contain a wide variety of programs. To use these images (including the RevBayes image), you will need to create an account. Docker Hub is free to join.Installing Docker DesktopIn order to use Docker on your computer, you will have to download and install Docker Desktop. The desktop app allows your computer to run Docker images, and also provides a GUI for interacting with images and containers. Whenever you want to use a Docker image, you will need to start the Docker daemon first, and opening Docker Desktop is the simplest way to do this (although there are a few other ways to start the Docker daemon). Docker has versions for Mac, Windows, and Linux. The RevBayes Docker image should work on any of these platforms.Example view of containers in Docker Desktop.This is a reference to Downloading the imageA Docker image containing RevBayes, TensorPhylo, Python, R, and other dependencies can be found at hub.docker.com/r/sswiston/phylo_docker. The easiest way to obtain this image is with a docker pull command.First, open Docker Desktop. This starts the Docker daemon so that you can run docker commands. If you do not open Docker Desktop first, you will receive an error when you try to run a docker command. Then, in the desktop app, sign into your Docker Hub account.Next, you will need to pull a phylo_docker image with a tag that matches your computer’s hardware architecture. The two tagged versions currently supported are phylo_docker:slim_amd64 and phylo_docker:slim_arm64. Consult the table below if you are unsure which tagged image to use.            OS      CPU      Manufacture date      Tagged image                  Windows or Linux      Intel or AMD      –      phylo_docker:slim_amd64              Mac OS X      Intel      2019 or earlier      phylo_docker:slim_amd64              Mac OS X      Silicon M1, M2, etc.      after 2019      phylo_docker:slim_arm64      Next, open your command line and enter the appropriate pull command:# For Intel/AMD computersdocker pull sswiston/phylo_docker:slim_amd64# For Apple Silicon (M1, M2, etc.) computersdocker pull sswiston/phylo_docker:slim_arm64Docker will automatically store the image on your computer in a directory reserved for Docker images. You will not have to manually locate this image; Docker will be able to find it.Special instructions for Windows usersYour Windows operating system uses Docker to run a PhyloDocker container. The container itself runs a virtual computing environment using the Linux operating system, Alpine.  If you are not familiar with Linux, find a short list of important commands here: link.  Docker expects Unix-style directory names, even if you are running Windows.  Converting from Windows to Unix-style directories is easy. The Windows-style directory path C:\\\\Users\\charles_darwin\\my_project becomes the Unix-style directory path /c/Users/mlandis/charles_darwin/my_project. Use Unix-style directory paths when mounting directories from your computer (the host) on a new container (see below).Using RevBayes via DockerRunning RevBayes image via command lineYou can also run the RevBayes Docker image directly from command line. This will still require opening Docker Desktop to start the Docker daemon.      Open Docker Desktop. You cannot run a Docker container from the command line unless the daemon is running. If you are not already signed in to your Docker Hub account, sign in now (there will be an option in the upper right corner of screen).        Open command prompt.        Run the command for opening the RevBayes Docker image:     # For Intel/AMD computers docker run --name [my_container] --volume [local_directory]:[container_directory] -it sswiston/phylo_docker:slim_amd64    # For Apple Silicon (M1, M2, etc.) computers docker run --name [my_container] --volume [local_directory]:[container_directory] -it sswiston/phylo_docker:slim_arm64        Some parts of this command are directly analogous to the optional settings from the RevBayes GUI.                  --name adds a name to your container. This is not strictly necessary, but it’s very helpful if you plan to have multiple containers opened at a time. Otherwise, Docker will give the container a randomized name.                    --volume [local_directory]: is how you tell Docker where to find your scripts on the host machine (your computer). To run the test script for this tutorial, you will have to tell Docker what directory to look in. You will need to use the absolute filepath to the directory. For example, if I put the test script in a directory called docker_tutorial on my desktop, I would put the filepath /Users/Sarah/Desktop/docker_tutorial. You can mount multiple directories from your host machine by adding multiple --volume arguments.                    [container_directory] is how you will access your files while inside the Docker container. Mounting a directory in this way essentially creates a “connection” between the directory on the outside of the container (on your host machine) and inside the container. While inside the container, you will be able to see all of the contents of a mounted directory, including other sub-directories. For example, if you run RevBayes and want it to read a file from your host machine, you will use this filepath while inside the container. If you want to save files to your host machine from inside the Docker container, you will also use this filepath. There isn’t a specific place inside the Docker container where you have to mount the directory, and it doesn’t have to have the same name as the directory on your host machine. For example, you could call it /tutorial, which would put a directory called tutorial in the container’s home directory with your test script in it.                    -it is for opening an interactive container. Docker containers can also be used to automatically run scripts and terminate when they are completed, but you will need an interactive container for this tutorial.                    sswiston/phylo_docker:slim_amd64 or sswiston/phylo_docker:slim_arm64 (hardware-dependent, see above) is the name of the Docker image you want to use.              Congrats, you are inside the Docker container! You should be able to access all of the programs and files in the container, and also the directory you mounted from your host machine.        Navigate to the location of your test script with cd [filepath], using the filepath that you mounted your directory to in Step 3.        Now you can use RevBayes to run the test script with the command rb test.Rev. This should open RevBayes and run the script, which will give a message indicating that it has been run correctly:     Processing file \"test.Rev\"   Congrats! RevBayes is working!Running RevBayes image via Docker Desktop  Docker Desktop behaves differently across versions  Some versions of Docker Desktop provide full support to run and interact with containers through the GUI. Unfortunately, its functionality changes across versions. To interact with a container through Docker Desktop v4.30.0 (current version as of writing this tutorial) you must first launch an interactive container using the commands above. Afterwards, you can interact with the running container through Docker Desktop. Previous versions of Docker Desktop allowed interactive sessions to be created from scratch, so this functionality will probably be restored in the future.You can use the Docker Desktop GUI to run the RevBayes Docker image (but see caveats in info box above). Once you have run the image, you will be able to use RevBayes inside the Docker container, and run scripts. Here are the steps you will need to get RevBayes running:      Open Docker Desktop. If you are not already signed in to your Docker Hub account, sign in now (there will be an option in the upper right corner of screen).        There will be a tab on the lefthand side called images. In this tab, you will see all of the images you have downloaded.        Hovering over an image should give a run option. Click this option.        This should bring you to a screen with a dropdown menu called optional settings. You will want to click on this menu and change a few settings.        Add a name to your container. This is not strictly necessary, but it’s very helpful if you plan to have multiple containers opened at a time. Otherwise, Docker will give the container a randomized name.        Enter a host path. This is how you tell Docker where to find your scripts on the host machine (your computer). To run the test script for this tutorial, you will have to tell Docker what directory to look in. You will need to use the absolute filepath to the directory. You can manually enter the filepath to this directory, or you can navigate to it using the ... button. For example, if I put the test script in a directory called docker_tutorial on my desktop, I would put the filepath /Users/Sarah/Desktop/docker_tutorial. You can mount multiple directories from your host machine using the + button.        Enter a container path. This is how you will access your files while inside the Docker container. Mounting a directory in this way essentially creates a “connection” between the directory on the outside of the container (on your host machine) and inside the container. While inside the container, you will be able to see all of the contents of a mounted directory, including other sub-directories. For example, if you run RevBayes and want it to read a file from your host machine, you will use this filepath while inside the container. If you want to save files to your host machine from inside the Docker container, you will also use this filepath. There isn’t a specific place inside the Docker container where you have to mount the directory, and it doesn’t have to have the same name as the directory on your host machine. For example, you could call it /tutorial, which would put a directory called tutorial in the container’s home directory with your test script in it.        Click the run option.        Another tab on the lefthand side of the screen should be called containers. Clicking this option should bring you to a screen showing all of the containers you currently have running.        There are two ways to access the container as it is running. The first way is to click on the running container inside the Containers panel, and then click on the Exec tab from the list of tabs at the top of the container menu. The second way is to find the running container inside the Containers panel, and then click on the icon with three vertical dots to the right of the container name. This will open a menu of options. Click on the “Open in terminal” menu item. Clicking this option will take you to the Exec tab, as in the first option. Congrats, you are inside the Docker container! You should be able to access all of the programs and files in the container, and also the directory you mounted from your host machine.        Navigate to the location of your test script with cd [filepath], using the filepath that you mounted your directory to in Step 7.        Now you can use RevBayes to run the test script with the command rb test.Rev. This should open RevBayes and run the script, which will give a message indicating that it has been run correctly:     Processing file \"test.Rev\"   Congrats! RevBayes is working!Running on a clusterSome high-performance computing clusters allow (or require) users to implement Docker images. For the most part, using a Docker image on a computing cluster is like using the image via command line on your own machine. You can mount directories, start a container, and use the programs inside to run scripts. However, there can be some additional considerations, depending on how the particular cluster is organized.Example: WUSTL RISWashington University in Saint Louis Research Infrastructure Services (WUSTL RIS) has a scientific compute platform that requires Docker to run jobs. Each job opens a separate Docker container. Interactive jobs allow users to access an interactive container where they can enter commands, start programs, and run scripts. Non-interactive jobs open a non-interactive container that automatically runs certain scripts, and then both the container and job terminate. In this example, we will show a non-interactive job.WUSTL RIS uses bsub commands to submit jobs. These commands can be written in one line, but they often have many arguments, so it can be helpful to put together a Bash script that constructs the bsub command. Here is an example:LSF_DOCKER_VOLUMES=\"[Storage Directory]:/project\"PROJECTDIR=\"/project/\"NAME=\"MY_JOB\"OUTDIR=\"/project/joblogs/\"bsub \\-G [Compute Group] \\-q general \\-n 5 -M 20GB -R \"rusage [mem=20GB] span[hosts=1]\" \\-cwd $PROJECTDIR \\-J $NAME \\-o $OUTDIR$NAME.stdout.txt \\-a 'docker(sswiston/phylo_docker:slim_amd64)' /bin/bash /project/rev_shell.shLet’s pick apart the elements of this script. There is a section at the top for defining variables, and then a bsub command using those variables.      LSF_DOCKER_VOLUMES is how you mount a directory to the Docker container for the job. This variable will not be part of the bsub command itself, but when a new job is starting, it will look for this information. It has two parts.                  [Storage Directory]: is how you tell Docker where to find your scripts in the cluster’s storage.                    /project is where the directory will be mounted inside the Docker container (in this example). There isn’t a specific place inside the Docker container where you have to mount the directory, and it doesn’t have to have the same name as the directory in the cluster’s storage. Mounting a directory in this way essentially creates a “connection” between the directory on the outside of the container (in the cluster’s storage) and inside the container. While inside the container, your scripts will be able to see all of the contents of a mounted directory, including other sub-directories.                  PROJECDIR, OUTDIR, and NAME are variables that are used in the bsub command to specify the project directory, output directory, and job name.  After defining important variables, there is a multi-line bsub command that actually submits the job. It has several arguments:      -G is the compute group (usually belonging to the lab PI) that the job belongs to.        -q is the queue that the job will join. The general queue is non-interactive.        -n, -M, and -R specify the amount of memory that the job will use.        -cwd sets the working directory inside the Docker container. In this example, note that we use the project directory that we mounted. This is useful because, when the job runs, it will look in this directory for scripts.        -J gives the job a name. This is not strictly necessary, but it is helpful for keeping track of running jobs.        -o creates an output file for the job. Your scripts may save certain products to other files, but this file will record the standard output, including possible job failures.        -a is the most important part of the bsub command. There are 3 parts.                  'docker(sswiston/phylo_docker:slim_amd64)' is the Docker image being used. The image will be pulled from Docker Hub.                    /bin/bash is the initial command that will run once the container is open.                    /project/rev_shell.sh is the script that will run with the /bin/bash command.            You may wonder why we don’t immediately call RevBayes and run a .Rev script. This is possible to do. However, the WUSTL RIS cluster overwrites the PATH variable inside the Docker container, which means that RevBayes can’t be found with a simple rb command. Instead, you would have to specify the full filepath to the RevBayes binary inside the container, which is /revbayes/projects/installer/rb. You may also want to add extra information to the rb command (like defining variables for your analyses). It can be easier to use a shell script to call RevBayes while inside the container. Here is an example:PATH=$PATH:/revbayes/projects/installer:/revbayes/projects/installer/rbrb_command=\"variable_1=\"test\";source(\\\"/project/rev_script.Rev\\\");\"echo $rb_command | rbIn this short script, RevBayes is added to the PATH variable inside the container. Then a RevBayes command is constructed. It has two parts: setting variable_1 to have a value of test, and sourcing the script rev_script.Rev. The final line pipes this command into RevBayes, which sets variable_1 equal to test and runs rev_script.Rev.This example highlights two main differences between using the RevBayes Docker image on your local machine and using it on the WUSTL RIS computing cluster. First, the analyses must be submitted as jobs (common amongst computing clusters). Second, the job submission process overwrites the PATH variable inside the Docker container, so you will have to know where to find the programs you want to use. Other computing clusters may have similar challenges. Before running lengthy jobs or batches of jobs, it is recommended that you try out a test job or two and make sure they work as expected. Generally, the Docker image will function the same on any platform, and all of the programs should run correctly with appropriate input scripts.",
        "url": "/tutorials/docker/",
        "index": "true"
      }
      ,
    
      "tutorials-chromo": {
        "title": "Chromosome Evolution",
        "content": "IntroductionA central organizing component of the higher-order architecture of thegenome is chromosome number, and changes in chromosome number have longbeen understood to play a fundamental role in evolution. This tutorialwill introduce phylogenetic models of chromosome number evolution, anddemonstrate how to use RevBayes to estimate the rates of chromosomenumber change and ancestral chromosome numbers. We will also show how touse the RevGadgets R package to make plots of ancestral chromosomenumber estimates and stochastic character maps of chromosome evolution.We will begin by providing an overview of the basic ChromEvol model(missing reference) and an example RevBayes analysis. This isfollowed by a discussion of a number of model extensions that enablejoint inference of phylogeny and chromosome numbers, tests forcorrelated rates of phenotype and chromosome evolution with the BiChroMmodel (missing reference), and incorporating cladogenetic changes inchromosome number. Next, we will introduce the ChromoSSE model(Freyman and Höhna 2018) which jointly estimates diversification ratesand chromosome number evolution. We also briefly discuss testinghypotheses of chromosome evolution by comparing different models usingreversible-jump MCMC and Bayes factors.If you use RevBayes for chromosome evolution analyses, please cite theoriginal papers that describe the chromosome evolution models as well asFreyman and Höhna (2018) which describes in detail the RevBayesimplementation of these models.Overview of chromosome number evolution modelsChromosome changes represent major evolutionary mechanisms that havelong been a focal point of study. Changes in chromosome number such asthe gain or loss of a single chromosome (dysploidy), or the doubling ofthe entire genome (polyploidy), can have phenotypic consequences, affectthe rates of recombination, and increase reproductive isolation amonglineages and thus drive diversification (missing reference).Recently, evolutionary biologists have increasingly studied themacroevolutionary consequences of chromosome changes within a molecularphylogenetic framework, mostly utilizing the likelihood-based ChromEvolmodels of chromosome number evolution introduced by(missing reference). The ChromEvol models have permittedphylogenetic studies of ancient whole genome duplication events, rapid“catastrophic” chromosome speciation, major reevaluations of theevolution of angiosperms, and new insights into the fate of polyploidlineages(missing reference). The basicChromEvol model has been extended to examine the association ofphenotype with chromosome evolution (missing reference), and toincorporate cladogenetic changes and diversification rates(Freyman and Höhna 2018).Here we describe the ChromEvol model as implemented in RevBayes, whichexcept for one detail noted below is the same mathematical modelintroduced in (missing reference). In further sections, we willshow how to set up extensions such as BiChroM and ChromoSSE which buildon the useful but basic ChromEvol model of chromosome number evolution.The ChromEvol modelIn ChromEvol the evolution of chromosome number is represented as acontinuous-time Markov process, similar to models of molecular evolutionand discrete morphological evolution. The continuous-time Markov processis described by an instantaneous rate matrix $Q$ where the value of eachelement represents the instantaneous rate of change within a lineagefrom a genome of $i$ chromosomes to a genome of $j$ chromosomes. For allelements of $Q$ in which either $i = 0$ or $j = 0$ we define$Q_{ij} = 0$. For the off-diagonal elements $i \\neq j$ with positivevalues of $i$ and $j$, $Q$ is determined by:\\[\\label{eq:anagenetic1}Q_{ij} =    \\begin{cases}        \\gamma_a                          &amp; j = i + 1,    \\\\        \\delta_a                          &amp; j = i - 1,    \\\\        \\rho_a                                &amp; j = 2i,       \\\\        \\eta_a                                 &amp; j = 1.5i,     \\\\        0                                   &amp; \\mbox{otherwise},       \\end{cases}\\]where $\\gamma_a$, $\\delta_a$, $\\rho_a$, and $\\eta_a$are the rates of chromosome gains, losses, polyploidizations, anddemi-polyploidizations. We use the subscript $a$ for all these rates todifferentiate the rates between anagenetic ($a$) and cladogenetic ($c$)events (see the ChromoSSE model in a later section).If we are interested in modeling scenarios in which the probability offusion or fission events are positively or negatively correlated withthe number of chromosomes we can define $Q$ as:\\[\\label{eq:anagenetic2}Q_{ij} =    \\begin{cases}        \\gamma_a e^{\\gamma_m (i - 1)}  &amp; j = i + 1,    \\\\        \\delta_a e^{\\delta_m (i - 1)}     &amp; j = i - 1,    \\\\        \\rho_a                                &amp; j = 2i,       \\\\        \\eta_a                                 &amp; j = 1.5i,     \\\\        0                                   &amp; \\mbox{otherwise},       \\end{cases}\\]where $\\gamma_m$ and $\\delta_m$ are rate modifiers ofchromosome gain and loss, respectively, that allow the rates ofchromosome gain and loss to depend on the current number of chromosomes.If the rate modifier $\\gamma_m = 0$, then$\\gamma_a e^{0 (i - 1)} = \\gamma_a$. If the rate modifier$\\gamma_m &gt; 0$, then $\\gamma_a e^{\\gamma_m (i - 1)} \\geq \\gamma_a$(i.e., rates increase with more chromosomes),and if $\\gamma_m &lt; 0$ then $\\gamma_a e^{\\gamma_m (i - 1)} \\leq \\gamma_a$(i.e., rates decrease with more chromosomes).Note that this parameterization differs slightly from the originalChromEvol model; here we assume the rates of chromosome change can varyexponentially as a function of the current chromosome number, whereasChromEvol as originally described by (missing reference) assumes alinear function. The theoretical reasons for this difference aredescribed in Freyman and Höhna (2018), however in practice on mostempirical datasets the difference appears insignificant.Demi-polyploidization is the union of a reduced and an unreduced gametesthat produces a cytotype with 1.5 times the number of chromosomes. Thenumber of chromosomes in a genome must of course be an integer, so forodd values of $i$, $Q_{ij}$ is set to $\\eta/2$ for the two integervalues of $j$ resulting when $j = 1.5i$ is rounded up and down.As in all continuous-time Markov models, the diagonal elements $i = j$of $Q$ are defined as:\\(Q_{ii} = - \\sum_{i \\neq j}^{C_m} Q_{ij}.\\)Theprobability of anagenetically evolving from chromosome number $i$ to $j$along a branch of length $t$ is then calculated by exponentiation of theinstantaneous rate matrix:\\(\\label{eq:anagenetic_probs}    P_{ij}(t) = e^{-Qt}.\\)Given a phylogeny and chromosome counts ofthe extant lineages, this model can be used in either a maximumlikelihood or Bayesian inference framework to estimate the rates ofchromosome change and the ancestral chromosome numbers.Hypothesis testing and model uncertaintyThe ChromEvol model described above is actually a class of models; forexample we could exclude demi-polyploidization by fixing it’s rate to 0.A common use for different models of chromosome evolution is to testhypotheses. For example, are polyploidization events occuring primarilyat speciation events and possibly driving diversification, or dopolyploidization events occur within lineages and unassociated withlineage splitting? To answer this, one could use RevBayes to set uptwo different models, one allowing cladogenetic polyploidization (seethe section ) and a second using a model with onlyanagenetic polyploidization (like the ChromEvol model described above).One could then calculate a Bayes factor to compare which model betterexplained the observed data. See the RevBayes tutorial General Introduction to Model selection for more information onhow to calculate Bayes factors in RevBayes.Another option in RevBayes is to use Bayesian model averaging. Out ofthe class of all chromosome evolution models described here, it ispossible that no single model will adequately describe the chromosomeevolution of a given clade. To explore the entire space of all possiblemodels of chromosome number evolution one could specify a reversiblejump Markov chain Monte Carlo (missing reference) that samples acrossmodels of different dimensionality, drawing samples from chromosomeevolution models in proportion to their posterior probability andenabling Bayes factors for each model to be calculated. This approachincorporates model uncertainty by permitting model-averaged inferencesthat do not condition on a single model; we draw estimates of ancestralchromosome numbers and rates of chromosome evolution from all possiblemodels weighted by their posterior probability. For general reviews ofthis approach to model averaging see (missing reference), and for its use inphylogenetics see (missing reference). Averaging over all models has beenshown to provide a better average predictive ability than conditioningon a single model (missing reference). Conditioning on a single modelignores model uncertainty, which can lead to an underestimation in theuncertainty of inferences made from that model (missing reference).In our case, this can lead to overconfidence in estimates of ancestralchromosome numbers and chromosome evolution parameter value estimates.For details on how to implement Bayesian model averaging in RevBayeswith chromsome evolution see Freyman and Höhna (2018).Next StepsThe basic ChromEvol model as described above can be extended in a numberof useful ways that will be covered in further sections. In the nextsection, however, we’ll set up and run a simple RevBayes analysisusing the ChromEvol model before moving on to the more complex models.Maximum a posterioriancestral chromosome number estimates for Aristolochia inferred usingRevBayes and plotted using the RevGadgets R package. The section describes how to perform this analysis.A simple ChromEvol analysisIn this example, we will use molecular sequence data and chromosomecounts from (missing reference) of the plant genus Aristolochia(commonly called Dutchman’s pipe plants). We will use a simple ChromEvolmodel to infer rates of chromosome evolution and ancestral chromosomenumbers.Tutorial FormatThis tutorial follows a specific format for issuing instructions andinformation.All command-line text, including all Rev syntax, are given inmonotype font. Furthermore, blocks of Rev code that are needed tobuild the model, specify the analysis, or execute the run are given inseparate shaded boxes. For example, we will instruct you to create aconstant node called example that is equal to 1.0 using the &lt;-operator like this:example &lt;- 1.0Data and FilesOn your own computer, create a directory called chromosome_tutorial (or any name you like).In this directory download and unzip the archive containing the datafiles: data.zip.This will create a folder called data that contains the filesnecessary to complete this exercise.Creating the Rev FileCreate a new directory (in chromosome_tutorial) called scripts. (If you do not have this folder, please refer to the directions inthe section .)When you execute RevBayes in this exercise, you should do so withinthe main directory you created (chromosome_tutorial),thus, if you are using a Unix-based operating system, we recommend thatyou add the RevBayes binary to your path.For complex models and analyses, it is best to create Rev script filesthat will contain all of the model parameters, moves, and functions. Inthis first section, you will create a file called from scratch and saveit in the scripts directory.The full scripts for these examples are also provided for download asscripts.zip. Please refer to these files to verify ortroubleshoot your own scripts.Open your text editor and enter the Rev code provided in this section.Save this Rev file into your scripts directory.This file will be the one you load into RevBayes to run the analysis.The file will contain the Rev code to load the data files, set up themodel, run the MCMC analysis, and summarize the results.Reading in DataFirst, we’ll read in the phylogeny. In this example the phylogeny isassumed known. In further examples we’ll jointly estimate chromosomeevolution and the phylogeny.phylogeny &lt;- readBranchLengthTrees(\"data/aristolochia.tree\")[1]We need to limit the maximum number of chromosomes allowed in our model,so here we use the largest observed chromosome count plus 10. This is anarbitrary limit on the size of the state space that could be increasedif necessary.max_chromo = 26Now we get the observed chromosome counts from a tab-delimited file.chromo_data = readCharacterDataDelimited(\"data/aristolochia_chromosome_counts.tsv\", stateLabels=(max_chromo + 1), type=\"NaturalNumbers\", delimiter=\"\\t\", headers=FALSE)Finally, we initialize a variable for our vector of moves and monitors.moves    = VectorMoves()monitors = VectorMonitors()The Chromosome Evolution ModelWe’ll use exponential priors with prior mean 0.1 to model the rates ofpolyploidy and dysploidy events along the branches of the phylogeny.gamma is the rate of chromosome gains, delta is the rate ofchromosome losses, and rho is the rate of polyploidization.gamma ~ dnExponential(10.0)delta ~ dnExponential(10.0)rho ~ dnExponential(10.0)Add MCMC moves for each of the rates.moves.append( mvScale(gamma, lambda=1, weight=1) )moves.append( mvScale(delta, lambda=1, weight=1) )moves.append( mvScale(rho, lambda=1, weight=1) )Now we create the rate matrix for the chromosome evolution model. Herewe will use a simple ChromEvol model that includes only the rate ofchromosome gain, loss, and polyploidization.Q := fnChromosomes(max_chromo, gamma, delta, rho)Parameters for demi-polyploidization and rate modifiers could also beadded at this step for more complex models. For example, we could haveincluded the rate of demi-polyploidization eta and rate modifiers likethis:Q := fnChromosomes(max_chromo, gamma, delta, rho, eta, gamma_l, delta_l)Here we assume an equal prior probability for the frequency ofchromosome numbers at the root of the tree. This does not mean that thefrequencies are actually equal, we just give it an equal priorprobability. Alternatively, we could have treated the root frequenciesas a free variable and estimated them from the observed data. Thisapproach will be illustrated in further examples.root_frequencies := simplex(rep(1, max_chromo + 1))Finally, we create the stochastic node for the chromosome evolutioncontinuous-time Markov chain (CTMC). We also clamp the observedchromosome count data to the CTMC.chromo_ctmc ~ dnPhyloCTMC(Q=Q, tree=phylogeny, rootFreq=root_frequencies, type=\"NaturalNumbers\")chromo_ctmc.clamp(chromo_data)All of the components of the model are now specified, so now we wrap itinto a single model object.mymodel = model(phylogeny)Set Up the MCMCThe next important step for our master Rev file is to specify the MCMCmonitors. For this, we create a vector called monitors that will eachoutput MCMC samples. First, a screen monitor that will output every 10iterations:monitors.append( mnScreen(printgen=10) )Next, an ancestral state monitor which will sample ancestral states andwrite them to a log file. We could additionally use themnStochasticCharacterMap monitor to sample stochastic character mapsof chromosome evolution (see the next section for an example).monitors.append( mnJointConditionalAncestralState(filename=\"output/ChromEvol_simple_anc_states.log\", printgen=10, tree=phylogeny, ctmc=chromo_ctmc, type=\"NaturalNumbers\") )And another monitor for logging all the model parameters. This willgenerate a file that can be opened in Tracerfor checkingMCMC convergence and parameter estimates.monitors.append( mnModel(filename=\"output/ChromEvol_simple_model.log\", printgen=10) )Now we set up the MCMC and include code to execute the analysis. In thisexample we set the chain length to 200, however for a real analysisyou would want to run many more iterations and check for convergence.mymcmc = mcmc(mymodel, monitors, moves)mymcmc.run(200)Summarize Ancestral StatesNow we need to add Rev code that will summarize the sampled ancestralchromosome numbers. First, read in the ancestral state trace generatedby the ancestral state monitor during the MCMC analysis:anc_state_trace = readAncestralStateTrace(\"output/ChromEvol_simple_anc_states.log\")Finally, summarize the values from the traces over the phylogeny. Herewe do a marginal reconstruction of the ancestral states, discarding thefirst 25% of samples as burnin. This will produce the fileChromEvol_simple_final.tree that contains the phylogeny along withestimated ancestral states. We can use that file with the RevGadgets Rpackage to generate a plot of the ancestral states.ancestralStateTree(phylogeny, anc_state_trace, \"output/ChromEvol_simple_final.tree\", burnin=0.25, reconstruction=\"marginal\")Note that we could also have calculated joint or conditional ancestralstates instead of (or in addition to) the marginal ancestral states. Ifwe had sampled stochastic character maps, we would summarize them withthe characterMapTree function.And now quit RevBayes:q()You made it! Be sure to save the file.Execute the RevBayes AnalysisWith all the parameters specified and all analysis components in place,you are now ready to run your analysis. The Rev scripts you justcreated will all be used by RevBayes and loaded in the appropriateorder.Begin by running the RevBayes executable. In Unix systems, type thefollowing in your terminal (if the RevBayes binary is in your path):Provided that you started RevBayes from the correct directory(RB_Chromsome_Evolution_Tutorial) the analysis should now run.Alternatively, from within RevBayes you could use the source()function to feed RevBayes your master script file:source(\"scripts/ChromEvol_simple.Rev\")This will execute the analysis and you should see output similar to this(though not the exact same values):When the analysis is complete, RevBayes will quit and you will have anew directory called output that will contain all of the files youspecified with the monitors.Plotting the ResultsNow we will plot the results of the MCMC analysis using the RevGadgetsR package. Start R and set your working directory to theRB_Chromsome_Evolution_Tutorial directory. Now run the command togenerate  below. There are many optionsto customize the look of the plot, for options take a look inside the Rscript.Next StepsThere are many extensions to the basic ChromEvol analysis demonstratedhere. In the next section we will look at how to set up more complexchromosome number evolution analyses.Basic extensions of the ChromEvol modelIn this section, we will extend the ChromEvol model in a number of ways.First, we will examine another approach for treating chromosome numberroot frequencies. This is followed by a brief example applyingstochastic character mapping to chromosome evolution models. Then willlook at jointly estimating the phylogeny and chromsome evolution, showhow to set up a BiChroM analysis, and demonstrate one way to addcladogenetic changes to a chromosome evolution analysis.Like before, scripts for these examples are also provided in thescripts.zip file. Please refer to these files toverify or troubleshoot your own scripts.Improved root frequenciesIn the last example we assumed the frequency of chromosome numbers atthe root of the tree were equal. This is equivalent to assigning anextremely informative prior that all root states are equally likely. Analternative approach is to treat the root frequencies as free parametersof the model and estimate them from the observed data. In a series ofunpublished simulations performed by the authors this resulted inincreased accuracy of ancestral root chromosome numbers estimates.To use this approach, the root_frequencies parameter must be redefinedas a stochastic node in our graphical model instead of a deterministicnode. Remove the following line from your Rev script:root_frequencies := simplex(rep(1, max_chromo + 1))We will instead use an uninformative flat Dirichlet prior for the rootfrequencies. First, we create a vector to hold the concentrationparameters for the Dirichlet distribution. Here we set all concentrationparameters to 1, which results in all sets of probabilities beingequally likely. We then pass the vector of concentration parameters intothe Dirichlet distribution and create the stochastic node representingroot frequencies.root_frequencies_prior &lt;- rep(1, max_chromo + 1)root_frequencies ~ dnDirichlet(root_frequencies_prior)Next, we must specify MCMC moves for the root frequencies. When themaximum number of chromosomes is high these parameters can havedifficulty converging. Therefore, we use two different MCMC moves. Thefirst is Beta Simplex move, which selects one element of theroot_frequencies vector and proposes a new value for it drawn from aBeta distribution. The second is Element Swap Simplex move, whichselects two elements of the root_frequencies vector and simply swapstheir values.moves.append( mvBetaSimplex(root_frequencies, alpha=0.5, weight=10)moves.append( mvElementSwapSimplex(root_frequencies, weight=10)You can experiment with different weights for each MCMC move.Make the modifications to the root frequencies in the scriptChromoSSE_simple.Rev and run the analysis again? What are theestimates of the root frequency (you can see this by looking at theoutput in Tracer)? Do the estimated ancestral stateschange?Stochastic character mapping of chromosome evolutionIn RevBayes both ancestral states and stochastic character maps can besampled from continuous-time Markov chain (CTMC) and state-dependentspeciation and extinction (SSE) models of character evolution.Stochastic character maps show the timing and number of transitionsalong the branches of the phylogeny, so they can be particularly usefulfor chromosome evolution estimates where the timing of, for example,whole genome duplication events might be of interest. This example isperformed on a non-ultrametric tree, but the same analysis could beperformed on time-calibrated trees.An example of stochastic charactermapping applied to chromosome number evolution using RevBayes. Shownis the marginal maximum a posteriori chromosome evolution history ofAristolochia using the simple ChromEvol analysis from the section .We have already shown how to sample ancestral states above, and here weshow the few extra lines of Rev code needed to sample stochasticcharacter maps. Stochastic character maps are drawn during the MCMC, sowe need to include the mnStochasticCharacterMap monitor.monitors.append( mnStochasticCharacterMap(ctmc=chromo_ctmc, filename=\"output/ChromEvol_maps.log\", printgen=10) )This monitor will create the output/ChromEvol_maps.log file. Just likethe other log files, each row in this file represents a different samplefrom the MCMC. Each column in the file, though, is the character historyfor a different node in the phylogeny. The last column of the file isthe full stochastic character map of the entire tree in SIMMAP(Bollback 2006) format. These can be plotted using thephytools R package (Revell 2012).After the MCMC simulation, we can calculate the maximum a posteriorimarginal, joint, or conditional character history. This process issimilar to the ancestral state summaries. First we read in thestochastic character map trace.anc_state_trace = readAncestralStateTrace(\"output/ChromEvol_maps.log\")Then we use the characterMapTree function. This generates two SIMMAPformatted files: 1) the maximum a posteriori character history, and 2)the posterior probabilities of the entire character history.characterMapTree(phylogeny, anc_state_trace, character_file=\"output/character.tree\", posterior_file=\"output/posterior.tree\", burnin=5, reconstruction=\"marginal\") is an example stochastic character map of ourAristolochia analysis plotted using phytools.Copy the script ChromoSSE_simple.Rev and add stochastic charactermapping monitor. Then, run the analysis and use the scriptplot_simmap.R to visualize the character mappings.Joint estimation of phylogeny and chromosome evolutionIn RevBayes the chromosome evolution models can be used jointly with amodel of molecular evolution enabling joint inference of the phylogenyand chromosome number evolution. This enables the chromosome numberanalysis to take into account phylogenetic uncertainty and allows thechromosome numbers to help inform the phylogeny.Setting up a model that jointly infers chromosome evolution andphylogeny requires mostly combining elements covered in the MolecularModels of Character Evolution tutorial with what has already beencovered in Section [sec:chromo_basic_analysis] of this tutorial. Wewill not repeat how to set up the chromosome model component, but we’llstep through what must be added to the example in the section above. Furthermore, we have provided afull working example script scripts/ChromEvol_joint.Rev.Reading in Molecular Data and Setting Clade ConstraintsThe first major difference from the basic ChromEvol example shown aboveis that we must additionally read in molecular sequence data:dna_seq = readDiscreteCharacterData(\"data/aristolochia_matK.fasta\")We will need some useful information about this data as well:n_species = dna_seq.ntaxa()n_sites = dna_seq.nchar()taxa = dna_seq.names()n_branches = 2 * n_species - 2Since we want to jointly infer ancestral states, we need to set an apriori rooting constraint on our phylogeny. So here we set an ingroupand outgroup.outgroup = [\"Aristolochia_serpantaria\", \"Aristolochia_arborea\",            \"Aristolochia_wardiana\", \"Aristolochia_californica\",            \"Aristolochia_saccata\", \"Aristolochia_mollisima\",            \"Aristolochia_tomentosa\", \"Aristolochia_neolongifolia_SETS52\",            \"Aristolochia_neolongifolia_SETS96\"]Here we loop through each taxon and if it is not present in the outgroupdefined above we add it to the ingroup.i = 1for (j in 1:taxa.size()) {    found = false    for (k in 1:outgroup.size()) {        if (outgroup[k] == taxa[j].getSpeciesName()) {            found = true            break        }    }    if (found == false) {        ingroup[i] = taxa[j].getSpeciesName()        i += 1    }}And now we make the vector of clade objects to constrain our treetopology.clade_ingroup = clade(ingroup)clade_outgroup = clade(outgroup)clade_constraints = [clade_ingroup, clade_outgroup]Tree ModelWe will specify a uniform prior on the tree topology, and add a MCMCmove on the topology.topology ~ dnUniformTopology(taxa=taxa, constraints=clade_constraints, rooted=TRUE)moves.append( mvNNI(topology, weight=10.0) )Next, we create a stochastic node for each branch length. Each branchlength prior will have an exponential distribution with rate 1.0. We’llalso add a simple scaling move for each branch length.for (i in 1:n_branches) {    br_lens[i] ~ dnExponential(10.0)    moves.append( mvScale(br_lens[i], lambda=2, weight=1)}Finally, build the tree by combining the topology with the branchlengths.phylogeny := treeAssembly(topology, br_lens)Molecular Substitution ModelWe’ll specify the GTR substitution model applied uniformly to all sites.Use a flat Dirichlet prior for the exchange rates.er_prior &lt;- v(1,1,1,1,1,1)er ~ dnDirichlet(er_prior)moves.append( mvSimplexElementScale(er, alpha=10, weight=3) )And also a flat Dirichlet prior for the stationary base frequencies.pi_prior &lt;- v(1,1,1,1)pi ~ dnDirichlet(pi_prior)moves.append( mvSimplexElementScale(pi, alpha=10, weight=2) )Now create a deterministic variable for the nucleotide substitution ratematrix.Q_mol := fnGTR(er, pi)Create a stochastic node for the sequence evolution continuous-timeMarkov chain (CTMC) and clamp the sequence data. Note we should have twoCTMC objects in this model: one for the model of molecular evolution andone for the model of chromosome evolution.dna_ctmc ~ dnPhyloCTMC(tree=phylogeny, Q=Q_mol, branchRates=1.0, type=\"DNA\")dna_ctmc.clamp(dna_seq)MCMC and Summarizing ResultsWe set up the MCMC just as before, except here we need to add a filemonitor to store the sampled trees.monitors.append( mnFile(filename=\"output/ChromEvol_joint.trees\", printgen=10, phylogeny) )Summarizing the results of the MCMC analysis are a little different.First we will calculate the maximum a posteriori (MAP) tree.treetrace = readAncestralStateTreeTrace(\"output/ChromEvol_joint.trees\", treetype=\"non-clock\")map_tree = mapTree(treetrace, \"output/ChromEvol_joint_map.tree\")Now we’ll summarize the ancestral chromosome numbers over the MAP tree.Read in the ancestral state trace:anc_state_trace = readAncestralStateTrace(\"output/ChromEvol_joint_states.log\")Finally, calculate the marginal ancestral states from the traces overthe MAP tree. Note that this time we have to pass both the tree traceand the ancestral state trace to the ancestralStateTree function.Since we sampled a joint distribution of ancestral state histories andtrees, we sampled some ancestral states for nodes that do not exist inthe MAP tree. Therefore the ancestral state probabilities beingcalculated for the MAP tree are conditional to the probability of thenode existing.ancestralStateTree(map_tree, anc_state_trace, treetrace, \"output/ChromEvol_joint_final.tree\" burnin=0.25, reconstruction=\"marginal\")Like before, we can plot the results using the RevGadgets R packageusing the script plot_ChromEvol_joint.R.Association chromosome evolution with phenotype (BiChroM)We may be interested in testing whether the rates of chromosome numberevolution are associated with a certain phenotype. Here we set up abinary phenotypic character and estimate separate rates of chromosomeevolution for each state of the phenotype. We’ll use a model thatdescribes the joint evolution of both the phenotypic character andchromosome evolution. This model (BiChroM) was introduced in(missing reference). In RevBayes, the BiChroM model can easily beextended to multistate phenotypes and/or hidden states, pluscladogenetic changes could be incorporated into the model.In this example we will again use chromosome count data from(missing reference) for the plant genus Aristolochia. For the phenotypewe will examine gynostemium morphology. Aristolochia flowers have anextensively modified perianth that traps and eventually releasespollinators to ensure cross pollination (this is why the flowersresemble pipes and are commonly called Dutchman’s pipes). Thegynostemium is a reproductive organ found only in Aristolchiaceae andOrchids that consists of fused stamens and pistil that pollinators mustinteract with during pollination. The subgenus Isotrema has highlyreduced three-lobed gynostemium. Other members of Aristolochia havegynostemium subdivided into 5 to 24 lobes. We’ll test for an associationof this phenotype with changes in the rates of chromosome evolution.  phenotype state 0 = 3 lobed gynostemium  phenotype state 1 = 5 to 24 lobed gynostemiumMuch of this exercise is a repeat of what was already covered in section ,so we will only touch on the modelcomponents that are different. We have provided a full working examplescript scripts/BiChroM.Rev. In this example the phylogeny is assumedknown, however one could combine this with the exercise above to jointlyinfer the phylogeny.Results of the example BiChroManalysis performed in RevBayes. This plot shows the output from .logfiles when loaded into Tracer. The rates of chromosomegains (gamma) and losses (delta) are higher for Aristolochialineages with complex gynostemium subdivided into 5 to 24 lobes (state0) compared to lineages with simple 3 lobed gynostemium (state 1).Maximum a posteriori estimates ofancestral chromosome number and gynostemium morphology forAristolochia inferred using the BiChroM model as implemented inRevBayes. States 1-26 represent the haploid number $n$ of chromosomefor lineages with gynostemium subdivided in 5 to 24 lobes. States 27-52represent the haploid number $n + 27$ of chromosomes for lineages withsimple 3 lobed gynostemium. The ancestral state for the common ancestorof all Aristolochia had a haploid number $n = 8$ and more complex 5 to24 lobed gynostemium. An evolutionary reduction to 3 lobes is inferredin the lineage leading to the extant Isotrema clade.Setting up the BiChroM modelThe first step will be to read in the observed data. This is done asbefore, with the exception that the data is set up a bit differently.The data matrix must now represent both the observed chromosome countsand the observed phenotype. So in this file states 1-26 represent thehaploid number $n$ of chromosome for lineages with gynostemiumsubdivided in 5 to 24 lobes, and states 27-52 represent the haploidnumber $n + 27$ for lineages with simple 3 lobed gynostemium. Note thestateLabels argument must now be set to 2 times the maximum number ofchromosomes.chromo_data = readCharacterDataDelimited(\"data/aristolochia_bichrom_counts.tsv\", stateLabels=2*(max_chromo + 1), type=\"NaturalNumbers\", delimiter=\"\\t\", headers=FALSE)Like before, we’ll use exponential priors to model the rates ofpolyploidy and dysploidy events along the branches of the phylogeny.However, here we set up two rate parameters for each type of chromosomechange – one for phenotype state 0 and one for phenotype state 1.gamma_0 ~ dnExponential(10.0)gamma_1 ~ dnExponential(10.0)delta_0 ~ dnExponential(10.0)delta_1 ~ dnExponential(10.0)rho_0 ~ dnExponential(10.0)rho_1 ~ dnExponential(10.0)Add MCMC moves for each of the rates.moves.append( mvScale(gamma_0, lambda=1, weight=1) )moves.append( mvScale(delta_0, lambda=1, weight=1) )moves.append( mvScale(rho_0, lambda=1, weight=1) )moves.append( mvScale(gamma_1, lambda=1, weight=1) )moves.append( mvScale(delta_1, lambda=1, weight=1) )moves.append( mvScale(rho_1, lambda=1, weight=1) )Now we create the rate matrix for the chromosome evolution model. Wewill set up two rate matrices, one for each phenotype state.Q_0 := fnChromosomes(max_chromo, gamma_0, delta_0, rho_0)Q_1 := fnChromosomes(max_chromo, gamma_1, delta_1, rho_1)Again, we could have include the rate of demi-polyploidization eta andrate modifiers like this:Q_0 := fnChromosomes(max_chromo, gamma_0, delta_0, rho_0, eta_0, gamma_l_0, delta_l_0)Q_1 := fnChromosomes(max_chromo, gamma_1, delta_1, rho_1, eta_1, gamma_l_1, delta_l_1)Now we create the rates of transitioning between phenotype states. Anymodel could be used (all rates equal models, Dollo models, etc.) buthere we estimate a different rate for each transition between states 0and 1.q_01 ~ dnExponential(10.0)q_10 ~ dnExponential(10.0)moves.append( mvScale(q_01, lambda=1, weight=1) )moves.append( mvScale(q_10, lambda=1, weight=1) )And finally we create the transition rate matrix Q_b for the jointmodel of phenotypic and chromosome evolution. First we will initializethe matrix with all zeros:s = Q_0[1].size()for (i in 1:(2 * s)) {    for (j in 1:(2 * s)) {        Q[i][j] := 0.0    }}And now we populate the matrix with the transition rates.for (i in 1:(2 * s)) {    for (j in 1:(2 * s)) {        if (i &lt;= s) {            if (j &lt;= s) {                if (i != j) {                    # chromosome changes within phenotype state 0                    Q[i][j] := abs(Q_0[i][j])                }            } else {                if (i == (j - s)) {                    # transition from phenotype state 0 to 1                    Q[i][j] := q_01                }            }        } else {            if (j &lt;= s) {                if (i == (j + s)) {                    # transition from phenotype state 1 to 0                    Q[i][j] := q_10                }            } else {                if (i != j) {                    # chromosome changes within phenotype state 1                    k = i - s                    l = j - s                    Q[i][j] := abs(Q_1[k][l])                }            }        }    }}Q_b := fnFreeK(Q, rescaled=false)The rest of the analysis is essentially the same as in section. Just make sure to pass the Q_b matrixinto the CTMC object.BiChroM Analysis ResultsIn  the rates of chromosome gains andlosses for each of the phenotype states are plotted. Aristolochialineages with complex gynostemium subdivided into many lobes have higherrates of dysploid changes than lineages with simple 3-lobed gynostemium.In  the marginal maximum a posteriori estimates ofancestral chromosome number and gynostemium morphology are plotted. Fromthis we can see that an evolutionary reduction occured on the lineageleading to the Isotreme clade. The common ancestor for allAristolochia is inferred to have complex many lobed gynostemium whichwas reduced to a more simple 3-lobed form in Isotrema.Incorporating cladogenetic and anagenetic chromosome changeChanges in chromosome number can increase reproductive isolation and maydrive the diversification of some lineages (missing reference).To test for the association of chromosome changes with speciation wemust extend our models to incorporate cladogenetic changes [for detailssee Freyman and Höhna (2018)]. Cladogenetic changes are changes thatoccur only at lineage splitting events. All models of chromosomeevolution that we have examined so far model only anagenetic changes,i.e., changes that occur within a lineage.We introduce here a simple model of cladogenetic change that handlescladogenetic events similarly to the widely usedDispersal-Extinction-Cladogenesis (missing reference) models ofbiogeographic range evolution. A major limitation of such models is thatthey only model cladogenetic changes at the observed speciation eventson the phylogeny. Many other unobserved speciation events likelyoccurred, but are not present in the reconstructed phylogeny due toincomplete taxon sampling and lineages going extinct. This can bias therelative rates of anagenetic and cladogenetic change. In section we will introduce the ChromoSSE model, whichremoves this bias by explicitly modeling unobserved speciation eventsbut at the cost of additional model complexity.Much of this exercise is a repeat of what was already covered in section, so we will only touch on the modelcomponents that must be changed to incorporate cladogenetic changes. Wehave provided a full working example scriptscripts/ChromEvol_clado.RevA Simple Cladogenetic ModelMaximum a posterioriancestral chromosome numbers for Aristolochia estimated using a simplecladogenetic and anagenetic chromosome evolution model. The start statesof each lineage (the state after cladogenesis) are plotted on the‘shoulders’ of each lineage, and show where cladogenetic eventsoccurred. This example analysis did not fully converge and shows a veryhigh number of cladogenetic events.The anagenetic transition rate matrix should be set up just as before.The cladogenetic changes, though, are modeled as a vector ofprobabilities that sum up to 1 (a simplex). Each element of the vectoris the probability of a certain type of cladogenetic event occurring. Toset this up, we’ll first draw a ‘weight’ for each type of cladogeneticevent from an exponential distribution. To keep the example simple weare excluding cladogenetic demi-polyploidization. We then pass each‘weight’ into a simplex to create the vector of probabilities.clado_no_change_pr ~ dnExponential(10.0)clado_fission_pr ~ dnExponential(10.0)clado_fusion_pr ~ dnExponential(10.0)clado_polyploid_pr ~ dnExponential(10.0)clado_demipoly_pr &lt;- 0.0clado_type := simplex([clado_no_change_pr, clado_fission_pr, clado_fusion_pr, clado_polyploid_pr, clado_demipoly_pr])The function fnChromosomesCladoProbs produces a matrix of cladogeneticprobabilities. This is a very large and sparse 3 dimensional matrix thatcontains the transition probabilities of every possible state of theparent lineage transitioning to every possible combination of states ofthe two daughter lineages.clado_prob := fnChromosomesCladoProbs(clado_type, max_chromo)We can’t forget to add moves for each cladogenetic event:moves.append( mvScale(clado_no_change_pr, lambda=1.0, weight=2) )moves.append( mvScale(clado_fission_pr, lambda=1.0, weight=2) )moves.append( mvScale(clado_fusion_pr, lambda=1.0, weight=2) )moves.append( mvScale(clado_polyploid_pr, lambda=1.0, weight=2) )Now we can create the cladogenetic CTMC model. We must pass in both theQ matrix that represents the anagenetic changes, and the clado_probsmatrix that represents the cladogenetic changes.chromo_ctmc ~ dnPhyloCTMCClado(Q=Q, tree=phylogeny, cladoProbs=clado_prob, rootFrequencies=root_frequencies, type=\"NaturalNumbers\", nSites=1)Most of the rest of the analysis is the same. For the ancestral statemonitor we want to be sure to specify withStartState=true so that wesample the states both at start and end of each branch. This enables usto reconstruct cladogenetic events.monitors.append( mnJointConditionalAncestralState(filename=\"output/ChromEvol_clado_anc_states.log\", printgen=10, tree=phylogeny, ctmc=chromo_ctmc, withStartStates=true, type=\"NaturalNumbers\") )When summarizing the ancestral state results we also want to specifyinclude_start_states=true so that we summarize the cladogeneticchanges.ancestralStateTree(phylogeny, anc_state_trace, \"output/ChromEvol_clado_final.tree\", include_start_states=true, burnin=0.25, reconstruction=\"marginal\")And that’s it!  shows the ancestral stateestimates plotted on the tree. The start states of each lineage (thestate after cladogenesis) are plotted on the ‘shoulders’ of eachlineage. You may want to try stochastic character mapping for adifferent and possibly better visualization of cladogenetic andanagenetic changes.Overview of the ChromoSSE modelA major challenge for all phylogenetic models of cladogenetic characterchange is accounting for unobserved speciation events due to lineagesgoing extinct and not leaving any extant descendants(Bokma 2002), or due to incomplete sampling of lineages in thepresent. Teasing apart the phylogenetic signal for cladogenetic andanagenetic processes given unobserved speciation events is a majordifficulty, and using a naive approach that does not account forunobserved speciation (like the ones discussed earlier in section) can bias the relative rates of cladogeneticand anagenetic change. The Cladogenetic State change Speciation andExtinction (ClaSSE) model (Goldberg and Igić 2012), on the other hand,reduces this bias by explicitly incorporating unobserved speciationevents. This is achieved by jointly modeling both character evolutionand the phylogenetic birth-death process. Not only does the ClaSSEframework enable the modeling of unobserved speciation, but it alsoprovides an easily extensible framework for testing state-dependentspeciation and extinction rates.An illustration of chromosomeevolution events that could occur during each small time interval$\\Delta t$ along the branches of a phylogeny as modeled in ChromoSSE(Freyman and Höhna 2018). The set of differential equations(subfigures a and b, respectively) sum over each possible chromosomeevolution event and are numerically integrated backwards through timeover the phylogeny to calculate the likelihood. The transition ratematrix for anagenetic changes $Q$ is explained in section. a) $D_{Ni}(t)$ is the probability that thelineage at time $t$ evolves into the observed clade $N$. To calculatethe change in this probability over $\\Delta t$ we sum over threepossibilities: no event occurred, an anagenetic change in chromosomenumber occurred, or a speciation event with a possible cladogeneticchromosome change occurred followed by an extinction event on one of thetwo daughter lineages. b) $E_i(t)$ is the probability that the lineagegoes extinct or is not sampled at the present. To calculate the changein this probability over $\\Delta t$ we sum over four possibilities: noevent occurred followed eventually by extinction, extinction occurred,an anagenetic change occurred followed by extinction, or a speciationevent with a possible cladogenetic change occurred followed byextinction of both daughter lineages.The ChromoSSE model (Freyman and Höhna 2018) is a special case of theClaSSE model that models chromosome changes. Compared to the previouslydiscussed CTMC models of chromosome evolution, SSE models requireadditional complexity since they must also model the speciation andextinction process. Simple extensions to the ChromoSSE model will enableexplicit tests of different extinction rates for polyploid and diploidlineages, and testing different rates of chromosome speciationassociated with phenotypes or habitat.The ChromoSSE likelihood calculationAll the previous models of chromosome number evolution discussed in thetutorial used the standard pruning algorithm (Felsenstein 1981) tocalculate the likelihood of chromosome evolution over the phylogeny. Forthe ChromoSSE model we must use a different approach; here thelikelihood is calculated using a set of ordinary differential equationssimilar to the Binary State Speciation and Extinction (BiSSE) model(Maddison et al. 2007). The BiSSE model was extended to incorporatecladogenetic changes by Goldberg and Igić (2012). FollowingGoldberg and Igić (2012), we define $D_{Ni}(t)$ as the probability that alineage with chromosome number $i$ at time $t$ evolves into the observedclade $N$. We let $E_i(t)$ be the probability that a lineage withchromosome number $i$ at time $t$ goes extinct before the present, or isnot sampled at the present. These two differential equations are shownand explained in . However, unlike the fullClaSSE model the extinction rate $\\mu$ does not depend on the chromosomenumber $i$ of the lineage. This can easily be modified in RevBayes toallow for different speciation and/or extinction rates depending onploidy or other character states.Next StepsIn the next section we’ll set up and run a RevBayes analysis using theChromoSSE model of cladogenetic and anagenetic chromosome evolution.A simple ChromoSSE analysisIn this example, we will again use molecular sequence data andchromosome counts from (missing reference) of the plant genusAristolochia. We will use a ChromoSSE model to infer rates ofchromosome evolution and ancestral chromosome numbers. For more complexexamples utilizing ChromoSSE with Bayesian model averaging andreversible-jump MCMC, see the scripts and explanations athttps://github.com/wf8/chromosse.Like in previous examples, we will here only highlight the majordifferences between a ChromoSSE analysis and the ChromEvol analysis setup in section . The full script to runthis ChromoSSE example is provided in the filescripts/ChromoSSE_simple.Rev.The model: a joint model of the tree and chromosome evolutionA major difference between the previously discussed models of chromosomenumber evolution and ChromoSSE is that ChromoSSE jointly describes theevolution of chromosome numbers and the tree. Since ChromoSSE assumesthe tree is generated via a birth-death process you should use anultrametric tree or time-calibrated tree. The tree may contain lineagesthat went extinct before the present, but the branch lengths should bein units of time, as opposed to units of substitutions per site. So herewe will swap out the tree used in previous examples for a tree that wastime-calibrated. The estimated rates of chromosome evolution will havethe same unit of time as the branch lengths of our tree. In this examplethe node ages are relative, but you could use a fossil-calibrated treewith absolute ages if you wanted rates of chromosome change in units ofmillions of years.phylogeny &lt;- readTrees(\"data/aristolochia-bd.tree\")[1]Anagenetic ChangesThe anagenetic part of the chromosome number evolution model involvespopulating the Q transition rate matrix with the rates of anageneticchromosome number changes. This is set up exactly the same as in theChromEvol analyses before.Cladogenetic ChangesAt each lineage splitting event a number of different types ofcladogenetic events could occur, including no change in chromosomenumbers, a dysploid gain or loss in a single daughter lineage, or achange in ploidy in a single daughter lineage. So we must set up aseparate speciation rate for each type of cladogenetic event.For each speciation rate, we will use an exponential prior with the meanvalue set to $r$, where $r$ is an approximation of the netdiversification rate. We calculate this approximation using$E(N_t) = N_0 e^{rt}$, which describes the expected number of species$N_t$ at time $t$ under a constant rate birth-death process where $N_0$is the number of species at $t=0$ (missing reference). The equation can berearranged to arrive at $r = ( \\ln(N_t) - \\ln(N_0) ) / t$.taxa &lt;- phylogeny.taxa()speciation_mean &lt;- ln( taxa.size() ) / phylogeny.rootAge()speciation_pr &lt;- 1 / speciation_meanEach cladogenetic event type is assigned its own speciation rate. We setthe rate of demi-polyploidization to 0.0 for simplicity.clado_no_change ~ dnExponential(speciation_pr)clado_fission ~ dnExponential(speciation_pr)clado_fusion ~ dnExponential(speciation_pr)clado_polyploid ~ dnExponential(speciation_pr)clado_demipoly &lt;- 0.0Like usual, we must add MCMC moves for the speciation rates.moves.append( mvScale(clado_no_change, lambda=5.0, weight=1) )moves.append( mvScale(clado_fission, lambda=5.0, weight=1) )moves.append( mvScale(clado_fusion, lambda=5.0, weight=1) )moves.append( mvScale(clado_polyploid, lambda=5.0, weight=1) )We next create a vector to hold the speciation rates, and also create adeterministic node total_speciation which will be a convenient way tomonitor the total speciation rate of the birth-death process.speciation_rates := [clado_no_change, clado_fission, clado_fusion, clado_polyploid, clado_demipoly]total_speciation := sum(speciation_rates)Finally, we map the speciation rates to the chromosome cladogeneticevents. The function fnChromosomesCladoEventsBD produces a matrix ofspeciation rates. This is a very large and sparse 3 dimensional matrixthat contains the speciation rates for all possible cladogenetic events.It contains the speciation rate for every possible state of the parentlineage transitioning to every possible combination of states of the twodaughter lineages.clado_matrix := fnChromosomesCladoEventsBD(speciation_rates, max_chromo)Extinction RateNext, we create a stochastic variable to represent therelative-extinction rate. Here, we define relative-extinction asextinction divided by speciation, so we use a uniform prior on theinterval ${0,1}$.rel_extinction ~ dnUniform(0, 1.0)rel_extinction(0.4)moves.append( mvScale(rel_extinction, lambda=5.0, weight=3.0) )We then make a vector of extinction rates for each state. In the basicChromoSSE model we assume all chromosome numbers have the sameextinction rate.for (i in 1:(max_chromo + 1)) {    extinction[i] := rel_extinction * total_speciation}The State-Dependent Speciation and Extinction ModelWe are now nearly ready to create the stochastic node that representsthe state-dependent speciation and extinction process. First, though, wemust set the probability of sampling species at the present. Weartificially use 1.0 here, but you should experiment with more realisticsettings as this will affect the overall speciation and extinction ratesestimated.rho_bd &lt;- 1.0Now we construct a variable that describes the evolution of both thetree and chromosome numbers drawn from a cladogenetic state-dependentbirth-death process. The dnCDBDP distribution is named for acharacter-dependent birth-death process, which is anothername for a state-dependent speciation and extinctionprocess.chromo_bdp ~ dnCDBDP( rootAge            = phylogeny.rootAge(),                      cladoEventMap      = clado_matrix,                      extinctionRates    = extinction,                      Q                  = Q,                      pi                 = root_frequencies,                      rho                = rho_bd )Since ChromoSSE is a joint model of both the tree and the chromosomenumbers, we must of course clamp both the observed tree and thechromosome count data.chromo_bdp.clamp(phylogeny)chromo_bdp.clampCharData(chromo_data)Finishing the ChromoSSE AnalysisThe rest of the analysis is nearly identical to the other examples wehave worked through, except for one detail when setting up the ancestralstate monitor. Now we must specify that we are using a state-dependentspeciation and extinction process (as mentioned above this is alsocalled a character-dependent birth-death process, or cdbdp) instead ofa continuous-time Markov process (ctmc) like we use before.monitors.append( mnJointConditionalAncestralState(filename=\"output/ChromoSSE_anc_states.log\", printgen=10, tree=phylogeny, cdbdp=chromo_bdp, withStartStates=true, type=\"NaturalNumbers\") )And that’s it! The results can be plotted using the same R scriptdemonstrated before to plot the cladogenetic ChromEvol analysis.",
        "url": "/tutorials/chromo/",
        "index": "true"
      }
      ,
    
      "tutorials-pps-morpho": {
        "title": "Assessing Phylogenetic Reliability Using RevBayes and $P^{3}$",
        "content": "OverviewThis tutorial presents a general approach for using posterior predictivesimulations (PPS) to determine the adequacy of morphological substitution models following (Mulvey et al. 2024). PPSassess the reliability of an evolutionary model for a given data set through the useof simulations and a number of test statistics. In this way, it is possible to determinewhether a model is adequately capturing the evolutionary dynamics of the data set ornot.IntroductionUnderstanding morphological evolution is a difficult task. Within palaeobiology there are a small number of relatively  simple evolutionary models to describe this complex process. Assessing the fit of these models to individual data sets is therefore crucial in order to have confidence in the inference results. Posterior prediction is a Bayesian approach to assess the fit of a model to a given data set(Bollback 2002; Brown 2014; Höhna et al. 2018). PPS works by simulating data setsbased on parameters sampled from the posterior distribution(). If the simulated data is found to be similar to theempirical data, according to the chosen test statistics, you can haveconfidence that the model is capturing the properties of the empirical data and can be used for an analysis. In this way, PPS provides the absolute fit of anevolutionary model to a given data set.In this tutorial, we will walk through how to use PPS to determine the fit of a modelto a data set of Hyaenodontidae taken from (missing reference).We will test two models in this tutorial. Each test is completely seperate fromthe other as we are determining the absolute fit of the model to the data set.A schematic presentation of posterior predictive simulation for morphological substitution models. Step 1 involves a carrying out an inference under the model ofinterest. Step 2 a number of data sets are then simulated using parameters taken fromthe posterior distribution estimated in step 1. Step 3 uses tests statistics to assesshow similar the simulated data sets are to the empirical. The test statistics employedhere are consistency index and retention index. P-values or effect sizes can be usedto determine whether a model is adequate or not for a given data set.Substitution ModelsThe models used here are described in more detail in the tutorial  Discrete morphology - Tree Inference. For a better understanding of the assumptions of these models, consultthe previous tutorial. We provide scripts for two subsitution models here:      Mk substitution model (Lewis 2001)        MkVP+G substitution model (Lewis 2001)              Model &amp; Extensions      Assumptions                  Mk      all transition rates are equal (Lewis 2001)                  G      allows for variation in substitution rates among sites (Yang 1994)                  V      accounts for ascertainment bias (Lewis 2001)                  P      partitions the data based on the number of character states      The assumptions of the morphological models used in this tutorial.Assessing Model Fit with Posterior PredictionHere we will highlight the main parts of the analysis. The morpholgoical data used here is of a family of extinct birds from the Miocene (Agnolin 2007). The data set contains 12 taxa, 51 characters with a maximum 4 character states. To quickly run the entire PPS pipeline you can run the pps_analysis_Mk.Rev script. To run this type the following command into RevBayessource(\"scripts/pps_analysis_Mk.Rev\")The test statistics and P-values can then be calculated by running the Test-stats.r scripts in r studio.Empirical MCMC AnalysisThe initial step in PPS involves generating a posterior distribution from which wewill later sample parameter values from, in order to simulated new data sets. Here we will specify our dataset, evolutionary model, and run a normal MCMC analysis.This code is in the pps_analysis_Mk.Rev script.Set up the workspaceFirst, let’s set up some workspace variables we’ll need, and define which substitutionmodel we will use for the analysis, either Mk or MkVP+G.analysis_name = \"pps_morpho_example\"model_name = \"Mk\"model_file_name = \"scripts/\" +model_name+\"_Model.Rev\"Next specify and read in the data file.inFile = \"data/Agnolin_2007a_paleobiodb.nex\"morpho &lt;- readDiscreteCharacterData(inFile)   Successfully read one character matrix from file data/Agnolin_2007a_paleobiodb.nex\"Specify the maximum number of states in the morphological alignment. For morphological datasets this value will change depending on the data set. We need this to construct a Q-matrix of the correct size.num_states =\"4\"Create vectors to store your moves and monitors for the analysis.moves    = VectorMoves()monitors = VectorMonitors()For the rest of the tutorial you can choose to either use the source() function to run the scripts or copy and paste the each line into revbayes as they are explained.Specifying the modelHere we will specify both the tree model and the substitution model. Typing the following command into RevBayes will run the model script.source( model_file_name )Processing file \"scripts/Mk_Model.Rev\"Processing of file \"scripts/Mk_Model.Rev\" completed Opening the model file, we can see how we have set it up.First we define some helper variables.taxa &lt;- morpho.names()num_taxa &lt;- taxa.size()num_branches &lt;- 2 * num_taxa - 3The next part of the file specifies the tree model.We put a prior on the branch lengths using an exponential distribution with a rate 0.2.br_len_lambda ~ dnExp(0.2)moves.append(mvScale(br_len_lambda, weight=2))We then set a uniform tree prior for the tree topology. This prior assumes that no tree is more likely a priori than any other.phylogeny ~ dnUniformTopologyBranchLength(taxa, branchLengthDistribution=dnExponential(br_len_lambda))moves.append(mvNNI(phylogeny, weight=num_branches/2.0))moves.append(mvSPR(phylogeny, weight=num_branches/10.0))moves.append(mvBranchLengthScale(phylogeny, weight=num_branches))tree_length := phylogeny.treeLength()In this script we are specifying an Mk model, where k is the maximum number ofobserved states. As the Mk model is a generalization of the JC model this is done using the fnJC() function. This creates a Q-matrix of size num_states. The resulting Q-matrix will have equal transition probabilities between all states.Q := fnJC(int(num_states)) We then create a PhyloCTMC object. This joins all the model parameters we created to modelthe morphological data. The is then clamped to the morphological evolution model.seq ~ dnPhyloCTMC(tree=phylogeny, Q=Q, type=\"Standard\")seq.clamp(morpho)Finally, we can create a workspace model variable using the model() function. We provide the model function a single node here using (phylogeny).mymodel = model(phylogeny)Run the MCMCTo run the analysis, we need to set up the MCMC. As with the previous scripts you can run this by typing the following command in RevBayessource(\"scripts/pps_MCMC.Rev\")In this script, we are specifying the number of iterations, the monitors and the burn.For this analysis we add a different file than the normal set up, the .var file. This file will be used in the next step to simulate the new data sets.monitors.append( mnModel(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.log\",printgen=10, separator = TAB)) monitors.append( mnFile(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.trees\",printgen=10, separator = TAB, phylogeny) )monitors.append( mnScreen(printgen=1000, tree_length) )monitors.append( mnStochasticVariable(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.var\",printgen=10) )Now that we have our model, moves, and monitors set up we can define our MCMC. This is done using the mcmc() function. nruns specifiies that there are two independent mcmc runs during the analysis.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")mymcmc.burnin(generations=200,tuningInterval=200)Running burn-in phase of Monte Carlo sampler for 200 iterations.   This simulation runs 2 independent replicates.   The simulator uses 4 different moves in a random move schedule with 30.7 moves per iteration   Progress:0---------------25---------------50---------------75--------------100********************************************************************To start the MCMC then run the following commandmymcmc.run(generations=10000,tuningInterval=200)Running MCMC simulation   This simulation runs 2 independent replicates.   The simulator uses 4 different moves in a random move schedule with 30.7 moves per iteration   Iter        |      Posterior   |     Likelihood   |          Prior   |    tree_length   |    elapsed   |        ETA   |-----------------------------------------------------------------------------------------------------------------------0           |       -996.887   |        -1026.4   |        29.5141   |       5.267817   |   00:00:00   |   --:--:--   |1000        |       -991.783   |       -1020.93   |        29.1461   |       5.635822   |   00:00:05   |   --:--:--   |2000        |        -993.55   |       -1023.25   |        29.7004   |       5.081564   |   00:00:11   |   00:00:44   |3000        |       -984.413   |       -1014.64   |        30.2221   |       4.559854   |   00:00:16   |   00:00:37   |4000        |       -986.254   |       -1016.16   |        29.9079   |       4.874004   |   00:00:22   |   00:00:33   |Once the analysis is finished you can generate a most credible clade summary tree using the mccTree() function.trace = readTreeTrace(\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.trees\")mccTree(trace, file=\"output_\" + model_name + \"/MCC.tre\")MCMC outputAs output from the analysis, you should have folder named output_Mk in your current directory. This file contains the log files and trees from both runs and a combination of both. You should use programs like Tracer to check for convergence at this stage.Before moving on to the next section it is important to clear the RevBayes environment of all the variables created during the previous step. This can be done using the following clear() function.clear()Posterior Predictive Data SimulationIn this section we will simulate new data sets based on the parameters sampled in the .varfile in part one of this tutorial.We first have to load the data and set up the model as we did in the previous step. As we went through this above you can simply copy and paste this into the terminal.inFile = \"data/Agnolin_2007a_paleobiodb.nex\"analysis_name = \"pps_morpho_example\"morpho &lt;- readDiscreteCharacterData(inFile)model_name = \"Mk\"num_states =\"4\"source( model_file_name )The script pps_Simulation.Rev contains the code to simulate new data.First we read in the trace file created during the previous MCMC.trace = readStochasticVariableTrace(\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.var\", delimiter=TAB)Now we use the posdteriorPredictiveSimulation() function to set up the simulations. We provide the function with the model, output directory, and the trace file.pps = posteriorPredictiveSimulation(mymodel, directory=\"output_\" + model_name + \"/\" + analysis_name + \"_post_sims\", trace)To start the simulation we use the pps.run() function. Here we can specify how many data sets we want to simulate by setting the thinning. The .var file contains a number of phylogenetic trees. If thinning is set to the default (0) the pps.run() function will simulate data for each tree. Setting thinning to 2 will simulate along every second tree and so on. In this way setting thinning = 2 simulates data for half of the trees in the .var file. Simulating 500 data sets has been found to be sufficient for these types of analysis. If you increase the number of iterations in the MCMC you may want to increase the thinning value here. Here we set thinning to 4.pps.run(thinning=4)Once you run this command a file should be created in the output_Mk directory called pps_morpho_example_post_sims.This is where all of the data simulated in revbayes will be stored. This simulation step should only take a few minutes.Calculating the P-values and effect sizes for the Test StatisticsTo determine if a model is adequate or not we need to compare the empirical data with the newly simulated data sets. If the simulated data sets are not significantly different from the empirical, we can conclude that the model is adequate for the data set. Here we use two test statistics, Consistency Index and Retention Index.Consistency Index is a measure of homoplasy in the data and retention index measures the degree to which potential synapomorphy is exhibited on the tree. These test statistics are calculated in R using the Test-stats.r script.This script calculates posterior P-values and effect sizes to determine the adequacy of the model. Here we calculate 4p-values, lower 1-tailed, Upper 1-tailed, 2-tailed, and the mid-point p-value. Values of less than 0.025 or greater the 0.975 indicate significance. The effect sizes can also be used to determine the adequacy of a model, though the p-values are considered more conservative. For the effect sizes, if the maximum and minimum value range through zero a model can be considered adequate.Visualising the resultsHistrograms show the distribution of values calculated for each test statistic from the simulated data sets. The green line shows the empirical value while the blue shows the median simulated value. The box plots show the effect sizes calculated for the test statistics. The median value should be closer to zero, models are considered adequate if the min and max values are range through zero.The Mk model would not be considered adequate for this data set. While it does well according to the retention index, the consistency index determined the simulated data to be significantly different to the empirical.Additional Exercise  Now that you know the Mk model is not appropriate for the data set try the MkVP+G model. This model is more complex than the Mk as it relaxes some of the assumption. Is this model adequate for the data set? Use the following command to rerun the entire analysis using the other model.source(\"scripts/pps_analysis_MkVP+G\")  Try testing the models with the other data set in this tutorial. This morpholgoical data used here is of Proviverrine hyaenodontids (an extinct family of  hypercarnivorous mammals) from (Egi et al. 2005). The data set contains 15 taxa, 65 characters with a maximum of 5 character states. Are is the same model adequate for this data? Remember to change the file names and the number of states to match the new dataset!",
        "url": "/tutorials/pps_morpho/",
        "index": "true"
      }
      ,
    
      "tutorials-intro": {
        "title": "Getting Started with RevBayes and Rev Language Syntax",
        "content": "OverviewThis is the very first tutorial for you in RevBayes. The goal of this set of tutorials is getting you started and familiar with the basics in RevBayes. If you have some familiarity with R or similar software, then this should be straight forward. Nevertheless, we recommendyou to work through these tutorials to learn all the specific quirks of RevBayes.Tutorials  Click on the first exercise to begin!  Installing RevBayes and first steps  Basic commands in RevBayes with Rev  Introduction to graphical models",
        "url": "/tutorials/intro/",
        "index": "true"
      }
      ,
    
      "tutorials-clocks": {
        "title": "Relaxed Clocks &amp; Time Trees",
        "content": "IntroductionCentral among the questions explored in biology are those that seek tounderstand the timing and rates of evolutionary processes. Accurateestimates of species divergence times are vital to understandinghistorical biogeography, estimating diversification rates, andidentifying the causes of variation in rates of molecular evolution.This tutorial will provide a general overview of divergence timeestimation using fossil calibration and relaxed-clock model comparisonin a Bayesian framework. The exercise will guide you through the stepsnecessary for estimating phylogenetic relationships and dating speciesdivergences using the program RevBayes (Höhna et al. 2014; Höhna et al. 2016).Getting StartedThe various exercises in this tutorial take you through the stepsrequired to perform phylogenetic analyses of the example datasets. Inaddition, we have provided the output files for every exercise so youcan verify your results. (Note that since the MCMC runs you perform willstart from different random seeds, the output files resulting from youranalyses will not be identical to the ones we provide you.)Download the data files listed above.The alignment in file, data/bears_irbp.nex, contains interphotoreceptorretinoid-binding protein (irbp) sequences for each extant species.In this exercise, we will compare among different relaxed clock modelsand estimate a posterior distribution of calibrated time trees. Thedataset we will use is an alignment of 10 caniform sequences, comprising8 bears, 1 spotted seal, and 1 gray wolf. Additionally, we will use theoccurrence time of the caniform fossil Hesperocyon gregarius to informour prior on the root age of the tree(i.e., the most-recent-common ancestor ofcaniforms). Creating Rev FilesThis tutorial sets up three different relaxed clock models and acalibrated birth-death model. Because of the complexity of the variousmodels, this exercise is best performed by specifying the models andsamplers in different Rev files. At the beginning of each section, youwill be given a suggested name for each component file; these namescorrespond to the provided Rev scripts that reproduce these commands.Directory StructureThis tutorial assumes that you have a very specific directory structurewhen running RevBayes. First, you may want to put theRevBayes binary in your path if you’re using a Unix-based operatingsystem. Alternatively, you can place the binary in a directory fromwhich you will execute RevBayes, e.g., the tutorial directory. Thetutorial directory can be any directory on your file system, but you maywant to create a new one so that you avoid conflicts with otherRevBayes tutorials.  Create a directory for this tutorial called RB_ClockModels_Tutorial (or any name you like), andnavigate to that directory. This is the tutorial directory mentionedabove.For this exercise, the Rev code provided assumes that within thetutorial directory exists subdirectories. These directories must havethe same names given here, unless you wish to also change the Rev codeto conform to your specific directory names.The first subdirectory will contain the data files (downloaded inthe Getting Started section).  Create a directory called data in your tutorial directory.  Save the tree and alignment files downloaded above (Getting Started) in the data directory.The second subdirectory will contain the Rev files you write toexecute the exercises in this tutorial.  Create a directory called scripts in your tutorial directory.This tutorial will guide you through creating all of the files necessaryto execute the analyses without typing the Rev language syntaxdirectly in the RevBayes console. Since the scripts must point tomodel and analysis files in a modular way, it is important to be awareof you directory structure and if you choose to do something different,make sure that the file paths given throughout the tutorial are correct.Finally, we’ll need a directory for all of the files written by ouranalyses. For some operations, RevBayes can create this directory onthe fly for you. However, it may be safer just to add it now.  Create a directory called output in your tutorial directory.The only files you need for this exercise are now in the datadirectory. Otherwise, you will create all of the Rev files specifyingthe models and analyses. The Birth-Death ModelThe birth-death process we will use is a constant-rate processconditioned on the age of the root of the tree ().The graphical model representation ofthe birth-death process conditioned on the root age in RevBayes.Create the Rev File  Open your text editor and create the birth-death model file called m_BDP_bears.Rev inthe scripts directory.  Enter the Rev code provided in this section in the new model file.Read in a Tree from a Previous StudySometimes it is convienent to read in a tree from a previous study. Thiscan be used as a starting tree or if there are nodes in the tree fromthe previous study that we wish to compare our estimates to. We willread in the tree estimated by (dos Reis et al. 2012).T &lt;- readTrees(\"data/bears_dosReis.tre\")[1]From the tree we can initialize some useful variables. (These can alsobe created from the data matrix using the same methods.)n_taxa &lt;- T.ntips()taxa &lt;- T.taxa()Finally, we initialize a variable for our vector of moves and monitors.moves    = VectorMoves()monitors = VectorMonitors()Birth-Death ParametersWe will begin by setting up the model parameters and proposal mechanismsof the birth-death model. We will use the parameterization of the birth-death process specifyingthe diversification and turnover. For a more detailed tutorial on thesimple birth-death model, please refer to the Simple Diversification Rate Estimationtutorial.DiversificationDiversification ($d$) is the speciation rate ($\\lambda$) minus theextinction rate ($\\mu$): $d = \\lambda - \\mu$.diversification ~ dnExponential(10.0) moves.append( mvScale(diversification, lambda=1.0, tune=true, weight=3.0) )TurnoverTurnover is: $r = \\mu / \\lambda$.turnover ~ dnBeta(2.0, 2.0) moves.append( mvSlide(turnover,delta=1.0,tune=true,weight=3.0) )Deterministic Nodes for Birth and Death RatesThe birth rate and death rate are deterministic functions of thediversification and turnover. First, create a deterministic node for$1 - r$, which is the denominator for each formula.denom := abs(1.0 - turnover) Now, the rates will both be positive real numbers that are variabletransformations of the stochastic variables.birth_rate := diversification / denomdeath_rate := (turnover * diversification) / denomSampling ProbabilityFix the probability of sampling to a known value. Since there areapproximately 147 described caniform species, we will create a constantnode for this parameter that is equal to 10/147.rho &lt;- 0.068Prior on the Root NodeThe fossil Hesperocyon gregarius is a fossil descendant of themost-recent common ancestor of all caniformes and has an occurrence timeof $\\sim$38 Mya. Thus, we can assume that the probability of the rootage being younger than 38 Mya is equal to 0, using this value to offseta prior distribution on the root-age.First specify the occurrence-time of the fossil.tHesperocyon &lt;- 38.0We will assume a lognormal prior on the root age that is offset by theobserved age of Hesperocyon gregarius. We can use the previousanalysis by (dos Reis et al. 2012) to parameterize the lognormal prior on the roottime. The age for the MRCA of the caniformes reported in their study was$\\sim$49 Mya. Therefore, we can specify the mean of our lognormaldistribution to equal $49 - 38 = 11$ Mya. Given the expected value ofthe lognormal (mean_ra) and a standard deviation (stdv_ra), we canalso compute the location parameter of the lognormal (mu_ra).mean_ra &lt;- 11.0stdv_ra &lt;- 0.25mu_ra &lt;- ln(mean_ra) - ((stdv_ra*stdv_ra) * 0.5)With these parameters we can instantiate the root age stochastic nodewith the offset value.root_time ~ dnLognormal(mu_ra, stdv_ra, offset=tHesperocyon)Time Tree Stochastic NodeNow that we have specified all of the parameters of the birth-deathprocess, we can create our stochastic node representing the treetopology and divergence times.timetree ~ dnBDP(lambda=birth_rate, mu=death_rate, rho=rho, rootAge=root_time, samplingStrategy=\"uniform\", condition=\"nTaxa\", taxa=taxa)Creating a Node-Age VariableWe may be interested in a particular node in the tree and thus wish tosave the age of that node to a log file. To do this, we can create adeterministic node for that node age. First, define the node by a set oftaxa using the clade() function. This will not restrict this node tobe monophyletic, but just create a node that is the MRCA of the taxalisted (even if that node has descendants that are not named).clade_Ursidae &lt;- clade(\"Ailuropoda_melanoleuca\",\"Tremarctos_ornatus\",\"Helarctos_malayanus\", \"Ursus_americanus\",\"Ursus_thibetanus\",\"Ursus_arctos\",\"Ursus_maritimus\",\"Melursus_ursinus\")Once we have defined the node, we can create a deterministic node tomonitor its age.tmrca_Ursidae := tmrca(timetree,clade_Ursidae)Proposals on the Time TreeNext, create the vector of moves. These tree moves act on node ages:moves.append( mvNodeTimeSlideUniform(timetree, weight=30.0) ) )moves.append( mvSlide(root_time, delta=2.0, tune=true, weight=10.0) )moves.append( mvScale(root_time, lambda=2.0, tune=true, weight=10.0) )moves.append( mvTreeScale(tree=timetree, rootAge=root_time, delta=1.0, tune=true, weight=3.0) )Then, we will add moves that will propose changes to the tree topology.moves.append( mvNNI(timetree, weight=8.0) )moves.append( mvNarrow(timetree, weight=8.0) )moves.append( mvFNPR(timetree, weight=8.0) )Now save and close the file. This file, with all the modelspecifications will be loaded by other Rev files.Specifying Branch-Rate ModelsThe next sections will walk you through setting up the files specifyingdifferent relaxed clock models. Each section will require you to createa separate Rev file for each relaxed clock model, as well as for eachmarginal-likelihood analysis.The Global Molecular Clock ModelThe global molecular clock assumes that the rate of substitution isconstant over the tree and over time.Create the Rev FileOpen your text editor and create the global molecular clock model filecalled in the scripts directory.Enter the Rev code provided in this section in the new model file.Keep in mind that we are creating modular model files that can besourced by different analysis files. Thus, the Rev code below willstill depend on variable initialized in different files.The Clock-RateThe clock-rate parameter is a stochastic node from a gamma distribution.clock_rate ~ dnGamma(2.0,4.0)moves.append( mvScale(clock_rate,lambda=0.5,tune=true,weight=5.0) )The Sequence Model and Phylogenetic CTMCSpecify the parameters of the GTR model and the moves to operate onthem.    sf ~ dnDirichlet(v(1,1,1,1))    er ~ dnDirichlet(v(1,1,1,1,1,1))    Q := fnGTR(er,sf)    moves.append( mvSimplexElementScale(er, alpha=10.0, tune=true, weight=3.0) )    moves.append( mvSimplexElementScale(sf, alpha=10.0, tune=true, weight=3.0) )And instantiate the phyloCTMC.    phySeq ~ dnPhyloCTMC(tree=timetree, Q=Q, branchRates=clock_rate, nSites=n_sites, type=\"DNA\")    phySeq.clamp(D)This is all we will include in the global molecular clock model file.Save and close the file called in the scripts directory.Estimate the Marginal LikelihoodNow we can use the model files we created and estimate the marginallikelihood under the global molecular clock model (and all other modelsettings). You can enter the following commands directly in theRevBayes console, or you can create another Rev script.Open your text editor and create the marginal-likelihood analysis fileunder the global molecular clock model. Call the file: and save it inthe scripts directory.Load Sequence Alignment — Read in the sequences and initializeimportant variables.D &lt;- readDiscreteCharacterData(file=\"data/bears_irbp.nex\")n_sites &lt;- D.nchar()mi = 1The Calibrated Time-Tree Model — Load the calibrated tree model fromfile using the source() function. Note that this file does not havemoves that operate on the tree topology, which is helpful when you planto estimate the marginal likelihoods and compare different relaxed clockmodels.source(\"scripts/m_BDP_bears.Rev\")Load the GMC Model File — Source the file containing all of theparameters of the global molecular clock model. This file is called .source(\"scripts/m_GMC_bears.Rev\")We can now create our workspace model variable with our fully specifiedmodel DAG. We will do this with the model() function and provide asingle node in the graph (er).mymodel = model(er)Run the Power-Posterior Sampler and Compute the Marginal Likelihoods —With a fully specified model, we can set up the powerPosterior()analysis to create a file of ‘powers’ and likelihoods from which we canestimate the marginal likelihood using stepping-stone or path sampling.This method computes a vector of powers from a beta distribution, thenexecutes an MCMC run for each power step while raising the likelihood tothat power. In this implementation, the vector of powers starts with 1,sampling the likelihood close to the posterior and incrementallysampling closer and closer to the prior as the power decreases.First, we initialize a monitor which will log the MCMC samples for eachparameter at every step in the power posterior.monitors[1] = mnModel(filename=\"output/GMC_posterior_pp.log\",printgen=10, separator = TAB)Next, we create the variable containing the power posterior. Thisrequires us to provide a model and vector of moves, as well as an outputfile name. The cats argument sets the number of power steps. Once wehave specified the options for our sampler, we can then start the runafter a burn-in/tuning period.pow_p = powerPosterior(mymodel, moves, monitors, \"output/GMC_bears_powp.out\", cats=50, sampleFreq=10) pow_p.burnin(generations=5000,tuningInterval=200)pow_p.run(generations=1000)  Compute the marginal likelihood using two different methods,stepping-stone sampling and path sampling.ss = steppingStoneSampler(file=\"output/GMC_bears_powp.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")ss.marginal() ### use path sampling to calculate marginal likelihoodsps = pathSampler(file=\"output/GMC_bears_powp.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")ps.marginal() If you have entered all of this directly in the RevBayes console, youwill see the marginal likelihoods under each method printed to screen.Otherwise, if you have created the separate Rev file in the scriptsdirectory, you now have to directly source this file in RevBayes(after saving the up-to-date content).Begin by running the RevBayes executable. In Unix systems, type thefollowing in your terminal (if the RevBayes binary is in your path):Now load your RevBayes analysis:source(\"scripts/mlnl_GMC_bears.Rev\")Once you have completed this analysis, record the marginal likelihoodsunder the global molecular clock model in Table .The Uncorrelated Lognormal Rates ModelThe uncorrelated lognormal (UCLN) model relaxes the assumption of asingle-rate molecular clock. Under this model, the rate associated witheach branch in the tree is a stochastic node. Each branch-rate variableis drawn from the same lognormal distribution.Given that we might not have prior information on the parameters of thelognormal distribution, we can assign hyper priors to these variables.Generally, it is more straightforward to construct a hyperprior on theexpectation (i.e., the mean) of a lognormal density rather than thelocation parameter $\\mu$. Here, we will assume that the mean branch rateis exponentially distributed and as is the stochastic node representingthe standard deviation. With these two parameters, we can get thelocation parameter of the lognormal by:\\(\\mu = \\log(M) - \\frac{\\sigma^2}{2}.\\) Thus, $\\mu$ is a deterministicnode, which is a function of $M$ and $\\sigma$. We can represent the vector of $N$ branch rates usingthe plate notation.Create the Rev FileOpen your text editor and create the uncorrelated-lognormalrelaxed-clock model file called in the scripts directory.Enter the Rev code provided in this section in the new model file.Keep in mind that we are creating modular model files that can besourced by different analysis files. Thus, the Rev code below willstill depend on variable initialized in different files.Independent Branch RatesBefore we can set up the variable of the branch-rate model, we must knowhow many branches exist in the tree.n_branches &lt;- 2 * n_taxa - 2We will start with the mean of the lognormal distribution.ucln_mean ~ dnExponential(2.0)And the exponentially distributed node representing the standarddeviation. We will also create a deterministic node, which is thevariance, $\\sigma^2$.ucln_sigma ~ dnExponential(3.0)ucln_var := ucln_sigma * ucln_sigmaNow we can declare the function that gives us the $\\mu$ parameter of thelognormal distribution on branch rates.ucln_mu := ln(ucln_mean) - (ucln_var * 0.5)The only stochastic nodes we need to operate on for this part of themodel are the lognormal mean ($M$ or ucln_mean) and the standarddeviation ($\\sigma$ or ucln_sigma).moves.append( mvScale(ucln_mean, lambda=1.0, tune=true, weight=4.0))moves.append( mvScale(ucln_sigma, lambda=0.5, tune=true, weight=4.0))With our nodes representing the $\\mu$ and $\\sigma$ of the lognormaldistribution, we can create the vector of stochastic nodes for each ofthe branch rates using a for loop. Within this loop, we also add themove for each branch-rate stochastic node to our moves vector.for(i in 1:n_branches){   branch_rates[i] ~ dnLnorm(ucln_mu, ucln_sigma)   moves.append( mvScale(branch_rates[i], lambda=1, tune=true, weight=2.))}Sidebar: Other Uncorrelated-Rates ModelsThe choice in the branch-rate prior does not necessarily have to be alognormal distribution. Depending on your prior beliefs about how branchrates vary across the tree, the rates can just as easily be assigned anexponential distribution (e.g., defines each branch rate as anindependent draw from an exponential distribution centered on 1) or agamma distribution (e.g., defines each branch rate as an independentdraw from a gamma distribution centered on 0.5) or any otherdistribution on positive-real numbers. The exercises outlined in thistutorial demonstrate how to compare different models of branch-ratevariation using Bayes factors, and it may also be important to consideralternative priors on branch rates using these approaches. Importantly,RevBayes is flexible enough to make the process of comparing thesemodels very straightforward. For the purposes of this exercise,specify a lognormal prior on the branch rates.Because we are dealing with semi-identifiable parameters, it often helpsto apply a range of moves to the variables representing the branch ratesand branch times. This will help to improve the mixing of our MCMC. Herewe will add 2 additional types of moves that act on vectors.moves.append( mvVectorScale(branch_rates,lambda=1.0,tune=true,weight=2.0) )moves.append( mvVectorSingleElementScale(branch_rates,lambda=30.0,tune=true,weight=1.0) )The mean of the branch rates is a convenient deterministic node tomonitor, particularly in the screen output when conducting MCMC.mean_rt := mean(branch_rates) The Sequence Model and Phylogenetic CTMCNow, specify the stationary frequencies and exchangeability rates of theGTR matrix.sf ~ dnDirichlet(v(1,1,1,1))er ~ dnDirichlet(v(1,1,1,1,1,1))Q := fnGTR(er,sf)moves.append( mvSimplexElementScale(er, alpha=10.0, tune=true, weight=3.0))moves.append( mvSimplexElementScale(sf, alpha=10.0, tune=true, weight=3.0))Now, we can put the whole model together in the phylogenetic CTMC andclamp that node with our sequence data.phySeq ~ dnPhyloCTMC(tree=timetree, Q=Q, branchRates=branch_rates, nSites=n_sites, type=\"DNA\")attach the observed sequence dataphySeq.clamp(D)Save and close the file called in the scripts directory.Estimate the Marginal LikelihoodJust as we did for the strict clock model, we can execute apower-posterior analysis to compute the marginal likelihood under theUCLN model.Open your text editor and create the marginal-likelihood analysis fileunder the global molecular clock model. Call the file: and save it inthe scripts directory.Refer to the section describing this process for the GMC model above.Write your own Rev language script to estimate the marginal likelihoodunder the UCLN model. Be sure to change the file names in all of therelevant places (e.g., your output file for the powerPosterior()function should be and be sure to source() the correct model file ).Once you have completed this analysis, record the marginal likelihoodsunder the UCLN model in Table .Compute Bayes Factors and Select ModelNow that we have estimates of the marginal likelihood under each of ourdifferent models, we can evaluate their relative plausibility usingBayes factors. Use Table  to summarize the marginallog-likelihoods estimated using the stepping-stone and path-samplingmethods.            Model      Path-Sampling      Stepping-Stone-Sampling                  Global molecular clock ($M_0$)                            Uncorrelated lognormal                            Supported model?                    Marginal likelihoods of the lobal molecular clock and uncorrelated lognormal models.Phylogenetics software programs log-transform the likelihood to avoidunderflow, becausemultiplying likelihoods results in numbers that are too small to be heldin computer memory. Thus, we must calculate the ln-Bayes factor (we willdenote this value $\\mathcal{K}$):\\[\\begin{equation}\\mathcal{K}=\\ln[BF(M_0,M_1)] = \\ln[\\mathbb{P}(\\mathbf X \\mid M_0)]-\\ln[\\mathbb{P}(\\mathbf X \\mid M_1)],\\label{LNbfFormula}\\end{equation}\\]where $\\ln[\\mathbb{P}(\\mathbf X \\mid M_0)]$ is the marginal lnLestimate for model $M_0$. The value resulting from equation can be converted to a raw Bayes factor by simply takingthe exponent of $\\cal{K}$\\[\\begin{equation}BF(M_0,M_1) = e^{\\cal{K}}.\\label{LNbfFormula2}\\end{equation}\\]Alternatively, you can directly interpret the strength of evidence in favor of $M_0$ in logspace by comparing the values of $\\cal{K}$ to the appropriate scale(Table , second column). In this case, we evaluate $\\cal{K}$in favor of model $M_0$ against model $M_1$ so that:  if $\\mathcal{K} &gt; 1$, model $M_0$ is preferredif $\\mathcal{K} &lt; -1$, model $M_1$ is preferred.Thus, values of $\\mathcal{K}$ around 0 indicate that there is nopreference for either model.Using the values you entered in Table  and equation, calculate the ln-Bayes factors (using $\\mathcal{K}$)for the different model comparisons. Enter your answers in Table using the stepping-stone and the path-sampling estimates ofthe marginal log likelihoods.            Model      Path-Sampling      Stepping-Stone-Sampling                  $M_0,M_1$                            Supported model?                    Marginal likelihoods of the lobal molecular clock and uncorrelated lognormal models.Estimate the Topology and Branch TimesAfter computing the Bayes factors and determining the relative supportof each model, you can choose your favorite model among the three testedin this tutorial. The next step, then, is to use MCMC to jointlyestimate the tree topology and branch times.Open your text editor and create the MCMC analysis file under the yourfavorite clock model. Call the file: and save it in the scriptsdirectory.This file will contain much of the same initial Rev code as the filesyou wrote for the marginal-likelihood analyses.### Load the sequence alignmentD &lt;- readDiscreteCharacterData(file=\"data/bears_irbp.nex\")### get helpful variables from the datan_sites &lt;- D.nchar(1)### initialize an iterator for the moves vectormi = 1This is how you should begin your MCMC analysis file. The next step isto source the birth-death model.### set up the birth-death model from filesource(\"scripts/m_BDP_bears.Rev\")Next load the file containing your favorite model (where the wildcard\\* indicates the name of the model you prefer: GMC, UCLN, orACLN).### load the model from file source(\"scripts/m_*_bears.Rev\")### workspace model wrapper ###mymodel = model(er)MCMC MonitorsBefore you instantiate the MCMC workspace object, you need to create avector of “monitors” that are responsible for monitoring parametervalues and saving those to file or printing them to the screen.First, create a monitor of all the model parameters except thetimetree using the model monitor: mnModel. This monitor takes allof the named parameters in the model DAG and saves their value to afile. Thus, every variable that you gave a name in your model files willbe written to your log file. This makes it very easy to get an analysisgoing, but can generate very large files with a lot of redundant output.monitors.append(mnModel(filename=\"output/TimetTree_bears_mcmc.log\", printgen=10))If the model monitor is too verbose for your needs, you should use thefile monitor instead: mnFile. For this monitor, you have to providethe names of all the parameters you’re interested in after the file nameand print interval. (Refer to the example files for how to set up thefile monitor for model parameters.)In fact, we use the file monitor for saving the sampled chronograms tofile. It is important that you do not save the sampled trees in thesame file with other numerical parameters you would like to summarize.That is because tools for reading MCMC log files—likeTracer (Rambaut and Drummond 2011) —cannotload files with non-numerical states. Therefore, you must save thesampled trees to a different file.monitors.append(mnFile(filename=\"output/TimeTree_bears_mcmc.trees\", printgen=10, timetree))Finally, we will create a monitor in charge of writing information tothe screen: mnScreen. We will report the root age and the age of theMRCA of all Ursidae to the screen. If there is anything else you’d liketo see in your screen output (e.g., the mean rate of the UCLN or ACLNmodel), feel free to add them to the list of parameters give to thismodel.monitors.append(mnScreen(printgen=10, root_time, tmrca_Ursidae))Setting-Up &amp; Executing the MCMCNow everything is in place to create the MCMC object in the workspace.This object allows you to perform a burn-in, execute a run of a givenlength, continue an analysis that might not have reached stationarity,and summarize the performance of the various proposals.mymcmc = mcmc(mymodel, monitors, moves)With this object instantiated, specify a burn-in period that will sampleparameter space while re-tuning the proposals (only for the moves withtune=true). The monitors do not sample the states of the chain duringburn-in.mymcmc.burnin(generations=2000,tuningInterval=100)Once the burn-in is complete, we want the analysis to run the full MCMC.Specify the length of the chain.mymcmc.run(generations=5000)When the MCMC run has completed, it’s often good to evaluate theacceptance rates of the various proposal mechanisms. The.operatorSummary() member method of the MCMC object prints a tablesummarizing each of the parameter moves to the screen.mymcmc.operatorSummary()Summarize the Sampled Time-TreesDuring the MCMC, the sampled trees will be written to a file that wewill summarize using the mapTree function in RevBayes. This firstrequires that you add the code for reading in the tree-trace file andperforming an analysis of those trees.tt = readTreeTrace(\"output/TimeTree_bears_mcmc.trees\", \"clock\")tt.summarize()### write MAP tree to filemapTree(tt, \"output/TimeTree_bears_mcmc_MAP.tre\")Save and close the file called in the scripts directory. Then, executethe MCMC analysis using:",
        "url": "/tutorials/clocks/",
        "index": "true"
      }
      ,
    
      "tutorials-morph-ase": {
        "title": "Introduction to Discrete Morphology Evolution",
        "content": "IntroductionDiscrete morphological models are not only useful for tree estimation,as was done in Tutorial Discrete morphology - Tree Inference, but also to ask specific questions about the evolution of the morphological character of interest.Specifically, there are a few types of analyses that we might be interest in.First, we can test different models of morphological evolution,such as reversible and irreversible models, and estimate rates under these models.For example, using an irreversible model of evolution, we can test, for example,for Dollo’s law of a complex character that can be lost but not gained again (Goldberg and Igić 2008).Additionally, we might be interest in ancestral state estimation,or mapping transition on the phylogeny.Commonly the central problem in statistical phylogenetics concerns marginalizing over all unobserved character histories that evolved along the branches of a given phylogenetic tree according to some model, $M$, under some parameters, $\\theta$.This marginalization yields the probability of observing the tip states, $X_\\text{tip}$,given the model and its parameters,$P( X_\\text{tip} | \\theta, M ) = \\sum_{X_\\text{internal}} P( X_\\text{internal}, X_\\text{tip} \\mid \\theta, M )$.One might also wish to find the probability distribution of ancestral stateconfigurations that are consistent with the tip state distribution,$P( X_\\text{internal} \\mid X_\\text{tip}, \\theta, M )$, and to sampleancestral states from that distribution.This procedure is known as ancestral state estimation.%Finally, we might be interested in testing for correlated evolution between discrete morphological characters.%For example,%This tutorial will provide a discussion of modeling morphological charactersand ancestral state estimation, and will demonstrate how to perform suchBayesian phylogenetic analysis using RevBayes (Höhna et al. 2016).The data  Create a directory on your computer for this tutorial.In this directory, create a subdirectory called data, and download the data files that you can find on the left of this page.We have taken the phylogeny from Magnuson-Ford and Otto (2012), who took it from Vos and Mooers (2006) and then randomly resolved the polytomies using the method of Kuhn et al. (2011) and the trait data from Redding et al. (2010). In the data folder, you should now have the following files:  primates_tree.nex:Dated primate phylogeny including 233 out of 367 species.  primates_activity_period.nex:A file with the coded character states for primate species activity time. This character has just two states: 0 = diurnal and 1 = nocturnal.  primates_habitat.nex:A file with the coded character states for primate species habitay. This character has just two states: 0 = forest and 1 = savanna.  primates_solitariness.nex:A file with the coded character states for primate species social system type. This character has just two states: 0 = group living and 1 = solitary.  primates_terrestrially.nex:A file with the coded character states for primate species terrestrially. This character has just two states: 0 = arboreal and 1 = terrestrial.  primates_males.nex:A file with the coded character states for the number of males within a group per primate species. This character has four states: 0 = single male, 1 = single and multi male, and 2 = multi male.  primates_mating_system.nex:A file with the coded character states for primate species mating-system type. This character has four states: 0 = monogamy, 1 = polygyny, 2 = polygynandry, and 3 = polyandry.  primates_diet.nex:A file with the coded character states for primate species diet. This character has six states: 0 = frugivore, 1 = insectivore, 2 = folivore, 3 = gummnivore, 4 = omnivore, and 5 = gramnivore.Primate phylogeny with traits displayed at the tips from Magnuson-Ford and Otto (2012).ScriptsFor more complex models and analyses, it’s useful to create separate Rev scripts that contain all the model parameters, moves, and functions for different model components (e.g. the substitution model and the clock model).  Create another subdirectory called scripts.In this tutorial, you will work primarily in your text editor and create a script file that can be easily managed and modified. Examples of all the commands used to perform each analysis are also provided at the top of this page under Scripts but try to write the complete scripts yourself from the beginning to ensure you understand all the steps involved and the differences between setting up each analysis.Exercises  Click on the first exercise to begin!  Ancestral state estimation  Testing for independent rates  Testing for irreversibility  Stochastic Character Mapping and Testing for Rate Variation  Testing for Correlation between Characters",
        "url": "/tutorials/morph_ase/",
        "index": "true"
      }
      ,
    
      "tutorials-intro-posterior-prediction": {
        "title": "Introduction to Posterior Prediction",
        "content": "OverviewThis tutorial introduces the basic principles of posterior predictive model checking. The goal of posterior prediction is to assess the fit between a model and data by answering the following question: Could the model we’ve assumed plausibly have produced the data we observed?To perform posterior prediction, we simulate datasets using parameter values drawn from a posterior distribution. We then quantify some characteristic of both the simulated and empirical datasets using a test statistic (or a suite of test statistics), and we ask if the value of the test statistic calculated for the empirical data is a reasonable draw from the set of values calculated for the simulated data. If the empirical test statistic value is very different from the simulated ones, our model is not doing a good job of replicating some aspect of the process that generated our data.At the end of this tutorial, you should understand the goal of posterior predictive model checking and the steps involved to carry it out in RevBayes.IntroductionA good statistical model captures important features of observed data using relatively simple mathematical principles. However, a model that fails to capture some important feature of the data can mislead us. Therefore, it is important to not only compare the relative performance of models (i.e., model selection), but also to test the absolute fit of the best model (Bollback 2002; Brown 2014; Brown 2014; Höhna et al. 2018; Brown and Thomson 2018). If the best available model could not have plausibly produced our observed data, we should be cautious in interpreting conclusions based on that model.Posterior prediction is a technique to assess the absolute fit of a model in a Bayesian framework (Bollback 2002; Brown and Thomson 2018). Posterior prediction relies on comparing the observed data to data simulated from the model. If the simulated data are similar to the observed, the model could reasonably have produced our observations. However, if the simulated data consistently differ from the observed, the model is not capturing some feature of the data-generating process.An ExampleTo illustrate the steps involved in posterior prediction, we’ll begin with a non-phylogenetic example. Here, we will examine a hypothetical dataset of trait values sampled from a sexually dimorphic population. However, for the purposes of our tutorial, we will say that we do not yet realize that sexual dimorphism exists. This example is discussed further in Brown and Thomson (2018).A set of trait values sampled from a population with sexual dimorphism.A Single-Normal ModelTo start analyzing our data, we fit a single Normal distribution to our trait values using MCMC. This is a reasonable starting point, because we know that many continuous traits are polygenic and normally distributed. This single-normal model includes two free parameters that we estimate from the observed data: the mean and standard deviation.For the sake of brevity, we will not discuss here how to fit a model using MCMC, but if you are interested the trait values can be found in data.txt, the MCMC analysis can be found in MCMC_SingleNormal.rev, and the results of this analysis (i.e., posterior samples of the mean and standard deviation) can be found in singleNormal_posterior.log. Figure 2 shows some of the parameter values sampled for the mean and standard deviation during the MCMC analysis.MCMC samples from the posterior distribution of the mean and standard deviation for a single Normal distribution.Based on one set of sampled parameter values (one of the dots in Figure 2), Figure 3 shows the resulting Normal distribution compared to the population trait data:A single Normal distribution fit to the population trait values (based on one draw of the mean and standard deviation sampled during MCMC).In this case, it is visually obvious that there are some important differences between the model we’ve assumed and the trait data. However, we’d like a quantitative method to assess this fit. Also, in the case of more complicated models and data like we typically encounter in phylogenetics, visual comparisons are often not possible.Posterior Predictive SimulationNow that we’ve fit our single-normal model, we need to simulate posterior predictive datasets. Remember that these are datasets of the same size as our observed data, but simulated using means and standard deviations drawn from our posterior distribution.Simulation of posterior predictive datasets (shown in light gray) by drawing samples of means and standard deviations from the posterior distribution.The code for this simulation with the single-normal model can be found in pps_SingleNormal.rev.# Read in model log file from MCMCprint(\"Reading MCMC log...\")postVals &lt;- readMatrix(\"../data/singleNormal_posterior.log\")# Read in empirical dataprint(\"Reading empirical data...\")empDataMatrix &lt;- readMatrix(\"../data/data.txt\")# Convert empirical data format to a vectorfor (i in 1:empDataMatrix.size()){\tempData[i] &lt;- empDataMatrix[i][1] }# Simulate datasets and store in a vector# Note: We are using the first 49 values as burn-in and we start simulating with the#       50th value sampled during our MCMC analysis.print(\"Posterior predictive simulations...\")for (gen in 50:postVals.size()){\tsims[gen-49] &lt;- rnorm(n=empData.size(), mean=postVals[gen][5], sd=postVals[gen][6])}Test StatisticsTo quantitatively compare our empirical and simulated data, we need to use some test statistic (or suite of test statistics). These statistics numerically summarize different aspects of a dataset. We can then compare the empirical test statistic value to the posterior predictive distribution. For the case of our trait data, we will try four possible test statistics: the 1st percentile, mean, median, and 90th percentile.Four possible test statistics to summarize particular characteristics of a dataset of trait values.Code to calculate these test statistics can also be found in pps_SingleNormal.rev.print(\"Sorting...\")# Sort each simulated datasetfor ( s in 1:sims.size() ){\tsims[s] &lt;- sort(sims[s])}# Sort empirical datasetempData &lt;- sort(empData)# Define generic function to calculate a chosen percentile, pfunction percentile(p,nums){\tpos &lt;- round( (p/100) * nums.size() )\treturn nums[pos]}print(\"Calculating test statistics...\")# Calculate empirical test statisticsemp_mean &lt;- mean(empData)emp_median &lt;- median(empData)emp_1st &lt;- percentile(1,empData)emp_90th &lt;- percentile(90,empData)# Calculate simulated data test statisticsfor ( s in 1:sims.size() ){\tsim_means[s] &lt;- mean(sims[s])\tsim_medians[s] &lt;- median(sims[s])\tsim_1sts[s] &lt;- percentile(1,sims[s])\tsim_90ths[s] &lt;- percentile(90,sims[s])}Note that the calculation of percentiles is not built-in to RevBayes, which was why we created this custom functionfunction percentile(p,nums){\tpos &lt;- round( (p/100) * nums.size() )\treturn nums[pos]}P-values and Effect SizesWe typically summarize the comparison of test statistic values between empirical and posterior predictive datasets using either a posterior predictive p-value or an effect size.A posterior predictive p-value tells us how many of our simulated datasets have test statistic values that are as, or more, extreme than the empirical. While useful, these p-values are unable to distinguish between cases where the observed test statisic falls just a little bit outside the posterior predictive distribution from cases where there is a very big difference between the simulated and empirical values.Effect sizes allow us to distinguish those different possibilities (Doyle et al. 2015). An effect size is calculated as the difference between the empirical test statistic value and the median of the posterior predictive distribution, divided by the standard deviation of the posterior predictive distribution. In other words, how many standard deviations away from the median is the observed value?The comparison between test statistic values from empirical and posterior predictive datasets can be summarized using both posterior predictive p-values and effect sizes. Figure from Höhna et al. (2018).P-values and effect sizes are calculated in pps_SingleNormal.rev with this codefunction pVal(e,s){\tgreaterThan &lt;- 0\tfor (i in 1:s.size()){\t\tif (e &lt; s[i]){\t\t\tgreaterThan &lt;- greaterThan + 1\t\t}\t}\treturn greaterThan/s.size()}print(\"Calculating p-values and effect sizes...\")# Calculate posterior predictive p-valuesp_means &lt;- pVal(emp_mean,sim_means)p_medians &lt;- pVal(emp_median,sim_medians)p_1sts &lt;- pVal(emp_1st,sim_1sts)p_90ths &lt;- pVal(emp_90th,sim_90ths)print(\"\")print(\"---&gt; Posterior Prediction Model Assessment Results &lt;---\")print(\"\")# Print p-valuesprint(\"P-value mean:\",p_means)print(\"P-value median:\",p_medians)print(\"P-value 1st percentile:\",p_1sts)print(\"P-value 90th percentile:\",p_90ths)# Calculate effect sizes for test statisticseff_means &lt;- abs(emp_mean-median(sim_means))/stdev(sim_means)eff_medians &lt;- abs(emp_median-median(sim_medians))/stdev(sim_medians)eff_1sts &lt;- abs(emp_1st-median(sim_1sts))/stdev(sim_1sts)eff_90ths &lt;- abs(emp_90th-median(sim_90ths))/stdev(sim_90ths)# Print effect sizesprint(\"\")print(\"Effect size mean:\",eff_means)print(\"Effect size median:\",eff_medians)print(\"Effect size 1st percentile:\",eff_1sts)print(\"Effect size 90th percentile:\",eff_90ths)print(\"\")The results should look something like this---&gt; Posterior Prediction Model Assessment Results &lt;---P-value mean:\t0.4973767P-value median:\t0.9632739P-value 1st percentile:\t0.003147954P-value 90th percentile:\t0.3231899Effect size mean:\t0.001805498Effect size median:\t1.860846Effect size 1st percentile:\t2.034433Effect size 90th percentile:\t0.4808086Note that this script only calculates p-values as the percentage of posterior predictive values that are less than the empirical value. Formally, this is known as a lower one-tailed p-value. Therefore, p-values near either 0 or 1 indicate poor fit between our model and our empirical data.To run this entire posterior predictive analysis at once, you could use this command from inside the scripts folder for this tutorial.source(pps_SingleNormal.rev)These results can also be summarized graphically (although not in RevBayes) like thisComparison of empirical and posterior predictive test statistic values. The four test statistics are shown in different rows. Vertical arrows show empirical values, and distributions show simulated values. The lighter distributions were generated by posterior predictive simulation and the darker distributions were generated by parametric bootstrapping (similar to posterior prediction, but using only maximum likelihood parameter estimates for simulation). More detail is available in Brown and Thomson (2018).A Two-Normal ModelAll of the analyses above can be replicated using a model that correctly employs two independent Normal distributions to separately capture the distinct trait values for males and females. The simplest form of a two-normal model is a mixture distribution. The means and standard deviations of two Normal distributions are estimated separately and we calculate a likelihood that averages across the possibility of each individual belonging to each possible group.If you want to run an MCMC analysis under this two-normal mixture model, you can use the script MCMC_TwoNormals.rev. However, the MCMC output from this analysis is already available in twoNormals_posterior.log, so you do not need to rerun the MCMC if you don’t want to. You can perform just the posterior prediction steps using pps_TwoNormals.rev.After running posterior prediction for the two-normal model, compare your results to the one-normal model. Do these results indicate a better fit?Interpreting Posterior Predictive ResultsIn general, test statistics with intermediate p-values (close to 0.5) indicate good fit and should have relatively small effect sizes. In our results from the one-normal model, both the mean (p-value = 0.49, effect size = 0.03) and the 90th percentile (p-value = 0.68, effect size = 0.51) do not indicate big discrepancies between what we’ve observed and what we’ve simulated.However, the median (p-value = 0.046, effect size = 1.70) and 1st percentile (p-value = 0.996, effect size = 1.93) statistics have small and large p-values, respectively, and correspondingly large effect sizes. These results do indicate a discrepancy between the assumptions of the model and the data we’ve observed.Should we be concerned about our model? Two of our test statistics do not indicate poor fit, while two others do indicate poor fit.The answer depends on what we want to learn about our population. If we are interested in inferring or predicting the average trait value of our population, we seem to be doing fine. However, if we wanted to predict the trait value of any given individual drawn from the population, we will tend to overpredict individuals with intermediate values and underpredict individuals with extreme trait values. If we were interested in understanding an evolutionary process like stabilizing selection, we might also be very concerned. A single-normal model would suggest that our population has quite a lot of trait variation between individuals, when in fact most of that difference is between sexes. Individuals within a sex have much more limited variation.Moving to Phylogenetic Posterior Prediction (P3)After going through this tutorial, you have hopefully gained a sense for the overall workflow of posterior prediction:  Fit a model (with MCMC)  Simulate posterior predictive datasets by drawing parameter values from the posterior  Compare simulated and empirical datasets using your chosen test statisticsA few important, general take-home points are:  Posterior prediction is a way to assess the absolute fit of a model to your data.  There is no single correct test statistic to use for posterior prediction.          Some statistics may be more sensitive than others to different kinds of model violations.        How to best proceed if you identify model violations will depend on the goals of your analysis.          Is the type of model violation you’ve identified relevant to the question you’re asking?      Is the model a better fit for some parts of your dataset than others?      Can you think of ways to extend your model that might provide a better fit to your data?      The scripts used for this introductory tutorial made the posterior predictive workflow as obvious as possible, but required us to code each step individually. Because some of these steps are more complicated or tedious for phylogenetic applications, the developers of RevBayes have packaged them in a pipeline called P3 (for Phylogenetic Posterior Prediction). The following tutorials will guide you through the use of P3. These tutorials are divided by the types of test statistics used - whether based solely on the data (data-based) or whether conditioned on the assumed model (inference-based):Assessing Phylogenetic Reliability Using RevBayes and P3 (Data Version)Assessing Phylogenetic Reliability Using RevBayes and P3 (Inference Version)PhyloseminarA recent phyloseminar discusses the example used in this tutorial and other aspects of posterior prediction:The Role of Model Fit in Resolving the Tree of Life",
        "url": "/tutorials/intro_posterior_prediction/",
        "index": "true"
      }
      ,
    
      "tutorials-timefig-dating": {
        "title": "Biogeographic dating using the TimeFIG model",
        "content": "OverviewThis tutorial demonstrates how to perform a biogeographic dating analysis using the TimeFIG model. Dating with TimeFIG jointly estimates phylogenetic divergence times, molecular substitution rates, biogeographic rates, and ancestral species ranges. The tutorial provides a conceptual background on challenges related to dating molecular phylogenies, alternative techniques for dating, and a framework to assess the sensitivity of divergence time estimates to methodological assumptions.This tutorial has three parts:  Part 1: Molecular phylogenetics (Day 1 of workshop)  Part 2: Biogeographic dating with node calibrations (Day 2 of workshop)  Part 3: Biogeographic dating with TimeFIG (Day 2 of workshop)include_all: falseAs with previous tutorials in this series, we will analyze a dataset for Hawaiian Kadua plant species. All input datasets are the same as before, with the addition of 10 molecular loci obtained from the Angiosperms353 protocol.Because these analyses build on each other, the tutorial focuses on what changes between the scripts. This tutorial is also bundled with RevBayes scripts that complete analyses equivalent to those written below. However, the scripts are often designed to be more modular and general, making them ideal to customize for analyses of new datasets, other than Hawaiian Kadua.  Important dataset and script!  Tutorial files: This tutorial requires numerous input files and scripts to run properly. Download and unzip the timefig_dating_project.zip archive to create a project directory named timefig_dating_project. Enter the new directory to run the tutorial exercises. Analysis scripts are available on the sidebar for easy access.  Important version info!  Note: This tutorial currently requires specific versions of RevBayes and TensorPhylo to run properly (see linked branches and commits).  We recommend that you complete the tutorial using a PhyloDocker container, which is pre-configured with the above versions of RevBayes and TensorPhylo. Instructions to install and use PhyloDocker are here: link.Molecular phylogeneticsMolecular phylogenetic models use these estimates of genetic distance to infer phylogenetic divergence, in terms of topology (relationships among lineages) and branch lengths (distances along lineages). Phylogenetic models often measure genetic distances in units of expected substitutions per site. These distances are, in fact, the product of time (e.g. millions of years) and the substitution rate (substitutions per site per time). This substitution rate is often called the molecular clock (missing reference).An accurate estimate of such a clock rate would enable molecular dating, because a research would only need to convert molecular distances into clock “ticks” to estimate the ages of species in units of geological time. Unfortunately, it is difficult if not impossible to measure the molecular clock rate in units of geological time across all species and genes from sequence data alone. For instance, scenarios of fast molecular evolution over a short time and slow evolution over a long time produce the same expected numbers of events. This is known as rate-time non-identifiability. The phylogenetic model assigns equal likelihood to both of these alternative scenarios, so the true rate and time values cannot be learned directly from molecular data without extrinsic evidence.This tutorial demonstrates what happens when you run a simple molecular phylogenetic analysis without any information to time-calibrate the tree. The tutorial reviews several important concepts for phylogenetic tree estimation, such as the relaxed molecular clock. It also demonstrates that extrinsic (non-molecular) evidence is needed to time-calibrate divergence times using standard phylogenetic approaches.For this example, we’ll compose a model where the phylogeny was generated by a constant-rate birth-death process and genes evolve under a relaxed molecular clock. We’ll refer to this as the divtime_uncalibrated analysis. Although the analysis can be run by calling source(\"./scripts/divtime_uncalibrated.Rev\"), we’ll execute all code by hand to better understand how the model works.Analysis setupNow to get started! First, we create variables to locate input datasets.# set up filesystem variablesanalysis      = \"divtime_uncalibrated\"dat_fp        = \"./data/kadua/\"phy_fn        = dat_fp + \"kadua.tre\"calib_fn      = dat_fp + \"kadua_calib.csv\"out_fn        = \"./output/\" + analysisThese variables will later configure the MCMC analysis. The empty vectors for moves and monitors will be populated later. We suggest setting a shorter analysis for only 500 MCMC iterations when using the script the first time. Use 5000 or more iterations for a full analysis# number of MCMC iterationsnum_gen = 500         # set num_gen = 5000 for full analysis# number of iterations between MCMC samplesprint_gen = 20# empty vector for MCMC movesmoves = VectorMoves()# empty vector for MCMC monitorsmonitors = VectorMonitors()Next, we read in a phylogenetic tree. To simplify this analysis, we assume the tree topology is known (fixed) while the divergence times are unknown (estimated). We will use the first (only!) tree stored in phy_fn to define the topology.# read a phylogenyphy          &lt;- readTrees(phy_fn)[1]# get its taxon settaxa         = phy.taxa()# get the number of taxanum_taxa     = taxa.size()# get the number of branches for a rooted treenum_branches = 2 * num_taxa - 2After that, we read in the molecular sequence alignments, each one corresponding to a different Angiosperms353 locus. We store the each locus as an element in a vector. This tutorial comes with 10 loci that can be used for a full analysis. However, we recommend using only two as you begin to speed things up.# create vector of molecular alignment matricesnum_loci = 2       # num_loci = 10 # visit each locusfor (i in 1:num_loci) {    # set alignment file name    mol_fn[i] = dat_fp + \"kadua_gene_\" + i + \".nex\"        # read alignment    dat_mol[i] = readDiscreteCharacterData(mol_fn[i])        # record alignment length    num_sites[i] = dat_mol[i].nchar()}We did not have perfect coverage for all taxa across all loci, so we will add missing sequence data for each taxon present in the tree but absent for a locus.# collect the taxon names as stringsfor (i in 1:num_taxa) {    taxa_names[i] = taxa[i].getName()}# visit each locusfor (i in 1:num_loci) {    # hide all taxa for this locus    dat_mol[i].excludeTaxa( dat_mol[i].taxa() )    # unhide all taxa from the phylogeny in this locus     dat_mol[i].includeTaxa( taxa_names )        # insert ???? chars for taxa in phylogeny but not this locus    dat_mol[i].addMissingTaxa( taxa_names )}Birth-death processNext, we will manually configure the birth-death model by entering commands. In the future, you can instead load the birth-death phylogenetic model setup definition using source(\"./scripts/phylo_bdp.Rev\").We first assign priors to the net diversification rate (birth - death) and turnover proportion (ratio of death to birth events). Throughout the tutorial, we will pair new model parameters with moves that eventually instruct MCMC how to update parameter values during model fitting.Net diversification rate controls how rapidly species accumulation, whereas turnover proportion controls the speed of extinction relative to speciation. This parameterization often behaves better for model fitting, and many biologists prefer to think in these terms.# net diversification rate (birth - death)diversification ~ dnExp(5)# turnover proportion (death / birth)turnover ~ dnBeta(2,2)# convert into birth and death ratesbirth := diversification / abs(1.0 - turnover)death := birth * turnover# moves to update ratesmoves.append( mvScale(diversification, weight=3) )moves.append( mvScale(turnover, weight=3) )Based on ongoing taxonomic work led by the National Tropical Botanical Garden, there are 32 Kadua taxa in our clade of interest, but only 27 are represented in our analysis. We use the $\\rho$ parameter to inform our birth-death process that some fraction are missing (unsampled) and not e.g. extinct.# proportion of sampled taxasample_prop &lt;- taxa.size() / 32Next, we assign a prior on the root age of the clade. For now, we assume we know nothing about the age of Hawaiian Kadua, except that the clade is probably younger than the Eocene-Oligocene boundary at 34 Ma.Set up root age prior. Note, we will modify this in the next section.# wide ignorance prior on root ageroot_age ~ dnUniform(0.0, 34)# move to update root_agemoves.append( mvScale(root_age, weight=15) )We now have what we need to construct a constant-rate birth process. In addition to the taxon set and various model parameters, the process conditions on a stopping condition (e.g. a time duration, producing some number of taxa, etc.) and a sampling strategy (how are sampled taxa selected). We use time as the stopping criterion and assume included taxa were sampled uniformly at random.# constant rate birth-death processtimetree ~ dnBDP(lambda=birth,                 mu=death,                 rho=sample_prop,                 rootAge=root_age,                 samplingStrategy=\"uniform\",                 condition=\"time\",                 taxa=taxa)# move to update internal node ages (move for root age is above)moves.append( mvNodeTimeSlideUniform(timetree, weight=2*num_taxa) )We now initialize the timetree variable with the phylogeny we read from file, stored in phy. This is primarily to set the topology, but it can also be used to initialize the model with reasonable starting values for divergence times. If you don’t initialize the tree, you will want to create moves to infer the tree topology.# initialize value of timetreetimetree.setValue(phy)Inferring topologyThe analyses in this tutorial can be modified to also estimate tree topology. To do so, add these moves:# nearest neighbor interchange movemoves.append( mvNNI(timetree, weight=2*num_taxa) )# fixed nodeheight prune regraft movemoves.append( mvFNPR(timetree, weight=1*num_taxa) )We want the crown node ages of important clades in Hawaiian Kadua to appear in our MCMC trace file. Calling source(\"./scripts/kadua_clade.Rev\") will construct six clades based on predefined Kadua taxon sets. We then create deterministic nodes to track the crown node ages of these clades using the tmrca() function, which will be monitored.# load Kadua clade statementssource(\"./scripts/kadua_clade.Rev\")# create variables to monitor clade agesage_ingroup           := tmrca(timetree, clade_ingroup)age_affinis           := tmrca(timetree, clade_affinis)age_centrantoides     := tmrca(timetree, clade_centrantoides)age_flynni            := tmrca(timetree, clade_flynni)age_littoralis        := tmrca(timetree, clade_littoralis)age_littoralis_flynni := tmrca(timetree, clade_littoralis_flynni)diff_age_root_ingroup := root_age - age_ingroupMultilocus substitution modelNow we have a variable representing our phylogeny. Next, we’ll model how molecular variation accumulates over time. For this, we will construct a partitioned substitution model with a relaxed molecular clock. This means rates of molecular evolution can vary among branches, among loci, and among sites. The following code can be executed by calling source(\"./scripts/mol_ctmc.Rev\"), but you should specify the model by hand to better understand its composition.First, we create the relaxed clock model. The following code creates a vector of clock rates that are lognormally distributed. Later, the rates in this vector will be used to define branch-varying clock rates. To do so, we first define a base clock rate, mu_mol_base# base molecular clock ratemu_mol_base ~ dnExp(10)# move to base molecular clock ratemoves.append( mvScale(mu_mol_base, weight=5) )Then, we draw branch rates whose mean equals that base clock rate and 95% of possible branchwise rate variation spans one order of magnitude (determined by the magic number 0.587405).# lognormal standard deviation for relaxed molecular clockmu_mol_sd &lt;- 0.587405# lognormal mean for relaxed molecular clock, designed to# have an expected value equal to mu_mol_baseln_mean := ln(mu_mol_base) - 0.5 * mu_mol_sd * mu_mol_sd# visit each branch in the treefor (i in 1:num_branches) {    # molecular clock rate for this branch    mu_mol_branch[i] ~ dnLnorm(ln_mean, mu_mol_sd)    # move to update this branch's clock rate    moves.append( mvScale(mu_mol_branch[i], weight=1) )}How relaxed is the clock?The parameter mu_mol_sd controls how much clock rates vary among branches. In the above example, we fix mu_mol_sd &lt;- 0.587405 to allow for a moderate amount of branch rate variation. Alternatively, this parameter could be estimated directly from the sequence data. In most cases, this will improve the fit of the model and indirectly improve divergence time estimates, but at an increased computational cost. For more rigorous analyses, consider replacing the constant variable for mu_mol_sd with a random variable, in the code above.# comment out the fixed variable# mu_mol_sd &lt;- 0.587405# replace with a random variablemu_mol_sd ~ dnExp(1)# move to update variablemoves.append( mvScale(mu_mol_sd, weight=5) )As MCMC fits the model, it will additionally estimate to what extent your clock is relaxed. Learn more about relaxed molecular clocks here (LINK).In a partitioned analysis, each locus has its own evolutionary rates. We will assign each locus its own rate scaling factor, rate matrix, and site-rate variation parameters. Read through this tutorial (LINK) to learn more about the design of partitioned analyses.To model among-locus rate variation, the substitution process multiplies the relaxed clock branch rate vector (mu_mol_branch) by a per-locus relative rate factor. This allows each locus to experience a different substitution rate, while also allowing all loci to share the same set of branchwise relaxed clock rates. We fix the relative rate factor for the first locus to the value 1. All remaining loci have relative rate factors drawn from a lognormal distribution with an expected value of 1 – i.e. they may be slower or faster than the first locus. Taking the product mu_mol_locus_rel[i] * mu_mol_branch gives us a new vector of per-locus branch rates that are rescaled by the relative rate factor, and stored into mu_mol[i]. We use this vector of relaxed branch-by-locus clock rates later when constructing the substitution model.# visit each locusfor (i in 1:num_loci) {        # first locus provides the base relative rate of 1    mu_mol_locus_rel[i] &lt;- 1.0        # assign random relative rates to all remaining loci    if (i &gt; 1) {            # relative rates have a mean of 1        mu_mol_locus_rel[i] ~ dnLognormal(-1/2, 1)                # add move to update per-locus relative rate        moves.append(mvScale(mu_mol_locus_rel[i], weight=3))    }        # construct rescaled branch rates for this locus    mu_mol[i] := mu_mol_locus_rel[i] * mu_mol_branch}Next, we specify the HKY rate matrix, Q_mol to define transition rates among nucleotides. The HKY rate matrix uses the kappa parameter to control relative rates of transitions (e.g. purine to purine, pyrimadine to pyrimadine) and transversions (purine to pyrimadine, pyrimadine to purine). The pi_mol parameter controls the stationary frequencies across nucleotides for the model. The prior mean on kappa is 1 and the prior mean on pi_mol is a flat Dirichlet distribution. Read more about the HKY matrix here (missing reference).# visit each locusfor (i in 1:num_loci) {    # transition transversion rate-ratio (expected value of 1)    kappa_mol[i] ~ dnLognormal(-1/2, 1)        # base frequencies    pi_mol[i] ~ dnDirichlet( [1,1,1,1] )        # HKY85 substitution matrix    Q_mol[i] := fnHKY(kappa=kappa_mol[i], baseFrequencies=pi_mol[i])        # add moves to updat TiTv ratio and base frequencies    moves.append(mvSimplex(pi_mol[i], alpha=3, offset=0.5, weight=3))    moves.append(mvScale(kappa_mol[i], weight=3))    }Molecular rates variation among sites follow a Gamma distribution, approximated with four discrete rate classes. When alpha is large, all classes have relative rate factors of 1. When alpha is small, most rate classes are near 0 and one rate class is far greater than 1. Read more about the +Gamma among site rate variation model here (missing reference).# visit each locusfor (i in 1:num_loci) {    # alpha controls whether all sites have the same or different rates    alpha_mol[i] ~ dnExp(0.1)        # among site rate variation model, +Gamma4    site_rates_mol[i] := fnDiscretizeGamma(shape=alpha_mol[i],                                           rate=alpha_mol[i],                                           numCats=4)                                           # add move to update alpha    moves.append(mvScale(alpha_mol[i], weight=3))    }Now we have all the model components we need to define a partitioned molecular substitution model. This is called the phylogenetic continuous-time Markov chain (or phyloCTMC) in RevBayes. The dnPhyloCTMC models patterns of nucleotide variation under the evolutionary model for a given phylogenetic tree.We create one dnPhyloCTMC model for each locus. Each locus evolves along the branches of timetree, the phylogeny whose divergence times we wish to estimate.# visit each locusfor (i in 1:num_loci) {    # substitution process along branches of phylogeny    x_mol[i] ~ dnPhyloCTMC(        Q=Q_mol[i],        tree=timetree,        branchRates=mu_mol[i],        siteRates=site_rates_mol[i],        rootFrequencies=pi_mol[i],        nSites=num_sites[i],        type=\"DNA\" )}We clamp the observed sequence alignment for each locus to the corresponding partitioned model component.# visit each locusfor (i in 1:num_loci) {    # clamp this molecular matrix to this molecular model    x_mol[i].clamp(dat_mol[i])}MCMCWe also apply special joint moves for updating the tree and molecular rates simultaneously. These moves are designed to take advantage of the fact that rate and time are not separately identifiable. That is, the likelihood remains unchanged if you multiply rates by 2 and divide times by 2. In short, these moves improve the performance of MCMC mixing.# move to scale time (up) opposite of rate (down)up_down_scale_tree = mvUpDownScale(lambda=1.0, weight=20)up_down_scale_tree.addVariable(timetree,       up=true)up_down_scale_tree.addVariable(root_age,       up=true)up_down_scale_tree.addVariable(mu_mol_branch,  up=false)up_down_scale_tree.addVariable(mu_mol_base,    up=false)moves.append(up_down_scale_tree)# move to scale base (up) and branch (up) ratesup_down_mol_rate = mvUpDownScale(lambda=1.0, weight=20)up_down_mol_rate.addVariable(mu_mol_branch,  up=true)up_down_mol_rate.addVariable(mu_mol_base,    up=true)moves.append(up_down_mol_rate)# move to rebalance rate and age parameters for internal tree noderate_age_proposal = mvRateAgeProposal(timetree, weight=20, alpha=5)rate_age_proposal.addRates(mu_mol_branch)moves.append(rate_age_proposal)Next, we construct monitors to capture information from our MCMC as it searches parameter space.# screen monitor, so you don't get boredmonitors.append( mnScreen(root_age, printgen=print_gen) )# file monitor for all simple model variablesmonitors.append( mnModel(printgen=print_gen, file=out_fn+\".model.txt\") )# file monitor for treemonitors.append( mnFile(timetree, printgen=print_gen, file=out_fn + \".tre\") )Now that our model, moves, and monitors are properly configured, we can run MCMC.# create model objectmymodel = model(timetree)# create MCMC objectmymcmc = mcmc(mymodel, moves, monitors)# run MCMCmymcmc.run(num_gen)Generate a maximum clade consensus tree after the job is complete.# read trace of sampled phylogenetic treestt = readTreeTrace (file=out_fn+\".tre\", treetype=\"clock\", burnin=0.2)# summarize phylogenies with maximum clade credibility treemcc_tree = mccTree(trace=tt, file=out_fn+\".mcc.tre\")Let’s view the MCC tree for the uncalibrated analysis in FigTree (Fig. ).Maximum clade credibility tree for Kadua without time-calibration.Notice that node ages vary wildly in the uncalibrated analysis, up to the maximum age of 34.0 Ma.Biogeographic dating with node calibrationsIn practice, biologists use extrinsic evidence to “calibrate” the molecular clock to a geological timescale. Fossil evidence can be used to constrain the minimum age of a phylogenetic divergence event (e.g. a child species cannot be older than its parent species), which effectively constrains time and, indirectly, rate estimates. Two main approaches have been used to deploy fossil-based calibrations. Prior-based calibrations assign a node age distribution to internal nodes on a phylogeny, selected and specified using expert knowledge. Process-based calibrations explicitly incorporate fossil morphology and ages as data during phylogenetic inference to assign ages to clades. Read these excellent papers to read more about fossil-based dating.Paleogeography provides a complementary source of information to estimate divergence times through biogeographic dating. Biogeographic dating is often used for studying clades that possess no useful fossils (e.g. clades of island plants, like Kadua). The logic for biogeographic dating is as follows: imagine a clade of ten species that are endemic to a volcanic island that is less than one million years old. If it is assumed a single lineage colonized the island after it emerged, then the maximum age of the clade must be younger than the island, allowing the biologist to calibrate the molecular clock.We’ll now repeat the previous exercise, but this time we will use calibration densities to constrain the ages of two nodes during inference. The root node age will be constrained by a secondary node age calibration, derived from a previous fossil-calibrated analysis on a family-level phylogeny that happened to include the most recent common ancestor of Hawaiian and non-Hawaiian Kadua (i.e., our root node). In addition, the maximum age of the Hawaiian Kadua radiation (the “ingroup” of our analysis) will be constrained to be equal to or less than the oldest High Islands (&lt;6.3 Ma).To proceed, we suggest that you make a new copy of ./scripts/divtime_unconstrained.Rev that is named ./scripts/divtime_nodeprior.Rev. Then, rather than typing all the commands, modify the content of the script as described below. After all modifications are in place, you can run the analysis by typing:source(\"./scripts/divtime_nodeprior.Rev\")If the script runs successfully, you should see MCMC diagnostics being printed to the screen. If the script fails to run, carefully read the error messages provided by RevBayes to correct the issue. RevBayes error messages generally report the location and the type of error.Assigning node age calibration densitiesOnce you have your new copy of the script, begin editing it. First, we’ll time-calibrate the root node that represents the most recent common ancestor of our clade. A previous fossil-based Bayesian analysis estimated the age of this node as 7.0 [3.0, 13.0] Ma (posterior mean and HPD95 credible interval).Previously, the root_age variable followed the distribution dnUniform(0.0, 34.0). We replace the wide prior for root_age with a narrower prior reflecting the secondary node age calibration.The new root_age prior density is easily replaced with the following commands:# comment out the uniform prior on root age!# root_age ~ dnUniform(0.0, 34.0)    # Calibration density #1: MRCA of sampled Kadua# secondary calibration corresponding# to MRCA of Hawaiian and non-Hawaiian Kadua   root_age ~ dnUniform(3.0, 13.0)root_age.setValue( phy.rootAge() )moves.append( mvScale(root_age, weight=15) )A shapelier node age density priorRather than implementing the secondary node age calibration as a flat prior on root_age, we could instead design with prior with more shape. For example, to use a truncated normal distribution with mean, min, and max ages matching the posterior mean, lower, and upper bounds of the 95% credible interval above. The standard deviation of the prior is similarly design so the width of the interval $\\pm 2\\sigma$ matches the width of the credible interval, then further doubled to be conservative (less informative).# comment out the uniform prior on root age!# root_age ~ dnUniform(0.0, 34.0)    # Alternate calibration density #1: MRCA of sampled# Kadua secondary calibration corresponding# to MRCA of Hawaiian and non-Hawaiian Kadua# get mean, min, max ages for secondary calibrationmean_age &lt;- dat_calib[1][2]min_age &lt;- dat_calib[1][3]max_age &lt;- dat_calib[1][4]# set width of HPD95 CI equal to width of +/- 2sdsd_age &lt;- abs(max_age - min_age) / 4# further double sd make prior more diffusesd_age &lt;- sd_age * 2# set root age priorroot_age ~ dnNormal(mean=mean_age,                    sd=2*sd_age,                    min=min_age,                    max=max_age)# initialize root ageroot_age.setValue( phy.rootAge() )# add move to update root agemoves.append( mvScale(root_age, weight=15) )The second calibration restricts the maximum age of the Hawaiian Kadua crown age. The code stored in ./scripts/kadua_clade.Rev names relevant clades, including clade_ingroup. We then use the tmrca function to track the time to the most recent common ancestor for members of each clade. These variables do not influence the model probability or model fitting in any way. They are only monitored during MCMC sampling.Recall from the previous tutorial on molecular phylogenetics that we created the ingroup_age variable to monitor crown node age of the Hawaiian ingroup. We want this age to be restricted, where the lower bound is the minimum allowable age for the node, 0.0 Ma (the present), and the upper bound is the maximum age of the High Islands, 6.3 Ma (the past). Enter the following commands, then we will work through the logic with the help of Figure .# Calibration density #2: MRCA of Hawaiian Kadua# MRCA of extant and sampled Hawaiian Kadua is assumed to be less than the# maximum (very conservative) age of the oldest High Island (Kauai, &lt;6.3 Ma)# get min and max age for priorage_bg_min &lt;- 0.0age_bg_max &lt;- 6.3# set lower/upper bounds as difference between min/max age and ingroup agecalib_lower := age_bg_min - age_ingroupcalib_upper := age_bg_max - age_ingroup# set calibration intervalcalib_clade ~ dnUniform(calib_lower, calib_upper)calib_clade.clamp(0.0)Calibration in RevBayes works by setting the calibration variable to fixed value of 0.0. You can then imagine the calibration density as a “sliding window” that assigns a probability to the fixed point of zero. The position of this sliding window depends on the difference between the clade’s age and relative to the window. In our case, the fixed point of 0.0 will always be greater than the lower bound of calib_lower := 0.0 - age_ingroup, because clades always have positive ages. The fixed point 0.0 will be less than the upper bound of calib_upper := 6.3 - age_ingroup only when age_ingroup &lt; 6.3. When age_ingroup &gt; 6.3, then the fixed value of calib_clade &gt; 6.3 - age_ingroup and the prior density will assign probability zero to the node age constraint being satisfied. Phylogenetic trees that violate the node age constraint will always be discarded and never appear in the posterior sample. Learn more about divergence time estimation using node age calibrations in RevBayes through this tutorial: link.Cartoon for how RevBayes node age calibration densities behave. This example assumes the maximum clade age is 6.3 Ma and the minimum clade age is 0.0 Ma (present, i.e effectively no lower bound) under a uniform calibration density. When the clade age increases, the density “slides” to the left. When the clade age decreases, the density “slides” to the right. MCMC will reject a clade ages that have low (or zero) probability, as marked in red.Let’s view the MCC tree for the prior-based time-calibration analysis in FigTree (Fig. ).Maximum clade credibility tree for Kadua without node-based priors for time-calibration.Notice that the clade is now much younger. The upper bound of the HPD95 for the age of Hawaiian Kadua is always less than 6.3 Ma, as by design.Biogeographic dating with TimeFIGBiogeographic dating has typically relied on prior-based constraints. On the positive side, prior-based constraints are extremely easy to apply and are computationally inexpensive. Constraining node ages also has downsides. For one, it requires that the biologist makes strong assumptions about many unknowable factors. For Hawaiian Kadua, the ideal node calibration prior would account for how long it takes for a progenitor species to colonize into and radiate upon a newly formed island. However, that is precisely what historical biogeographers want to infer from data. What factors would you use to constrain the maximum age of Kadua? How would you justify their use? How would you encode them into a prior calibration density?In addition to this conceptual challenge, there is also a methodological concern. Using a biogeographic hypothesis to date a clade (e.g. “we assume the islands were colonized after they originate”) means that the resulting dated phylogeny cannot be safely used to test downstream biogeography hypotheses (“when were the islands first colonized by the clade?”). Doing so is circular. For example, a prior that restricts all Kadua to be younger than Kauai eliminates the possibility that ancestral species colonized the older Northwestern Islands before the emergence of the High Islands.Biogeographic dating with a process-based approach uses paleogeographically-informed biogeographic rates to extract information about divergence times through a dataset (Landis 2020). Unlike molecular rates of evolution, these biogeographic rates vary in response to the paleogeographic features at a given geological time. This establishes a timescale for the phylogeny that enables the molecular clock to be estimated. Previous approaches for process-based biogeographic dating relied on simpler models that did not consider how speciation and extinction rates vary among regions over time (missing reference). TimeFIG models this important relationship between paleogeographically-varying features, species ranges, biogeographic rates, and divergence times.We now proceed with a full TimeFIG analysis for biogeographic dating. Because we already ran a TimeFIG analysis on a fixed tree in the previous tutorial, this tutorial will instead demonstrate how to convert the fixed-tree TimeFIG analysis to treat the tree as a random variable. Broadly speaking, we’ll apply what we learned about molecular phylogenetics in the first part of this tutorial in combination with what we learned from the previous fixed-tree TimeFIG analysis.Note, we already know how to set up a molecular phylogenetic analysis and a TimeFIG analysis from previous exercises. Read those tutorials for details if you skipped ahead. Rather than re-implementing the complete model by hand a second time, we’ll simply import Rev scripts that instantiate molecular phylogenetic and TimeFIG model components to design our model.This analysis can be run by calling source(\"./scripts/divtime_timefig.Rev\"), however you should build the model through the console when using it for the first time.Let’s get started! First, we load the TensorPhylo plugin.# location of your TensorPhylo libraryplugin_fp = \"/Users/mlandis/.local/lib/tensorphylo\"# load TensorPhylo pluginloadPlugin(\"TensorPhylo\", plugin_fp)We will use the same basic fileset as the molecular phylogenetics exercise.# filesystem variablesanalysis      = \"divtime_timefig\"dat_fp        = \"./data/kadua/\"phy_fn        = dat_fp + \"kadua.tre\"out_fn        = \"./output/\" + analysisNow, we also load species range data and paleogeographical data for the TimeFIG analysis.# new filesystem variablesbg_fn         = dat_fp + \"kadua_range_n7.nex\"label_fn      = dat_fp + \"kadua_range_label.csv\"geo_fp        = \"./data/hawaii/\"feature_fn    = geo_fp + \"feature_summary.csv\"times_fn      = geo_fp + \"age_summary.csv\"Next, we create various MCMC analysis settings, to be used later.# number of processors for TensorPhylonum_proc = 6# number of MCMC iterationsnum_gen = 500         # set num_gen = 5000 for full analysis# number of interations between MCMC samplesprint_gen = 20# empty vector for MCMC movesmoves = VectorMoves()# empty vector for MCMC monitorsmonitors = VectorMonitors()Load the input datasets for the analysis. First, we load a tree variable for its taxon set and topology of species relationships.# read the phylogenetic treephy          &lt;- readTrees(phy_fn)[1]# store its taxon settaxa         = phy.taxa()# get the number of taxanum_taxa     = taxa.size()# compute the number of branches in a rooted treenum_branches = 2 * num_taxa - 2Next, we load in our biogeographic character matrix of species presence (1) and absences (0) across regions. Recall that we convert these presence-absence vectors into an integer-based representation. We also use this matrix to determine the num_regions and num_ranges, which will be $2^N - 1$ when max_range_size == num_regions and smaller otherwise.# read the biogeographic matrix of region presence/absence datadat_01         = readDiscreteCharacterData(bg_fn)# get the number of regionsnum_regions    = dat_01.nchar()# set a maximum range sizemax_range_size = 4# compute the number of ranges from num_regions and max_range_sizenum_ranges     = 0for (k in 1:max_range_size) {    num_ranges += choose(num_regions, k)}# convert the presence/absence matrix into integer-valued rangesdat_nn         = formatDiscreteCharacterData(dat_01, format=\"GeoSSE\", numStates=num_ranges)We’ll save a copy of the range state labels to csv file for later use.# save relationships between 01 and integer coding to filedesc           = dat_nn.getStateDescriptions()write(\"index,range\\n\", filename=label_fn)for (i in 1:desc.size()) {    write((i-1) + \",\" + desc[i] + \"\\n\", filename=label_fn, append=true)}Load in the molecular dataset. As before, we’ll use only two loci to experiment with the script. Set the number of loci to 10 for a full analysis.# create vector of molecular alignment matricesnum_loci = 2         # set num_loci = 10 for full analysis# visit each locusfor (i in 1:num_loci) {    # set alignment file name    mol_fn[i] = dat_fp + \"kadua_gene_\" + i + \".nex\"        # read alignment    dat_mol[i] = readDiscreteCharacterData(mol_fn[i])        # record alignment length    num_sites[i] = dat_mol[i].nchar()}As before, align the taxon labels with the tree, adding ambiguous states for missing taxa in each locus.# collect the taxon names as stringsfor (i in 1:num_taxa) {taxa_names[i] = taxa[i].getName()}# visit each locusfor (i in 1:num_loci) {    # hide all taxa for this locus    dat_mol[i].excludeTaxa( dat_mol[i].taxa() )    # unhide all taxa from the phylogeny in this locus     dat_mol[i].includeTaxa( taxa_names )        # insert ???? chars for taxa in phylogeny but not this locus    dat_mol[i].addMissingTaxa( taxa_names )}Now we load the model that defines relationships between regional features and biogeographic rate factors.# load model components for feature informed GeoSSE ratessource(\"./scripts/geo_timefig.Rev\")Then we load the script that specifies the TimeFIG model.# load model components for GeoSSE using TensorPhylosource(\"./scripts/phylo_timefig.Rev\")Note, this script is identical to the script used in the previous TimeFIG tutorial that assumed a fixed phylogeny, with two exceptions. First, rather than assuming the root_age variable is fixed as a constant node, we assign it a uniform prior from 0 to 34 Ma, as we did in the uncalibrated analysis. The script also constructs MCMC moves to update all node ages, including the root node.# NOTE: you do not need to enter these lines code to the console# estimate root node ageroot_age ~ dnUniform(0, 34)moves.append( mvScale(root_age, weight=15) )# estimate internal node agesmoves.append( mvNodeTimeSlideUniform(timetree, weight=2*num_taxa) )Next, we load the molecular model, identical to that used earlier in this tutorial.# load model components for multilocus molecular substitution processsource(\"./scripts/mol_ctmc.Rev\")Previously, we initialized the timetree variable with the value of phy to set the tree topology. We also clamped the sequence data for each molecular locus to each CTMC, x_mol[i], in the multilocus analysis. Unlike before, we also need to clamp our biogeographic range data to the timetree variable.# set GeoSSE tree topology (and initial divergence times)timetree.setValue(phy)# clamp GeoSSE biogeographic datatimetree.clampCharData(dat_nn)# clamp molecular datafor (i in 1:num_loci) {    x_mol[i].clamp(dat_mol[i])}Now our TimeFIG model is configured and associated with our molecular, biogeographic, and paleogeographic data. Next, we construct joint moves to help MCMC explore tree and rate space.# move to scale time (up) opposite of rate (down)up_down_scale_tree = mvUpDownScale(lambda=1.0, weight=20)up_down_scale_tree.addVariable(timetree,       up=true)up_down_scale_tree.addVariable(root_age,       up=true)up_down_scale_tree.addVariable(mu_mol_branch,  up=false)up_down_scale_tree.addVariable(mu_mol_base,    up=false)moves.append(up_down_scale_tree)# move to scale base (up) and branch (up) ratesup_down_mol_rate = mvUpDownScale(lambda=1.0, weight=20)up_down_mol_rate.addVariable(mu_mol_branch,  up=true)up_down_mol_rate.addVariable(mu_mol_base,    up=true)moves.append(up_down_mol_rate)# move to rebalance rate and age parameters for a noderate_age_proposal = mvRateAgeProposal(timetree, weight=20, alpha=5)rate_age_proposal.addRates(mu_mol_branch)moves.append(rate_age_proposal)We also create the same original set of monitors.# screen monitor, so you don't get boredmonitors.append( mnScreen(root_age, printgen=print_gen) )# file monitor for all simple model variablesmonitors.append( mnModel(printgen=print_gen, file=out_fn+\".model.txt\") )# file monitor for treemonitors.append( mnFile(timetree, printgen=print_gen, file=out_fn + \".tre\") )We also create monitors to track the biogeographic rates per region per time interval.# file monitor for biogeographic ratesfor (k in 1:num_times) {    bg_mon_fn = out_fn + \".time\" + k + \".bg.txt\"    monitors.append( mnFile( filename = bg_mon_fn, printgen=print_gen,                             rho_e, rho_w, rho_d, rho_b,                             r_e[k], r_w[k],                             r_d[k][1], r_d[k][2], r_d[k][3], r_d[k][4],                             r_d[k][5], r_d[k][6], r_d[k][7],                             r_b[k][1], r_b[k][2], r_b[k][3], r_b[k][4],                             r_b[k][5], r_b[k][6], r_b[k][7],                             m_e[k][1], m_w[k][1],                             m_d[k][1], m_d[k][2], m_d[k][3], m_d[k][4],                             m_d[k][5], m_d[k][6], m_d[k][7],                             m_b[k][1], m_b[k][2], m_b[k][3], m_b[k][4],                             m_b[k][5], m_b[k][6], m_b[k][7] ) )}You can also create a ancestral state monitor to sample ancestral ranges that are reflect with the phylogenetic, biogeographic, and paleogeographic dynamics of the system. Note, that the ancestral states generated during a given MCMC iteration are consistent with the value of the (random) phylogeny and model parameters at that same iteration.# monitor ancestral ranges at internal nodesmonitors.append( mnJointConditionalAncestralState(    tree=timetree, glhbdsp=timetree, printgen=print_gen,    filename=out_fn+\".states.txt\",    withTips=true, withStartStates=true, type=\"NaturalNumbers\") )Similarly, you can construct a monitor to generate stochastic mappings that represent the timing and sequence of historical biogeographic events for a given MCMC iteration. That said, stochastic mapping under SSE models can be computationally intensive, so recommend leaving the option disabled or to generate stochastic mappings infrequently.# monitor stochastic mappings along branches of tree# NOTE: uncomment if needed, but can cause performance issues# monitors.append( mnStochasticCharacterMap(#    glhbdsp=timetree, printgen=print_gen*10,#    filename=out_fn+\".stoch.txt\",#    use_simmap_default=false) )With our model, moves, and monitors all in place, we can build and run our MCMC analysis. We recommend using moveschedule=\"single\" as an option when learning to run this analysis for the first time. Full analysis shoudl instead use moveschedule=\"random\".# create model objectmymodel = model(timetree)# create MCMC objectmymcmc = mcmc(mymodel, moves, monitors, moveschedule=\"single\")     # set moveschedule=\"random\" for full analysis# run MCMCmymcmc.run(num_gen)Now, let’s inspect the MCC tree for the TimeFIG-based time-calibration analysis in FigTree (Fig. ).Maximum clade credibility tree for Kadua using a process-based TimeFIG model for time-calibration.Notice how the clade is younger than the uncalibrated and prior-based node age calibration analyses.Reviewing the marginal posterior densities of the ingroup crown age help reveal why the prior-based BDP calibration and process-based TimeFIG calibration differ.Comparison of marginal posterior ingroup ages for Hawaiian Kadua.Notice that the posterior ingroup age for the prior-based calibration is never older than 6.3 Ma, the maximum age constraint we applied (green density in  Fig. ). This constraint means the crown age of Hawaiian Kadua must be younger than the oldest High Islands (Kauai and Niihua).While the posterior-based TimeFIG calibration produces a young mean age for the Hawaiian ingroup’s crown node, it also allows for the ingroup to be older than the oldest High Islands (red density in  Fig. ). This upper tail in the ingroup age density captures scenarios in which Hawaiian Kadua colonized the now-High Islands from the now-Low Islands more than once.Other analyses, such generating figures for ancestral ranges or regional biogeographic rates through time, are done in the same manner as with previous tutorials. The difference here is that rather than assuming fixed phylogenetic divergence times, the phylogeny can be estimated as part of the analysis. The ability to jointly estimate phylogeny and biogeography is especially crucial in scenarios where paleogeography is expected to shape when and where species diversified, as in the case of Hawaiian Kadua.  Example output  Note: Complete FIG analyses can take several hours to run. To exploreFIG analysis output as part of a workshop, we recommend that youdownload precomputed “example output” from the top left menu on thispage. Save these files into your local output directory and view resultsand/or run the following plotting code.This section shows how generate plots for FIG analysis results using the FIG Tools repository, which primarily uses R, RevGadgets, ggplot, and igraph for visualization.NOTE: Your output may look slightly different than the output shown below. If you want to exactly replicate the results of the tutorial, you must set a seed at the beginning of the kadua_geosse.Rev script by adding the RevBayes command seed(1).To proceed, we’ll exit RevBayes and work from the command line prompt in shell. We assume that ./timefig_dating is a subdirectory from your current location. To generate the images below, first save a copy of FIG tools to your filesystem:# Download .zip file (open in browser our save in command line)wget https://github.com/hawaiian-plant-biogeography/fig_tools/archive/refs/heads/main.zip# Unzip file as \"fig_tools-main\"unzip main.zip# Rename directorymv fig_tools-main fig_toolsNext, copy the files in ./fig_tools/scripts into your TimeFIG project directory as ./timefig_dating/plot:# copycp -R ./fig_tools/scripts ./timefig_dating/plotThese scripts assume you are in the base of your analysis directory:cd ./timefig_datingNow we can generate plots using FIG tools. First, we generate one plot with a maximum clade credibility tree with node age estimates and another plot with ancestral range estimates using these commands:# prepare tree and state output for plottingrb --args ./output/divtime_timefig.tre ./output/divtime_timefig.states.txt --file ./plot/make_tree.Rev# make MCC tree plotRscript ./scripts/plot_mcc_tree.R ./output/out.mcc.tre# make ancestral tree plotRscript ./plot/plot_states_tree.R ./output/out.states.tre ./output/out.mcc.tre ./data/kadua/kadua_range_label.csv GNKOMHZMaximum clade credibility tree for Hawaiian Kadua (another version).Ancestral ranges for Hawaiian Kadua.In addition, we generate a plot of within-region speciation rates, $r_w(i,t)$, for each region $i$ at time $t$, which shows elevated speciation in islands soon after emergence. The code for this is:# make region rate vs. time plotsRscript ./plot/plot_features_vs_time_grid.R ./data/hawaii/feature_summary.csv ./data/hawaii/age_summary.csv ./data/hawaii/feature_description.csv GNKOMHZWithin-region speciation rate estimates for Kadua. Dark colors are high rates, light colors are low rates, and gray indicates the region did not exist during that interval (missing feature). Range labels represent the following set of regions: G=Gardner, N=Necker, K=Kauai, O=Oahu, M=Maui Nui, H=Hawaii, Z=Remaining non-Hawaiian regions.Notice the differences in ancestral range and rate estimates when comparing these results to those from the simpler TimeFIG analysis in the previous tutorial.Lastly, this script will plot a network that summarizes relationships between regional features, feature effect parameters, and core biogeographic processes:# make feature vs. rate network plotRscript ./plot/plot_feature_rate_network.R ./output/divtime_timefig.model.txt ./data/hawaii/feature_description.csvNetwork diagram displaying the relationships between regional features (gold), feature effect parameters ($\\phi$ and $\\sigma$ in green), and rate modifier functions ($m$ in cyan). Edges colors indicate positive (blue) versus negative (red) relationships and widths indicate weak (thin) versus (strong) interactions.",
        "url": "/tutorials/timefig_dating/",
        "index": "true"
      }
      ,
    
      "tutorials-multispecies-coalescent": {
        "title": "Gene tree - species tree reconstruction",
        "content": "Overview: Gene tree-species tree modelsEver since @Zuckerkandl1965a, researchers have acknowledged thatphylogenies reconstructed from homologous gene sequences could differfrom species phylogenies. As molecular sequences accumulated, the linkbetween gene trees and species trees started to be modeled. The firstmodels were based on parsimony, and aimed for instance at reconciling agene tree with a species tree by minimizing the number of events of geneduplication and gene loss. In the past dozen years, probabilistic modelshave been proposed to reconstruct gene trees and species trees in arigorous statistical framework. Models and algorithms have quickly grownin complexity, to model biological processes with increasing realism, toaccommodate several processes at the same time, or to handlegenome-scale data sets. In this overview we will not detail thesemodels, and we invite the interested reader to take a look at recentreviews (e.g.,(missing reference)).Processes of discordThere are several reasons why a gene tree may differ from a speciestree. Of course, a gene tree may differ from the species tree justbecause a mistake was made during the analysis of the gene sequences, atany point in a pipeline going from the sequencing itself to the genetree reconstruction. Such a mistake would produce an incorrect genetree. Here we do not mean this kind of discord, but rather discord thatcomes from a real biological process that generates true gene historiesthat differ from true species histories. These processes include geneduplication, gene loss, gene transfer (used loosely here to also includereticulation, hybridization between species), and incomplete lineagesorting (Fig. [fig1]). In this tutorial we focus on Incomplete lineagesorting, which will be discussed in more details in the followingsubsection.Fig. [fig1] suggests that for all processes the gene tree can be seenas the product of a branching process operating inside the species tree.Indeed, all processes are modeled as some type of birth-death processrunning along the species tree. For duplication/loss models, birthcorrespond to gene duplication events, and death to gene loss events.Transfers can be added to the model by introducing another type ofbirth, with a child lineage appearing in another branch of the speciestree. Incomplete lineage sorting is also modeled with a birth-death typeof model, the coalescent. All these models can be made heterogeneous,for instance by allowing different sets of parameters for differentbranches of the species tree. This is useful to model differences inrates of duplication, loss or transfer among species, or to modeldifferent effective population sizes in a species tree. InRevBayes so far only models of incomplete lineage sortinghave been implemented (models of duplication and loss and transfer willsoon be added). Thanks to RevBayes modular design, thereis quite a lot of flexibility in specifying the model, for instance byassociating different parameters to different branches of the speciestree, or by combining the gene tree-species tree model to other types ofmodels, for instance models of trait evolution, or models of relaxedmolecular clock.Gene tree discordance is a problem for species tree reconstructionThere have been several approaches to species tree reconstruction:concatenation and supertree approaches, which have been used for quitesome time now, and more recently methods that rely on gene tree-speciestree models.      Concatenation simply consists in taking all gene alignments,concatenating them into one super alignment, and then analyzing itas if it were a single gene sequence. More sophisticated approachesallow different partitions for different genes, but the mainassumption at the heart of this approach is that all sites of allgenes have evolved according to the same species tree. Thisassumption is often not correct because all the processes of discordpresented above conspire to make gene trees different from thespecies tree. In practice, this matters: simulation studies havefound that in the presence of incomplete lineage sorting, in someparticular areas of the parameter space, concatenation will oftenreturn an incorrect species tree (Leaché and Rannala 2011). Concatenation mayalso be a questionable approach in prokaryotic phylogenetics, wherethe quest for a tree of life has been difficult, to the point thatsome doubted that one could find a meaningful species treerepresenting vertical descent. Nonetheless, the concatenationapproach may be fairly robust to lateral gene transfers, as itreturns good species trees (arguably better than small subunit orlarge subunit rRNA trees) in a range of prokaryotic groups(missing reference).        Supertree approaches differ from concatenation notably by discardingsequence information once individual gene trees have been built.Contrary to concatenation approaches that combine individual genealignments, supertree approaches combine individual gene trees toobtain a species tree. Most supertree methods are not based on anexplicit model of the processes causing discordance between genetrees and species tree (although there are exceptions, notablymodelling incomplete lineage sorting, see below). Instead, they aimat finding a tree that would best describe the distribution of genetrees, according to some fairly arbitrary criterion. In practice,these methods have been found to provide reasonable results in manycases, but in simulations they are usually less accuratethan concatenation.        Methods that rely on gene tree-species tree models appear verypromising as they explicitly model the processes of discord. Theadvantage of these models is that we account for processes that weknow have taken a part in generating the data, thus possiblyimproving the accuracy and robustness of our inferences. Further,these models can be combined withe.g.,models of sequence evolution,models of co-evolution between gene trees, or models of traitevolution. However, these models are computationally challenging touse, because they require estimating jointly gene trees, speciestrees, and other parameters that entertain strong correlations. As aconsequence, in many gene tree-species tree models, devising awell-mixing MCMC strategy can be problematic.  Modeling incomplete lineage sorting: the multispecies coalescentIncomplete lineage sorting is a population-level process. In a species,at a given time, there are several alleles for a given locus in thegenome. These alleles have their own history, they diverged from eachother at various times in the past. This history can differ from thespecies history, because several alleles can persist through aspeciation event, and because, even without selective effects, thesorting of alleles during a speciation event is random and can result ina tree that differs from the species tree (Fig. [fig1]d). In allcases, incongruence between the gene tree and the species tree occurswhen alleles persist over the course of several speciation events. Whenreconstructing a gene tree, one therefore gets the history of thealleles that have been sampled (at best), not the history of thespecies.In 2003, Rannala and Yang proposed a powerful way to model the sortingof alleles along a phylogeny of several species (Rannala and Yang 2003), themultispecies coalescent (Fig. [fig2]). This model is at the origin ofmost model-based approaches to reconstruct gene and species trees(missing reference). The multispecies coalescent models theevolution of a population of alleles along a species tree. Along thespecies tree, it allows different branch lengths, in units of time, andalso allows different effective population sizes. Computing theprobability of a gene tree given a species tree and other parameters isquite easy. Basically it works by cutting the gene tree into independentspecies-specific subtrees, computing probabilities for each of thosesubtrees, and combining them all at the end to get the probability ofthe gene tree according to the multispecies coalescent, given thecurrent parameter values. Cutting the gene tree into species-specificsubtrees is quite easy, because we can use the dates of speciationevents to identify parts of the gene trees that are before and afterspeciation events. The resulting subtrees are represented with the greyboxes in Fig. [fig2]. In this figure, each subtree corresponds to oneparticular population, either extant or ancestral. Inside each subtree,given its length, the effective population size, and dates ofcoalescence (divergences of alleles), the coalescent model providessimple formulas for computing the probability of the gene subtree givenother parameters. Because we consider that these subtree probabilitiesare all independent of one another, they are then multiplied to get thegene tree probability given current parameter values.Two parameters associated to branches of the species tree have a directimpact on the expected amount of gene tree-species tree incongruence:      Time between speciations. The more a branch length increases,the more the pool of alleles is expected to change. Alleles aretherefore less likely to persist for several speciation events ifthe branches between these speciation events are long.        Effective population size between speciations. In populationswith small effective population sizes, chance events can cause largeshifts in allele frequencies, and possibly disappearance of alleles.In large populations, because an allele is likely carried by a largenumber of individuals, its disappearance is less likely, thepopulation of alleles is more stable. Alleles are therefore lesslikely to persist for several speciation events if the branchesbetween these speciation events are characterized by small effectivepopulation sizes.  Overall, larger amounts of gene tree-species tree incongruence areexpected in phylogenies characterized by short branches with largepopulation sizes. A corollary of that is that larger amounts of genetree-gene tree incongruence are expected as well. To measure thesusceptibility of species phylogenies to generate incomplete lineagesorting, the concept of coalescent time units has been introduced.Coalescent time units are obtained when branch length $\\lambda$, innumber of generations, is divided by effective population size $N_e$. Asa consequence, in a species tree whose branches are expressed incoalescent time units, a branch length of $1~coalescent~time~unit $means a branch length of $N_e~generations$. Once branch lengths on thespecies tree are measured in coalescent time units, it becomes easy tospot species trees that generate a lot of incongruence: those are shorttrees.   The multispecies coalescent.A) A gene tree, including 3 human alleles, 2 Chimp alleles, one Gorillaallele, and one Orang-outan allele. $\\tau$ parameters are speciationtimes, $t$ parameters are divergence time in the gene tree, the greysquares represent the ancestral populations, with their respeciivesizes. B) The corresponding species tree. In this model, the speciationtimes define minimal boundaries for allele divergence times.[Replicated from Fig. 1 in @Rannala2003.]The exercises assume you have a working installation of RevBayes. Inthis introductory tutorial, we will apply the multispecies coalescentmodel to 10 gene alignments from 23 primate species. We will specify themultispecies coalescent, with different effective population sizes foreach branch of the species tree. We will assume that:      The species tree is drawn from a constant birth-death process.        Along the branches of the species tree, a multispecies coalescentprocess generates gene trees. Different effective population sizesare assigned to each branch of the species tree.        Along each gene tree, gene sequences are evolved according to an HKYmodeland a strict global clock. To save computing time, we do notuse gamma distributed rate variation among sites.        Here, we run an MCMC on this model, using data from 10 genes in 23mammalian species.  Scripts are all placed in$tutorials/RB_MultispeciesCoalescent_Tutorial/scripts/$.      Open RevBayes        Let’s load all 10 gene alignments.    ### Read in sequence data for all geneslocus_names = [\"COIII\", \"FGA\", \"GHRmeredith\", \"lrpprc_169\", \"npas3\", \"sim1\", \"tex2\", \"ttr\", \"zfy\", \"zic3\"]num_loci = locus_names.size()# read in each data matrix separatelyfor ( i in 1:num_loci ) {    data[i] &lt;- readDiscreteCharacterData(\"data/\" + locus_names[i] + \".fasta\")}# alternatively we could have read in from data/merged.nex too (although that contains the empty sequences ...)# Get some useful variables from a species tree.# We need these variables later on, but we will not use the species tree as starting value.primate_tree = readTrees(\"data/primates.tree\")[1]n_species &lt;- primate_tree.ntips()taxa &lt;- primate_tree.taxa()n_branches &lt;- 2 * n_species - 1 # number of branches in a rooted tree# set my move indexmi = 0            We specify a constant-rate birth-death process as our prior on thespecies tree. The birth-death process has a speciation andextinction rate as its parameters. We use here a transformation andspecify priors on the speciation rate and relative extinction rate.Additionally, we calibrate the tree by assuming that the crown ageof primates is around 75 MYA. Thus, we specify a normal distributionwith mean 75 and standard deviation 2.5 as the prior on the rootage. Since the root age can only be a positive real number wetruncate the normal distribution at 0.    # Specify a prior on the diversification and turnover ratespeciation ~ dnGamma(2,2)relativeExtinction ~ dnBeta(1,1)# now transform the diversification and turnover rates into speciation and extinction ratesextinction := speciation * relativeExtinction# specify a prior on the root age (our informed guess is about 75-80 mya)root ~ dnNormal(mean=75,sd=2.5,min=0.0, max=Inf)sampling_fraction &lt;- 23 / 450 # 23 out of the ~ 450 primate species# create some moves that change the stochastic variables# all moves are sliding and scaling proposalsmoves[++mi] = mvSlideBactrian(speciation,tune=true,weight=2)moves[++mi] = mvSlideBactrian(relativeExtinction,tune=true,weight=2)moves[++mi] = mvScaleBactrian(speciation,lambda=1,tune=true,weight=2)moves[++mi] = mvScaleBactrian(relativeExtinction,lambda=1,tune=true,weight=2)# construct a variable for the tree drawn from a birth death processpsi ~ dnBDP(lambda=speciation, mu=extinction, rootAge=root, rho=sampling_fraction, taxa=taxa )            Mixing in the multispecies coalescent model is particularlychallenging. Although, in principle and assuming our MCMC is workingas intended, after an infinite amount of time we would converge to acorrect estimate of the posterior distribution over the speciestree, gene trees and other parameters, it is advisable to start fromreasonable starting values for the species tree and the gene trees.In our case, we have already reconstructed candidate gene treesusing RevBayes. These gene trees have been produced using thescripts:$tutorials/RB_MultispeciesCoalescent_Tutorial/scripts/mcmc_GeneTree.Rev$and$tutorials/RB_MultispeciesCoalescent_Tutorial/construct_all_GeneTrees.sh$).They have been placed in the folder$tutorials/RB_MultispeciesCoalescent_Tutorial/output_GeneTrees$,and Maximum A Posteriori (MAP) trees have been constructed. TheseMAP trees will provide good starting gene trees, and will also beused to generate a starting species tree using a function thatcomputes a maximum tree (Edwards et al. 2007). The maximum tree “is thetree with the largest possible speciation times in the space ofspecies trees restricted by available gene trees” (Liu et al. 2010).Conveniently, tip names in the gene trees correspond to the speciesnames: we can thus use the maximum tree function on these treeswithout worrying about potential name inconsistencies.    # read in each gene tree separatelyj=1for ( i in 1:num_loci ) {    gene_trees[i] &lt;- readTrees(\"output_GeneTrees/\" + locus_names[i] + \"_MAP.tree\")[1]    print(\"Gene tree \"+i+ \" has \"+ gene_trees[i].ntips() + \" tips.\")}# We set the species tree to a good starting value.# This good starting value is obtained from the Maximum Tree method (Liu, 2006).# The same method is used in BEST to obtain a good starting species tree.recTree &lt;- maximumTree(gene_trees)psi.setValue(recTree)root.setValue(recTree.rootAge())write(\"\\t\\tProposed starting species tree: \")write( psi)write(\"\\t\\tWith root age: \" + root)            Now that we have a species tree, we can specify the prior on thegene trees by using a multispecies coalescent process. First, weneed to load in a map of the individual names to the species names.This map ensures we can associate the individuals to the speciesthey belong to. Have a look in one of these files, for exampleprimates_COIII_species_map.txt. We will assume that each branchof the species tree, which represents a population, has its ownpopulation size. Thus, our prior is that each population size perbranch is identically distributed from an exponential distributionwith rate 0.1 (giving an expectation of 10 and thus a relativelyflat prior distribution). Note that we use fixed population sizesfor the terminal branches because we have only a single individualper species and thus have no information about its population size.One could use other models for the population sizes. For example onecould assume that all branches have the same population size. Oncethe gene trees have been declared, we set their value to the MAPtrees that we have read previously.    # We assume independent effective population size parameters for each branch of the species tree.for (i in 1:n_species) {  Ne[i] &lt;- 10.0}for (i in (n_species+1):n_branches) {  Ne[i] ~ dnExponential(0.01)  moves[++mi] = mvScale(Ne[i],lambda=.1,tune=true,3.0)  moves[++mi] = mvSlide(Ne[i],tune=true,2.0)}# We could also assume a single effective population size for the entire species tree.#Ne ~ dnGamma(shape=1.0,rate=1.0)#moves[++mi] = mvScale(Ne,1,true,1.0)for (i in 1:num_loci) {   # We need to read in files providing the link between gene names and species names   taxon_map = readTaxonData(\"data/species_maps/primates_\" + locus_names[i] + \"_species_map.txt\")   # The gene tree from the multispecies coalescent process   # Note that if Ne had been a vector of effective population sizes,   # allowing 1 parameter per branch of the species tree, the same line would work.   geneTree[i] ~ dnMultiSpeciesCoalescent(speciesTree=psi, Ne=Ne, taxa=taxon_map)   # We set a good starting value   geneTree[i].setValue(gene_trees[i])}            Although we have defined both the species tree and the gene trees,we have not set up the moves that we use to sample them. We will usesimple tree moves, that deal with a single tree at a time, and joingspecies tree-gene tree moves. Those latter moves are importantbecause they explicitly take into account the interdependencybetween the species tree and the gene trees. These moves are firstdeclared with the species tree as main argument. Then, each genetree is added to the moves.    ## General tree moves used on the species treemoves[++mi] = mvNarrow(psi, weight=5.0)moves[++mi] = mvNNI(psi, weight=1.0)moves[++mi] = mvFNPR(psi, weight=3.0)moves[++mi] = mvGPR(psi, weight=3.0)moves[++mi] = mvSubtreeScale(psi, weight=3.0)moves[++mi] = mvNodeTimeSlideUniform(psi, weight=5.0)## Joint species tree/gene tree movesmove_species_narrow_exchange = mvSpeciesNarrow( speciesTree=psi, weight=20 )move_species_subtree_scale_beta = mvSpeciesSubtreeScaleBeta(psi, weight=5)move_species_subtree_scale = mvSpeciesSubtreeScale(psi, weight=5)## Moves that alter gene treesfor (i in 1:num_loci) {    # moves on each gene tree    moves[++mi] = mvNNI(geneTree[i], 5.0)    moves[++mi] = mvNarrow(geneTree[i], 5.0)    moves[++mi] = mvFNPR(geneTree[i], 3.0)    moves[++mi] = mvGPR(geneTree[i], 2.0)    moves[++mi] = mvSubtreeScale(geneTree[i], 5.0)    moves[++mi] = mvTreeScale(geneTree[i], 1.0, true, 3.0)    moves[++mi] = mvNodeTimeSlideUniform(geneTree[i], 20.0)    # Associating the joint species tree/gene tree moves to each gene tree    move_species_narrow_exchange.addGeneTreeVariable( geneTree[i] )    move_species_subtree_scale_beta.addGeneTreeVariable( geneTree[i] )    move_species_subtree_scale.addGeneTreeVariable( geneTree[i] )}## We must not forget to include the joint moves into the vector of moves!moves[++mi] = move_species_narrow_exchangemoves[++mi] = move_species_subtree_scale_betamoves[++mi] = move_species_subtree_scale            Now we have gene trees, complete with branch lengths, and the movesoperating on them. The next element we need is a clock rate whichtransforms/scales the branch times into branch lengths thatrepresent the expected number of substitutions. Here we will assumefor simplicity that every gene evolves under a global strict clockbut has its own independent clock rate. We can later look into theestimate to see how much the clock rate estimates actually differacross genes. One could also choose to use a relaxed clock modelinstead of a strict clock.    for ( i in 1:num_loci ) {   clock_rate[i] ~ dnExponential(1.0)   moves[++mi] = mvScale(clock_rate[i], weight=2.0)   moves[++mi] = mvSlide(clock_rate[i], weight=3.0)}            Next we need our model for the substitution process. Hence, we justneed to define the substitution matrix. We use a single HKY matrixthat will apply to all sites per gene. Additionally, we assume thatsites all evolve according to the same rate, to save computing time.    for ( i in 1:num_loci ) {    #### specify the HKY substitution model applied uniformly to all sites ###    kappa[i] ~ dnLognormal(0,1)    moves[++mi] = mvScale(kappa[i],weight=1.0)    moves[++mi] = mvSlide(kappa[i], weight=1.0)    pi_prior[i] &lt;- v(1,1,1,1)    pi[i] ~ dnDirichlet(pi_prior[i])    moves[++mi] = mvSimplexElementScale(pi[i],weight=2.0)    #### create a deterministic variable for the rate matrix ####    Q[i] := fnHKY(kappa[i],pi[i])}            Finally, we can create our distribution for character evolution. Wewill use the common ‘PhyloCTMC‘ distribution, which is a continuoustime Markov process along a phylogenetic tree. We create a ‘seq‘variable and attach/clamp each gene to one of the ‘seq‘ variables.    for ( i in 1:num_loci ) {    # the sequence evolution model    seq[i] ~ dnPhyloCTMC(tree=geneTree[i], Q=Q[i], branchRates=clock_rate[i], type=\"DNA\")    # attach the data    seq[i].clamp(data[i])}            Now we have defined all the bricks of the model, and create ourmodel object from it.    # We get a handle on our model.# We can use any node of our model as a handle, here we choose to use the topology.mymodel = model(psi)# Ideally one would run more MCMC samples.# In the interest of time it may be worth running the analysis under the prior.# mymodel.ignoreAllData()            Finally, we need to perform inference under the model, using thedata. For clarity, we put the results in an output folder, and thegene trees will be further included within a folder of their own.    output_folder = \"output_MSC/\"# Monitors to check the progression of the programmonitors[1] = mnScreen(printgen=100, psi)monitors[2] = mnModel(filename=output_folder+\"posterior_MSC_primates.log\",printgen=10, separator = TAB)monitors[3] = mnFile(filename=output_folder+\"posterior_MSC_primates.trees\",printgen=10, separator = TAB, psi)for ( i in 1:num_loci ) {    # We add a monitor for each gene tree    monitors[i+3] = mnFile(filename=output_folder+\"geneTrees/posterior_\" + locus_names[i] + \".trees\",printgen=10, separator = TAB, geneTree[i])}# Here we use a plain MCMC. You could also set nruns=2 for an analysis with 2 replicates.# or use mcmcmc with heated chains.mymcmc = mcmc(mymodel, monitors, moves, nruns=1)mymcmc.burnin(generations=1000,tuningInterval=50)mymcmc.run(generations=3000)mymcmc.operatorSummary()            Now we can perform some post-run analyses, for instance to obtainand save the MAP species tree.    # Now, we will analyze the tree output.# Let us start by reading in the tree tracetreetrace = readTreeTrace(output_folder+\"posterior_MSC_primates.trees\", treetype=\"clock\")# and get the summary of the tree tracetreetrace.summarize()mapTree(treetrace,output_folder+\"posterior_MSC_primates_MAP.tree\")      Things to think aboutDo you find that the full multispecies coalescent mixes well?Is the mixing markedly different when the model is run under the prior?What is the impact of species tree moves on the mixing?Given the acceptance ratios of some of the moves, would you be temptedto change the frequency at which they are tried? If so how?What other improvements could be made to the script?",
        "url": "/tutorials/multispecies_coalescent/",
        "index": "false"
      }
      ,
    
      "tutorials-fbd": {
        "title": "Combined-Evidence Analysis and the Fossilized Birth-Death Process for Stratigraphic Range Data",
        "content": "OverviewThis tutorial demonstrates how to specify the models used in a Bayesian“combined-evidence” phylogenetic analysis of extant and fossil species,combining morphological and molecular data as well as stratigraphicrange data from the fossil record [e.g., (Ronquist et al. 2012; Zhang et al. 2016; Gavryushkina et al. 2017)]. We begin with a conciseintroduction to the models used in this analysis in the  section, followed by a detailed example analysis in  demonstrating how to apply these models inRevBayes (Höhna et al. 2017) and use Markov chain Monte Carlo (MCMC) toestimate the posterior distribution of dated phylogenies for datacollected from living and fossil bears (family Ursidae).IntroductionThe “combined-evidence” analysis described in this tutorial uses aprobabilistic graphical model (Höhna et al. 2014) integrating three separatelikelihood components or data partitions (): onefor molecular data (), one formorphological data (), and one forfossil stratigraphic range data ().In addition, all likelihood components are conditioned on a treetopology with divergence times which is modeled according to a separateprior component (defined in ).Modular components of the graphical model used in the “combined-evidence” analysis described in this tutorial.In  we provide an example of a type of treeestimated from a combined-evidence analysis. This example shows thecomplete tree (A) and the “reconstructed tree”(B).We will describe the distinction between these two trees in the section on .One possiblerealization of the specimen-level fossilized birth-death (described in section) (A) The complete tree shows all lineages both sampled (solidlines) and unsampled (dotted lines).(B) The reconstructed tree shows only the sampled specimens, both fossil and extant.Lineage Diversification and SamplingThe joint prior distribution on tree topologies and divergence times ofliving and extinct species used in this tutorial is described by thefossilized birth-death (FBD) process (Stadler 2010; Heath et al. 2014; Stadler et al. 2018). Thismodel simply treats the fossil observations as part of the processgoverning the tree topology and branch times (the ‘Time Tree Model’ node in). The fossilized birth-death process provides amodel for the distribution of speciation and sampling events i.e. tree topology, speciation times, number of sampled living taxa, andlineage samples before the present(e.g. non-contemporaneous samples likefossils or viruses). This type of tree is shown in . Importantly, this model can be used with orwithout character data for the historical samples. Thus, it provides areasonable prior distribution for analyses combining morphological orDNA data for both extant and fossiltaxa—i.e. the so-called “combined-evidence” or “total evidence”approaches described by Ronquist et al. (2012) and extended by Zhang et al. (2016) andGavryushkina et al. (2017). When matrices of discrete morphological charactersfor both living and fossil species are unavailable, the fossilizedbirth-death model imposes a time structure on the tree bymarginalizingover all possible attachment points for the fossils on the extant tree(Heath et al. 2014), therefore, some prior knowledge of phylogeneticrelationships is important.The FBD model () describes the probability of thetree and fossils conditional on the birth-death parameters:$f[\\mathcal{T} \\mid \\lambda, \\mu, \\rho, \\psi, \\phi]$, where$\\mathcal{T}$ denotes the tree topology, divergence times fossiloccurrence times and the times at which the fossils attach to the tree.The birth-death parameters $\\lambda$ and $\\mu$ denote the speciation andextinction rates, respectively. The ‘fossilrecovery rate’ is denoted $\\psi$ and describes the rate at which fossilsare sampled along lineages of the complete tree. The samplingprobability parameter $\\rho$ represents the probability that an extantspecies is sampled, and $\\phi$ represents the time at which the processoriginated (called the ‘origin time’).A graphical model of the fossilized birth-death model describing the generation of the time tree (in) used in this tutorial. The parameters of thefossilized birth-death process are labeled in orange. The speciation,extinction and fossilization rates are stochastic nodes (circles) drawnfrom exponential distributions, while the origin time is uniformlydistributed. The sampling probability is constant node (square) andequal to one for the tree in  and for the analysis in the exercise given in this tutorial. This model represents the phylogenetic continuous-time Markovchain that links the tree model to the other model components and theobserved sequence data. For more information on probabilistic graphicalmodels and their notation, please see (Höhna et al. 2014).In the example FBD tree shown in , thediversification process originates at time $\\phi$, giving rise to $n=10$species in the present, with both sampled fossils andextant species. All of the lineages represented in A (both solid and dotted lines) show thecomplete tree. This is the tree of all extant and extinct lineagesgenerated by the process.The complete tree is distinct from thereconstructed tree (B) which is the treerepresenting only the lineages sampled as extant taxa or fossils. Fossilobservations (non-extant red circles in ) are recoveredover the lifetime of the process along the lineages of the completetree. If a lineage does not have any descendants sampled in the present (or at the moment it goes extinct),it is lost and cannot be observed, these are the dotted lines in A. The probability must be conditioned on the origintime of the process $\\phi$. The origin ($\\phi$) of a birth-death processis the starting time of the stem lineage, thus this conditions on asingle lineage giving rise to the tree.An important characteristic of the FBD model is that it accounts for theprobability of sampled ancestor-descendant pairs (Foote 1996). Giventhat fossils are sampled from lineages in the diversification process,the probability of sampling fossils that are ancestors to taxa sampledat a later date is correlated with the turnover rate ($r=\\mu/\\lambda$),the fossil recovery rate ($\\psi$), and the probability of sampling an extant taxon ($\\rho$). This feature is important,particularly for datasets with many sampled fossils. In the example(), several of the fossils have sampleddescendants. These fossils have solid black lines leading to thepresent.Assignment of fossil specimens to taxonomic speciesThe most basic version of the FBD treats individual fossil specimens as separate taxonomic entities. This is the standard specimen-level “Fossilized Birth Death Process” (implemented as FBDP in RevBayes).However, in most cases taxonomic species are represented in the fossil record by multiple fossil specimens sampled atdifferent stratigraphic ages. These stratigraphic ranges are the first and last occurrences observed for a single morpho-species in the fossilrecord (for extant species, the last occurrence is the present day).In order to compute the density of the FBD while accounting for this stratigraphic species range data, we need to assume some model of speciation that will allow us to assign fossil specimens to species. Stadler et al. (2018) describe an extension to the FBD which assigns lineages to taxonomic species through a process of asymmetric or “budding” speciation. This model assumes that at each asymmetric speciation event, one descendant species represents a new species, while the other descendant represents the continuation of the parent species. In this way, each lineage (and therefore all the fossil specimens sampled along that lineage) can be mapped to a unique species. An example realization of such a speciation process is shown in .\\(\\implies\\)One possible realization of asymmetric speciation (light blue) along one lineage of the fossilized birth death tree from .(A) The highlighted lineage originates through an asymmetric speciation event by branching upward,and then continues past additional speciation events by downward branching.Fossil specimens lying along this path are assigned to the same taxonomic species.(B) The same lineage is highlighted in the oriented tree with lineages representing the same species collapsed into straight lines.Stadler et al. (2018) show how to compute the density of the “sampled tree”, which is obtained by pruning all unsampled lineages after asymmetric species identities have been assigned in the complete tree (). This gives rise to the “Fossilized Birth Death Range Process” (implemented as FBDRP in RevBayes). This is the model we will be using in this tutorial. It is important to note that the tips in the sampled tree represent the age of the youngest sample for each species.The “sampled tree” is produced by pruning unsampled lineages from the oriented tree in B and collapsing intermediate fossil samples other than the first and last occurrences into stratigraphic ranges.Nucleotide Sequence EvolutionThe model component for the molecular data uses a generaltime-reversible model of nucleotide evolution and gamma-distributed rateheterogeneity across sites (the Substitution Model and Sites Model in ). Thismodel of sequence evolution is covered thoroughly in theNucleotide substitution modelstutorial.Lineage-Specific Rates of Sequence EvolutionRates of nucleotide sequence evolution can vary widely among lineages,and so models that account for this variation by relaxing the assumptionof a strict molecular clock (Zuckerkandl and Pauling 1962) can allow for moreaccurate estimates of substitution rates and divergence times(Drummond et al. 2006). The simplest type of relaxed clock model assumes thatlineage-specific substitution rates are independent or “uncorrelated”.One example of such an uncorrelated relaxed model is the uncorrelatedexponential relaxed clock, in which the substitution rate for eachlineage is assumed to be independent and identically distributedaccording to an exponential density (). This is Branch Rates Model for the Molecular Data () that we will use in this tutorial. Another possible uncorrelated relaxedclock model is the uncorrelated lognormal model, described in theRelaxed Clocks &amp; Time Treestutorial [also see Thorne and Kishino (2002)].A graphical model of theuncorrelated exponential relaxed clock model. In this model, the clockrate on each branch is independent and identically distributed accordingto an exponential density with mean drawn from an exponential hyperpriordistribution.Morphological Character EvolutionFor the vast majority of extinct species, fossil morphology is theprimary source of phylogenetically informative characters. Therefore, anappropriate model of morphological character evolution is needed toreliably infer the positions of these species in a phylogeneticanalysis. The Mk model (Lewis 2001) uses a generalized Jukes-Cantormatrix to allow for the incorporation of morphology into likelihood andBayesian analyses. In its simplest form, this model assumes thatcharacters change states symmetrically—that a given character is aslikely to transition from a one state to another as it is to reverse. Inthis tutorial we will consider only binary morphological characters,i.e. characters that are observed in one oftwo states, 0 or 1. For example, the assumption of the single-rate Mkmodel applied to our binary character would mean that a change from a 0state to a 1 state is as likely as a change from a 1 state to a 0 state.This assumption is equivalent to assuming that the stationaryprobability of being in a 1 state is equal to $1/2$.In this tutorial, we will apply a single-rate Mk model as a prior onbinary morphological character change. If you are interested extensionsof the Mk model that relax the assumptions of symmetric state change,please see Discrete morphology - Tree Inference.Because of the way morphological data are collected, we need to exercisecaution in how we model the data. Traditionally, phylogenetic trees werebuilt from morphological data using parsimony. Therefore, only parsimonyinformative characters were collected—that is, characters that areuseful for discriminating between phylogenetic hypotheses under themaximum parsimony criterion. This means that many morphological datasetsdo not contain invariant characters orautapomorphies, as theseare not parsimony informative. However, by excluding these slow-evolvingcharacters, estimates of the branch lengths can be inflated(Felsenstein 1992; Lewis 2001). Therefore, it is important to use modelsthat can condition on this data-acquisition bias. RevBayes has twoways of doing this: one is used for datasets in which only parsimonyinformative characters are observed; the other is for datasets in whichparsimony informative characters and parsimony uninformative variablecharacters (such as autapomorphies) are observed.The Morphological ClockJust like with the molecular data ,our observations of discrete morphological characters are conditional onthe rate of change along each branch in the tree. This model componentdefines the of the in the generalized graphical model shown in . The relaxed clock model we described for themolecular data in  it allows thesubstitution rate to vary through time and among lineages. For themorphological data, we will instead use a “strict clock” model(Zuckerkandl and Pauling 1962), in which the rate of discrete character change isassumed to be constant throughout the tree. The strict clock is thesimplest morphological branch rate model we can construct (graphicalmodel shown in ).The graphical-modelrepresentation of the branch-rate model governing the evolution ofmorphological characters. This model is consistent with a strictmorphological clock, where every branch has the same rate of change($c$) and that rate is drawn from an exponential distribution with arate parameter of $\\delta_c$.Example: Estimating the Phylogeny and Divergence Times of Fossil and Extant BearsIn this exercise, we will combine different types of data from 22species of extant and extinct bears to estimate a posterior distributionof calibrated time trees for this group. We have molecular sequence datafor ten species, which represent all of the eight living bears and twoextinct species sequenced from sub-fossil specimens (Arctodus simus,Ursus spelaeus). The sequence alignment provided is a 1,000 bp fragmentof the cytochrome-b mitochondrial gene (Krause et al. 2008). The morphologicalcharacter matrix unites 18 taxa (both fossil and extant) with 62 binary(states 0 or 1) characters (Abella et al. 2012). For the fossil species,occurrence times are obtained from the literature or fossil databaseslike the Fossilworks PaleoDB or the FossilCalibration Database, or from your ownpaleontological expertise. The 14 fossil species used in this analysisare listed in  along with the age range for thespecies and relevant citation. Finally, there are two fossil species(Parictis montanus, Ursus abstrusus) for which we do not havemorphological character data (or molecular data) and we must use priorinformation about their phylogenetic relationships to incorporate thesetaxa in our analysis. This information will be applied using cladeconstraints.Data and FilesOn your own computer or your remote machine, create a directory for this tutorial.In this directory, create another directory called data, and download the datafiles which you can find at the top of this page.In the data folder, you will find the following files:      bears_taxa.tsv: a tab-separated table listing every bear species(both fossil and extant) and their occurrence age ranges. For extanttaxa, the minimum age is 0.0(i.e. the present).        bears_cytb.nex: an alignment in NEXUS format of 1,000 bp ofcytochrome b sequences for 10 bear species. This alignment includes8 living bears and 2 extinct sub-fossil bears.        bears_morphology.nex: a matrix of 62 discrete, binary (coded 0or 1) morphological characters for 18 species of fossil andextant bears.  Age range data for fossil and extant bear species.            Fossil Species      Age      Reference                  Ailuropoda melanoleuca      0.0-1.24      (Abella et al. 2012)              Helarctos malayanus      0.0-1.78      (Abella et al. 2012)              Melursus ursinus      0.0-1.8      (Abella et al. 2012)              Tremarctos ornatus      0.0-0.0      (Abella et al. 2012)              Ursus americanus      0.0-1.84      (Abella et al. 2012)              Ursus arctos      0.0-1.71      (Abella et al. 2012)              Ursus maritimus      0.0-0.65      (Abella et al. 2012)              Ursus thibetanus      0.0-1.18      (Abella et al. 2012)              Agriarctos spp.      4.9–7.75      (Abella et al. 2011; Abella et al. 2012)              Ailurarctos lufengensis      5.8–8.2      (Jin et al. 2007; Abella et al. 2012)              Arctodus simus      0.012–2.588      (Churcher et al. 1993; Krause et al. 2008)              Ballusia elmensis      13.7–16      (Ginsburg and Morales 1998; Abella et al. 2012)              Indarctos vireti      7.75–8.7      (Montoya et al. 2001; Abella et al. 2012)              Indarctos arctoides      8.7–9.7      (Geraads et al. 2005; Abella et al. 2012)              Indarctos punjabiensis      4.9–9.7      (Baryshnikov 2002; Abella et al. 2012)              Kretzoiarctos beatrix      11.2–11.8      (Abella et al. 2011; Abella et al. 2012)              Parictis montanus      33.9–37.2      (Clark and Guensburg 1972; Krause et al. 2008)              Ursavus primaevus      13.65–15.97      (Andrews and Tobien 1977; Abella et al. 2012)              Ursavus brevihinus      15.97–16.9      (Heizmann et al. 1980; Abella et al. 2012)              Ursus abstrusus      1.8–5.3      (Bjork 1970; Krause et al. 2008)              Ursus spelaeus      0.027–0.25      (Loreille et al. 2001; Krause et al. 2008)              Zaragocyon daamsi      20–22.8      (Ginsburg and Morales 1995; Abella et al. 2012)      Getting StartedCreate a new directory called scripts.When you execute RevBayes in this exercise, you will do so within themain directory you created. Thus, if you are using a Unix-based operating system, we recommend thatyou add the RevBayes binary to your path.Creating Rev FilesFor complex models and analyses, it is best to create Rev script filesthat will contain all of the model parameters, moves, and functions. Inthis exercise, you will work primarily in your text editor andcreate a set of modular files that will be easily managed andinterchanged. You will write the following files from scratch and savethem in the scripts directory:      mcmc_CEFBDRP_Ranges.Rev: the master Rev file that loads the data, theseparate model files and specifies the monitors and MCMC sampler.        model_FBDRP.Rev: specifies the model parameters and movesrequired for the fossilized birth-death range process prior on the tree topology,divergence times, fossil occurrence ranges, anddiversification dynamics.        model_UExp.Rev: specifies the components of theuncorrelated exponential model of lineage-specific substitutionrate variation.        model_GTRG.Rev: specifies the parameters and moves for thegeneral time-reversible model of sequence evolution withgamma-distributed rates across sites (GTR+$\\Gamma$).        model_Morph.Rev: specifies the model describing discretemorphological character change (binary characters) under a strictmorphological clock.  All of the files that you will create are also provided in theRevBayes tutorial at the top of the page to download. Please refer to these files toverify or troubleshoot your own scripts.Start the Master Rev File and Import DataIn this section you will begin the master file that you will load intoRevBayes when you’ve completed all of the components of the analysis.  Open your text editor and create the master Revfile called mcmc_CEFBDRP_Ranges.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.In this file you will write the Rev commands forloading in the taxon list and managing the data matrices. Then, startingin section , you will move on to writingmodule files for each of the model components. Once the model files arecomplete, you will return to editing mcmc_CEFBDRP_Ranges.Rev and complete theRev script with the instructions given in section, you will move on to writing the .Load Taxon ListBegin the Rev script by loading in the list of taxon names from thebears_taxa.tsv file using the readTaxonData function.taxa &lt;- readTaxonData(\"data/bears_taxa.tsv\")This function reads a tab-delimited file and creates a variable calledtaxa that is a list of all of the taxon names relevant to thisanalysis. This list includes all of the fossil and extant bear speciesnames in the first columns and minimum/maximum ages in the second/third columns.Load Data MatricesRevBayes uses the function readDiscreteCharacterData to load adata matrix to the workspace from a formatted file. This function can beused for both molecular sequences and discrete morphological characters.Load the cytochrome-b sequences from file and assign the data matrix toa variable called cytb.cytb &lt;- readDiscreteCharacterData(\"data/bears_cytb.nex\")Next, import the morphological character matrix and assign it to thevariable morpho.morpho &lt;- readDiscreteCharacterData(\"data/bears_morphology.nex\")Add Missing TaxaIn the descriptions of the files in section, we mentioned that the two data matriceshave different numbers of taxa. Thus, we must add any taxa that are notfound in the molecular (cytb) partition(i.e. are only found in the fossil data) tothat data matrix as missing data (with ? in place of all characters),and do the same with the morphological data partition (morpho). Inorder for all the taxa to appear on the same tree, they all need to bepart of the same dataset, as opposed to present in separate datasets.This ensures that there is a unified taxon set that contains all of ourtips.cytb.addMissingTaxa( taxa )morpho.addMissingTaxa( taxa )Create Helper VariablesBefore we begin writing the Rev scripts for each of the modelcomponents, we need to instantiate a couple “helper variables” that willbe used by downstream parts of our model specification files. Thesevariables will be used in more than one of the module files so it’s bestto initialize them in the master file.Create a new constant node called n_taxa that is equal to the numberof species in our analysis (22).n_taxa &lt;- taxa.size()Next, create a workspace variable called moves. This variable is a vector that will contain all of the MCMC moves usedto propose new states for every stochastic node in the model graph. Eachtime a new stochastic node is created in the model, we can append the move to this vector.moves = VectorMoves()One important distinction here is that moves is part of the RevBayesworkspace and not the hierarchical model. Thus, we use the workspaceassignment operator = instead of the constant node assignment &lt;-.  Save your current working version of mcmc_CEFBDRP_Ranges.Rev in the scripts directory.We will now move on to the next Rev file and will completemcmc_CEFBDRP_Ranges.Rev in section .The Fossilized Birth-Death ProcessIn this section we will define the models described in section above. Ifnecessary, please review the graphical models depicted for thefossilized birth-death process ().  Open your text editor and create the fossilized birth-death model filecalled model_FBDRP.Rev in the scripts directory.Speciation and Extinction RatesTwo key parameters of the FBD process are the speciation rate (the rateat which lineages are added to the tree, denoted by $\\lambda$ in) and the extinction rate (the rate at whichlineages are removed from the tree, $\\mu$ in ).We’ll place exponential priors on both of these values. Each parameteris assumed to be drawn independently from a different exponentialdistribution with rates $\\delta_{\\lambda}$ and $\\delta_{\\mu}$respectively (see ). Here, we will assume that$\\delta_{\\lambda} = \\delta_{\\mu} = 10$. Note that an exponentialdistribution with $\\delta = 10$ has an expected value (mean) of $1/10$.Create the exponentially distributed stochastic nodes for thespeciation_rate and extinction_rate using the ~ operator.speciation_rate ~ dnExponential(10)extinction_rate ~ dnExponential(10)For every stochastic node we declare, we must also specify proposalalgorithms (called moves) to sample the value of the parameter inproportion to its posterior probability. If a move is not specified fora stochastic node, then it will not be estimated, but fixed to itsinitial value.The rate parameters for extinction and speciation are both positive,real numbers (i.e. non-negative floatingpoint variables). For both of these nodes, we will use a scaling move(mvScale), which proposes multiplicative changes to a parameter.Many moves also require us to set a tuning value, called lambda formvScale, which determine the size of the proposed change. Here, wewill use three scale moves for each parameter with different values oflambda. By using multiple moves for a single parameter, we will improvethe mixing of the Markov chain.moves.append( mvScale(speciation_rate, lambda=0.01, weight=1) )moves.append( mvScale(speciation_rate, lambda=0.1,  weight=1) )moves.append( mvScale(speciation_rate, lambda=1.0,  weight=1) )moves.append( mvScale(extinction_rate, lambda=0.01, weight=1) )moves.append( mvScale(extinction_rate, lambda=0.1,  weight=1) )moves.append( mvScale(extinction_rate, lambda=1,    weight=1) )You will also notice that each move has a specified weight. Thisoption allows you to indicate how many times you would like a given moveto be performed at each MCMC cycle. The way that we will run our MCMCfor this tutorial will be to execute a schedule of moves at each stepin our chain instead of just one move per step, as is done inMrBayes (Ronquist and Huelsenbeck 2003) or BEAST(Drummond et al. 2012; Bouckaert et al. 2014). Here, if we were to run our MCMC withour current vector of 6 moves, then our move schedule would perform 6moves at each cycle. Within a cycle, an individual move is chosen fromthe move list in proportion to its weight. Therefore, with all six movesassigned weight=1, each has an equal probability of being executed andwill be performed on average one time per MCMC cycle. For moreinformation on moves and how they are performed in RevBayes, pleaserefer to the Introduction to Markov chain Monte Carlo (MCMC) Sampling and Nucleotide substitution models tutorials.In addition to the speciation ($\\lambda$) and extinction ($\\mu$) rates,we may also be interested in inferring diversification ($\\lambda - \\mu$)and turnover ($\\mu/\\lambda$). Since these parameters can be expressed asa deterministic transformation of the speciation and extinction rates,we can monitor (that is, track the values of these parameters, and printthem to a file) their values by creating two deterministic nodes usingthe := operator.diversification := speciation_rate - extinction_rateturnover := extinction_rate/speciation_rateProbability of Sampling Extant TaxaAll extant bears are represented in this dataset. Therefore, we will fixthe probability of sampling an extant lineage ($\\rho$ in) to 1. The parameter rho will be specified as aconstant node using the &lt;- operator.rho &lt;- 1.0Because $\\rho$ is a constant node, we do not have to assign a move tothis parameter.The Fossil Sampling RateSince our data set includes serially sampled lineages, we must alsoaccount for the rate of sampling back in time. This is the fossilsampling (or recovery) rate ($\\psi$ in ), which wewill instantiate as a stochastic node (named psi). As with thespeciation and extinction rates(see ), we will use anexponential prior on this parameter and use scale moves to sample valuesfrom the posterior distribution.psi ~ dnExponential(10) moves.append( mvScale(psi, lambda=0.01, weight=1) )moves.append( mvScale(psi, lambda=0.1,  weight=1) )moves.append( mvScale(psi, lambda=1,    weight=1) )The Origin TimeWe will condition the FBD process on the origin time ($\\phi$ in) of bears, and we will specify a uniformdistribution on the origin age. For this parameter, we will use asliding window move (mvSlide). A sliding window samples a parameteruniformly within an interval (defined by the half-width delta).Sliding window moves can be tricky for small values, as the window mayoverlap zero. However, for parameters such as the origin age, there islittle risk of this being an issue.origin_time ~ dnUnif(37.0, 55.0)moves.append( mvSlide(origin_time, delta=0.01, weight=5.0) )moves.append( mvSlide(origin_time, delta=0.1,  weight=5.0) )moves.append( mvSlide(origin_time, delta=1,    weight=5.0) )Note that we specified a higher move weight for each of the proposalsoperating on origin_time than we did for the three previousstochastic nodes. This means that our move schedule will propose fivetimes as many updates to origin_time than it will tospeciation_rate, extinction_rate, or psi.The FBD Distribution ObjectAll the parameters of the FBD process have now been specified. The nextstep is to use these parameters to define the FBD tree priordistribution, which we will call fbd_dist. Note that, because we areusing stratigraphic range data to represent our sampled species,we use the FBDRP distribution function(as opposed to the specimen-level distribution function FBDP; see )fbd_dist = dnFBDRP(origin=origin_time, lambda=speciation_rate, mu=extinction_rate, psi=psi, rho=rho, taxa=taxa)Clade ConstraintsNote that we created the distribution as a workspace variable using theworkspace assignment operator =. This is because we still need toinclude a topology constraint in our final specification of the treeprior. Specifically, we do not have any morphological or molecular datafor the fossil species Ursus abstrusus. Therefore, in order to use theage of this fossil as an observation, we need to specify to which cladeit belongs. In this case, Ursus abstrusus belongs to the subfamilyUrsinae, so we define a clade for the total group Ursinae includingUrsus abstrusus.clade_ursinae = clade(\"Melursus_ursinus\", \"Ursus_arctos\", \"Ursus_maritimus\",                       \"Helarctos_malayanus\", \"Ursus_americanus\", \"Ursus_thibetanus\",                       \"Ursus_abstrusus\", \"Ursus_spelaeus\")constraints = v(clade_ursinae)Then we can specify the final constrained tree prior distribution bycreating a vector of constraints, and providing it along with theworkspace FBD distribution to the constrained topology distribution.Here we use the stochastic assignment operator ~ to create astochastic node for our constrained FBD tree variable (calledfbd_tree).fbd_tree ~ dnConstrainedTopology(fbd_dist, constraints=constraints)It is important to recognize that we do not know if Ursus abstrusus isa crown or stem Ursinae. Because of this, we defined this cladeconstraint so that it constrained the total group Ursinae and thisuncertainty is taken into account. As a result, our MCMC willmarginalize over both stem and crown positions of U. abstrusus andsample the phylogeny in proportion to its posterior probability,conditional on our model and data.Additionally, we do not have morphological data for the fossil speciesParictis montanus. However, we will not create a clade constraint forthis taxon because it is a very old, stem-fossil bear. Thus, the MCMCmay propose to place this taxon anywhere in the tree (except within theclade constraint we made above). This allows us to account for themaximum amount of uncertainty in the placement of P. montanus.Moves on the Tree Topology and Node AgesNext, in order to sample from the posterior distribution of trees, weneed to specify moves that propose changes to the topology (mvFNPR)and node times (mvNodeTimeSlideUniform). Included with these moves isa proposal that will collapse or expand a fossil branch(mvCollapseExpandFossilBranch). This will change a fossil that is asampled ancestor (see  andSect. ) so that it is on its own branch and viceversa. In addition, when conditioning on the origin time, we also needto explicitly sample the root age (mvRootTimeSlideUniform).moves.append( mvFNPR(fbd_tree, weight=15.0) )moves.append( mvCollapseExpandFossilBranch(fbd_tree, origin_time, weight=6.0) )moves.append( mvNodeTimeSlideUniform(fbd_tree, weight=40.0) )moves.append( mvRootTimeSlideUniform(fbd_tree, origin_time, weight=5.0) )Incorporating Specimen-Level Fossil Age UncertaintyIf we are using the specimen-level FBDP distribution (see ),in order to account for uncertainty in the ages of fossil specimens,we can incorporate intervals on the fossil ages.These intervals can represent, for example, stratigraphic ranges or measurement error.We do this by assuming each fossil can occur withuniform probability anywhere within its observed interval. This issomewhat different from the typical approach to node calibration. Here,instead of treating the calibration density as an additional priordistribution on the tree, we treat it as the likelihood of our fossildata given the tree parameter. Specifically, we assume the likelihood ofa particular fossil observation $\\mathcal{F}_i$ is equal to one if thefossil’s inferred age on the tree $t_i$ falls within its observed timeinterval $(a_i,b_i)$, and zero otherwise:\\[f[\\mathcal{F}_i \\mid a_i, b_i, t_i] = \\begin{cases}1 &amp; \\text{if } a_i &lt; t_i &lt; b_i\\\\0 &amp; \\text{otherwise}\\end{cases}\\]In other words, we assume the likelihood is equal to oneif the inferred age is consistent with the observed data. We canrepresent this likelihood in RevBayes using a distribution that isproportional to the likelihood,i.e. non-zero when the likelihood is equalto one. This model component representsthe observed in the modular graphical model shown in .A graphical model of thefossil age likelihood model used in this tutorial. The likelihood offossil observation $\\mathcal{F}_i$ is uniform and non-zero when theinferred fossil age $t_i$ falls within the observed time interval$(a_i,b_i)$.Sampling Fossil Specimen AgesWhen using the specimen-level FBD distribution FBDP, we can account for uncertainty in the age estimates of ourfossils specimens using the observed minimum and maximum stratigraphic ages.First, we loop over the the list of taxa. For each fossil observation, we create auniform random variable representing the likelihood. Remember, we canrepresent the fossil likelihood using any uniform distribution that isnon-zero when the likelihood is equal to one.For example, if $t_i$ is the inferred fossil age and $(a_i,b_i)$ is theobserved stratigraphic interval, we know the likelihood is equal to onewhen $a_i &lt; t_i &lt; b_i$, or equivalently $t_i - b_i &lt; 0 &lt; t_i - a_i$. Solet’s represent the likelihood using a uniform random variable uniformlydistributed in $(t_i - b_i, t_i - a_i)$ and clamped at zero.fossils = fbd_tree.getFossils()for(i in 1:fossils.size()){    t[i] := tmrca(fbd_tree, clade(fossils[i]))    a_i = fossils[i].getMinAge()    b_i = fossils[i].getMaxAge()    F[i] ~ dnUniform(t[i] - b_i, t[i] - a_i)    F[i].clamp( 0 )}Finally, we add a move that samples the ages of the fossil nodes on thetree.moves.append( mvFossilTimeSlideUniform(fbd_tree, origin_time, weight=5.0) )Monitoring Parameters of Interest using Deterministic NodesThere are additional parameters that may be of particular interest to usthat are not directly inferred as part of this graphical model. As withthe diversification and turnover nodes specified in, we can createdeterministic nodes to sample the posterior distributions of theseparameters. Create a deterministic node called num_samp_anc thatwill compute the number of sampled ancestors in our fbd_tree.num_samp_anc := fbd_tree.numSampledAncestors();We are also interested in the age of the most-recent-common ancestor(MRCA) of all living bears. To monitor the age of this node in our MCMCsample, we must use the clade function to identify the node.Importantly, since we did not include this clade in our constraints thatdefined fbd_tree, this clade will not be constrained to bemonophyletic. Once this clade is defined we can instantiate adeterministic node called age_extant with the tmrca function thatwill record the age of the MRCA of all living bears.clade_extant = clade(\"Ailuropoda_melanoleuca\",\"Tremarctos_ornatus\",\"Melursus_ursinus\",                    \"Ursus_arctos\",\"Ursus_maritimus\",\"Helarctos_malayanus\",                    \"Ursus_americanus\",\"Ursus_thibetanus\")age_extant := tmrca(fbd_tree, clade_extant)Finally, we will monitor the tree after removing taxa for which we didnot have any molecular or morphological data. The phylogenetic placementof these taxa is based only on their occurrence times and any cladeconstraints we applied (see ).Because no data are available to resolve their relationships to otherlineages, we will treat their placement as nuisanceparameters andremove them from the output.We will remove two fossil taxa, Parictis montanus and Ursusabstrusus, from every tree in the trace file before summarizing thesamples. Use the fnPruneTree function to create a deterministic treevariable pruned_tree from which these taxa have been pruned. We willmonitor this tree instead of fbd_tree.pruned_tree := fnPruneTree(fbd_tree, prune=v(taxa[17],taxa[20]))  You have completed the FBD model file. Save model_FBDRP.Rev in the scripts directory.The Uncorrelated Exponential Relaxed Clock ModelWe will now define the molecular relaxed clock model.  Open your text editor and create the lineage-specific branch-rate modelfile called model_UExp.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.For our hierarchical, uncorrelated exponential relaxed clock model(described in section  and shown in), we first define the mean branch rate as anexponential random variable. Then, we specify scale proposal moves onthe mean rate parameter.branch_rates_mean ~ dnExponential(10.0)moves.append( mvScale(branch_rates_mean, lambda=0.01, weight=1.0) )moves.append( mvScale(branch_rates_mean, lambda=0.1,  weight=1.0) )moves.append( mvScale(branch_rates_mean, lambda=1.0,  weight=1.0) )Before creating a rate parameter for each branch, we need to get thenumber of branches in the tree. For rooted trees with $n$ taxa, thenumber of branches is $2n-2$.n_branches &lt;- 2 * n_taxa - 2Then, use a for loop to define a rate for each branch. The branch ratesare independent and identically exponentially distributed with meanequal to the mean branch rate parameter we specified above. For eachrate parameter we also create scale proposal moves.for(i in 1:n_branches){    branch_rates[i] ~ dnExp(1/branch_rates_mean)    moves.append( mvScale(branch_rates[i], lambda=1.0,  weight=1.0) )    moves.append( mvScale(branch_rates[i], lambda=0.1,  weight=1.0) )    moves.append( mvScale(branch_rates[i], lambda=0.01, weight=1.0) )}Lastly, we use a vector scale move to propose changes to all branchrates simultaneously. This way we can sample the total branch rateindependently of each individual rate, which can improve mixing.moves.append( mvVectorScale(branch_rates, lambda=0.01, weight=4.0) )moves.append( mvVectorScale(branch_rates, lambda=0.1,  weight=4.0) )moves.append( mvVectorScale(branch_rates, lambda=1.0,  weight=4.0) )You have completed the molecular relaxed clock model file. Save model_UExp.Rev inthe scripts directory.The General Time-Reversible + Gamma Model of Nucleotide Sequence EvolutionIn this section we will define our nucleotide sequence evolution model.  Open your text editor and create the molecular substitution model filecalled model_GTRG.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.For our nucleotide sequence evolution model, we need to define a generaltime-reversible (GTR) instantaneous-rate matrix(i.e. $Q$-matrix). A nucleotide GTR matrixis defined by a set of 4 stationary frequencies, and 6 exchangeabilityrates. We create stochastic nodes for these variables, each drawn from auniform Dirichlet prior distribution.sf_hp &lt;- v(1,1,1,1)sf ~ dnDirichlet(sf_hp)er_hp &lt;- v(1,1,1,1,1,1)er ~ dnDirichlet(er_hp)We need special moves to propose changes to a Dirichlet random variable,also known as a simplex (a vector constrained sum to one). Here, we usea mvSimplexElementScale move, which scales a single element of asimplex and then renormalizes the vector to sum to one. The tuningparameter alpha specifies how conservative the proposal should be,with larger values of alpha leading to proposals closer to the currentvalue.moves.append( mvSimplexElementScale(er, alpha=10.0, weight=5.0) )moves.append( mvSimplexElementScale(sf, alpha=10.0, weight=5.0) )Then we can define a deterministic node for our GTR $Q$-matrix using thespecial GTR matrix function (fnGTR).Q_cytb := fnGTR(er,sf)Next, in order to model gamma-distributed rates across, we create anexponential parameter $\\alpha$ for the shape of the gamma distribution,along with scale proposals.alpha_cytb ~ dnExponential( 1.0 )moves.append( mvScale(alpha_cytb, lambda=0.01, weight=1.0) )moves.append( mvScale(alpha_cytb, lambda=0.1,  weight=1.0) )moves.append( mvScale(alpha_cytb, lambda=1,    weight=1.0) )Then we create a Gamma$(\\alpha,\\alpha)$ distribution, discretized into 4rate categories using the fnDiscretizeGamma function. Here,rates_cytb is a deterministic vector of rates computed as the mean ofeach category.rates_cytb := fnDiscretizeGamma( alpha_cytb, alpha_cytb, 4 )Finally, we can create the phylogenetic continuous time Markov chain(PhyloCTMC) distribution for our sequence data, including thegamma-distributed site rate categories, as well as the branch ratesdefined as part of our exponential relaxed clock. We set the value ofthis distribution equal to our observed data and identify it as a staticpart of the likelihood using the clamp method.phySeq ~ dnPhyloCTMC(tree=fbd_tree, Q=Q_cytb, siteRates=rates_cytb, branchRates=branch_rates, type=\"DNA\")phySeq.clamp(cytb)  You have completed the GTR model file. Save model_GTRG.Rev inthe scripts directory.We will now move on to the next model file.Modeling the Evolution of Binary Morphological CharactersIn this section we will define the model of morphological character evolution.  Open your text editor and create the morphological character model filecalled model_Morph.Rev in the scripts directory.As stated in the introduction () we willuse Mk to model our data. Because the Mk model is a generalization ofthe Jukes-Cantor model, we will initialize our Q matrix from a Jukes-Cantormatrix.Q_morpho := fnJC(2)As in the molecular data partition, we will allow gamma-distributed rateheterogeneity among sites.alpha_morpho ~ dnExponential( 1.0 )rates_morpho := fnDiscretizeGamma( alpha_morpho, alpha_morpho, 4 )moves.append( mvScale(alpha_morpho, lambda=0.01, weight=5.0) )moves.append( mvScale(alpha_morpho, lambda=0.1,  weight=3.0) )moves.append( mvScale(alpha_morpho, lambda=1,    weight=1.0) )The phylogenetic model also assumes that each branch has a rate ofmorphological character change. For simplicity, we will assume a strictexponential clock—meaning that every branch has the same rate drawn froman exponential distribution (see ).clock_morpho ~ dnExponential(1.0)moves.append( mvScale(clock_morpho, lambda=0.01, weight=4.0) )moves.append( mvScale(clock_morpho, lambda=0.1,  weight=4.0) )moves.append( mvScale(clock_morpho, lambda=1,    weight=4.0) )As in our molecular data partition, we now combine our data and ourmodel in the phylogenetic CTMC distribution. There are some uniqueaspects to doing this for morphology.You will notice that we have an option called coding. This optionallows us to condition on biases in the way the morphological data werecollected (ascertainment bias). The option coding=variable specifiesthat we should correct for coding only variable characters (discussed in).phyMorpho ~ dnPhyloCTMC(tree=fbd_tree, siteRates=rates_morpho, branchRates=clock_morpho, Q=Q_morpho, type=\"Standard\", coding=\"variable\")phyMorpho.clamp(morpho)You have completed the morphology model file. Save model_Morph.Rev inthe scripts directory.We will now move on to the next model file.Complete Master Rev File  Return to the master Rev file you created in section called mcmc_CEFBDRP_Ranges.Rev in the scripts directory.Source Model ScriptsRevBayes uses the source function to load commands from Revfiles into the workspace. Use this function to load in the model scriptswe have written in the text editor and saved in the scripts directory.source(\"scripts/model_FBDRP.Rev\") # FBD tree priorsource(\"scripts/model_UExp.Rev\") # UExp relaxed clocksource(\"scripts/model_GTRG.Rev\") # Molecular substitution model (GTR+G)source(\"scripts/model_Morph.Rev\") # Morphological character change modelCreate Model ObjectWe can now create our workspace model variable with our fully specifiedmodel DAG. We will do this with the model function and provide asingle node in the graph (sf).mymodel = model(sf)The object mymodel is a wrapper around the entire model graph andallows us to pass the model to various functions that are specific toour MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our master Rev file is to specify themonitors and output file names. For this, we create a vector calledmonitors that will each sample and record or output our MCMC.monitors = VectorMonitors()The first monitor we will create will monitor every named randomvariable in our model graph. This will include every stochastic anddeterministic node using the mnModel monitor. The only parameter thatis not included in the mnModel is the tree topology. Therefore, theparameters in the file written by this monitor are all numericalparameters written to a tab-separated text file that can be opened byaccessory programs for evaluating such parameters. We will also name theoutput file for this monitor and indicate that we wish to sample ourMCMC every 10 cycles.monitors.append( mnModel(filename=\"output/bears.log\", printgen=10) )The mnFile monitor writes any parameter we specify to file. Thus, ifwe only cared about the speciation rate and nothing else (this is not atypical or recommended attitude for an analysis this complex) wewouldn’t use the mnModel monitor above and just use the mnFilemonitor to write a smaller and simpler output file. Since the treetopology is not included in the mnModel monitor (because it is notnumerical), we will use mnFile to write the tree to file by specifyingour pruned_tree variable in the arguments. Remember, we aremonitoring the tree with nuisance taxa pruned out (see).monitors.append( mnFile(filename=\"output/bears.trees\", printgen=10, pruned_tree) )The last monitor we will add to our analysis will print information tothe screen. Like with mnFile we must tell mnScreen which parameterswe’d like to see updated on the screen. We will choose the age of theMCRCA of living bears (age_extant), the number of sampled ancestors(num_samp_anc), and the origin time (origin_time).monitors.append( mnScreen(printgen=10, age_extant, num_samp_anc, origin_time) )Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now createthe workspace variable that defines our MCMC run. We do this using themcmc function that simply takes the three main analysis componentsas arguments.mymcmc = mcmc(mymodel, monitors, moves)The MCMC object that we named mymcmc has a member method calledrun. This will execute our analysis and we will set the chainlength to 10000 cycles using the generations option.mymcmc.run(generations=10000)Once our Markov chain has terminated, we will want RevBayes to close.Tell the program to quit using the q() function.q()  You made it! Save all of your files.Execute the MCMC AnalysisWith all the parameters specified and all analysis components in place,you are now ready to run your analysis. The Rev scripts you justcreated will all be used by RevBayes and loaded in the appropriateorder.Begin by running the RevBayes executable. In Unix systems, type thefollowing in your terminal (if the RevBayes binary is in your path):Provided that you started RevBayes from the correct directory, you can then use thesource function to feed RevBayes your master script file(mcmc_CEFBDRP_Ranges.Rev).source(\"scripts/mcmc_CEFBDRP_Ranges.Rev\")This will execute the analysis and you should see the various parameters you included when you created mnScreen printed to the screen every 10 generations.When the analysis is complete, RevBayes will quit and you will have anew directory called output that will contain all of the files youspecified with the monitors (see ).Evaluate and Summarize Your ResultsIn this section, we will evaluate the mixing and convergence of ourMCMC simulation using the program Tracer. We can alsosummarize the marginal distributions for particular parameters we’reinterested in. Tracer(Rambaut and Drummond 2011) is a tool for visualizing parameters sampled by MCMC.This program is limited to numerical parameters, however, and cannot beused to summarize or analyze MCMC samples of the tree topology (thiswill be discussed further in ).The Tracerwindow. To add data, click on the “+” sign, highlighted in red aboveOpen Tracer and import the bears.log file in theFile &gt; Import New Trace Files. Or click the button on theleft-hand side of the screen to add your log file (see ).The Estimates window in Tracer showing thehistogram of the PosteriorImmediately upon loading your file (see ),you will see the list of Trace Files on the left-handside (you can load multiple files). The bottom left section, calledTraces, provides a list of every parameter in the logfile, along with the mean and the effective sample size (ESS) for theposterior sample of that parameter. The ESS statistic provides a measureof the number of independent draws in our sample for a given parameter.This quantity will typically be much smaller than the number ofgenerations of the chain. In Tracer, poor to fair valuesfor the ESS will be colored red and yellow. You will likely see a lot ofred and yellow numbers because the MCMC runs in this exercise are tooshort to effectively sample the posterior distributions of mostparameters. A much longer analysis is provided in the outputdirectory.The inspection window for your selected parameter is theEstimates window, which shows a histogram and summarystatistics of the values sampled by the Markov chain.  shows the marginal distribution of thePosterior statistic for the bears.log file in theoutput directory.  Look through the various parameters and statistics in the list ofTraces.  Are there any parameters that have really low ESS? Why do you think that might be?Next, we can click over to the Trace window. Thiswindow shows us the samples for a given parameter at each iteration ofthe MCMC. The left side of the chain has a shaded portion that has beenexcluded as “burn-in”. Samples taken near the beginning of chain areoften discarded or “burned” because the MCMC may not immediately beginsampling from the target posterior distribution, particularly if thestarting condition of the chain is far from the region of highestposterior density.  shows thetrace for the extinction rate.The Trace window in Tracer. This windowshows a line plot of every sampled value for the extinction rate thatwas saved to file. The lighter shaded portion is the set of samplesdiscarded as “burn-in” and are not used to compute the summarystatistics found in the Estimates window.The Trace window allows us to evaluate how well ourchain is sampling the target distribution. For a fairly short analysis,the output in  shows reasonablemixing—there is no consistent pattern or trend in the samples, nor arethere long intervals where the statistic does not change. The presenceof a trend or large leaps in a parameter value might indicate that yourMCMC is not mixing well. You can read more about MCMC tuning andimproving mixing in the tutorials Introduction to Markov chain Monte Carlo (MCMC) Sampling.  Look through the traces for your parameters.  Are there any parameters in your log files that show trends or large leaps? What steps might you take to solve these issues?In Tracer you can view the marginal probabilitydistributions of your parameters in the Marginal Prob Distribution window. Using this tool, you can compare thedistributions of several different parameters (by selecting them both).  Go to the diversification parameter in the Marginal Prob Distribution window.  ⇨ What is the mean value estimatedfor the net diversification rate ($d$)?  ⇨ What does the marginaldistribution tell you about the net diversification? (Hint:$d = \\lambda - \\mu$)While specifying the model, remember that we created severaldeterministic nodes that represent parameters that we would like toestimate, including the net diversification rate. Tracerallows us to view the summaries of these parameters since they appear inour log files.Go to the age_extant parameter in the Estimateswindow.⇨ What is the mean and 95% highest posterior density of the age of the MRCA for all living bears?Since you have evaluated several of the parameters by viewing the tracefiles and the ESS values, you may be aware that the MCMC analysis youconducted for this tutorial did not sufficiently sample the jointposterior distribution of phylogenetic parameters. More explicitly,your run has not converged. It is not advisable to base yourconclusions on such a run and it will be critical to perform multiple,independent runs for many more MCMC cycles. For further discussion ofrecommended MCMC practices in RevBayes, please see the Introduction to Markov chain Monte Carlo (MCMC) Sampling tutorials.Summarize TreeIn addition to evaluating the performance and sampling of an MCMC runusing numerical parameters, it is also important to inspect the sampledtopology and tree parameters. This is a difficult endeavor, however. Onetool for evaluating convergence and mixing of the tree samples isRWTY (Warren et al. 2016). In thistutorial, we will only summarize the sampled trees, but we encourage youto consider approaches for assessing the performance of the MCMC withrespect to the tree topology.Ultimately, we are interested in summarizing the sampled trees andbranch times given that our MCMC has sampled all of the importantparameters in proportion to their posterior probabilities. RevBayesincludes some functions for summarizing the tree topology and other treeparameters.We will complete this part of the tutorial using RevBayesinteractively.  Begin by running the RevBayes executable. You should dothis from within the tutorial directory.Read in the MCMC sample of trees from file.trace = readTreeTrace(\"output/bears.trees\")By default, a burn-in of 25% is used when creating the tree trace (250trees in our case). You can specify a different burn-in fraction, say50%, by typing the command trace.setBurnin(500).Now we will use the mccTree function to return a maximum cladecredibility (MCC) tree. The MCC tree is the tree with the maximumproduct of the posterior clade probabilities. When considering treeswith sampled ancestors, we refer to the maximum sampled ancestor cladecredibility (MSACC) tree (Gavryushkina et al. 2017).mccTree(trace, file=\"output/bears.mcc.tre\" )When there are sampled ancestors present in the tree, visualizing thetree can be fairly difficult in traditional tree viewers. We will makeuse of a browser-based tree viewer calledIcyTree, created by TimVaughan. IcyTree has manyunique options for visualizing phylogenetic trees and can producepublication-quality vector image files(i.e. SVG). Additionally, it correctlyrepresents sampled ancestors on the tree as nodes, each with only onedescendant ().Maximum sampled ancestor cladecredibility (MSACC) tree of bear species used in this tutorial. Numbersabove fossil nodes indicate the posterior probability of being a sampledancestorNavigate to https://icytree.org/ and open the fileoutput/bears.mcc.tre in IcyTree.  Try to replicate the tree in  (Hint: Style &gt; MarkSingletons) Why might a node with a sampled ancestor bereferred to as a singleton?  How can you see the names of the fossils that are putative sampled ancestors?  Try mousing over differentbranches (see . What are the fieldstelling you?  What is theposterior probability that Zaragocyon daamsi is a sampled ancestor?Another newly available web-based tree viewer isPhylogeny.IO (Jovanovic and Mikheyev 2016). Try this site fora different way to view the tree.",
        "url": "/tutorials/fbd/",
        "index": "false"
      }
      ,
    
      "tutorials-timefig-orig": {
        "title": "The Time-heterogeneous Feature-informed Geographic State Speciation and Extinction (TimeFIG) model",
        "content": "IntroductionIn the previous example, we saw how the MultiFIG model (Swiston and Landis 2023) allows us to test hypotheses about the relationships between certain environmental features and evolutionary processes using feature effect rates, as well as infer biogeographic event parameters and ancestral areas using GeoSSE. We used MultiFIG to investigate the evolution of the South American lizard genus Liolaemus based on species ranges, present-day regional features, and a time-calibrated phylogeny. However, we know that regional features change over time, and this may impact our ancestral state reconstructions and estimates of feature/process relationships. For example, consider an island system where regions form and subside. Knowing whether regions exist during a particular time period should impact which states are possible during this time period (ranges including absent regions should be disallowed). This should have an effect on our ancestral state reconstructions. As another example, consider a region that was very small for most of its history, with high extinction rates because of its small size (a negative area/extinction relationship). If the region recently became large, and we don’t include any information about the region’s history, we might infer the opposite relationship between size and extinction! The TimeFIG model (CITATION TBD) addresses the time-heterogeneity of regional features using “time slices” (discrete time periods), allowing regional features to have different values during each time slice, while assuming the relationships between features and processes remain constant.In this tutorial, we will model the evolution and biogeography of Hawaiian Silverswords using six regions, four regional features, and five time slices. We will also jointly estimate divergence times (using molecular data), which will allow information about the geological history of the Hawaiian archipelago to inform the dates of cladogenetic events.The Hawaiian Hot Spot ArchipelagoIsland archipelagos are ideal microcosms for studying biogeographic patterns of dispersal, speciation, and extinction. Among islands, the Hawaiian archipelago holds particular value for biogeographers in part because of its unique paleogeography.  Each  island in the Hawaiian “chain” is produced through volcanic eruption from a hotspot in the mid pacific and then moves northwest along a tectonic assembly line during which  subsidence and erosion case gradual decay. Thus, the hawaiian chain acts as a geological time-capsule, with hundreds of progressively older, more eroded islands stretching northwest towards the arctic. The vast majority of Hawaiian biodiversity is concentrated within four larger, younger high island systems of varying age, Kauai (~ 6.15 MYA), Oahu (~ 4.135 MYA), Maui Nui(~ 2.55 MYA), and Hawaii(~ 1.20 MYA). The age and origin of the many independent radiation of plants, animals, and fungi that have occurred in hawaii has been a perennial topic of evolutionary studies. One hypothesis that has been difficult to test concerns the age of onset of endemic hawaiian evolutionary radiations. The extreme isolation of the Hawaiian islands makes colonization from distant sources highly improbably, but various biogeographers have hypothesized that the now eroded northwest islands could have provided a landing pad for such lineages that is much older, making dispersal more likely, followed by dispersal to and subsequent radiation in the modern high islands.The Hawaiian archipelago is a system in which phylogenetic models of historical biogeography will produce much more accurate reconstructions if they incorporate change over time in paleogeography than if change in island feature is ignored. In this tutorial, we apply a TimeFIG model to an endemic hawaiian plant lineage, the silversword alliance (~34 spp.) to infer paleogeographically-informed parameter estimates for biogeographic event rates, effect rates of regional features, and ancestral areas. We also relax the assumption that our input phylogeny is a fully time-calibrated phylogeny, and use “a relaxed rock” approach to update divergence time estimates among silversword taxa. This last bit allows us to test the hypothesis that silverswords colonized older northwest islands before dispersing into, and radiating in the modern high island chain.The 4 regional features investigated in this analysis and the 8 associated parameters relating these features to core biogeographic processes.The analysis utilizes 5 different time slices, numbered starting from the present. These time slices are delimited by 4 historical time points: T1, T2, T3, and T4. Distributions may be assigned to these time points to account for uncertainty.Setup  Important version info!!  This tutorial is the one of a series of lessons explaining how to build increasingly powerful but computationally demanding GeoSSE-type models for biogeographic analyses. Inference under these models is powered by the Tensorphylo plugin for RevBayes, located here: bitbucket.org/mrmay/tensorphylo/src/master (May and Meyer).This tutorial, and following tutorials for GeoSSE-type models, will also require a development version of RevBayes built from the hawaii_fix branch (this message will be removed when the branch is merged).As an alternative to building the development version of RevBayes and installing Tensorphylo, you can instead use the RevBayes Docker image, which comes pre-configured with Tensorphylo enabled. The RevBayes Docker tutorial is located here: revbayes.github.io/tutorials/docker.Running a TimeFIG analysis in RevBayes requires several important data files, including a file representing a phylogeny and a biogeographic data matrix describing the ranges for each species. silversword.mcc.tre is a phylogeny of the Hawaiian Silverswords. It is a dated tree (and we will use it to initialize our MCMC), but we will estimate new divergence times in this analysis using the molecular data in silversword.mol.nex. silversword.range.nex assigns ranges to each species for a six-region system: Kauai, Oahu, Maui Nui, Hawaii, and an outgroup region. For each species (row) and region (column), the file reports if the species is present (1) or absent (0) in that region. There are also feature files that contain regional feature data, a feature_summary.csv file that describes all the regional feature files (where they are found and what kind of data they contain), and an age_summary.csv file that gives prior distributions for the times that delimit our discrete time slices.If you prefer to run a single script instead of entering each command manually, the RevBayes script called timefig.Rev contains all of the commands that are used in the tutorial. There is also an R script for plotting the analysis results. The data and script can be found in the Data files and scripts box in the left sidebar of the tutorial page. Somewhere on your computer, you should create a directory (folder) for this tutorial. Inside the tutorial directory, you should create a scripts directory. This is the directory where you will run RevBayes commands, or where you will put the timefig.Rev and timefig.R scripts. Then, you should create a data directory inside the tutorial directory. The scripts/commands for the tutorial expect that the primary data files (silversword.mcc.tre, silversword.mol.nex, silversword.ranges.nex, feature_summary.csv, and age_summary.csv) will be in this directory, while the feature files (the data, not the summary file) will be in a subdirectory called features. However, you can always modify the filepaths to locate the data wherever you choose to download it.TimeFIG in RevBayesGetting startedAfter starting up RevBayes from within your local scripts directory, you can load the TensorPhylo plugin. You will need to know where you downloaded the plugin. For example, if you cloned the TensorPhylo directory into your home directory at ~/tensorphylo, you would use the following command to load the plugin:loadPlugin(\"TensorPhylo\", \"~/tensorphylo/build/installer/lib\")Note that if you’re using the RevBayes Docker image, then the Tensorphylo plugin is installed in the / (root) directory:loadPlugin(\"TensorPhylo\", \"/tensorphylo/build/installer/lib\")It is also a good idea to set a seed. If you want to exactly replicate the results of the tutorial, you should use the seed 1.seed(1)We also want to tell RevBayes where to find our data (and where to save our output later). If you have set up your tutorial directory in a different way than suggested, you will need to modify the filepaths.fp         = \"../\"dat_fp     = fp + \"data/\"out_fp     = fp + \"output/\"mol_fn     = dat_fp + \"silversword.mol.nex\"bg_fn      = dat_fp + \"silversword.range.nex\"phy_fn     = dat_fp + \"silversword.mcc.tre\"feature_fn = dat_fp + \"feature_summary.csv\"age_fn     = dat_fp + \"age_summary.csv\"DataNow, we will start reading in data and constructing the TimeFIG model. Let’s start by loading the phylogenetic tree.phy &lt;- readTrees(phy_fn)[1]In order to set up our analysis, we will want to know some information about this tree: the root age, the taxa, the number of taxa, and the number of branches.tree_height  &lt;- phy.rootAge()taxa         = phy.taxa()num_taxa     = taxa.size()num_branches = 2 * num_taxa - 2Next, we will read in the molecular data, and calculate the number of sites. For this analysis, we are only using a single locus, but it can be performed with multiple loci.dat_mol   = readDiscreteCharacterData(mol_fn)num_sites = dat_mol.nchar()We also want to read in the biogeographic data. First, we’ll read the age file that tells us how many time slices to include (5) and what times delimit those slices (1.20 MYA, 2.55 MYA, 4.135 MYA, and 6.15 MYA). Note that for $n$ times, there will be $n+1$ time slices. The age_summary.csv file also includes information that would help establish a uniform prior on each of these times (start_age and end_age), but we will be using the mean_age without setting a prior (no uncertainty).times_dat   = readDataDelimitedFile(age_fn, delimiter=\",\", header=true)num_times   = times_dat.size() + 1for (i in 1:(num_times-1)) times[i] &lt;- times_dat[i][2]Next, we will read in the region data.bg_01 = readDiscreteCharacterData(bg_fn)We want to get some information about this range data: how many regions there are, how many ranges can be constructed from these regions, and how many region pairs there are.num_regions = bg_01.nchar()num_ranges  = abs(2^num_regions - 1)num_pairs   = num_regions^2 - num_regionsWe want to format the range data to be used in a GeoSSE-type analysis. This will take the binary range data and output integer states.bg_dat = formatDiscreteCharacterData(bg_01, format=\"GeoSSE\", numStates=num_ranges)We also want to get our feature data. Using the RevBayes function readRegionalFeatures, we can look at the feature_summary.csv file and automatically look for feature data. The feature_summary.csv file is specially formated to be read by RevBayes, consisting of 5 columns. The first column is time_index, telling us which time slice the feature data corresponds to. Time slices are numbered from the present starting with 1. The second column is feature_index. Each feature type (within-region categorical, within-region quantitative, between-region categorical, and between-region quantitative) has a container that can contain several features, so we want to index the features within those containers. In this analysis, we will only have one feature of each type, so the index will always be 1. The third column is feature_relationship. This column is for indicating whether the feature is a within-region feature or a between-region feature, with options ‘within’ or ‘between’. The fourth column is feature_type, for indicating whether the feature is quantitative of categorical. Finally, the fifth column is feature_path, which gives a filepath for the actual file containing the data for that feature.geo_features &lt;- readRegionalFeatures(feature_fn, delimiter=\",\",nonexistent_region_token=\"nan\")Next, we transform the feature data into feature layers, a RevBayes object that we will use later for informing our biogeographic rates. First, we normalize the features (important for scaling reasons). Then, for each time slice [i], we pull each feature type out of our geo_features object and create the layers.geo_features.normalize(\"within\")geo_features.normalize(\"between\")for (i in 1:num_times) {    feature_CW[i] &lt;- geo_features.get(\"within\",\"categorical\",i)    feature_QW[i] &lt;- geo_features.get(\"within\",\"quantitative\",i)    feature_CB[i] &lt;- geo_features.get(\"between\",\"categorical\",i)    feature_QB[i] &lt;- geo_features.get(\"between\",\"quantitative\",i)    for (j in 1:feature_CW[i].size()) {layer_CW[i][j] &lt;- feature_CW[i][j].get()}    for (j in 1:feature_QW[i].size()) {layer_QW[i][j] &lt;- feature_QW[i][j].get()}    for (j in 1:feature_CB[i].size()) {layer_CB[i][j] &lt;- feature_CB[i][j].get()}    for (j in 1:feature_QB[i].size()) {layer_QB[i][j] &lt;- feature_QB[i][j].get()}}Model setupIn the TimeFIG model, there are four processes: within-region speciation, extinction, between-region speciation, and dispersal. Rates per region or region pair for each time slice are calculated using feature data, feature effect parameters, and base rate parameters. We will set the prior on base rate parameters to the exponential distribution dnExp(1). We will set the prior on feature effect parameters to the normal distribution dnNormal(0,1). Then we will use the RevBayes function fnFeatureInformedRates to combine the feature data and feature effect parameters to create $m$ vectors/matrices for each time slice, representing relative rates per region or region pair. Finally, we will multiply $m$ by the base rate parameter to get model rates $r_w$, $r_e$, $r_b$, and $r_d$ for each time slice.Let’s start by creating distributions that we will use for all $\\rho$, $\\phi$, and $\\sigma$ parameters.sigma_dist  = dnNormal(0,1)phi_dist    = dnNormal(0,1)rho_dist    = dnExp(1)Now we will set up our rates for the four core processes. We will set up within-region speciation rates first. We won’t worry about multiplying $m_w$ by the base rate yet, because the fnBiogeographyCladoEventsBD function will do this later. Note that while m_w has different values for each time slice, the rho_w, sigma_w, and phi_w parameters associated with each feature are the same for all time slices; only the feature data changes.rho_w ~ rho_distfor (i in 1:feature_CW[1].size()) sigma_w[i] ~ sigma_distfor (i in 1:feature_QW[1].size()) phi_w[i] ~ phi_distfor (i in 1:num_times) m_w[i] := fnFeatureInformedRates(layer_CW[i], layer_QW[i], sigma_w, phi_w, null_rate=0)Extinction rates are set up similarly, and we will incorporate $\\rho$ this time. From these extinction rates (which are actually single-region extinction rates), we will set up global extinction rates for each possible range in the state space. In the TimeFIG model, lineage-level extincion events occur when a species goes globally extinct (i.e. it loses the last region from its range). Therefore, we will assign all multi-region ranges an extinction rate of 0, and we will assign all single-region ranges an extinction rate equal to the local extirpation rate. Note, ranges are numbered such that indices 1, 2, through num_regions correspond to ranges that respectively contain only region 1, region 2, up through the last region in the system. Similar to within-region speciation rates, we will construct a different m_e and r_e vector for each time slice, but the rho_e, simga_e, and phi_e parameters are shared among time slices.rho_e ~ rho_distfor (i in 1:feature_CW[1].size()) sigma_e[i] ~ sigma_distfor (i in 1:feature_QW[1].size()) phi_e[i] ~ phi_distfor (i in 1:num_times) m_e[i] := fnFeatureInformedRates(layer_CW[i], layer_QW[i], sigma_e, phi_e, null_rate=1e3)for (i in 1:num_times) r_e[i] := rho_e * m_e[i][1]for (i in 1:num_times) {    for (j in 1:num_ranges) {        mu[i][j] &lt;- abs(0)        if (j &lt;= num_regions) mu[i][j] := r_e[i][j]}}Between-region speciation rates are set up similarly. Again, we do not need to incorporate $\\rho$ yet. We also don’t have to worry about incorporating range split score; RevBayes will do this automatically when we create the cladogenetic probability matrix.rho_b ~ rho_distfor (i in 1:feature_CB[1].size()) sigma_b[i] ~ sigma_distfor (i in 1:feature_QB[1].size()) phi_b[i] ~ phi_distfor (i in 1:num_times) m_b[i] := fnFeatureInformedRates(layer_CB[i], layer_QB[i], sigma_b, phi_b, null_rate=1)Finally, for dispersal rates, we will set up dispersal rates.rho_d ~ rho_distfor (i in 1:feature_CB[1].size()) sigma_d[i] ~ sigma_distfor (i in 1:feature_QB[1].size()) phi_d[i] ~ phi_distfor (i in 1:num_times) m_d[i] := fnFeatureInformedRates(layer_CB[i], layer_QB[i], sigma_d, phi_d, null_rate=0)for (i in 1:num_times) {    for (j in 1:num_regions) {r_d[i][j] := rho_d * m_d[i][j]}}From these rates, we can use RevBayes functions to construct the rate matrices used by the analysis. Importantly, these rate matrices are different for each time slice, and this is how RevBayes knows to use different rates during different discrete periods of time. First are the anagenetic rate matrices, which give rates of anagenetic processes. We are not restricting the number of regions that a species can live in at any given time, so we set the maxRangeSize equal to the number of regions. Settings maxRangeSize may be used to reduce the number of range patterns in the model, particularly when num_regions is large.for (i in 1:num_times) {    Q_bg[i] := fnBiogeographyRateMatrix(        dispersalRates=r_d[i],        extirpationRates=r_e[i],        maxRangeSize=num_regions)}We also construct cladogenetic event matrices, describing the absolute rates of different cladogenetic events. We are not restricting the sizes of ‘split’ subranges following between-region speciation, so we set the max_subrange_split_size equal to the number of regions. From these matrices, we can obtain the total speciation rates per state during each time slice, as well as a cladogenetic probability matrices for each time slice.for (i in 1:num_times) {    clado_map[i] := fnBiogeographyCladoEventsBD(        speciation_rates=[ rho_w, rho_b ],        within_region_features=m_w[i][1],        between_region_features=m_b[i],        max_range_size=num_regions,        max_subrange_split_size=num_regions)    lambda[i] := clado_map[i].getSpeciationRateSumPerState()    omega[i]  := clado_map[i].getCladogeneticProbabilityMatrix()}Lastly, we need to assign a probability distribution to range of the most recent common ancestor of all species, prior to the first speciation event. In this analysis, we will assume all ranges were equally likely for that ancestor.pi_bg_prior &lt;- rep(1,num_ranges)pi_bg       &lt;- simplex(pi_bg_prior)With all of the rates constructed, we can create a stochastic variable drawn from this TimeFIG model with state-dependent birth, death, and speciation processes. This establishes how the various processes interact to generate a tree with a topology, divergence times, and terminal taxon states (ranges). Then we can clamp the variable with the present-day range states, allowing us to infer model parameters based on our observed data. Since we plan to jointly estimate divergence times, we will set a prior on the root age of the tree. The tree that we are using to initialize the analysis (and fix the topology) already has suggested dates, so we will assign a uniform prior centered on the suggested root age.root_age ~ dnUniform(tree_height-10, tree_height+10)timetree ~ dnGLHBDSP(    rootAge     = tree_height,    lambda      = lambda,    mu          = mu,    eta         = Q_bg,    omega       = omega,    lambdaTimes = times,    muTimes     = times,    etaTimes    = times,    omegaTimes  = times,    pi          = pi_bg,    rho         = 1,    condition   = \"time\",    taxa        = taxa,    nStates     = num_ranges,    nProc       = 4)timetree.clampCharData(bg_dat)We also want to set up a molecular model that describes how the molecular sequences evolve over our timetree. This will allow for divergence time estimation. There are many possible models of molecular evolution that you could define here (see this tutorial for some options), but we will use HKY+Gamma.mu_mol_base ~ dnExp(10)mu_mol_branch_rel ~ dnDirichlet(rep(2, num_branches))mu_mol := mu_mol_base * mu_mol_branch_relkappa ~ dnGamma(2,2)pi_mol ~ dnDirichlet( [1,1,1,1] )Q_mol := fnHKY(kappa=kappa, baseFrequencies=pi_mol)alpha ~ dnExp(0.1)site_rates := fnDiscretizeGamma(shape=alpha, rate=alpha, numCats=4, median=true)We can put these molecular model parameters together into one object using the RevBayes function dnPhyloCTMC(), which will associate this molecular model with our timetree.x_mol ~ dnPhyloCTMC(    Q=Q_mol,    tree=timetree,    branchRates=mu_mol,    siteRates=site_rates,    rootFrequencies=pi_mol,    nSites=num_sites,    type=\"DNA\" )MCMCTo aid the initial likelihood computation, we will initialize some parameters before starting MCMC. This will not fix the parameter values, but will give the MCMC chain a “reasonable” place to start.rho_w.setValue(0.1)rho_e.setValue(0.1)rho_b.setValue(0.1)rho_d.setValue(0.1)for (i in 1:sigma_w.size()) sigma_w[i].setValue(0.01)for (i in 1:sigma_e.size()) sigma_e[i].setValue(0.01)for (i in 1:sigma_b.size()) sigma_b[i].setValue(0.01)for (i in 1:sigma_d.size()) sigma_d[i].setValue(0.01)for (i in 1:phi_w.size()) phi_w[i].setValue(0.01)for (i in 1:phi_e.size()) phi_e[i].setValue(0.01)for (i in 1:phi_b.size()) phi_b[i].setValue(0.01)for (i in 1:phi_d.size()) phi_d[i].setValue(0.01)timetree.setValue(phy)root_age.setValue(tree_height)For this analysis, we will perform an MCMC of 1000 generations, with 100 generations of hyperparameter-tuning burnin. Despite being relatively short compared to a full analysis, THIS WILL STILL TAKE A LONG TIME. An analysis of this length may not achieve convergence, so these settings should only be used for testing purposes. You can alter this MCMC by changing the number of iterations, the length of the burnin period, or the move schedule. We will also set up the MCMC to record every 10 iterations.n_gen    = 1000n_burn   = n_gen/10printgen = 10We want MCMC to update our molecular branch rates and parameters. We will add simple scaling moves to many of the values, but we will use special moves for our simplexes. Some of these moves are given a higher weight, which indicates that they will be performed several times per iteration.mvi = 1mv[mvi++] = mvScale(mu_mol_base, weight=5)mv[mvi++] = mvSimplex(mu_mol_branch_rel, numCats=1, alpha=3, kappa=1, weight=num_branches)mv[mvi++] = mvSimplex(mu_mol_branch_rel, numCats=5, alpha=3, kappa=1, weight=num_branches)mv[mvi++] = mvScale(kappa)mv[mvi++] = mvScale(alpha)mv[mvi++] = mvSimplex(pi_mol, alpha=3)We also want MCMC to update all of the base rate $\\rho$ parameters, as well as the $\\sigma$ and $\\phi$ parameters. We will use a scaling move for the base rates, since they should always have positive values. We will use a sliding move for the feature effect parameters, since they can have positive or negative values.mv[mvi++] = mvScale(rho_w)mv[mvi++] = mvScale(rho_e)mv[mvi++] = mvScale(rho_b)mv[mvi++] = mvScale(rho_d)for (i in 1:sigma_d.size()) {mv[mvi++] = mvSlide(sigma_d[i])}for (i in 1:sigma_b.size()) {mv[mvi++] = mvSlide(sigma_b[i])}for (i in 1:sigma_e.size()) {mv[mvi++] = mvSlide(sigma_e[i])}for (i in 1:sigma_w.size()) {mv[mvi++] = mvSlide(sigma_w[i])}for (i in 1:phi_d.size()) {mv[mvi++] = mvSlide(phi_d[i])}for (i in 1:phi_b.size()) {mv[mvi++] = mvSlide(phi_b[i])}for (i in 1:phi_e.size()) {mv[mvi++] = mvSlide(phi_e[i])}for (i in 1:phi_w.size()) {mv[mvi++] = mvSlide(phi_w[i])}Finally, we will add a couple of tree moves to update the root age and node ages.mv[mvi++] = mvScale(root_age, weight=5)mv[mvi++] = mvNodeTimeSlideUniform(timetree, weight=5)We also want MCMC to keep track of certain things while it runs. We want it to print some output to the screen so we can see how it is running (mnScreen). We also want it to save model parameters to a file (mnModel). Finally, if we want to use the output for ancestral state reconstruction, we want to save states and stochastic character mappings (mnJointConditionalAncestralStates and mnStochasticCharacterMap). All of the output files will be saved in the output directory so that it can be accessed later.mni = 1mn[mni++] = mnScreen(printgen=1)mn[mni++] = mnModel(printgen=printgen, filename=out_fp+\"model.log\")mn[mni++] = mnJointConditionalAncestralState(glhbdsp=timetree, tree=timetree, printgen=printgen, filename=out_fp+\"states.log\", withTips=true, withStartStates=true, type=\"NaturalNumbers\")mn[mni++] = mnStochasticCharacterMap(glhbdsp=timetree, printgen=printgen, filename=out_fp+\"stoch.log\")mn[mni++] = mnFile(timetree, printgen=printgen, filename=out_fp+\"trace.tre\")Then we can start up the MCMC. It doesn’t matter which model parameter you use to initialize the model, so we will use m_w. RevBayes will find all the other parameters that are connected to m_w and include them in the model as well. Then we create an MCMC object with the moves, monitors, and model, add burnin, and run the MCMC.mdl = model(m_w)ch = mcmc(mv, mn, mdl)ch.burnin(n_burn, tuningInterval=10)ch.run(n_gen)After the MCMC analysis has concluded, we can create a maximum clade credibility (MCC) tree based on our posterior set of trees (the tree trace). We can then summarize the ancestral states we obtained and map them onto the MCC tree, creating an ancestral state tree. This tree will be written to the file ase.tre . It may take a little while. We can also summarize our stochastic mapping, creating the events.log file.tree_trace = readTreeTrace(file=out_fp+\"trace.tre\", treetype=\"clock\", burnin=0.1)mcc_tree = mccTree(tree_trace, file=out_fp+\"mcc.tre\")state_trace = readAncestralStateTrace(file=out_fp+\"states.log\")state_tree_trace = readAncestralStateTreeTrace(file=out_fp+\"trace.tre\", treetype=\"clock\")state_tree = ancestralStateTree(tree=mcc_tree,                                tree_trace=state_tree_trace,                                ancestral_state_trace_vector=state_trace,                                include_start_states=true,                                file=out_fp+\"ase.tre\",                                summary_statistic=\"MAP\",                                burnin=0.1)stoch = readAncestralStateTrace(file=out_fp+\"stoch.log\")summarizeCharacterMaps(stoch,mcc_tree,file=out_fp+\"events.tsv\",burnin=0.1)OutputOne interesting thing we can do with the output of the TimeFIG analysis is plot the time-calibrated tree with ancestral states. This can be done using RevGadgets, an R packages that processes RevBayes output. You can use R to generate a tree with ancestral states by running the timefig.R script, or by executing the following code in R. Before plotting the ancestral state tree, we create two vectors. The vector labels is useful because it maps actual region labels onto state numbers, so the legend can be easily interpreted. If you used your own data, you would have to provide your own state labels.library(RevGadgets)library(ggplot2)tree_file = \"../output/ase.tre\"states_file = \"../figures/states.png\"labels &lt;- c(\"0\" = \"R\",\"1\" = \"K\",\"2\" = \"O\",\"3\" = \"M\",\"4\" = \"H\",\"5\" = \"Z\",\"6\" = \"RK\",\"7\" = \"RO\",\"8\" = \"KO\",\"9\" = \"RM\",\"10\" = \"KM\",\"11\" = \"OM\",\"12\" = \"RH\",\"13\" = \"KH\",\"14\" = \"OH\",\"15\" = \"MH\",\"16\" = \"RZ\",\"17\" = \"KZ\",\"18\" = \"OZ\",\"19\" = \"MZ\",\"20\" = \"HZ\",\"21\" = \"RKO\",\"22\" = \"RKM\",\"23\" = \"ROM\",\"24\" = \"KOM\",\"25\" = \"RKH\",\"26\" = \"ROH\",\"27\" = \"KOH\",\"28\" = \"RMH\",\"29\" = \"KMH\",\"30\" = \"OMH\",\"31\" = \"RKZ\",\"32\" = \"ROZ\",\"33\" = \"KOZ\",\"34\" = \"RMZ\",\"35\" = \"KMZ\",\"36\" = \"OMZ\",\"37\" = \"RHZ\",\"38\" = \"KHZ\",\"39\" = \"OHZ\",\"40\" = \"MHZ\",\"41\" = \"RKOM\",\"42\" = \"RKOH\",\"43\" = \"RKMH\",\"44\" = \"ROMH\",\"45\" = \"KOMH\",\"46\" = \"RKOZ\",\"47\" = \"RKMZ\",\"48\" = \"ROMZ\",\"49\" = \"KOMZ\",\"50\" = \"RKHZ\",\"51\" = \"ROHZ\",\"52\" = \"KOHZ\",\"53\" = \"RMHZ\",\"54\" = \"KMHZ\",\"55\" = \"OMHZ\",\"56\" = \"RKOMH\",\"57\" = \"RKOMZ\",\"58\" = \"RKOHZ\",\"59\" = \"RKMHZ\",\"60\" = \"ROMHZ\",\"61\" = \"KOMHZ\",\"62\" = \"RKOMHZ\")states &lt;- processAncStates(tree_file, state_labels=labels)plotAncStatesMAP(t=states,                 timeline=T,                 geo=F,                 time_bars=F,                 node_size=2,                 node_color_as=\"state\",                 node_size_as=NULL,                 tip_labels_offset=0.1) +                 ggplot2::theme(legend.position=\"bottom\",                                legend.title=element_blank())ggsave(output_file, width = 9, height = 9)Ancestral state reconstruction of Hawaiian Silverswords. Nodes are colored based on the range with the highest probability. Range labels represents sets of regions (R = northwest islands, K = Kauai, O = Oahu, M = Maui Nui, H = Hawaii, Z = outgroup region).As we can see from the figure above, we do not find evidence that the silverswords dipsersed first to now eroded northwest island before colonizing and radiating in Kauai.",
        "url": "/tutorials/timefig_orig/",
        "index": "false"
      }
      ,
    
      "tutorials-dice": {
        "title": "Understanding Continuous-Time Markov Models",
        "content": "Simulating DNA SequencesIn this tutorial, you will develop an intuition for continuous-time Markov models of the sort used by RevBayes, as well as by other programs, such as PAUP, MrBayes, PhyloBayes, and others. The basic phylogenetic model, used in all of these programs has several components. You are probably familiar with two of them: a phylogenetic tree describing the relationships among the species with the branch lengths specified in terms of expected amount of change. The difficult part of the phylogenetic model is how character change is modeled along the branches of the tree. All of these programs assume that characters evolve along the branches of the tree according to a continuous-time Markov model.Generating uniform and exponential random variablesIf you had a highschool social life as awkward as this author’s, you already own a 10-sided die (d10, in the gaming lingo). If you actually had friends in high school, however, you may need to buy such a die. Go to a gaming store and tell them that you want to buy a “d10.” Alternatively, buy a 10-sided die from Amazon.Examine your new die. Note that it has ten faces, with each face numbered from 0 to 9. You can generate a random number on the interval (0,1) by repeatedly rolling your die. You will assume the “0.” of the number and use the die to randomly generate the digit in the tenths place (0._), hundreths place (0._ _), thousandths place (0. _ _ _) etc. until you have the random number to the desired precision. Let’s try it! I rolled my die three times and saw, in order, the numbers 0, 4, and 7. My uniform(0,1) random number, then, is $u = 0.047$. Try it yourself. With three rolls of the die, you can generate a random number to a precision of three decimal places. Do you understand why the number you generate in this manner is uniformly-distributed on the interval (0,1)? For reference, the uniform(0,1) probability distribution looks like,and is sometimes referred to as the rectangular distribution, for obvious reasons.The uniform distribution is only one of many distributions. You have probably heard of at least some of the more common distributions (the normal, log-normal, binomial, gamma, Poisson, exponential, $\\chi^2$, Student’s t) and perhaps you’ve heard of some of the more obscure distributions, too  (Wishart, Normal inverse Wishart, and Weibull, among others). When simulating DNA sequence evolution on a tree, in addition to the uniform(0,1) random numbers, we will need to generate exponential random variables.The exponential distribution is used to model waiting times. Imagine that something occurs at a constant rate, $\\lambda$. The time until that something occurs is exponentially distributed with parameter $\\lambda$. The exponential distribution looks like,While we can use the die to generate a uniformly-distributed random number, we cannot directly generate an exponentially-distributed number. That said, we can generate an exponential random variable from our uniformly-distributed random number using some math. First, generate a uniform(0,1) random number using your die, called $u$. We can convert this uniform(0,1) random number to an exponential random number using the following equation:\\(t = -{\\log(u) \\over \\lambda}\\)where $\\lambda$ is the rate at which something occurs and $\\log$ is the natural log function. (You can access the natural log function on your smart phone by going to the calculator app and turning the phone on its side, thereby revealing the full functionality of the calculator.) The variable $t$ is exponentially distributed.Let’s try it. We will generate an exponential random number when the rate parameter is $\\lambda = 10$. With my die, I generated a uniform(0,1) random number: $u = 0.948$. Using my calculator, I convert it to an exponential random number:\\(t = -{\\log(0.948) \\over 10} = 0.00534\\)The parameters for the simulationWe now have the machinery needed to generate uniform and exponential random numbers. For the simulation of DNA sequences on a tree, however, we need to choose some simulation parameters. Specifically, we need the tree topology, branch lengths, and rate matrix of the continuous-time Markov model that describes how the DNA sequences change over time.We will assume the following tree for the simulations:We will simulate on this tree for no particular reason except that I like this tree. Note the branch lengths on the tree. The branch lengths are in terms of expected number of substitutions per site. Again, the branch lengths were an arbitrary choice that I made.The last part of the model that must be specified is the rate matrix of the continuous-time Markov process that describes how the DNA sequences change on the tree. We will assume that sequences evolve according to the HKY85 model of DNA substitution, that has rate matrix:\\[{\\mathbf Q} = \\{q_{ij}\\} = \\left( \\begin{array}{cccc}\\cdot               &amp;  \\pi_C &amp; \\kappa \\pi_G &amp; \\pi_T \\\\\\pi_A &amp; \\cdot               &amp; \\pi_G &amp; \\kappa \\pi_T \\\\\\kappa \\pi_A &amp; \\pi_C &amp; \\cdot               &amp; \\pi_T \\\\\\pi_A &amp; \\kappa \\pi_C &amp; \\pi_G &amp; \\cdot               \\\\\\end{array} \\right) \\mu\\]We will make a few important points about the rate matrix. First, the rate matrix may have free parameters. For example,the HKY85 model has the parameters $\\kappa$, $\\pi_A$, $\\pi_C$, $\\pi_G$, and $\\pi_T$. The parameter $\\kappa$ is the transition/transversion rate bias; when $\\kappa = 1$ transitions occur at the same rate as transversions.Typically, the transition/transversion rate ratio, estimated using maximum likelihood or Bayesian inference, isgreater than one; transitions occur at a higher rate than transversions.The other parameters – $\\pi_A$, $\\pi_C$, $\\pi_G$, and $\\pi_T$ – are the base frequencies, and have a biological interpretationas the frequency of the different nucleotides and are also, incidentally, the stationary probabilities of the process.Second, the rate matrix, ${\\mathbf Q}$, can be used to calculatethe transition probabilities and the stationary distribution of the substitution process. The transition probabilities andstationary distribution play a key role in calculating the likelihood.We will assume the following values for the HKY85 parameters: $\\kappa = 5$, $\\pi_A = 0.4$, $\\pi_C = 0.3$, $\\pi_G = 0.2$, and $\\pi_T = 0.1$.These values result in the following scaled rate matrix:\\[{\\mathbf Q} = \\{q_{ij}\\} = \\left( \\begin{array}{rrrr} -0.886 &amp;  0.190 &amp;  0.633 &amp;  0.063 \\\\ 0.253 &amp; -0.696 &amp;  0.127 &amp;  0.316 \\\\ 1.266 &amp;  0.190 &amp; -1.519 &amp;  0.063 \\\\ 0.253 &amp;  0.949 &amp;  0.127 &amp; -1.329 \\\\\\end{array} \\right)\\]The stationary probabilities for this rate matrix are $\\pi_A = 0.4$, $\\pi_C = 0.3$, $\\pi_G = 0.2$, and $\\pi_T = 0.1$.Interpreting the rate matrixThe rate matrix specifies how changes occur on a phylogenetic tree. Consider the very simple case of a singlebranch on a phylogenetic tree. Let’s assume that the branch is $v=0.5$ in length. Our first task is to determine the nucleotide at the root of this tree. Although it is tempting to simply pick a nucleotide at the root of the tree with each nucleotide having a probability of $1/4$, doing so is not consistent with the process we are assuming, as described in the rate matrix, ${\\mathbf Q}$. Rather, we should choose the state at the root of the tree from the stationary probabilities. I made four intervals, with the following probabilities:\\[0.0 - 0.4 \\rightarrow A \\\\0.4 - 0.7 \\rightarrow C \\\\0.7 - 0.9 \\rightarrow G \\\\0.9 - 1.0 \\rightarrow T\\]I rolled the die to generate a uniiform(0,1) random number and obtained $u = 0.709$. The nucleotide at the root, then, is the nucleotide $G$.The situation we have is something like this,in which we have a single branch of length $v = 0.5$ starting in the nucleotide $G$.How can we simulate the evolutionof the site starting from the $G$ at the ancestor? The rate matrix tells us how to do this. First of all, because the current state of the process is $G$,  the only relevant row of the rate matrix is the third one:\\[{\\mathbf Q} = \\{q_{ij}\\} = \\left( \\begin{array}{cccc}\\cdot   &amp;   \\cdot &amp;    \\cdot &amp;  \\cdot \\\\\\cdot   &amp;   \\cdot &amp;    \\cdot &amp;  \\cdot \\\\1.266 &amp;  0.190 &amp; -1.519 &amp;  0.063 \\\\\\cdot   &amp;   \\cdot &amp;    \\cdot &amp; \\cdot \\\\\\end{array} \\right)\\]The overall rate of change away from nucleotide $G$ is $q_{GA} + q_{GC} + q_{GT} = 1.266 + 0.190 + 0.063 = 1.519$.Equivalently, the rate of change away from nucleotide $G$ is simply $-q_{GG} = 1.519$. In a continuous-time Markov model, the waiting time between substitutions is exponentially distributed. The exact shape of the exponential distribution is determined by its rate, which is the same as the rate of the corresponding process in the ${\\mathbf Q}$ matrix. For instance, if we are in state $G$, we  wait an exponentiallydistributed amount of time with rate 1.519 until the next substitution occurs.I generated an exponential(1.519) random variable by first generating a uniform(0,1) random number with my die.The first number it generated is $u = 0.794$. This means that the next time at which a substitution occurs is0.152 up from the root of the tree [i.e., $t = -{1 \\over 1.519} \\log(0.794)$]. We can now color a portion of the branch because we know the process was in state $G$ from the root of the single-branch tree ($t=0.0$) to $t=0.152$:The rate matrix also specifies the probabilities of a change from $G$to the nucleotides $A$, $C$, and $T$.  These probabilities are\\(\\begin{array}{ccc}G \\rightarrow A: {1.266\\over 1.519}=0.833, &amp;  G \\rightarrow C: {0.190\\over 1.519}=0.125, &amp; G \\rightarrow T: {0.063 \\over 1.519}=0.042 \\\\\\end{array}\\)To determine what nucleotide the process changes to we would generate another uniform(0,1) random number (againcalled $u$). If $u$ is between 0 and 0.833, we will say that we had a change from $G$ to $A$. If the random numberis between 0.833 and 0.958 we will say that we had a change from $G$ to $C$. Finally, if the random number $u$ isbetween 0.958 and 1.000, we will say we had a change from $G$ to $T$. The next number generated using the diewas $u = 0.102$, which means the change was from $G$ to $A$. The process is now in a different state (the nucleotide$A$) and the relevant row of the rate matrix is\\[{\\mathbf Q} = \\{q_{ij}\\} = \\left( \\begin{array}{cccc}-0.886 &amp;  0.190 &amp;  0.633 &amp;  0.063 \\\\\\cdot &amp; \\cdot &amp;  \\cdot &amp;  \\cdot \\\\\\cdot &amp;  \\cdot &amp; \\cdot &amp;  \\cdot \\\\\\cdot &amp;  \\cdot &amp;  \\cdot &amp; \\cdot \\\\\\end{array} \\right)\\]We wait an exponentially distributed amount of time with parameter$\\lambda = 0.886$ until the next substitution occurs. When the substitution occurs, it is to a $C$, $G$, or $T$with probabilities ${0.190 \\over 0.886} = 0.214$, ${0.633 \\over 0.886} = 0.714$, and ${0.063 \\over 0.886} = 0.072$,respectively.This process of generating random and exponentially-distributed times until the next substitution occursand then determining (randomly) what nucleotide the change is to is repeated until the process exceeds the lengthof the branch. The state the process is in when it passes the end of the branch is recorded. To complete the simulation on the branch, I generated another uniform random variable using the die. The number was $u = 0.371$, which means that the next substitution would occur 1.119 units above the substitutionfrom $G \\rightarrow A$. The process is in the state $A$ when it passed the end of the branch:The only non-random part of the entire procedure was the initial choice of the parameters. All other aspects of the simulation used a uniform random number generatorand our knowledge of the rate matrix to simulate a single realization of the HKY85 process of DNA substitution.Simulating on a more complicated treeSimulating on the treeis only slightly more complicated than simulating data on the single-branch tree. The steps are as follows:  First, generate the state at the root of the tree. This step requires knowledge of the stationary probabilities for the Markov process specified by the rate matrix, ${\\mathbf Q}$. The stationary distribution is the probability of capturing the process in a particular state when it has been run for a very long (technically, infinitely long) time. The stationary probabilities for the rate matrix we chose are $\\pi_A = 0.4$, $\\pi_C = 0.3$, $\\pi_G = 0.2$, and $\\pi_T = 0.1$.  Visit each branch in turn in preorder sequence (that is, from the root to the tips of the tree). If you visit the branches in preorder sequence, you will know the state at the root of the branch.Pattern probabilitiesThe tree we simulate DNA sequence evolution on has only four tips. This means that there are a total of $4^4 = 256$ possible patterns of nucleotides we could have observed at the tips of the tree. For example, one of the possible patterns is GTTC: Species I has the nucletide G, Species II and Species III are assigned the nucleotide T, and Species IV is assigned C.The probability of simulating any of the 256 patterns is given in the following table:            Pattern      Prob.      Pattern      Prob.      Pattern      Prob.      Pattern      Prob.                  AAAA      0.199465      AGAA      0.014711      CAAA      0.018317      CGAA      0.001490              AAAC      0.004185      AGAC      0.000725      CAAC      0.000628      CGAC      0.000210              AAAG      0.014711      AGAG      0.019868      CAAG      0.001490      CGAG      0.002878              AAAT      0.001395      AGAT      0.000242      CAAT      0.000166      CGAT      0.000048              AACA      0.009075      AGCA      0.000843      CACA      0.005277      CGCA      0.000669              AACC      0.000703      AGCC      0.000315      CACC      0.004524      CGCC      0.002262              AACG      0.000843      AGCG      0.002202      CACG      0.000669      CGCG      0.002304              AACT      0.000121      AGCT      0.000048      CACT      0.000375      CGCT      0.000188              AAGA      0.028625      AGGA      0.005985      CAGA      0.003304      CGGA      0.001065              AAGC      0.000702      AGGC      0.000755      CAGC      0.000210      CGGC      0.000209              AAGG      0.005985      AGGG      0.032738      CAGG      0.001065      CGGG      0.006655              AAGT      0.000234      AGGT      0.000252      CAGT      0.000048      CGGT      0.000059              AATA      0.003025      AGTA      0.000281      CATA      0.000959      CGTA      0.000120              AATC      0.000121      AGTC      0.000048      CATC      0.000360      CGTC      0.000180              AATG      0.000281      AGTG      0.000734      CATG      0.000120      CGTG      0.000420              AATT      0.000154      AGTT      0.000073      CATT      0.000404      CGTT      0.000202              ACAA      0.004185      ATAA      0.001395      CCAA      0.000628      CTAA      0.000166              ACAC      0.005482      ATAC      0.000350      CCAC      0.009592      CTAC      0.000415              ACAG      0.000725      ATAG      0.000242      CCAG      0.000210      CTAG      0.000048              ACAT      0.000350      ATAT      0.001594      CCAT      0.000415      CTAT      0.001214              ACCA      0.000703      ATCA      0.000121      CCCA      0.004524      CTCA      0.000375              ACCC      0.019527      ATCC      0.000752      CCCC      0.167489      CTCC      0.005866              ACCG      0.000315      ATCG      0.000048      CCCG      0.002262      CTCG      0.000188              ACCT      0.000752      ATCT      0.001546      CCCT      0.005866      CTCT      0.007452              ACGA      0.000702      ATGA      0.000234      CCGA      0.000210      CTGA      0.000048              ACGC      0.001837      ATGC      0.000116      CCGC      0.004796      CTGC      0.000208              ACGG      0.000755      ATGG      0.000252      CCGG      0.000209      CTGG      0.000059              ACGT      0.000116      ATGT      0.000535      CCGT      0.000208      CTGT      0.000607              ACTA      0.000121      ATTA      0.000154      CCTA      0.000360      CTTA      0.000404              ACTC      0.001781      ATTC      0.000517      CCTC      0.011625      CTTC      0.001716              ACTG      0.000048      ATTG      0.000073      CCTG      0.000180      CTTG      0.000202              ACTT      0.000517      ATTT      0.004711      CCTT      0.001716      CTTT      0.013873              GAAA      0.045565      GGAA      0.005060      TAAA      0.006106      TGAA      0.000497              GAAC      0.001004      GGAC      0.000453      TAAC      0.000166      TGAC      0.000048              GAAG      0.005060      GGAG      0.017648      TAAG      0.000497      TGAG      0.000959              GAAT      0.000335      GGAT      0.000151      TAAT      0.000099      TGAT      0.000038              GACA      0.002514      GGCA      0.000532      TACA      0.000959      TGCA      0.000120              GACC      0.000315      GGCC      0.000194      TACC      0.000548      TGCC      0.000274              GACG      0.000532      GGCG      0.002904      TACG      0.000120      TGCG      0.000420              GACT      0.000048      GGCT      0.000036      TACT      0.000215      TGCT      0.000108              GAGA      0.014437      GGGA      0.008240      TAGA      0.001101      TGGA      0.000355              GAGC      0.000476      GGGC      0.001251      TAGC      0.000048      TGGC      0.000059              GAGG      0.008240      GGGG      0.056794      TAGG      0.000355      TGGG      0.002218              GAGT      0.000159      GGGT      0.000417      TAGT      0.000038      TGGT      0.000030              GATA      0.000838      GGTA      0.000177      TATA      0.001119      TGTA      0.000143              GATC      0.000048      GGTC      0.000036      TATC      0.000231      TGTC      0.000116              GATG      0.000177      GGTG      0.000968      TATG      0.000143      TGTG      0.000488              GATT      0.000073      GGTT      0.000040      TATT      0.000893      TGTT      0.000447              GCAA      0.001004      GTAA      0.000335      TCAA      0.000166      TTAA      0.000099              GCAC      0.001837      GTAC      0.000116      TCAC      0.001389      TTAC      0.000240              GCAG      0.000453      GTAG      0.000151      TCAG      0.000048      TTAG      0.000038              GCAT      0.000116      GTAT      0.000535      TCAT      0.000240      TTAT      0.002009              GCCA      0.000315      GTCA      0.000048      TCCA      0.000548      TTCA      0.000215              GCCC      0.009764      GTCC      0.000376      TCCC      0.019456      TTCC      0.001275              GCCG      0.000194      GTCG      0.000036      TCCG      0.000274      TTCG      0.000108              GCCT      0.000376      GTCT      0.000773      TCCT      0.001275      TTCT      0.006924              GCGA      0.000476      GTGA      0.000159      TCGA      0.000048      TTGA      0.000038              GCGC      0.001823      GTGC      0.000117      TCGC      0.000694      TTGC      0.000120              GCGG      0.001251      GTGG      0.000417      TCGG      0.000059      TTGG      0.000030              GCGT      0.000117      GTGT      0.000530      TCGT      0.000120      TTGT      0.001005              GCTA      0.000048      GTTA      0.000073      TCTA      0.000231      TTTA      0.000893              GCTC      0.000891      GTTC      0.000258      TCTC      0.004935      TTTC      0.003240              GCTG      0.000036      GTTG      0.000040      TCTG      0.000116      TTTG      0.000447              GCTT      0.000258      GTTT      0.002355      TCTT      0.003240      TTTT      0.031522      ExercisesSimulate a site on the four-species tree described in this lab using the rate matrix.\\[{\\mathbf Q} = \\{q_{ij}\\} = \\left( \\begin{array}{rrrr} -0.886 &amp;  0.190 &amp;  0.633 &amp;  0.063 \\\\ 0.253 &amp; -0.696 &amp;  0.127 &amp;  0.316 \\\\ 1.266 &amp;  0.190 &amp; -1.519 &amp;  0.063 \\\\ 0.253 &amp;  0.949 &amp;  0.127 &amp; -1.329 \\\\\\end{array} \\right)\\]Why do we start the simulation by drawing from the stationary distribution?",
        "url": "/tutorials/dice/",
        "index": "true"
      }
      ,
    
      "tutorials-paths": {
        "title": "Tutorial modules",
        "content": "This page is intended to guide users in assembling a set of tutorials that will help them learn how to use RevBayes for a given analysis. Each module lists the tutorials that focus on a particular topic, as well as those that will provide prerequisite knowledge or resources for deeper learning.  We encourage users with no RevBayes experience to start with the RevBayes fundamentals module.RevBayes fundamentalsThis module is for users who have no experience with RevBayes. The tutorials contained here walkthe user through the basics of installing and using RevBayes, and some of the theory behindRevBayes analyses.  Pre-requisites: Basic knowledge on Bayesian statistics.  Getting started with RevBayes. This tutorial includes getting RevBayes working on your machine, and learning about the fundamentals of setting up models in the software.  MCMC fundamentals. Three tutorials lay out the basics of Markov Chain Monte Carlo analysis in RevBayes, using three different examples: Airline fatalities, coin flips, and archery. It is recommended that absolute beginners go through all three.By the end of this module, you should be able to build RevBayes, write simple Rev code, and build simple models on which to run MCMC analyses. From here, you can select the module(s) below thatbest represent your interests and intended analyses.MCMC assessmentThis module is for users looking to debug or analyze their MCMC results deeper. The tutorials will walk the user through best practices when analyzing posterior samples, and tips on how to improve your analyses if your analysis has not converged.  Pre-requisites: RevBayes fundamentals, and results from some analysis (e.g. from completion of one of the modules below).  Assessing convergence of your MCMC results: Convergence assessment. This tutorial will walk you through theory and examples on how to assess the convergence of your MCMC chain.  Practical exercises on assessing convergence: Debugging your MCMC. This tutorial contains more exercises on assessing convergence, and provides tips on how to improve convergence in your analysis.  Introduction to RevGadgets walks you through the use of RevGadgets, an R package that is an optional but convenient tool for postprocessing of RevBayes results.Molecular phylogenetic inferenceThis module is intended for users looking to infer a phylogenetic tree using molecular (DNA) data.Users will learn how to set up basic molecular evolution model, and be presented with more advancedtutorials to tailor their learning to their desired analyses.  Pre-requisites: RevBayes fundamentals.  Theoretical background: Understanding continuous-time Markov models. This tutorial will get you up to speed with the theory behind molecular evolution models.  Basic molecular phylogenetic inference: Nucleotide substitution models. This tutorial will walk you through a simple example of molecular phylogeny inference.  If interested in partitioning molecular data: Partitioned data analysis.  If interested in choosing between models of molecular evolution:          General introduction to model selection.      Model selection for one locus.      Model selection for partition models.      By the end of this module, you should be able to infer a phylogenetic tree with a simple molecular evolution model. Depending on your interests, you will also have learned about how to set uppartitioned models, and choose the best models for your dataset.Polymorphism-aware molecular phylogeneticsThis module is intended for users looking to infer a phylogenetic tree using molecular data including polymorphisms. Users will learn how to modify their molecular evolution models to account for the presence of polymorphisms in the dataset, including, if desired, balanced selection. Note that the tutorials in this module use code on a separate RevBayes branch, requiring users to run git checkout dev_PoMo_SNP within their local RevBayes repo before running analyses.  Pre-requisites: RevBayes fundamentals, Molecular phylogenetic inference.  Polymorphism-aware phylogenetic models.          If interested in including balancing selection in your model: Polymorphism-aware phylogenetic models with balancing selection.      Morphological phylogenetic inferenceThis module is intended for users looking to infer a phylogenetic tree using morphological data.Users will learn how to set up basic morphological evolution models, including for characterswith more than two states.  Pre-requisites: RevBayes fundamentals.  Theoretical background: Understanding continuous-time Markov models. This tutorial will get you up to speed with the theory behind morphological evolution models.  Morphological phylogenetics with binary traits: Tree inference with discrete morphology. This tutorial will walk you through a simple example of morphological phylogeny inference, using only binary (i.e. two-state) characters.  Expanding your dataset: Multistate characters. This tutorial will teach you how to accommodate characters with more than two states in your morphological phylogenetics analyses.Dating a phylogenyThis module is intended for users looking to date a time-calibrated phylogenetic tree. Users will learn how to set up analyses using both molecular clocks and fossil data for node dating.  Pre-requisites: RevBayes fundamentals, and either Molecular phylogenetic inference or Morphological phylogenetic inference. Note that while the tutorials below use molecular models, it is straightforward to adapt the clock model to a morphological analysis.  Master dating tutorial: Dating trees. This tutorial includes:          Global molecular clocks: The basics of molecular clocks.      Uncorrelated relaxed clocks: More complex (and biologically realistic) molecular clocks.      Node dating with fossils: Including fossil data for node dating. The master dating tutorial also links to a more advanced tip-dating tutorial (see the Total-evidence analysis with tip-dating module).        Bringing it all together: Relaxed clocks &amp; time trees. This tutorial walks the user through a full analysis, including performing model selection between multiple clock models.  If interested in enforcing the relative order of nodes in your analysis: Dating with relative constraints.Estimating diversification ratesThis module is intended for users looking to infer rates of speciation and extinction. Users will learn how to set up analyses using models in the birth-death model family, including the use of fossil data. While most of the tutorials here assume a fixed phylogenetic tree, some recommendations are made to point the user towards the direction of joint estimation as well.  Pre-requisites: RevBayes fundamentals.  Theoretical background: Introduction to diversification rate estimation. This tutorial will walk you through the theory behind the birth-death models used in the following tutorials.  Constant-rate models: Simple diversification-rate estimation. This tutorial will walk you through the basics of setting up a birth-death model for estimating rates of speciation and extinction from a fixed phylogenetic tree.The following tutorials are to be picked based on your intended analyses, as they lay out different hypotheses regarding diversification dynamics and how to set up a BD model to test those.  If interested in time-heterogeneous (or environmentally-dependent) diversification dynamics: Episodic diversification rate estimation, Environmental-dependent speciation and extinction rates.  If interested in mass-extinction estimation, including fossil taxa: Mass-extinction estimation.  If interested in jointly estimating topology and diversification rates: Relaxed clocks &amp; time-trees (consider completing the Molecular phylogenetic inference and Dating a phylogeny modules).Total-evidence analysis with tip-datingThis module is intended for users looking to infer a time-calibrated phylogenetic tree with both molecular and fossil data, and fossil ages.  Pre-requisites: Dating a phylogeny (and both Molecular phylogenetic inference and Morphological phylogenetic inference), and Estimating diversification rates.  The fossilized birth-death model. This tutorial will push you to put together the skills learned in many previous tutorials, including setting up molecular and morphological evolution, clock, and birth-death models.  If interested in more complex diversification dynamics, explore the advanced tutorials in Estimating diversification rates. Note that dnFBDP should be used instead of dnBDP/dnEBDP.State-dependent diversification-rate estimationThis module is intended for users looking to infer trait-dependent diversification rates using the state-dependent speciation and extinction (SSE) model family. As with Estimating diversification rates, the tutorials assume a fixed tree, but tips are included for joint estimation.  Pre-requisites: RevBayes fundamentals, and Estimating diversification rates.  Theoretical background: Background on state-dependent diversification-rate estimation. This tutorial will introduce you to the theory behind the SSE model family, building from the fundamentals on BD models you learned on the Estimating diversification rates module.  Testing the effects of discrete traits on diversification: State-dependent diversification with BiSSE and MuSSE. This tutorial will walk you through the setup of the simplest SSE models: binary and multiple SSE (BiSSE and MuSSE), which are used to test the effects of binary and multiple state discrete traits on diversification, respectively.  If interested in testing whether unobserved effects might impact diversification for your data: State-dependent diversification with hidden SSE (HiSSE).  If interested in the presence of cladogenetic state transitions: State-dependent diversification with cladogenetic SSE (ClaSSE).  If interested in the effect of chromosome number on diversification: Chromosome evolution.  If interested in jointly estimating topology and state-dependent diversification rates, adapt Relaxed clocks &amp; time-trees to use dnCDBDP or dnGLHBDSP instead of dnBDP (consider completing the Molecular phylogenetic inference and Dating a phylogeny modules).",
        "url": "/tutorials/paths/",
        "index": "false"
      }
      ,
    
      "tutorials-ted-workflow": {
        "title": "Total-evidence dating, model sensitivity, and model comparison",
        "content": "OverviewTotal-evidence (or combined-evidence) dating allows us to estimate time-calibrated phylogenies for extinct and extant species in one coherent statistical framework.In this framework, fossils are treated as tips in the phylogeny, and their phylogenetic position and branch lengths are inferred directly from morphological data rather than specified a priori.While this relieves us from the difficult (sometimes impossible) task of deriving reliable fossil-calibration densities for use in node-dating, it requires us to specify a model that (in addition to the standard components of a phylogenetic model) describes how morphological characters evolve, and how lineages diversify and produce fossils over time.As with all phylogenetic analyses (and divergence-time estimation in particular), inferences using total-evidence dating may be sensitive to the models we use.Therefore, when applying total-evidence dating, it’s a good idea to use different models to assess whether they affect your inferences, and if they do, to assess the relative and absolute performance of the different models.The purpose of this tutorial is to help you manage the task of using a potentially large number of total-evidence dating models and assessing their performance, similar to the workflow we use for our own work (May et al. 2021).It is intended to be adapted to new datasets, and to allow you to add or modify models as appropriate for your own analyses.This tutorial is structured as follows.In the first section (), we discuss the general structure of the total-evidence-dating model, and the organizational scheme of the analysis scripts we provide.In the second section (), we show how to estimate the posterior distribution under a given model using Markov-chain Monte Carlo.In the third section (), we present some tools for assessing how much different modeling assumptions affect tree topologies and divergence-time estimates.In the fourth section (), we show how to compare the relative fit of competing models using Bayes factors, which can be useful if posterior estimates are sensitive to different models.In the final section (), we show how to use posterior-predictive simulation to assess whether our models provide a good absolute (rather than relative) description of morphological evolution.⚠ This tutorial involves running many analyses and comparing many models. Be aware that it can take several hours to complete! We will also presume some familiarity with many aspects of phylogenetic modeling; be sure to refer to the prerequisites if you are not already familiar with the models we are using.A note about R dependenciesMany of the post-processing steps in this tutorial require you to use R, especially the package RevGadgets.Before you begin, you should make sure you have the following R package dependencies installed:  RevGadgets  ggplot2  ape  phytools  phangorn  smacof  gtools  gridExtraYou can install these packages in R using install.packages(library name).Consider doing the RevGadgets tutorial if you’re not comfortable working in R.IntroductionThe Total-Evidence Dating ModelThe total-evidence dating analysis has five model components:  A tree model that describes how lineages are distributed over time.  A molecular clock model that describes how rates of molecular evolution vary over the branches of the phylogeny (if at all).  A molecular substitution model that describes the process of evolution between different nucleotide states.  A morphological clock model that describes how rates of morphological evolution vary over the branches of the phylogeny (if at all).  A morphological transition model that describes the process of evolution between different morphological character states (e.g., between states 0 and 1 for a binary character).For any one of these model components, we must choose one of several possible models.For example, we may choose to use a uniform tree distribution or a fossilized birth-death process for the tree model.If we choose to use a fossilized birth-death process, we have to decide whether rates of diversification and/or fossilization vary over time (or even among clades).Because these assumptions can have strong effects on our ultimate inferences, we may wish to perform analyses under various different models and compare the relative and absolute fit of these models to our data.To learn more about these different model components, see the CTMC, molecular dating, FBD, and morphological phylogenetics tutorials.Data and Script FilesThe data and scripts for this tutorial have a special structure.To download all the files in the appropriate structure, click HERE, and then unpack the archive.You will want to run all of the scripts from this tutorial in the top-level directory of TED_workflow.The example dataset is a pruned down version of the marattialean fern dataset analyzed in May et al. (2021).We also provide scripts for most of the models we used for that study.However, the workflow is intended to be adapted to other datasets, and you can even add new models or variants of the existing models for your own studies.The archive also includes a headers directory, a modules directory, and a posterior_summary directory, all of which we explain below.Script OrganizationWhen we run a single analysis, it is often most convenient to write a single script that specifies every part of our model.However, when we run a potentially large number of analyses under different models, it can be helpful to adopt a different approach.While there are many conceivable approaches, one that we find useful is to create headers, which define the models to be used for a particular analysis, a template, which stitches together the models provided by the header file, and module files which implement the individual models ().Structure of the analyses. The header file (left) defines which models a particular analysis will use. The header file sources the template file. The template file (middle) uses the variables defined in the header file to find and source the appropriate module files. The module files (right column) specify specific model components, for example the Mk model or the constant-rate fossilized birth-death model.This tutorial focuses on the structure of a total-evidence dating analysis, and on assessing different models, rather than on the specific details of any one model.One thing to keep in mind is that it’s critical that different modules for a given model component define a common set of variables, so that we can swap different modules in without having to change other parts of the code.For example, all diversification models will have to define parameters $\\lambda$ and $\\mu$, to be used by the fossilized birth-death process.The details of how the different diversification models fill in $\\lambda$ and $\\mu$ will, of course, depend on the model.We’ll hide details of particular models in folds, like so (in case you want to dig in):Module: An example model moduleHere’s a Jukes-Cantor model of sequence evolution, partitioned by alignment:# the partitioned JC substitution model# REQUIRED: mole_Q (one per partition), mole_site_rates (one per partition), and mole_relative_rates (one per partition)# define the Q matrix and site rates per partitionfor(i in 1:naln) {  # the Q matrices  mole_Q[i] &lt;- fnJC(4)  # the site rates  # NOTE: this model doesn't have ASRV, but we have to define it anyway  mole_site_rates[i] &lt;- [1.0]}# relative-rate multipliers among partitionsmole_proportional_rates ~ dnDirichlet(rep(1, naln))moves.append( mvBetaSimplex(mole_proportional_rates, weight = naln) )# rescale the rates so the mean is 1mole_relative_rates := abs(mole_proportional_rates * naln)Note that the comments at the top of the module file list what variables have to be defined in this module.These are variables that will ultimately be used by other parts of the model, so all modules for a given model component must define those variables.For example, all substitution models must define one $Q$ matrix per data partition, a set of site rates to accommodate site-rate variation within partitions, and a set of relative rates among partitions.A Simple Header FileWe’ll see how this works by starting with the header file for an MCMC analysis under a simple total-evidence model (located in headers/MCMC/strict_Mk.Rev).The first thing we’re going to do is specify which tree model to use.In this tutorial, we’ll assume we’re always using a variant of the fossilized birth-death process as our tree model.However, there are variants of the fossilized birth-death process that assume that rates are constant over time, or that they vary over time.We create a variable, diversification_model, whose value is a string that refers to a specific diversification model (which defines speciation and extinction rates).We create an analogous variable that defines which fossilization model to use, fossilization_model.In this case, we’ll assume that both all of the parameters are constant over time.# tree modeldiversification_model = \"constant\"fossilization_model   = \"constant\"Next, we say which molecular model we want to use.In this case, we are using a strict molecular clock model and an HKY substitition model.# molecular modelmole_clock_model = \"strict\"substn_model     = \"HKY\"Likewise, we’ll use a strict morphological clock model, and a simple model of morphological evolution, the Mk model.# morphological modelmorph_clock_model = \"linked\"morph_model       = \"Mk\"Next, we’ll specify what type of analysis we want to do.In this example, we’re going to start with a standard MCMC analysis; later, we’ll also do “power posterior” and “posterior-predictive simulation” analyses.# the type of analysisanalysis = \"MCMC\"There’s nothing more frustrating than running two (or more) analyses but forgetting to change the output files, so all your outputs get overwritten!To prevent this from happening, we’ll create some variable in the header file that define where the output should be stored, and also build the name of the output directory based on the variables defined above (so that different analyses will end up with different output filenames).To achieve this, we start by defining the overall output directory:# the output directoryoutput_dir      = \"output_MCMC\"We then create another variable, output_extra that you can use to append any additional information to the output file name.output_extra    = \"_run_01\"(We’re using output_extra to specify a given run of the same analyses.This lets us quickly do multiple runs by duplicating the header and changing the run number.However, in principle, you could use this variable to keep track of any additional information you want.)Next, we use string concatenation (+) to programmatically create the output filename based on the output_dir and the analysis-specific variables.output_filename = output_dir + \"/div_\" + diversification_model + \"_foss_\" + fossilization_model + \"_moleclock_\" + mole_clock_model + \"_moleQ_\" + substn_model + \"_morphclock_\" + morph_clock_model + \"_morphQ_\" + morph_model + \"_\" + analysis + output_extra + \"/\"Finally, the header file sources the template.Rev file.You can think of this as the header file handing all of the relevant information we’ve just defined to the template file, which then puts together all the corresponding models into one analysis.# source the template filesource(\"modules/template.Rev\")The Template File: Reading the DataThe job of the template file is to take the values specified in the header to put together an analysis.The template file is located in modules/template.Rev.Let’s look at it line-by-line.Like most Rev scripts, the first thing we’ll do in the template is create a container for moves:# moves containermoves = VectorMoves()as well as some useful constants (in this case, H is the standard deviation for a lognormal distribution that spans one order of magnitude, which we use for some prior distributions):# convenient constantsH = ln(10) / (qnorm(0.975) - qnorm(0.025))Again like a normal Rev script, we’ll load our data.For your own datasets, you’ll want to substitute your own data files for these variables.We’ll begin by reading in our molecular dataset:# read the sequence datamoledata = readDiscreteCharacterData(\"data/rbcL.nex\", alwaysReturnAsVector = TRUE)naln     = moledata.size() # the number of alignmentsThe argument alwaysReturnAsVector = TRUE enforces that the molecular data is always assumed to be a vector.Whether you are reading in a nexus file with a single alignment or many alignments, the result will always be a vector of alignments.If we read one alignment, we’d just end up with a vector of length one.This means that moledata.size() returns the number of alignments in the vector, not the number of sites in the alignment.(Also note that our example molecular data file contains three alignments, one per codon position.)Next, we read our morphological data:# read the morphological datamorphdata = readDiscreteCharacterData(\"data/morpho.nex\")Now, we read in the taxon data (including the ages associated with each taxon, as described here):# read the taxon datataxa = readTaxonData(\"data/taxa.tsv\", delimiter=TAB)ntax = taxa.size()nbranch = 2 * ntax - 2It will be handy to know the number of fossils in our dataset.To do this, we count the number of taxa in our taxa.tsv file that aren’t sampled at time 0:# count the number of fossilsnum_fossils = 0for(i in 1:taxa.size()) {  if (taxa[i].getMinAge() != 0) {    num_fossils = num_fossils + 1  }}We also need to make sure all of our character data objects have the same species in them.We add missing data to each data object for any species that aren’t sampled for that dataset.# add missing taxa to the sequence datafor(i in 1:naln) {  x = moledata[i]  x.addMissingTaxa(taxa)  moledata[i] = x}# add missing taxa to the morphological datamorphdata.addMissingTaxa(taxa)Finally, some of our fossilized birth-death models will allow diversification and/or fossilization rates to vary among geological epochs.We’ll read in a file that encodes these epochs, epoch_timescale.csv, and define our breakpoints accordingly.You may wish for rates to vary over ages, periods, or some other arbitrary way; this code should work for any arbitrary piecewise timescale, as long as it is formatted the same as epoch_timescale.csv.# read in the timescaletimescale = readDataDelimitedFile(\"data/epoch_timescale.csv\", header = true, delimiter=\",\")num_bins  = timescale.size()for(i in 1:num_bins) {    breakpoints[i] &lt;- timescale[i][2]}The Template File: Specifying the ModelsHere is where the rubber meets the road!We’ll start using the variables defined in the header file to create our model.The Tree ModelThe first object we will create is the tree.We first need to define some dataset-specific variables, for example the number of extant taxa in the tree, and the total number of extant taxa (these values will be used to compute the sampling fraction):############################## specifying the tree model ############################### first we specify some dataset-specific parametersextant_taxa = 10    # total number of extant taxa in the treetotal_taxa  = 111   # total number of extant taxa in the groupWe also need to specify the minimum and maximum age of the group (the age of the lineage ancestral to the root of the tree):origin_min  = 419.2 # latest origin is the beginning of the Devonianorigin_max  = 485.4 # earliest origin is the beginning of the OrdovicianWe assume the ancestral lineage is uniformly distributed between this minimum and maximum age.We therefore draw it from a uniform prior distribution and place MCMC proposals on it:# draw the origin timeorigin_time ~ dnUniform(origin_min, origin_max)moves.append( mvSlide(origin_time, weight = 1.0) )Now we create the diversification model (that defines speciation and extinction rates) using the variables defined in the header.Once again, we use string concatenation to look up the appropriate file:# specify the diversification modelsource(\"modules/diversification_models/\" + diversification_model + \".Rev\")In this case, the above code will evaluate to source(\"modules/diversification_models/constant.Rev\"), because we defined diversification_model = \"constant\" in our header file!Module: The Constant-Rate Diversification ModelIn this model, (modules/diversification_models/constant.Rev) we assume speciation ($\\lambda$) and extinction ($\\mu$) rates are constant over time.We parameterize the model using the net-diversification rate ($\\lambda - \\mu$) and the relative extinction rate ($\\mu \\div \\lambda$).We use an empirical prior on the diversification rate such that the prior mean diversification rate gives rise to the known number of taxa at the present (see below for details):# the constant-rate diversification model# REQUIRED: lambda, mu (both one per time interval)# empirical prior on the diversification ratediversification_prior_mean &lt;- ln(total_taxa) / origin_timediversification_prior_sd   &lt;- H# the diversification ratediversification ~ dnLognormal( ln(diversification_prior_mean) - diversification_prior_sd * diversification_prior_sd * 0.5, diversification_prior_sd)moves.append( mvScale(diversification, weight = 1.0) )The standard deviation of this distribution is H, which implies that the true diversification rate is within one order of magnitude of the prior mean.We draw the relative-extinction rate from a uniform prior between 0 and 1 (if the relative extinction rate was greater than one, the process almost certainly would have died before reaching the present):# the relative extinction raterelext ~ dnUniform(0, 1)moves.append( mvSlide(relext, weight = 1.0) )Finally, we transform the net-diversification and relative-extinction rates into the speciation and extinction rates:# transform to real parameterslambda := rep(abs(diversification / (1 - relext)), num_bins + 1)mu     := abs(lambda * relext)Note that we replicate lambda one time per time interval (num_bins + 1).We are doing this because some diversification models will actually let lambda vary among time intervals, and other parts of the code (for example, the FBD distribution) will not know whether we are using a constant or epochal model.Replicating lambda lets us use downstream code that will work the same for constant or epochal models.The abs() functions just guarantee that both of these rates are positive numbers.Here’s how the empirical prior works.For simplicity, we imagine the process begins with at time 0 and diversifies forward in time, $t$, under a deterministic growth model with growth rate (diversification rate) $\\lambda - mu$.The number of species at time $t$ is then:\\[\\begin{equation*}N(t) = N(0) \\exp^{(\\lambda - \\mu)t}\\end{equation*}\\]We can turn this into an empirical prior on the mean by plugging in the number of extant species into $N(t)$, the number of initial species (one) into $N(0)$ and solving for $\\lambda - \\mu$:\\[\\begin{equation*}\\lambda - \\mu = \\frac{ln[N(t)/N(0)]}{t} = \\frac{ln[N(t)]}{t}\\end{equation*}\\]which is reflected in the prior mean code:diversification_prior_mean &lt;- ln(total_taxa) / origin_timeNext, we source the fossilization model:# specify the fossilization modelsource(\"modules/fossilization_models/\" + fossilization_model + \".Rev\")Module: The Constant-Rate Fossilization ModelThis model is defined in modules/fossilization_models/constant.Rev.As with the constant-rate diversification model, this model assumes that the fossilization rate is constant over time.Here, we use an empirical prior that centers the fossilization rate such that, on average, we expect to see the number of fossils we observe in the empirical dataset (see below for details):# the constant-rate fossilization model# REQUIRED: psi (one per time interval)# empirical prior on the fossilization ratefossilization_prior_mean &lt;- num_fossils * diversification_prior_mean / (exp(diversification_prior_mean * origin_time) - 1)fossilization_prior_sd   &lt;- 2 * HWe then draw the fossilization rate from the corresponding Lognormal prior:# the fossilization ratepsi_global ~ dnLognormal( ln(fossilization_prior_mean) - fossilization_prior_sd * fossilization_prior_sd * 0.5, fossilization_prior_sd)moves.append( mvScale(psi_global, weight = 1.0) )As with the constant diversification model, we replicate the global fossilization rate so there is one per time interval:# define the timelinespsi := rep(psi_global, num_bins + 1)Like the diversification parameter, we’ve used an empirical prior on the fossilization rate, $\\psi$.If there are $N(t)$ species at time $t$, each of which leaves a fossil with rate $\\psi$, then the rate at which fossils are produced by the entire population at time $t$ is $\\psi \\times N(t)$.Assuming deterministic population growth at rate $\\lambda - \\mu$, we can compute the total number of fossils produced by the population up to time $t$, $F(t)$, by integrating time up to $t$:\\[\\begin{equation*}F(t) = \\int_0^t \\psi N(s) ds = \\frac{\\psi (\\exp^{(\\lambda - \\mu)t} - 1)}{\\lambda - \\mu}\\end{equation*}\\](assuming we began with a single lineage at time $t = 0$).To derive an empirical prior mean for the fossilization rate, we plug the observed number of fossils in our dataset into $F(t)$ and solve for $\\psi$, which leads to:\\[\\begin{equation}\\psi = \\frac{F(t) (\\lambda  - \\mu) }{ \\exp^{(\\lambda - \\mu)t} - 1 }\\end{equation}\\]which is reflected in the code as:fossilization_prior_mean &lt;- num_fossils * diversification_prior_mean / (exp(diversification_prior_mean * origin_time) - 1)Now that we have defined our origin_time and the diversification and fossilization models, we can draw the tree from a fossilized birth-death distribution:# make the FBD treetimetree ~ dnFBDP(originAge = origin_time,                  lambda    = lambda,                  mu        = mu,                  psi       = psi,                  timeline  = breakpoints,                  condition = \"survival\",                  rho       = Probability(extant_taxa / total_taxa),                  taxa      = taxa)where breakpoints defines where the rate parameters change, if applicable, and we condition on the process surviving (leaving at least one extant descendant).We have to place MCMC proposals on the tree topology and branch lengths:# MCMC proposals on the treemoves.append( mvFNPR(timetree,                       weight = ntax                             ) )moves.append( mvNarrow(timetree,                     weight = 5 * ntax                         ) )moves.append( mvNodeTimeSlideBeta(timetree,          weight = 5 * ntax                         ) )moves.append( mvRootTimeSlideUniform(timetree,       weight = ntax,        origin = origin_time) )moves.append( mvCollapseExpandFossilBranch(timetree, weight = num_fossils, origin = origin_time) )Next, we place proposals on the fossils: whether or not they are sampled ancestors, and also on their exact age (to accommodate stratigraphic uncertainty):# MCMC proposals on whether fossils are sampled ancestorsmoves.append( mvCollapseExpandFossilBranch(timetree, weight = num_fossils, origin = origin_time) )num_sampled_ancestors := timetree.numSampledAncestors()# MCMC proposals on the fossil agesfossils = timetree.getFossils()for(i in 1:fossils.size()) {  # get the fossil age  t[i] := tmrca(timetree, clade(fossils[i]))  # specify the age contraint  a = fossils[i].getMinAge()  b = fossils[i].getMaxAge()  F[i] ~ dnUniform(t[i] - b, t[i] - a)  F[i].clamp( 0 )  # specify a proposal on the fossil age  moves.append( mvFossilTimeSlideUniform(timetree, origin_time, fossils[i], weight = 1.0) )}Finally, we keep track of a tree for the extant species and the tree lengths.# keep track of the extant treeextant_tree := fnPruneTree(timetree, fossils)# keep track of the tree lengthTL        := timetree.treeLength()TL_extant := extant_tree.treeLength()The Molecular Clock ModelNow we specify the molecular clock model:######################################### specifying the molecular clock model #########################################source(\"modules/mole_clock_models/\" + mole_clock_model + \".Rev\")In this header file, we assumed a strict molecular clock.Module: The Strict Molecular ClockThis simple model (defined in modules/mole_clock_models/strict.Rev) assumes that the rate of evolution is the same across branches of the tree.It therefore has a single parameter, which we parameterize on the log scale:# the strict molecular clock model# REQUIRED: mole_branch_rates (either one value, or one value per branch), mole_branch_rate_mean# the strict clock model on the log scalemole_clock_rate_log ~ dnUniform(-10, 1)moves.append( mvSlide(mole_clock_rate_log) )mole_clock_rate_log.setValue(-7)We then exponentiate to get the clock rate on the real scale:# exponentiatemole_clock_rate := exp(mole_clock_rate_log)Our relaxed clock models will allow one rate per branch, defined by the variable mole_branch_rates (a vector with one element per branch).To keep our code generic, we’ll make sure we define mole_branch_rates for the constant model, but just set it equal to the strict clock rate:# the branch-specific ratesmole_branch_rates := mole_clock_rateFinally, we compute the mean rate of evolution among branches (sometimes we will use this for relaxed clock models, but in this case it is again equal to the clock rate itself).# the mean of the branch ratesmole_branch_rate_mean := mole_branch_ratesThe Molecular Substitition ModelWe source the molecular substitution model as so:###################################### specifying the substitution model ######################################source(\"modules/substn_models/\" + substn_model + \".Rev\")In this analyses, we’re using the HYK substitution model, partitioned among alignments (which correspond to codon positions in the example rbcL dataset).Module: The HKY Substitution ModelThis model is defined in modules/substn_models/HYK.Rev.It has a single parameter, $\\kappa$, which defines the transition to transversion ratio, and a vector of stationary frequencies, $\\pi$.We allow each molecular partition to have different $\\kappa$ and $\\pi$ parameters.# the partitioned HKY substitution model# REQUIRED: mole_Q (one per partition), mole_site_rates (one per partition), and mole_relative_rates (one per partition)# define the Q matrix and site rates per partitionfor(i in 1:naln) {  # the transition/transversion ratio  mole_kappa[i] ~ dnLognormal(ln(1) - H * H * 0.5, H)  moves.append( mvScale(mole_kappa[i], weight = 1.0) )  # the stationary frequency  mole_pi[i] ~ dnDirichlet(rep(1,4))  moves.append( mvBetaSimplex(mole_pi[i], weight = 1.0) )  # the Q matrices  mole_Q[i] := fnHKY(mole_kappa[i], mole_pi[i])  # the site rates  # NOTE: this model doesn't have ASRV  mole_site_rates[i] &lt;- [1.0]}Because this model doesn’t allow rate variation among sites within a partition, we set mole_site_rates[i] &lt;- [1.0].We also want each partition to have a different overall rate.We specify a proportional rate per partition from a Dirichlet distribution (which has a sum of one), then multiply by the number of partitions (so that the mean rate is 1):# relative-rate multipliersmole_proportional_rates ~ dnDirichlet(rep(1, naln))moves.append( mvBetaSimplex(mole_proportional_rates, weight = naln) )# rescale the rates so the mean is 1mole_relative_rates := abs(mole_proportional_rates * naln)(Once again, abs simply reassures RevBayes that these rates are positive numbers.)We then specify the phylogenetic CTMC models for each alignment, which depend on the tree, substitution model and molecular clock model:# make the CTMC for molecular datafor(i in 1:naln) {  seq[i] ~ dnPhyloCTMC(timetree, mole_Q[i], branchRates = mole_branch_rates, siteRates = mole_site_rates[i] * mole_relative_rates[i])  seq[i].clamp( moledata[i] )}The Morphological Clock ModelOur model must also specify how rates of morphological evolution vary among lineages (if at all).We specify this model by sourcing the appropriate module file:############################################# specifying the morphological clock model #############################################source(\"modules/morph_clock_models/\" + morph_clock_model + \".Rev\")We’re using a “linked” morphological clock model, which assumes rates of morphological evolution are proportional to rates of molecular evolution (per branch).In this case, because we’re using a strict molecular clock, this implies that rates of molecular evolution also follow a strict clock (though the absolute rate will be different between molecular and morphological characters).Module: The Linked Morphological ClockHere, we use the linked morphological clock, specified in the modules/morph_clock_models/linked.Rev module file.This model has a single parameter: the absolute rate of morphological evolution, which we parameterize on the log scale and then exponentiate onto the real scale:# the linked morphological clock model# REQUIRED: morph_branch_rates (either one value, or one value per branch)# draw the log of the rate from a uniform distributionmorph_clock_rate_log ~ dnUniform(-10, 1)moves.append( mvSlide(morph_clock_rate_log) )morph_clock_rate_log.setValue(-7)# exponentiatemorph_clock_rate := exp(morph_clock_rate_log)Now we compute the branch-specific rates of morphological evolution.Because we’re assuming these are proportional to the branch-specific rates of molecular evolution, we simply rescale the molecular branch rates like so:# the branch-specific rates# in this model, these are a multiple of the molecular branch rates# so, the morphological branch rate = morphological clock rate * molecular branch rate / morphological clock ratemorph_branch_rates := morph_clock_rate * mole_branch_rates / mole_branch_rate_meanBecause all molecular clock models will define a mole_branch_rate_mean, this module will work in combination with all molecular clock models.In this example, we are effectively just rescaling the (global) molecular clock rate to get the (global) morphological clock rate.The Morphological Transition ModelThis model component describes how morphological characters change among states.(We’ll assume for simplicity that the characters are binary.These modules would have to be modified to accommodate multistate characters.See the multistate tutorial for ideas of how to achieve this.)We source the morphological transition model:################################################## specifying the morphological transition model ##################################################source(\"modules/morph_models/\" + morph_model + \".Rev\")In this analysis, we’re using an Mk model (Lewis 2001).Module: The Mk ModelIn this analysis we’re assuming that rates of transition between character states are the same, i.e., that the $Q$ matrix is symmetrical.Because the rate matrix is normalized to have an average rate of 1, this model has no free parameters:# the Mk model of morphological evolution# REQUIRED: morph_Q (either one, or one per mixture category), morph_site_rates, site_matrices (TRUE or FALSE)# the Mk modelmorph_Q &lt;- fnJC(2)# relative rates among sitesmorph_site_rates &lt;- [1.0]# make sure we don't use site matricessite_matrices = FALSEThe rates of change among characters are the same, so we use morph_site_rates &lt;- [1.0], and the rate matrix $Q$ is the same for all characters, so we set site_matrices = FALSE.Just as with molecular substitution models, we hand the tree, transition model, and morphological-clock model to a phylogenetic CTMC, and clamp our observed data:# make the CMTC for morphological datamorph ~ dnPhyloCTMC(timetree, morph_Q, branchRates = morph_branch_rates, siteRates = morph_site_rates, coding = \"variable\", type = \"Standard\", siteMatrices = site_matrices)morph.clamp( morphdata )You may notice some difference between this CTMC and the ones we used for the molecular data.First, we’re using coding = \"variable\" because we’re assuming we only included characters that are variable within our focal group.This corresponds to the $v$ correction proposed by Lewis (2001).Second, we also supply a siteMatrices argument.This argument (when TRUE) indicates that the Q matrix may vary among characters, which happens when we use mixtures of rate matrices among characters (e.g. as described by the discrete morphology tutorial).The AnalysisNow that we’ve specified the entire model, we source the specified analysis file:##################### running analysis #####################source(\"modules/analysis/\" + analysis + \".Rev\")This code is responsible for running whathever analyses we provide, whether it is MCMC, stepping-stone analysis, posterior-predictive simulation, etc.We’ll begin by running a simple MCMC, which we will talk about in the next section.Inferring the Posterior Distribution for One ModelNow that we’ve specified a header file and a template file, we’ll want to run an MCMC to estimate the posterior distribution for our total-evidence analysis.We do this by calling the analysis/MCMC.Rev module.Let’s look at this script line-by-line.The first thing we’ll want to do is decide how many runs to do (nruns), how many generations to run for (ngen), and how often to write sampled to file (printgen).We’ll do a burnin analysis, so we’ll also need to decide how many burnin generations to run for (nburnin):# analysis settingsnruns    = 1printgen = 10nburnin  = 2000ngen     = 20000We’ll also want to keep track of our MCMC analysis using monitors.We’ll use a screen monitor to log the progress to our screen, as well as a model monitor to keep track of model parameters, and file monitors to keep track of the full tree and the extant tree.# the monitorsmonitors = VectorMonitors()monitors.append( mnScreen(printgen = printgen) )monitors.append( mnModel(filename  = output_filename + \"params.log\", printgen = printgen, exclude = [\"F\"]) )monitors.append( mnFile(filename   = output_filename + \"tree.trees\", printgen = printgen, timetree) )monitors.append( mnFile(filename   = output_filename + \"extant_tree.trees\", printgen = printgen, extant_tree) )Note that these monitors make use of the output_filename that was constructed in our header file.This ensures that, if we use a different model, we don’t risk accidentally overwriting or losing track of our output files.Next, we create a model object by providing at least one of our parameters.The timetree is a natural choice because it will always be in the model (regardless of what model components we are using).# the modelmymodel = model(timetree)Now, we create our MCMC analysis, which depends on our model, monitors and moves, as well as some information about how many runs to do, and how to combine the output files when we do multiple runs:# make the analysismymcmc = mcmc(mymodel, monitors, moves, nruns = nruns, combine = \"mixed\")If we specified a burnin (that is, more than zero burnin generations, nburnin &gt; 0), we now run that part of the analysis.This analysis will adjust MCMC proposals every tuningInterval iterations to improve the acceptance rates (to a target value of 23-44%).We also print out the MCMC proposal information with operatorSummary, which tells us how often each proposal was accepted during the burnin phase.# run the burninif (nburnin &gt; 0 ) {  mymcmc.burnin(generations = nburnin, tuningInterval = 100)  mymcmc.operatorSummary()}Note that we normally throw out the first chunk of our MCMC sampled (before the chain has converged) as “burnin”.This burnin analysis is a bit different: it is a special “warm-up” MCMC where the tuning parameters are adjusted to achieve good acceptance rates.While the chain may converge in this period, there are no guarantees!It is good practice to still check the samples generated by the run step (next) to decide how many need to discard as burnin.Now we are ready to run our MCMC analysis for ngen generations.# run the analysismymcmc.run(generations = ngen)After the analysis completes, you’ll want to create summary trees.If you did more than one run (nruns &gt; 1), you’ll create one summary tree for each run as well as the combined runs.We’ll make maximum-clade-credibilty (MCC) trees, but you could also modify or extend this code to produce maximum a posteriori (MAP) trees.# make the summary treesif ( nruns == 1 ) {  # just make summary trees for the one run  full_trees = readTreeTrace(output_filename + \"tree.trees\", \"clock\")  mccTree(full_trees, output_filename + \"MCC_tree.tre\")  extant_trees = readTreeTrace(output_filename + \"extant_tree.trees\", \"clock\")  mccTree(extant_trees, output_filename + \"MCC_extant_tree.tre\")} else {  # make a combined summary tree  full_trees = readTreeTrace(output_filename + \"tree.trees\", \"clock\")  mccTree(full_trees, output_filename + \"MCC_tree.tre\")  extant_trees = readTreeTrace(output_filename + \"extant_tree.trees\", \"clock\")  mccTree(extant_trees, output_filename + \"MCC_extant_tree.tre\")  # and run-specific summary trees  for(i in 1:nruns) {    full_trees = readTreeTrace(output_filename + \"tree_run_\" + i + \".trees\", \"clock\")    mccTree(full_trees, output_filename + \"MCC_tree_run_\" + i + \".tre\")    extant_trees = readTreeTrace(output_filename + \"extant_tree_run_\" + i + \".trees\", \"clock\")    mccTree(extant_trees, output_filename + \"MCC_extant_tree_run_\" + i + \".tre\")  }}⚠ For the purposes of this tutorial, we’ll assume these MCMC worked!However, for your own analyses, you’ll definitely want to make sure that your analyses converge and sample adequately from the posterior distribution.See the MCMC Diagnosis tutorial for more details.Plotting the inferred treeCongratulations! You’ve inferred a total-evidence-dated phylogeny!You can plot this tree in FigTree, or use RevGadgets (Tribble et al. 2022) to create a publication-quality figure.We’ve provided some RevGadgets code for plotting this tree.Boot up Rstudio (or your preferred R console), and check out the script posterior_summary/plot_trees.R.The first thing we do is load RevGadgets.library(RevGadgets)Now we make a variable that stores where the summary tree is located.# specify a tree filetreefile &lt;- \"output_MCMC/div_constant_foss_constant_moleclock_strict_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/MCC_tree.tre\"Then we read the tree.# read the treetree &lt;- readTrees(treefile)Finally, we plot the tree using plotFBDTree.# plot the treep &lt;- plotFBDTree(tree = tree, timeline = TRUE, tip_labels_italics = FALSE,            tip_labels_remove_underscore = TRUE,            geo_units = \"periods\",            node_age_bars = TRUE, age_bars_colored_by = \"posterior\",            label_sampled_ancs = TRUE,            age_bars_color = rev(colFun(2))) +  ggplot2::theme(legend.position=c(0.75, 0.4))Adjust this plot as you please, then export it as a pdf (or other type of image) like so:pdf(\"figures/tree.pdf\", width = 10, height = 8)print(p)dev.off()which produces .The MCC tree inferred for Marattiales under a constant-rate FBD model, a strict molecular/morphological clock model, an HKY substitution model, and an Mk morphological model.The bars represent 95% posterior credible intervals for node ages.The color of each bar indicates the posterior probability of the clade (for internal nodes), of being a tip (for fossil tips), or for being a sampled ancestor (for sampled ancestor nodes).Exercise: Comparing phylogenies estimated under different total-evidence modelsNow that we’ve run one header file start-to-finish, it’s time to start using other models.This is where the header/template/module structure really pays off.To specify a new model, we simply create a header file and a module file for the new model.Let’s modify our first header file, headers/MCMC/strict_Mk.Rev to use an uncorrelated lognormal relaxed molecular clock.Duplicate this file and rename it UCLN_Mk.Rev.Then, change the value of mole_clock_model like so:mole_clock_model = \"UCLN\"Module: The Uncorrelated Lognormal Molecular Clock ModelThe uncorrelated lognormal (UCLN) relaxed molecular clock allows the rates of molecular evolution to vary among branches.Each branch draws its rate from an underlying lognormal distribution with some mean and variance.Since we don’t know what the mean and variance are, we treat them as free parameters, place a prior on them, and estimate them from the data.In the parlance of hierarchical modeling, we would the mean and standard deviation “hyperparameters” of the branch-rate prior.We’ll use the same prior on the mean as we used on the strict clock rate:# the UCLN morphological clock model# REQUIRED: morph_branch_rates (either one value, or one value per branch)# draw the log of the mean from a uniform distributionmole_clock_rate_mean_log ~ dnUniform(-10, 1)moves.append( mvSlide(mole_clock_rate_mean_log) )mole_clock_rate_mean_log.setValue(-7)# exponentiate to get the true meanmole_clock_rate_mean := exp(mole_clock_rate_mean_log)We also need to estimate the standard deviation of this lognormal clock model.We use an exponential distribution with a mean value of H, which means we expect molecular rates to vary over branches by about an order of magnitude.Using an exponential prior here lets the standard deviation more easily shrink to zero, which corresponds to a strict molecular clock.# draw the standard deviation from an exponentialmole_clock_rate_sd ~ dnExponential(abs(1 / H))moves.append( mvScale(mole_clock_rate_sd) )Now that we have the mean and standard deviation, we’ll draw each branch rate from the corresponding lognormal prior.We draw these on the log scale, then exponentiate (just as we did with the mean):# the branch-specific ratesfor(i in 1:nbranch) {  # draw the log of the rate  mole_branch_rates_log[i] ~ dnNormal(mole_clock_rate_mean_log - mole_clock_rate_sd * mole_clock_rate_sd * 0.5, mole_clock_rate_sd)  moves.append( mvSlide(mole_branch_rates_log[i]) )  mole_branch_rates_log[i].setValue(mole_clock_rate_mean_log - mole_clock_rate_sd * mole_clock_rate_sd * 0.5)  # exponentiate to get the rate  mole_branch_rates[i] := exp(mole_branch_rates_log[i])}We keep track of the mean rate among branches for use with the linked morphological clock model:# the mean of the branch ratemole_branch_rate_mean := mean(mole_branch_rates)Because we parameterized the log of the branch rates, we can use very nice proposals on the branch rates that simultaneously update the mean and variance of the lognormal distribution:# add a joint move on the branch rates and hyperparametersmoves.append( mvVectorSlideRecenter(mole_branch_rates_log, mole_clock_rate_mean_log) )moves.append( mvShrinkExpand(mole_branch_rates_log, mole_clock_rate_sd) )Note that the output filename will automatically be updated to reflect the change in model!Also, since we’re still using the linked morphological clock, this model allows morphological branch rates to vary among branches (in proportion to the molecular branch rates).Now, prepare header files by duplicating and modifying the strict_Mk.Rev header file, and run the following analyses:  A strict clock/Mk model (the first analyses we ran).  The same as 1, but with an uncorrelated relaxed molecular clock (the modified header file we just made).  An uncorrelated exponential relaxed molecular clock.  An uncorrelated lognormal relaxed molecular clock, with an F81 mixture model among morphological characters.  A model with rates of diversification and fossilization that vary among epochs (but is otherwise the same as strict_Mk.Rev).⚠ These header files are already provided for you as headers/MCMC, but it’s good practice to create your own header files to get a sense of how everything fits together.To perform these analyses, you’ll need to use the following module files:Module: The Uncorrelated Exponential Relaxed Molecular ClockThis relaxed clock model has a single parameter: the mean rate of evolution among branches.The branch-rates then follow an exponential distribution with the specified mean.The logic is otherwise similar to the UCLN model.This model is defined in modules/mole_clock_models/UCE.Rev# the UCE clock model# REQUIRED: mole_branch_rates (either one value, or one value per branch), mole_branch_rate_mean# draw the log of the mean from a uniform distributionmole_clock_rate_mean_log ~ dnUniform(-10, 1)moves.append( mvSlide(mole_clock_rate_mean_log) )mole_clock_rate_mean_log.setValue(-7)# exponentiate to get the true meanmole_clock_rate_mean := exp(mole_clock_rate_mean_log)# the branch-specific ratesfor(i in 1:nbranch) {  # draw the rate  mole_branch_rates[i] ~ dnExponential(1 / mole_clock_rate_mean)  moves.append( mvScale(mole_branch_rates[i]) )}# the mean of the branch ratemole_branch_rate_mean := mean(mole_branch_rates)# add a joint move on the branch rates and hyperparametersmoves.append( mvVectorScale(mole_branch_rates, mole_clock_rate_mean) )Module: The F81 Mixture ModelThe Mk model assumes that relative transition rates are the same among all character states.For binary characters, F81 assumes that each character state has a stationary frequency, $\\pi$, that is estimated from the data.This stationary frequency reflects the tendency for the character to evolve toward one state or the other.While this model may be appropriate for a single character, it is difficult to justify for many characters because the state labels are arbitrary (0 and 1 don’t have the same meaning for all characters), and the process of evolution is certainly very different among characters.We therefore use a relaxed version of the F81 model, called an F81 mixture model, that allows the stationary frequency to vary among characters.We draw a set of stationary frequencies from a discretized Beta distribution, and average the likelihood of each character over all possible stationary frequencies.We define a shape parameter, $\\alpha$, that describes how much variation there is among characters.When $\\alpha &lt; 1$, that means that, on average, character evolution is ery biased; conversely, when $\\alpha &gt; 1$, character evolution tends to be balanced (0 -&gt; 1 rates and 1 -&gt; 0 rates are more similar.)Please see the morphological phylogenetics tutorial for more details of this model.Here’s how we specify this model in RevBayes:# the F81 mixture model of morphological evolution# REQUIRED: morph_Q (either one, or one per mixture category), morph_site_rates, site_matrices (TRUE or FALSE)# process variation among charactersnum_pi_cats_morph = 4morph_pi_alpha ~ dnExponential(1)moves.append( mvScale(morph_pi_alpha) )morph_pis := fnDiscretizeBeta(morph_pi_alpha, morph_pi_alpha, num_pi_cats_morph)for(i in 1:num_pi_cats_morph) {  morph_pi[i] := simplex([ abs(morph_pis[i]), abs(1.0 - morph_pis[i])])  morph_Q[i]  := fnF81(morph_pi[i])}# relative rates among charactersmorph_site_rates &lt;- [1.0]# make sure we don't use site matricessite_matrices = TRUENote that this model (as defined above) assumes that the overall rate of evolution is the same among characters (morph_site_rates &lt;- [1.0]).We could relax this assumption, for example, by allowing rates to vary among characters according to a Gamma distribution (analogous to the ${+}\\Gamma$ model of molecular evolution.Check out the modules/morph_models/MkG.Rev model for an example of how to specify the discrete Gamma model.Note that the discrete Gamma and F81 mixture model can be combined!But to do that, you’d have to write a new module file, like F81MixG.Rev!We’ll leave that one to you :)Module: The Epochal FBD ModelThe fossilized birth-death model can easily accommodate diversification (speciation and extinction) and/or fossilization-rate variation over time.In our analyses, we’ll assume (for convenience) that these rates are different among different geological epochs, though you could use any time intervals you like by modifying the data/epoch_timescale.csv file.In this example, we’ll let both diversification and fossilization rates to vary.Let’s look at the epoch-variable diversification model in modules/diversification_models/epochal.Rev.Rather than letting each epoch have an independent rate, we’ll simplify things by assuming there are three rate categories, and try to infer which rate category each epoch belongs to.We begin by specifying the prior on the diversification rates per epoch.As with the constant-rate model, we use an empirical approach to specify the mean.# the epochal diversification model# REQUIRED: lambda, mu (both one per time interval)# empirical prior on the diversification ratediversification_prior_mean &lt;- ln(total_taxa) / origin_timediversification_prior_sd   &lt;- H# the diversitication ratediversification_prior = dnLognormal( ln(diversification_prior_mean) - diversification_prior_sd * diversification_prior_sd * 0.5, diversification_prior_sd)Note that we create a prior distribution object (diversification_prior) rather than a parameter here!We do the same thing for the relative-extinction rate:# empirical prior on the relative-extinction raterelext_prior_mean &lt;- 1.0relext_prior_sd   &lt;- H# the relative extinction raterelext_prior = dnLognormal( ln(relext_prior_mean) - relext_prior_sd * relext_prior_sd * 0.5, relext_prior_sd)We let this distribution have a prior mean of 1, because it’s possible that the extinction rate is higher or lower than the speciation rate for some epochs.Now we specify how many categories to use.We’ll keep it simple and use three categories.# specify the mixture modelnum_div_cats = 3We now draw the diversification and relative-extinction rates for each category.# draw the rates for each categoryfor(i in 1:num_div_cats) {  diversification_rate_cat[i] ~ diversification_prior  moves.append( mvScale(diversification_rate_cat[i]) )}# draw the rates for each categoryfor(i in 1:num_div_cats) {  relext_rate_cat[i] ~ relext_prior  moves.append( mvScale(relext_rate_cat[i]) )}Some categories may include more epochs than others.We therefore want to let the different categories have different “weights”, or prior probabilities that any given epoch is drawn from that category.Again we do this separately for diversification and relative-extinction rates.# draw the mixture weights for each categorydiv_mixture_weights ~ dnDirichlet(rep(1, num_div_cats))moves.append( mvBetaSimplex(div_mixture_weights, weight = 1.0) )moves.append( mvElementSwapSimplex(div_mixture_weights, weight = 1.0) )# draw the mixture weights for each categoryrelext_mixture_weights ~ dnDirichlet(rep(1, num_div_cats))moves.append( mvBetaSimplex(relext_mixture_weights, weight = 1.0) )moves.append( mvElementSwapSimplex(relext_mixture_weights, weight = 1.0) )Now that we’ve defined the rate categories, we draw each epoch rate from the rate categories (using a mixture distribution, i.e., a model that says the rate for an epoch takes one of a set of values with some probability):# draw the rates for each epochfor(i in 1:(breakpoints.size() + 1)) {\t# diversification rate  diversification[i] ~ dnMixture(diversification_rate_cat, div_mixture_weights)  moves.append( mvMixtureAllocation(diversification[i], weight = 1.0) )  # relative-extinction rate  relext[i] ~ dnMixture(relext_rate_cat, relext_mixture_weights)  moves.append( mvMixtureAllocation(relext[i], weight = 1.0) )}Finally, we transform the diversification and relative-extinction rates into speciation and extinction rates.# transform to real parameterslambda := abs(diversification / (1 - relext))mu     := abs(lambda * relext)To allow fossilization rates to vary, we basically repeat this process but for the psi parameter.See the modules/fossilization_models/epochal.Rev model for the code.After running the analyses, plot the summary tree for each analysis.Do divergence-time estimates vary among models?How about posterior probabilities for tree topologies or tip/sampled-ancestor relationships?It may be difficult or impossible to get a good sense of how these inferences vary by eye-balling the tree.In the next section, we’ll explore some more useful ways of exploring the impact of model choice on our estimates.Assessing model sensitivityIn this section, we’ll discuss how to tell whether different models affect phylogenetic estimates.We’ll use two tools for assessing model sensitivity: lineage-through-time (LTT) plots to visualize the inferred number of lineages over time and multidimensional scaling (MDS) plots to visualize differences in posterior distributions of trees.The code for plotting LTT curves and MDS plots can all be found in the R script posterior_summary/sensitivity.RFor these examples, we’ll assume you’ve already estimated the posterior distribution of total-evidence trees for the five models we explored in the last exercise.(If you have just skipped to this section, you can generate the necessary output by running the following header files: strict_Mk.Rev, UCLN_Mk.Rev, UCE_Mk.Rev, UCLN_F81Mix.Rev, and epochal_Mk.Rev in the headers/MCMC directory.)Lineage-Through-Time PlotsA lineage-through-time curve displays the number of branches in the inferred tree at any given time, so we can use LTTS to summarize overall differences in divergence-time estimates between different models.We’ll use R and RevGadgets to plot the LTT curves for our different models.First, we load some required packages and code:library(RevGadgets)library(phytools)source(\"posterior_summary/utils.R\")Now, we read in the trees that were sampled by a set of analyses:# read the samplesstrict_samples  &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_strict_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")UCLN_samples    &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")UCE_samples     &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_UCE_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")epochal_samples &lt;- readTrees(\"output_MCMC/div_epochal_foss_epochal_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")F81Mix_samples  &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_F81Mix_MCMC_run_01/tree.trees\", tree_name = \"timetree\")The treename argument tells RevGadgets what the tree variable was called in your RevBayes analyses.Since we named our tree timetree in our template file, we use tree_name = \"timetree\".Next, we combine the trees into a single list:# combine the samples into one listcombined_samples &lt;- list(strict  = strict_samples[[1]],                         UCLN    = UCLN_samples[[1]],                         UCE     = UCE_samples[[1]],                         epochal = epochal_samples[[1]],                         F81Mix  = F81Mix_samples[[1]])Note that we’ve named each element of the list.For example, strict  = strict_samples[[1]] indicates that the first element of the list will be named strict.This lets us color our LTT curves by the model name.Now, we compute the lineage through time curves for each model:# plot the LTTsLTTs &lt;- processLTT(combined_samples, num_bins = 1001)We evaulate the number of lineages at a finite set of time points, defined by num_bins.Turning up the value of num_bins may make the curves look smoother, but they will also take longer to compute and may exaggerate MCMC noise.Finally, we plot the LTT curves with plotLTT.plotLTT(LTTs, plotCI = FALSE)For ease of interpretation, we’re omitting the 95% credible intervals around the number of lineages, but you may choose to turn them on with plotCI = TRUE.(You can also adjust the size of the credible interval, e.g., you can use the 50% credible interval by specifying CI = 0.5 in the processLTT function.)We can save our LTT plot to a file like so:pdf(\"LTTs.pdf\", height = 4)print(plotLTT(LTTs, plotCI = FALSE))dev.off()which produces :Lineage-through-time curves for the five models we used. The strict clock model appears to imply a quite different diversity trajectory, in particular, it predicts more species appeared earlier.The effect of the remaining models appears to depend on time: the UCE and epochal models predict a smaller number of species in the earlier part of the history; the epochal model predicts the largest number of lineages at the end of the Pennsylvanian; and the UCE model predicts more recent divergence times near the present.Multidimensional Scaling PlotsThe lineage-through-time curves lose some information, both because we just examined the posterior average number of lineages (at least in the above example; in principle we can also plot the LTT credible intervals), and because it obscured tree topology and branch lengths.We can use multidimensional scaling (MDS) of tree-distance metrics to compare the tree topologies and branch lengths inferred under these models (see Hillis et al. (2005), Huang et al. (2016)).This involves computing a “distance” between each pair of trees within and between the posterior distributions of trees for each model.MDS then projects these pairwise distances into a lower dimensional—and therefore easier to visualize—representation of tree space.Two convenient distance metrics are the Robinson-Foulds distance (Robinson and Foulds 1981), which measures the topological distance between two trees, and the Kühner-Felsenstein distance (Kühner and Felsenstein 1994), which incorporates both topology and branch lengths.We’ll be using the R package phangorn (Schliep 2011) to compute the distances, and RevGadgets to create the plots.We’ll start by reading in the data (you don’t have to repeat this step if you’ve already read in the trees to make LTT plots, above):# read the samplesstrict_samples  &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_strict_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")UCLN_samples    &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")UCE_samples     &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_UCE_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")epochal_samples &lt;- readTrees(\"output_MCMC/div_epochal_foss_epochal_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_Mk_MCMC_run_01/tree.trees\", tree_name = \"timetree\")F81Mix_samples  &lt;- readTrees(\"output_MCMC/div_constant_foss_constant_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_F81Mix_MCMC_run_01/tree.trees\", tree_name = \"timetree\")Again, we combine the tree samples into a single named list:# combine the samples into one listcombined_samples &lt;- list(strict  = strict_samples[[1]],                         UCLN    = UCLN_samples[[1]],                         UCE     = UCE_samples[[1]],                         epochal = epochal_samples[[1]],                         F81Mix  = F81Mix_samples[[1]])Now we call the processMDS function with the argument type = \"RF\" to compute RF distances:# make the RF MDS plotsRF_MDS &lt;- processMDS(combined_samples, n = 100, type = \"RF\")The argument n determines how many trees to use from each posterior distribution.In this case, we’re using 100 trees from each of five analyses, so there will be a total of 500 trees.Keep in mind that we have to compute the distance for each pair of trees, so the total number of distances grows quickly as n increases.Large values of n will provide better representations of tree space, but will also take a potentially very long time to compute!Be wary of increasing n too much.We then create the MDS plot:# plot the RF MDSRF_plot &lt;- plotMDS(RF_MDS)# save the plotpdf(\"figures/mds_RF.pdf\")print(RF_plot)dev.off()which produces Multidimensional scaling of tree space sampled by different models using the Robinson-Foulds distance.The RF metric measures the topological distance between pairs of trees, so this MDS plot represents how different inferred tree topologies are among different models.The strict clock model in particular appears to be sampling tree topologies in only a subset of the tree space explored by other models.We can produce an MDS plot of Kühner-Felsenstein distance likewise:# make the KF MDS plotsKF_MDS &lt;- processMDS(combined_samples, n = 100, type = \"KF\")# plot the KF MDSKF_plot &lt;- plotMDS(KF_MDS)# save the plotpdf(\"figures/mds_KF.pdf\")print(KF_plot)dev.off()which produces Multidimensional scaling of tree space sampled by different models using the Kühner-Felsenstein distance.The KF metric incorporates both tree topology and branch lengths.Because this MDS plot looks very similar to the MDS plot for RF distances (; note that the directions of the axes are arbitrary, so rotations don’t matter), we might conclude that the main differences between these models are in the inferred tree topologies.(Of course, we don’t expect this to be a general result, just a feature of the example dataset!)Comparing Model Fit with Bayes FactorsIf we assess model sensitivity and determine that our estimates depend on the choice of model (as they often do), we will naturally want to ask: Which of the models best describes my dataset? or which of the estimates should I trust the most/report in my results?.We can use Bayes factors to compare the relative fit of different models to our data, which allows us to decide which results are the most trustworthy.The Bayes factor represents how well one model, $M_0$, fits the data relative to an alternative model, $M_1$.The Bayes factor between models $M_0$ and $M_1$ ($\\text{BF}_{01}$) is calculated as:\\[\\begin{equation*}\\text{BF}_{01} = \\frac{ P(X \\mid M_0) }{ P(X \\mid M_1) }\\end{equation*}\\]where $P(X \\mid M_i)$ is the marginal likelihood of model $M_i$.A Bayes factor greater than 1 indicates support for model $M_0$, while a Bayes factor between 0 and 1 indicates support for model $M_1$.We often report \\(2 \\ln \\text{BF}_{01}\\) (twice the Bayes factor on the log scale), in which case values greater than 0 indicate support for model $M_0$ and less than 0 indicate support for model $M_1$.The nice thing about the log scale is that it is symmetrical around 0: \\(2\\ln\\text{BF}_{01} = 5\\) represents the same amount of support for $M_0$ as \\(2\\ln\\text{BF}_{01} = -5\\) represents for $M_1$.The marginal likelihood for a given model (the denominator of Bayes’ theorem) is the probability of the data (the likelihood) averaged over all possible parameter values in proportion to their prior probability.Because it is the average probability of the data, Bayes factors intuitively represent our preference for models that have a higher average probability of producing the data.Also, because the must integrate over each prior distribution, increasing the number of parameters will tend to reduce the marginal likelihoods unless the additional parameters improve the fit of the model; that is, Bayes factors provide a natural way of penalizing additional parameters.For more details of Bayes factors and how to interpret them, please see the Model Selection tutorial.Estimating the Marginal Likelihood with Power-Posterior AnalysisThe only difference between estimating the posterior distribution and estimating the marginal likelihood is that we must run a variant of Markov-chain Monte Carlo called power-posterior analysis.This involves running a set of $k$ MCMC runs, each of which experiences a “distorted” version of the posterior distribution.Each run is sometimes called a “stone” or a “cat” (category).This distortion is represented by a parameter, $\\beta$, which is used to “heat” the likelihood function:\\[\\begin{equation*}P_\\beta(\\theta \\mid X) = \\frac{ P(X \\mid \\theta)^\\beta P(\\theta)}{ P_\\beta(X) }\\end{equation*}\\](Because we raise the likelihood function to a power, this is called a power posterior analysis.)We run a set of Markov chains with $\\beta$ ranging from 0 (the prior) to 1 (the posterior), and use the resulting samples of the likelihoods sampled by each run to estimate the marginal likelihood using either a “Path-Sampling” estimator or a “Stepping-Stone” estimator.In theory, these two estimates are the same, but they can be different if we don’t use enough stones or if we don’t sample enough from each stone.See the Model Selection tutorial for more details about how this algorithm works.Practically, rather than using modules/analysis/MCMC.Rev, we need to set up a new analysis type for the power-posterior analysis.(This analysis script is already provided in modules/analysis/PP.Rev.)To run a power-posterior analysis, we first have to decide on some settings, in particular: how frequently to write a sample of the chain to a file, how many stones to use, and how many generations to run per stone:# analysis settingsprintgen = 2nstones  = 30ngen     = 1000The number of generations is per stone, so the total number of generations is the number of stones times the number of generations!The quality of the marginal-likelihood estimate will depend on the number of generations; we number of stones and generations we use is small because our dataset is relatively small, but you will probably need to use more to get accurate estimates for larger datasets.We then create a screen monitor to give us a progress bar:# the monitorsmonitors = VectorMonitors()monitors.append( mnScreen(printgen = printgen) )(Note that we aren’t creating other monitors here, like an mnModel or mnFile monitor to keep track of parameter samples. Because the chains in a power-posterior analysis experience a distorted posterior distribution, the sampled parameters are not a valid approximation of the posterior distribution, except when $\\beta = 1$.If you want to estimate the posterior distribution, you should use the MCMC.Rev analysis module.)As with MCMC, we define a model object:# the modelmymodel = model(timetree)Next, we define our power-posterior analysis object.This function takes the number of stones (the cats) argument, as well as the filename for the samples per stone.# make the analysismymcmc = powerPosterior(mymodel, monitors, moves, filename = output_filename + \"/stones/pp.log\", cats = nstones - 1, sampleFreq = printgen)(This function automatically decides where to place the $\\beta$ values for each stone, but advanced users may want to control where the stones go using the powers or alpha arguments, describes in the documentation. The placement of stones can have some affect on the accuracy of marginal-likelihood estimates, but the default values are usually pretty good.)We then run the power-posterior analysis.# run the analysismymcmc.run(generations = ngen)After the analysis finishes, we read in the samples to compute both the path-sampling and stepping-stone estimates of the marginal likelihood.If these estimates are different, it indicates that we did not use enough stones and/or did not sample enough generations per stone!We print each estimate to screen, but also write them into a file named ml.txt for later reference.# compute the path-sampling estimate of the marginal likelihoodps = pathSampler(file = output_filename + \"/stones/pp.log\", powerColumnName = \"power\", likelihoodColumnName = \"likelihood\")ps_ml = ps.marginal()\"Path-sampling estimate of ML: \" + ps_ml# compute the stepping-stone estimate of the marginal likelihoodss = steppingStoneSampler(file = output_filename + \"/stones/pp.log\", powerColumnName = \"power\", likelihoodColumnName = \"likelihood\")ss_ml = ss.marginal()\"Stepping-stone sampling estimate of ML: \" + ps_ml# write the estimates to filewrite(ps_ml, ss_ml, filename = output_filename + \"/ml.txt\")⚠ NOTE: The values reported by these marginal likelihood estimates are in fact the log marginal likelihoods! If you want to compute the Bayes factor between two models, plug these log marginal likelihoods into the following equation:\\[\\begin{equation*}2 \\ln \\text{BF}_{01} = 2 \\times \\left[ \\text{log-marginal-likelihood of }M_0 - \\text{log-marginal-likelihood of }M_1 \\right]\\end{equation*}\\]That’s it!The rest of the model files do not need to change, because the power-posterior analysis does not involve modifications to the model itself.Exercise 2: Comparing among modelsNow that we’ve written a PP.Rev analysis script for doing a power-posterior analysis, we want to compare the fit of our models.Prepare a header file for each of the four models in the tables below (a subset of the models we used above for the MCMC analyses).(We also provide the relevant header files in headers/PowerPosterior.Note that these model comparisons are not exhaustive, and you might want to consider more combinations of models to pinpoint which parts of the model are affecting model fit.)Use these header files to estimate the marginal likelihood for each model.For each model, assess whether the path-sampling and and stepping-stone estimates are similar.            Model      Path-Sampling      Stepping-Stone-Sampling                  strict_Mk                            UCLN_Mk                            UCE_Mk                            UCLN_F81Mix                    Marginal likelihoods for total-evidence models.Now, compare the relative fit by computing the Bayes factor between each pair of models.Which is the favored model?            Model      strict_Mk      UCLN_Mk      UCE_Mk      UCLN_F81Mix      epochal_Mk                  strict_Mk                                                 UCLN_Mk                                                 UCE_Mk                                                 UCLN_F81Mix                                         Bayes factors between total-evidence models.Assessing Model Adequacy with Posterior-Predictive SimulationIn addition to comparing the relative fit of competing models using Bayes factors, we may wish to assess whether a given model (perhaps the best fit model) provides an adequate description of the true process that give rise to our data, sometimes called “model adequacy”.This may be particularly important for morphological data, since we may be especially skeptical that our models of morphological evolution can provide a realistic description of the process of morphological evolution.We will use posterior-predictive simulation to assess model adequacy, as described in the P^3 tutorial.The basic idea of posterior-predictive simulation is to ask: if we simulate data from our model, does that data resemble our observed data (in some quantifiable way)?A model that is adequate will be able to simulate datasets that resemble our observed dataset, while an inadequate model will simulate datasets that do not resemble our observed data.We capture the notion of “resemblance” by computing a statistic for a dataset, and looking at the distribution of that statistic computed over simulated datasets compared to the same statistic computed on our observed dataset.To simulate a single dataset from our model, we take one sample from our posterior distribution, and forward simulate a character dataset given the parameters of that sample from the posterior.We repeat this procedure many times (e.g., one per posterior sample) to generate a posterior-predictive distribution of simulated datasets, then compute our statistic for each simulated dataset.We can compute compute a posterior-predictive p-value as the fraction of simulated statistics that are greater than the observed statistic:\\[\\begin{equation*}P = \\frac{1}{n} \\sum_{i=1}^n T(X^\\text{sim}_i) &gt; T(X^\\text{obs})\\end{equation*}\\]where $n$ is the number of simulated datasets, $T(X)$ is our test statistic, $X^\\text{\\sim}_i$ is the $i^\\text{th}$ simulated dataset, and $X^\\text{obs}$ is the observed dataset.Note that this is the same as subtracting the simulated and observed statistics, and then computing the fraction of these differences that is larger than zero:\\[\\begin{equation*}P = \\frac{1}{n} \\sum_{i=1}^n \\left[ T(X^\\text{sim}_i) - T(X^\\text{obs}) \\right] &gt; 0\\end{equation*}\\]A p-value greater than \\(1 - \\alpha \\div 2\\) or less than \\(\\alpha \\div 2\\) indicates model inadequacy at the critical value of \\(\\alpha\\).For example, a p value greater than 0.975 or less than 0.025 indicates inadequacy at the $\\alpha = 0.05$ level.There are many statistics one could consider using, and which statistics are best at diagnosing inadequacy is discussion.In this tutorial, we’ll recreate the statistics we used in (May et al. 2021): the total parsimony score among discrete morphological characters (intended to characterize whether the model adequately describes overall rates of morphological evolution), and the variance in parsimony scores among characters (intended to characterize whether the model adequately describes how the process varies among characters).Simulating the Posterior-Predictive DatasetsSimilar to the power-posterior analysis we’ll create a new posterior-predictive analysis script (called modules/analysis/PPS.Rev) for simulating morphological character datasets from our posterior distribution.Let’s look at this analysis line-by-line.The first step of the posterior-predictive analysis is a fairly standard MCMC run, as we implemented in modules/analysis/MCMC.Rev.As usual, we first decide how many runs to do, how many generations to run the MCMC for, etc.# analysis settingsnruns    = 1printgen = 10nburnin  = 2000ngen     = 20000We then specify our model monitors; in this case, we’re including an additional monitor, mnStochasticVariable, that keeps track of all the model parameters in one file to be used to generate the posterior simulations.# the monitorsmonitors = VectorMonitors()monitors.append( mnScreen(printgen = printgen) )monitors.append( mnModel(filename = output_filename + \"params.log\", printgen = printgen, exclude = [\"F\"]) )monitors.append( mnFile(filename  = output_filename + \"tree.trees\", printgen = printgen, timetree) )monitors.append( mnFile(filename  = output_filename + \"extant_tree.trees\", printgen = printgen, extant_tree) )monitors.append( mnStochasticVariable(filename = output_filename + \"stoch.var\", printgen = printgen) )Next, we make our model and MCMC analysis, then run the chain and create a summary tree.We’ll hide this code behind a fold because we’ve already seen in before in our MCMC analyses.Running the MCMC analysis# the modelmymodel = model(timetree)# make the analysismymcmc = mcmc(mymodel, monitors, moves, nruns = nruns)# run the burninif (nburnin &gt; 0 ) {  mymcmc.burnin(generations = nburnin, tuningInterval = 100)  mymcmc.operatorSummary()}# run the analysismymcmc.run(generations = ngen)# make the summary treesif ( nruns == 1 ) {  # just make summary trees for the one run  full_trees = readTreeTrace(output_filename + \"tree.trees\", \"clock\")  mccTree(full_trees, output_filename + \"MCC_tree.tre\")  extant_trees = readTreeTrace(output_filename + \"extant_tree.trees\", \"clock\")  mccTree(extant_trees, output_filename + \"MCC_extant_tree.tre\")} else {  # make a combined summary tree  full_trees = readTreeTrace(output_filename + \"tree.trees\", \"clock\")  mccTree(full_trees, output_filename + \"MCC_tree.tre\")  extant_trees = readTreeTrace(output_filename + \"extant_tree.trees\", \"clock\")  mccTree(extant_trees, output_filename + \"MCC_extant_tree.tre\")  # and run-specific summary trees  for(i in 1:nruns) {    full_trees = readTreeTrace(output_filename + \"tree_run_\" + i + \".trees\", \"clock\")    mccTree(full_trees, output_filename + \"MCC_tree_run_\" + i + \".tre\")    extant_trees = readTreeTrace(output_filename + \"extant_tree_run_\" + i + \".trees\", \"clock\")    mccTree(extant_trees, output_filename + \"MCC_extant_tree_run_\" + i + \".tre\")  }}Now we read in the posterior samples and simulate our posterior-predictive datasets:# read in the posterior samplestrace = readStochasticVariableTrace(output_filename + \"stoch.var\")# setup the PPS simulationspps = posteriorPredictiveSimulation(mymodel, directory = output_filename + \"/simulations\", trace)# run the PPS simulationspps.run()(Note that we’re reading in the stochastic variable trace created by mnStochasticVariable.)The pps.run() command will generate one simulated dataset per MCMC sample, and write them in the /simulations subdirectory.We repeat this procedure for each model under consideration, though in principle we can do model adequacy with a single model!We provide the analysis headers in headers/PPS—these are the same models we have used in our previous sections.Summarizing Posterior-Predictive SimulationsNow that we’ve simulated our datasets for each model, it’s time to move to R to compute our statistics and compute posterior-predictive p-values.(This code is provided in the script posterior_summary/PPS.R)We begin by loading the required packages:library(RevGadgets)source(\"posterior_summary/utils.R\")Next, we read in our observed morphological dataset.# read the observed datadata_file &lt;- \"data/morpho.nex\"data      &lt;- readMorphoData(data_file)Now, we specify the output directories for each of our posterior-predictive analyses.# specify the output directory for each modeloutput_strict_Mk   &lt;- \"output_PPS/div_constant_foss_constant_moleclock_strict_moleQ_HKY_morphclock_linked_morphQ_Mk_PPS_run_01/\"output_UCLN_Mk     &lt;- \"output_PPS/div_constant_foss_constant_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_Mk_PPS_run_01/\"outout_UCE_Mk      &lt;- \"output_PPS/div_constant_foss_constant_moleclock_UCE_moleQ_HKY_morphclock_linked_morphQ_Mk_PPS_run_01/\"output_UCLN_F81Mix &lt;- \"output_PPS/div_constant_foss_constant_moleclock_UCLN_moleQ_HKY_morphclock_linked_morphQ_F81Mix_PPS_run_01/\"Next, we read in the simulated morphological datasets with readMorphoPPS:# read the output filessamples_strict_Mk   &lt;- readMorphoPPS(output_strict_Mk)samples_UCLN_Mk     &lt;- readMorphoPPS(output_UCLN_Mk)samples_UCE_Mk      &lt;- readMorphoPPS(outout_UCE_Mk)samples_UCLN_F81Mix &lt;- readMorphoPPS(output_UCLN_F81Mix)Now, we compute the posterior-predictive statistics with processMorphoPPS:# compute the statisticsstats_strict_Mk   &lt;- processMorphoPPS(data, samples_strict_Mk)stats_UCLN_Mk     &lt;- processMorphoPPS(data, samples_UCLN_Mk)stats_UCE_Mk      &lt;- processMorphoPPS(data, samples_UCE_Mk)stats_UCLN_F81Mix &lt;- processMorphoPPS(data, samples_UCLN_F81Mix)By default, this function will compute the parsimony-sum and parsimony-variance statistics, as described in May et al. (2021).You can also make your own user-defined statistics.However, this may take a bit of work, which you may only want to do if you’re a fairly advanced R user.Advanced: User-Defined Test StatisticsThere is a hidden statistics argument to the processMorphoPPS function.This argument accepts a list with elements that are either names of default statistics (which can be \"Parsimony Sum\" or \"Parsimony Variance\"), or functions.You can provide any user defined function which has arguments tree, observed_data, and simulated_data.The tree argument should expect to receive the tree that was used to simulate a particular dataset; the observed_data object expects a matrix object with named rows for species, and numeric values for character stats in columns.For example, we can look at the internal parsimony_sum statistic to see how the function should work.This function translates the provided datasets to the phangorn format phyDat, then computes the parsimony scores using the phangorn function parsimony.It then computes the statistic as the difference in parsimony scores between the simulated and observed datasets.parsimony_sum &lt;- function(tree, observed_data, simulated_data) {  # get the state space  all_chars &lt;- as.vector(observed_data)  all_chars &lt;- all_chars[all_chars %in% c(\"?\",\"-\") == FALSE]  states    &lt;- sort(unique(all_chars))  # transform to phyDat for phangorn  observed_phydat  &lt;- phyDat(observed_data, type = \"USER\", levels = states)  simulated_phydat &lt;- phyDat(simulated_data, type = \"USER\", levels = states)  # compute the parsimony scores for the observed and simulated data  observed_statistic  &lt;- sum(parsimony(tree, observed_phydat, site=\"site\"))  simulated_statistic &lt;- sum(parsimony(tree, simulated_phydat, site=\"site\"))  # compute the statistic  statistic &lt;- simulated_statistic - observed_statistic  return(statistic)}This function should serve as a template for any new statistics you choose to implement.We’ve now computed all of our posterior-predictive statistics.It’s time to plot the posterior-predictive distributions and p-values!There are several ways to do this, but one easy way is to plot the posterior-predictive distributions as boxplots and annotate them with p-values using boxplotPostPredStats:pdf(\"figures/pps.pdf\", height = 10)print(boxplotPostPredStats(combined_stats))dev.off()which produces .For this example dataset and these models, all of the models appear to adequately describe the process of morphological evolution!However, we should not expect this to be a general result: different datasets will behave differently, and the example dataset we are using is quite small and may not provide enough information to diagnose inadequacy.Posterior-predictive distributions of parsimony-sum and parsimony-variance statistics under each model.  Since we’ve subtracted the observed statistic from the simulated statistic, the posterior-predictive p-value is the fraction of statistics that are greater than 0.  In this case, all of the models appear to be adequate at the $\\alpha = 0.05$ level: all of the distributions overlap with 0, and none of the p-values are less than 0.025 or greater than 0.975.ConclusionThis concludes the total-evidence-dating workflow tutorial!We’ve shown you how to specify complex analyses in a generic way, that hopefully you can extend and modify to accommodate your own dataset.We’ve also provided some examples of how to assess model sensitivity—the degree to which phylogenetic divergence-time estimates are sensitive to modeling choices—and how to compare models on both relative (Bayes factor) and absolute (posterior-predictive simulation) scales.We encourage you to try this workflow out with your own dataset, and let us know how it goes!For a more complete set of models and analyses, you can check out the supplemental archive of our Marattiales study—which explored a larger number of models and model combinations than we presented here—at our GitHub supplemental repository.",
        "url": "/tutorials/ted_workflow/",
        "index": "false"
      }
      ,
    
      "tutorials-geosse": {
        "title": "Geographic state-dependent speciation-extinction (GeoSSE) model",
        "content": "GeoSSE modelThe geographic state-dependent speciation-extinction (or GeoSSE) model is phylogenetic model of biogeographic change (Goldberg et al. 2011). GeoSSE allows species to diversify through four main event classes: dispersal, extinction, within-region speciation, and between-region speciation. GeoSSE models are frequently used to test biogeographic hypotheses that concern relationships between these event rates and different regions. For example, is speciation faster on an island than on a mainland for a clade of ferns? Or, is dispersal faster into or out of the Andes for a clade of lizards?Model overviewIn the GeoSSE model, lineage “states” represent possible geographic ranges, comprised of one or more discrete regions. For example, in a two-region scenario, there are three possible ranges: A, B, and AB. Lineages split and transition among these states according to four core processes: within-region speciation, local extinction (extirpation), between-region speciation, and dispersal (). Within- and between-region speciation are cladogenetic processes that create new phylogenetic lineages, which may inherit ranges that differ from the ancestral species. Extinction and dispersal are anagenetic processes, occurring along the branches of an evolutionary tree. Within-region speciation and extinction happen inside a single region, whereas between-region speciation and dispersal involve two or more regions ().An example tree showing GeoSSE event types: within-region speciation (w), extinction (e), between-region speciation (b), and dispersal (d).Here is a summary of the four event types:  Extinction causes a species to lose one region from its range. Only when a species loses the final region from its range, does the entire species go entirely extinct as a lineage. A widespread lineage can only go extinct by losing each of its regions individually until one remains, then losing that last region. For example, a species with range AB might go extinct in region A, leaving it with range B. If it experiences no dispersal events (below) and suffers extinction in its last region B, the species is completely extinct.  Dispersal causes a species to expand its range into a new region. The rate of range expansion is the sum of pairwise dispersal rates from each starting region into the new region. For example, a species with range A that disperses into region B afterwards has range AB.  Within-region speciation: One daughter lineage inherits the entire ancestral range (which may be one or more regions), while the other daughter inherits a single region from the ancestral range. For example, the ancestral species has range AB and its two daughter species have ranges AB and A.  Between-region speciation: The range of a widespread ancestral species in two or more regions is subdivided and inherited by two new daughter lineages. Between-region speciation rates are always symmetric (separation between A and B is the same as separation between B and A). For example, the widespread ancestor with range AB splits and give rise to daughters with ranges A and B.The standard GeoSSE model does not allow for other kinds of evolutionary events. For example, an ancestor with a widespread range (of two or more regions) cannot produce daughters that both possess the entire ancestral range (a widespread sympatry scenario). In addition, GeoSSE only allows for a single event to occur within an instant of time.                   Within region      Between region                  Anagenetic      Extinction (e)      Dispersal (d)              Cladogenetic      Within-region speciation (w)      Between-region speciation (b)      The four GeoSSE event classes.The GeoSSE model allows each region or region pair to possess its own rate for each process. For example, the within-region speciation rate for region A may not necessary equal the within-region speciation rate for region B. Similarly, the dispersal rate from A to B does not necessarily equal the dispersal rate from B to A. When constructing the GeoSSE model, each rate will be represented with its own parameter. We will represent these rates with the following vectors and matrices: $r_w$ for the vector of within-region speciation rates, $r_e$ for  the vector of extinction rates, $r_b$ for the matrix of between-region speciation rates, and $r_d$ for the matrix of dispersal rates.Transition diagram for the GeoSSE model with two regions, based on Figure 1 from (Goldberg et al. 2011). Anagenetic processes are represented with dashed arrows, while cladogenetic processes are represented with solid arrows.As the name suggests, GeoSSE is a member of a broader class of methods that include state-dependent diversification – that is, the discrete character state of a lineage may impact its rates of speciation, extinction, and state transition. These models are known as SSE models. Other examples of SSE models include BiSSE (binary state speciation and extinction model) and ClaSSE (cladogenetic state change speciation and extinction model). For more information about how these methods jointly model character evolution and the birth-death process, see the associated tutorials. The GeoSSE model is a special case of the ClaSSE model that is structured and parameterized for biogeographical scenarios.This tutorial gives a step-by-step explanation of how to perform a GeoSSE analysis in RevBayes. We will model the evolution and biogeography of the Hawaiian Kadua using two regions: old islands, and young islands. For more information on Kadua and the Hawaiian islands, you can visit the intro tutorial for the Kadua series.NOTE: Although this tutorial is written for a two-region biogeographic analysis, it is designed to be applicable to analyses involving more regions. In general, we anticipate it should perform well for as many as eight regions (255 distinct ranges) or more with additional optimizations. However, for the purposes of this tutorial, we group the Hawaiian islands into two categories so that we can easily enumerate all of the model rates.Setup  Important version info!  Note: This tutorial currently requires specific versions of RevBayes and TensorPhylo to run properly (see linked branches and commits).  We recommend that you complete the tutorial using a PhyloDocker container, which is pre-configured with the above versions of RevBayes and TensorPhylo. Instructions to install and use PhyloDocker are here: link.Running a GeoSSE analysis in RevBayes requires two important data files: a file representing the time-calibrated phylogeny and a biogeographic data matrix describing the ranges for each species. In this tutorial, kadua.tre is a time-calibrated phylogeny of Kadua. kadua_range_n2.nex assigns ranges to each species for a two-region system: an “old islands” region and a “young islands” region. For each species (row) and region (column), the file reports if the species is present (1) or absent (0) in that region.If you prefer to run a single script instead of entering each command manually, the RevBayes script called geosse.Rev contains all of the commands that are used in the tutorial. The data and script can be found in the Data files and scripts box in the left sidebar of the tutorial page. Somewhere on your computer, you should create a directory (folder) for this tutorial. This is the main directory for the tutorial, and you will run all of your commands from here. Inside the tutorial directory, you should create a scripts directory. This is the directory where you put the geosse.Rev script. Then, you should create a data directory inside the tutorial directory, and download the two datafiles to this directory.GeoSSE in RevBayesGetting startedAfter starting up RevBayes from within your main tutorial directory, you can load the TensorPhylo plugin. You will need to know where you downloaded the plugin. For example, if you cloned the TensorPhylo directory into your home directory at ~/tensorphylo, you would use the following command to load the plugin:loadPlugin(\"TensorPhylo\", \"~/tensorphylo/build/installer/lib\")Note that if you’re using the PhyloDocker image, then the Tensorphylo plugin is installed in /.plugins, where RevBayes is able to find it without including a filepath:loadPlugin(\"TensorPhylo\")We also want to tell RevBayes where to find our data (and where to save our output later). If you have set up your tutorial directory in a different way than suggested, you will need to modify the filepaths.# FILESYSTEMfp          = \"./\"dat_fp      = fp + \"data/\"out_fp      = fp + \"output/\"bg_fn       = dat_fp + \"kadua_range_n2.nex\"phy_fn      = dat_fp + \"kadua.tre\"lbl_fn      = dat_fp + \"kadua_range_label_n2.nex\"DataNext, we will read in the data. Let’s start with the phylogenetic tree.phy &lt;- readTrees(phy_fn)[1]In order to set up our analysis, we will want to know some information about this tree: the root age, the taxa and their names, and the number of taxa.tree_height &lt;- phy.rootAge()taxa = phy.taxa()num_taxa = taxa.size()We also want to read in the range data.bg_01 = readDiscreteCharacterData(bg_fn)We want to get some information about this range data: how many regions there are, how many ranges can be constructed from these regions, and how many region pairs there are.num_regions = bg_01.nchar()num_ranges = abs(2^num_regions - 1)num_pairs = num_regions^2 - num_regionsFinally, we want to format the range data to be used in a GeoSSE analysis. This will take the binary range data and output integer states. Note that the integers used to represent ranges are first sorted by range size, then sorted by range patterns given each size-class, following general format of the table in the Introduction to Phylogenetic Models of Discrete Biogeography tutorial.bg_dat = formatDiscreteCharacterData(bg_01, format=\"GeoSSE\", numStates=num_ranges)The range assignments for this exercise with two regions are:            Range      Vector      State                  A      10      1              B      01      2              AB      11      3      Species ranges as region-sets, presence-absence vectors, and numerical states for two-region system.If you are interested in learning how to set up the GeoSSE rates manually without using the formatDiscreteCharacterData function, or if you want to further customize the model (ie. GeoHiSSE), the ClaSSE tutorial gives an example of hand-coded rates.Model setupIn the GeoSSE model, there are four processes: within-region speciation, extinction, between-region speciation, and dispersal. For each process, each distinct event is assigned its own rate that depends on the involved regions or region pairs. This will result in two rate vectors r_w and r_e with lengths equal to the number of regions, and two square rate matrices r_b and r_d with a number of entries equal to the number of region pairs. We will construct the event rates by multiplying the region- or pair-specific relative rate parameters in m_x for each event class $x \\in { w, e, b, d}$ against the appropriate base rate parameter rho_x to produce the absolute rates r_x. All rho_x parameters will be drawn from the exponential distribution dnExp(1). We will use Dirichlet distributions to generate relative rates.We will set up within-region speciation rates first.rho_w ~ dnExp(1)m_w_simplex ~ dnDirichlet(rep(1,num_regions))m_w := m_w_simplex * num_regionsr_w := rho_w * m_wTo obtain our vector of relative rates, m_w, we first create the simplex m_w_simplex, which is a vector containing num_regions random values that will be estimated, where each value is between 0 and 1 and all values sum to 1. The Dirichlet(1) distribution assigns equal probability to any combination of values in the simplex, making it a “flat prior”. Setting the alpha value to be large sets higher prior probability on relative rates being similar to one another. We design the model in this way so that users can better control how relative rates of within-region speciation are distributed among regions. We then multiply m_w_simplex by num_regions to produce the mean relative rate value of 1 for any region represented in the resulting relative rate vector, m_w. Lastly, we multiply these relative rates by the absolute scaling factor, rho_w, to obtain our vector of absolute rates, r_w.Extinction rates are set up similarly. The same general logic applies as before. However, these rates are applied only to extinction and not to within-region speciation.rho_e ~ dnExp(1)m_e_simplex ~ dnDirichlet(rep(1,num_regions))m_e := m_e_simplex * num_regionsr_e := rho_e * m_eFrom these extinction rates (which are actually single-region extinction rates), we will set up lineage-level extinction rates for each possible range in the state space. In the GeoSSE model, lineage-level extincion events occur when a species goes globally extinct (i.e. it loses the last region from its range). Therefore, we will assign all multi-region ranges an extinction rate of 0, and we will assign all single-region ranges an extinction rate equal to the local extirpation rate. Note, ranges are numbered such that indices 1, 2, through num_regions correspond to ranges that respectively contain only region 1, region 2, up through the last region in the system.for (i in 1:num_ranges) {    mu[i] &lt;- 0.0    if (i &lt;= num_regions) {        mu[i] := r_e[i]    }}For between-region speciation, we want to assign rates to each region pair. However, we want these rates to be symmetric, so we only want num_pairs/2 unique values. The same value will be assigned to m_b[i][j] as m_b[j][i]. We can do this by creating an initial simplex from a Dirichlet distribution, and assigning successive values from this simplex as we traverse the m_b matrix.rho_b ~ dnExp(1)m_b_simplex ~ dnDirichlet(rep(1,num_pairs/2))m_b_idx = 1for (i in 1:num_regions) {    m_b[i][i] &lt;- 0.0    for (j in 1:num_regions) {        if (i &lt; j) {            m_b[i][j] := abs(m_b_simplex[m_b_idx] * num_pairs)            m_b[j][i] := abs(m_b_simplex[m_b_idx] * num_pairs)            m_b_idx += 1        }        r_b[i][j] := rho_b * m_b[i][j]    }}For a two-region system with just one pair of regions, m_b_simplex will contain only a single relative-rate factor with the value of 1. That means the value of r_b for between-region speciation is driven entirely by rho_b. However, when the code is used for analyses with num_regions &gt; 2, the simplex m_b_simplex will contain different values. By allowing these values to vary, we allow widespread ranges to split into daughter ranges at different rates depending on the resulting split. These rates are computed using a range-split score (Landis et al. 2022), which we will not cover in this tutorial (RevBayes will complete this calculation automatically).Finally, for dispersal rates, we want to assign rates to each region pair. These rates are allowed to be asymmetric, so we need num_pairs unique values.rho_d ~ dnExp(1)m_d_simplex ~ dnDirichlet(rep(1,num_pairs))m_d_idx = 1for (i in 1:num_regions) {    m_d[i][i] &lt;- 0.0    for (j in 1:num_regions) {        if (i != j) {            m_d[i][j] := abs(m_d_simplex[m_d_idx++] * num_pairs)        }        r_d[i][j] := rho_d * m_d[i][j]    }}From these rates, we can use RevBayes functions to construct the rate matrices used by the analysis. The first is an anagenetic rate matrix, which gives rates of anagenetic processes. We are not restricting the number of regions that a species can live in at any given time, so we set the maxRangeSize equal to the number of regions. Settings maxRangeSize may be used to reduce the number of range patterns in the model, particularly when num_regions is large.Q_bg := fnBiogeographyRateMatrix(    dispersalRates=r_d,    extirpationRates=r_e,    maxRangeSize=num_regions)We also construct a cladogenetic event matrix, describing the absolute rates of different cladogenetic events. We are not restricting the sizes of ‘split’ subranges following between-region speciation, so we set the max_subrange_split_size equal to the number of regions. From this matrix, we can obtain the total speciation rates per state, as well as a cladogenetic probability matrix.clado_map := fnBiogeographyCladoEventsBD(    speciation_rates=[rho_w,rho_b],    within_region_features=m_w,    between_region_features=m_b,    max_range_size=num_regions,    max_subrange_split_size=num_regions)lambda := clado_map.getSpeciationRateSumPerState()omega := clado_map.getCladogeneticProbabilityMatrix()Lastly, we need to assign a probability distribution to range of the most recent common ancestor of all species, prior to the first speciation event. In this analysis, we will assume all ranges were equally likely for that ancestor.pi_base &lt;- rep(1,num_ranges)pi &lt;- simplex(pi_base)With all of the rates constructed, we can create a stochastic variable drawn from this GeoSSE model with state-dependent birth, death, and speciation processes. This establishes how the various processes interact to generate a tree with a topology, divergence times, and terminal taxon states (ranges). Then we can clamp the variable with the fixed tree and present-day range states, allowing us to infer model parameters based on our observed data.We will use the dnGLHBDSP distribution that interfaces with the Tenorsphylo plugin to model a Generalized Lineage Heterogeneous Birth Death Sampling Process, which is a generalized model (as the name suggests) that can express simpler models, such as GeoSSE models.Although most of the model variable arguments provided to construct the timetree variable have been described above, we pass a few additional arguments to define how we compute the model likelihood. First, we instruct the model to condition on the process evolving for tree_height units of time by setting condition=\"time\". Alternatively, condition can be used to condition on the process e.g. producing a given number of taxa or surviving until the present (producing &gt;2 taxa). Second, we permit Tensorphylo to use four processors with nProc=4 to speed up computation.timetree ~ dnGLHBDSP(    rootAge     = tree_height,    lambda      = lambda,    mu          = mu,    eta         = Q_bg,    omega       = omega,    pi          = pi,    rho         = 1.0,    condition   = \"time\",    taxa        = taxa,    nStates     = num_ranges,    nProc       = 4)timetree.clamp(phy)timetree.clampCharData(bg_dat)MCMCFor this analysis, we will perform a short MCMC of 1000 generations, with 100 generations of hyperparameter-tuning burnin. An analysis of this length may not achieve convergence, so these settings should only be used for testing purposes. You can alter this MCMC by changing the number of iterations, the length of the burnin period, or the move schedule. We will also set up the MCMC to record every 10 iterations.n_gen = 1000n_burn = n_gen/10printgen = 10We want MCMC to update all of the base rate rho parameters, as well as the relative rate Dirichlet simplexes. We will use a scaling move for the base rates, since they should always have positive values. These moves will each be performed once per iteration. Simplexes have a unique kind of move in RevBayes. Instead of performing one simplex move per generation, we will make the number of moves per iteration equal to the number of elements in the simplex.mvi = 1mv[mvi++] = mvScale(rho_w, weight=1)mv[mvi++] = mvScale(rho_e, weight=1)mv[mvi++] = mvScale(rho_b, weight=1)mv[mvi++] = mvScale(rho_d, weight=1)mv[mvi++] = mvSimplex(m_e_simplex, weight=m_e.size())mv[mvi++] = mvSimplex(m_w_simplex, weight=m_w.size())mv[mvi++] = mvSimplex(m_b_simplex, weight=m_b_simplex.size())mv[mvi++] = mvSimplex(m_d_simplex, weight=m_d_simplex.size())We also want MCMC to keep track of certain things while it runs. We want it to print some output to the screen so we can see how it is running (mnScreen). We also want it to save model parameters to a file (mnModel). Finally, if we want to use the output for ancestral state reconstruction, we want to save states and stochastic character mappings (mnJointConditionalAncestralStates and mnStochasticCharacterMap). All of the output files will be saved in the output directory so that it can be accessed later.mni = 1mn[mni++] = mnScreen(printgen=printgen)mn[mni++] = mnModel(printgen=printgen, filename=out_fp+\"model.txt\")mn[mni++] = mnJointConditionalAncestralState(glhbdsp=timetree, tree=timetree, printgen=printgen, filename=out_fp+\"states.txt\", withTips=true, withStartStates=true, type=\"NaturalNumbers\")mn[mni++] = mnStochasticCharacterMap(glhbdsp=timetree, printgen=printgen, filename=out_fp+\"stoch.txt\")Let’s also store information for how the integer-valued ranges (0, 1, 2) relate to the regional presence-absence representation of ranges (A=10, B=01, AB=11).write(\"index,range\\n\", file=lbl_fn)state_labels = Q_bg.getStateDescriptions()for (i in 1:state_labels.size()) {    write( (i-1), \",\", state_labels[i], \"\\n\", file=lbl_fn, append=true, separator=\"\")}Then we can start up the MCMC. It doesn’t matter which model parameter you use to initialize the model, so we will use m_w. RevBayes will find all the other parameters that are directly or indirectly connected to m_w and include them in the model as well. Then we create an MCMC object with the moves, monitors, and model, add burnin, and run the MCMC.mdl = model(m_w)ch = mcmc(mv, mn, mdl)ch.burnin(n_burn, tuningInterval=10)ch.run(n_gen)After the MCMC analysis has concluded, we can summarize the ancestral states we obtained, creating an ancestral state tree. This tree will be written to the file ase.tre. It may take a little while.f_burn = 0.2x_stoch = readAncestralStateTrace(file=out_fp+\"stoch.txt\")x_states = readAncestralStateTrace(file=out_fp+\"states.txt\")summarizeCharacterMaps(x_stoch,timetree,file=out_fp+\"events.tsv\",burnin=f_burn)state_tree = ancestralStateTree(tree=timetree,                   ancestral_state_trace_vector=x_states,                   include_start_states=true,                   file=out_fp+\"ase.tre\",                   summary_statistic=\"MAP\",                   reconstruction=\"marginal\",                   burnin=f_burn,                   nStates=3,                   site=1)writeNexus(state_tree,filename=out_fp+\"ase.tre\")OutputOne interesting thing we can do with the output of the GeoSSE analysis is plot ancestral states. This can be done using RevGadgets, an R packages that processes RevBayes output. You can use R to generate a tree with ancestral states by executing the following code in R. You can also examine the output files, like model.txt, to assess the relative rates of different processes occurring in different regions.NOTE: Your output may look slightly different than the output shown below. If you want to exactly replicate the results of the tutorial, you must set a seed at the beginning of the geosse.Rev script by adding the RevBayes command seed(1).library(RevGadgets)library(ggplot2)tree_file = \"./output/ase.tre\"output_file = \"./output/states.png\"states &lt;- processAncStates(tree_file, state_labels=c(\"0\"=\"Old\", \"1\"=\"Young\", \"2\"=\"Both\"))plotAncStatesMAP(t=states,                 node_size=2,                 node_size_as=NULL) +                 ggplot2::theme(legend.position=\"bottom\",                                legend.title=element_blank())ggsave(output_file, width = 9, height = 9)Ancestral state reconstruction of Kadua.GeoSSE with more regionsAs noted above, the script is written generally, so it can be applied to biogeographic systems with more than two regions. Consider re-running the script, but instead using the seven-region dataset from the FIG tutorial: link to kadua_range_n7.nex.How many species ranges are there in a three-region versus a seven-region system? How many parameters are there in a three-region versus a seven-region system? Which processes require the most new parameters as the number of regions increases? What problems do you expect to encounter running this GeoSSE script for an analysis with more regions?",
        "url": "/tutorials/geosse/",
        "index": "true"
      }
      ,
    
      "tutorials-mcmc-troubleshooting": {
        "title": "Debugging your Markov chain Monte Carlo (MCMC)",
        "content": "IMPORTANT This tutorial is currently a work in progress, meaning that some sections may be incomplete or lack details. Send feedback by opening an issue on the Github repository.OverviewMarkov Chain Monte Carlo is a common method for approximating the posterior distribution of parameters in a mathematical model. Despite how well-used the method is, many researchers have misunderstandings about how the method works. This leads to difficulties with diagnosing issues with MCMC analyses when they arise. For this lesson, we will not be performing MCMC analyses themselves. However, we will work with MCMC traces, which are logs of the values sampled for each parameter in our phylogenetic model. These are draws from the posterior. We can examine these logs to assess if our analysis has reached convergence, or stationarity.This tutorial closely follows the text of this paper, which describes strategies for troubleshooting MCMC. We assume that you have completed the introductory MCMC tutorials (Introduction to Markov chain Monte Carlo (MCMC) Sampling), and will not be covering the basic mechanics of the MCMC algorithm. This tutorial, instead, will focus on giving examples of issues that may impact the efficiency and convergence of your MCMC simulation, and will give strategies for solving this problems.Reading a TraceBefore we can understand if there are problems with our MCMC trace, we need to understand how to examine an MCMC for convergence. In this tutorial, we will mainly focus on fairly simple visual convergence diagnostics. But please note that this tutorial offers some more sophisticated techniques for asessing convergence. For this, we will use the software Tracer.Start by opening Tracer. You can load in MCMC traces, such as the two provided with this tutorial by clicking the + button (see ). Load in the two traces labeled ‘good’ provided with this tutorial. These are traces that will demonstrate convergence. We estimated these two traces from this tutorial Discrete morphology - Tree Inference. When you load these in, you will see both files, as well as a “combined” trace, showing the total sample contained within both. Note that the combined view will only include analyses that share the same parameters. If you are comparing models or different datasets, they may not combine.The Tracer interface, showing where to click to load in your MCMC trace file. No trace is loaded yet.Click on the trace button. This shows the parameter and its values (Y-Axis) and the number of generations (X-axis). If you look at each of these files, you will see that, for each parameter, the lines seem to jump around a central value. This is because this analysis has converged (). It has found stationarity, or the point at which increased sampling no longer affects the distribution of values estimated. If you select both traces, you will see that the two traces have both found stationarity at similar values.On the left hand side of the screen, we see a set of parameters along with their effective sample size (ESS). The ESS is the most commonly used convergence diagnostic in phylogenetics. The ESS is specific to a posterior sample and to a given parameter, and describes the number of uncorrelated (independent) samples that would be needed to approximate the posterior distribution of a parameter with similar precision to that posterior sample. It is usually defined as ESS $= N/\\tau$, in which $N$ is the number of generations and $\\tau$ is the autocorrelation time.Due to autocorrelation, the ESS is typically smaller than the number of steps in the MCMC chain, because the difference between two successive samples is usually quite small. If we were drawing completely independent samples, the difference between sample $i$ and sample $i+1$ could be quite large (i.e., an independent sample could be drawn from anywhere in parameter space, so a series of such samples may explore the different areas of that space more quickly than when done step by step by an MCMC chain). We can see that each of our ESS values is over 200, which is generally considered good.An MCMC trace with adequate MCMC generations. This trace is converged.For comparison, let’s look at some poor traces. First, let’s look at the traces labeled too_few (). These traces, as the name may imply have simply not bee run long enough. This trace was estimated from the same tutorial scripts and data as the first trace, however, we ran it for 1/10th the number of MCMC generations. Load in both of the traces. You will immediately notice that all the ESSes are in red, indicating this analysis has not made a sufficient number of draws to approximate the posterior distriuction. If you look at the trace, you will see that it is sampling around a central value, but not densely sampling around it. There is no clear directionality to the trace. This indicates that stationarity has been found, but more sampling needs to occur to confirm this. More generations of MCMC analysis is the solution to this problem.An MCMC trace with too few generations. More generations are needed for this to converge.In the next trace, we have used the same scripts, but are performing MCMC moves less frequently. Load in the two traces labeled few_moves. If you need a reminder on MCMC moves, please see this tutorial. This means that we are sampling fewer new values for important parameters. In this case, we are undersampling the alpha parameter the gamma distribution on Among-Character Rate Variation (ACRV). What you will observe is that in these traces is that the alpha parameter is poorly-sampled. The ESS is very low. ACRV is used to esitmate the rates_morpho parameters - these, correspondingly, have low ESS, as well. When we examine the trace, it is blocky-looking (). This is sometimes called a “skyline” or “Manhattan” shape. This is caused by the parameter being unmoved for several generations, as it is being moved too infrequently. The solution here is to increase the frequency of the move.An MCMC trace with too few moves. This is sometimes referred to as poor mixing - the move is not performed enough to adequately sample parameters.The final issue that we are going to diagnose is unreasonable starting values. This issue can be diagnosed by observing that the trace does not stabilize. Load in the files titled poor_start (). What you will observe, again, is that the values for the alpha parameter of ACRV has a very low ESS. What is different, however, is that the trace shows a steep climb from the starting value and does not reach stationarity. In this case, one would want to examine their starting values and priors on their parameters.This parameter is mixing, but has not reached stationarity, as evidenced by its vertical incline.Now that we have some simple looks at what these issues look like, we will now dive deeper into the causes of these problems and how they can be fixed.Selecting priorsThis part of the tutorial deals with the choice of prior distributions for continuous parameters. Note that demographic models (birth-death and coalescent models) are also technically prior distributions for the phylogeny itself, however they will be covered in a separate section.Available distributionsThe range of available distributions for priors is quite large, but we present here the most common examples. When deciding on a prior, the choice of the distribution will change the distribution of the probabilities within the range of plausible values, and determine whether we believe the true parameter value to be towards the lower end, the upper end, or anywhere in the range. The distribution chosen also determines whether the parameter value has a strict bound, i.e. a minimum or maximum that it cannot go beyond. Distributions with a so-called “long tail” allow parameter values to go increasingly high, but with increasingly low probability. The second factor in choosing a prior is the range of plausible values determined by the distribution, which will depend not only on the distribution chosen but also on the parameters chosen for the distribution. For distributions which have no hard bounds, we generally consider that the values between the 2.5% and 97.5% quantiles are “plausible” under the distribution. Values outside of this range generally have a very low prior probability.  Uniform distribution: a distribution which assigns an identical probability to all values between a lower and upper bound. This distribution is parametrized by the lower and upper values of the range. Itis appropriate for parameters with a (known) fixed range, but where the distribution of values within a range is not well known, for instance a sampling probability or an age range for a sample.    unif ~ dnUnif(lower = 37.0, upper = 55.0)        Exponential distribution: a distribution on positive values which has high probabilities for values close to 0, with a long tail. This distribution is parametrized by the rate $\\lambda$ of the distribution, where the mean is $1/\\lambda$. It is appropriate for values which are always positive but can be very low, such as the diversification rate.    exp ~ dnExponential(lambda = 10)        LogNormal distribution: a distribution on positive values where the majority of the weight is on values within a certain range, defined by the mean and standard deviation parameters. Both values close to 0 and high values have a low probability under this distribution. This distribution also has a long tail, but unlike the exponential distribution, it seeks to impose a minimum on the range of plausible values. It can thus be a better choice for parameters which are unlikely to be very close to 0, such as the clock rate.    lnorm ~ dnLognormal(mean = 0.0, sd = 0.5)        Gamma distribution: similar to the lognormal distribution, the majority of the weight is on positive values within a certain range, away from 0 or from high values. It can thus be applied to the same type of parameters. This distribution is parametrized by the shape and the rate. The mean of the distribution is given by $mean = shape / rate$ and the variance by $var = shape / rate^2$.    gamma ~ dnGamma(shape = 2.0, rate = 4.0)        Beta distribution: this distribution is restricted to values between 0 and 1, but can have a wide range of shapes depending on the parameters set. For instance, a beta distribution can put higher probability on lower values, higher values, or values in the middle of the range. So it is a good choice for parameters which are probabilities and for which we have some knowledge about the distribution, for instance a sampling probability that we know is likely between $0.001$ and $0.01$.    beta ~ dnBeta(alpha = 1.0, beta = 0.8)      Commonly used distributions in phylogenetics are generally restricted to positive values, as this corresponds to most biological parameters. However, there are also distributions available for parameters which can take negative values, such as the normal or uniform distributions.Finally, more complex distributions such as mixture distributions are available for parameters with a more complicated behaviour. A common situation is a scenario where a parameter can be either a fixed value (usually 0) or estimated under a distribution. For instance, imagine we are trying to detect whether a mass extinction happened in our particular dataset. We know that a mass extinction happened at a specific time, but we are not sure whether it actually impacted our focal clade. In this case we may set the extinction probability during the time of the supposed mass extinction to be either 0 (corresponding to no mass extinction in this clade) or Uniform[0.7, 0.95] (if a mass extinction did happen).me ~ dnReversibleJumpMixture(0, dnUnif(0.7, 0.95), 0.5)Evaluating the influence of the priorWe can evaluate the influence of the prior on estimates by running the same analysis under several different prior distributions. We have set up a toy analysis containing a phylogeny inferred from a morphological matrix under an FBD prior. We estimating the phylogeny, origin time and the parameters of the clock and substitution models. The full details of the inference can be found in the prior_influence.Rev file.We will try several distributions for the clock rate prior:  a uniform distribution    clock_morpho ~ dnUnif(0.0, 2.0)        an exponential distribution    clock_morpho ~ dnExponential(1.0)        a lognormal distribution    clock_morpho ~ dnLognormal(-0.18, 0.6)      Note that we have chosen distributions which all have the same mean $m = 1.0$, this is important to make sure that the differences we observe come from the shape of the prior distribution rather than simply from the fact that we set a different expectation for the value of the parameter. However, because of the differences in shape, the actual range of plausible values can be very different between distributions. We can check the actual ranges by using RevBayes to check the quantiles of each distribution.qUniform(p = 0.025, 0.0, 2.0)qUniform(p = 0.975, 0.0, 2.0)qexponential(p = 0.975, 1.0)qlognormal(p = 0.025, -0.18, 0.6)qlognormal(p = 0.975, -0.18, 0.6)Note that because of its shape, the exponential distribution does not really have a minimum for its plausible range, as values very close to 0 still have a high prior probability under this distribution. This gives us ranges of [0.05 ; 1.95] for the uniform distribution, [0.0 ; 3.69] for the exponential distribution, and [0.26 ; 2.71] for the lognormal distribution.Running the inference under these different prior distributions gives us the posterior distribution of the clock rate shown in . We can see that the estimated value changes depending on the choice of prior, as the median estimate is $0.45$ for the lognormal prior, $0.019$ for the uniform prior and $0.017$ for the exponential prior. In a Bayesian analysis, the final estimate is influenced both by the prior distribution, and by the signal provided by the data and the other priors (in particular the prior on origin time in this case). This explains why the estimate under the lognormal prior is so different: the data drives the estimate towards low values, but this prior has a much higher bound for plausible values than the other two distributions. On the other hand, the uniform and exponential distributions mostly differ by their shape in the upper part of the value range, which is not where our posterior is located, and so we do not see large differences between these two priors.Comparison of the posterior distribution of the clock rate under different priors, visualized into Tracer.Finally, we observe that changes in the prior distribution can have effects not only on the parameter which is directly impacted, but also on other correlated parameters. For instance, the prior on the clock rate also has an effect on the estimate for the $\\alpha$ parameter which controls among-site rate heterogeneity, as shown in .Comparison of the posterior distribution of the among-site rate heterogeneity parameter under different priors for the clock rate, visualized into Tracer.Note that in this example, we subsampled our original alignment to artificially obtain a matrix with very few characters, which increases the influence of the prior. In a real analysis, it is unlikely that we would use an alignment with so few sites. However, this scenario can also happen for instance due to partitioning: when an alignment is split into several partitions, the amount of information present in the alignment can be split very unevenly, such that some partitions contain almost no signal. If separate substitution and clock models are used for each partition, the estimates for the parameters corresponding to the low-information partition will be heavily influenced by the prior chosen.Common prior issuesVague priorsUsing very wide or very vague priors can appear tempting, for several reasons. First, sometimes only little information, or contradictory information, is available about the value of certain parameters. Second, setting vague priors lets us avoid some of the work needed to establish more narrow priors, e.g. checking the literature for previous estimates, or thinking deeply about the biological meaning of the parameters. Finally, vague priors provide little information to the inference, and so the final estimates will better reflect the signal coming from the data as opposed to the expectations we have placed on the results.However, this assumes that there is in fact signal in the data for the specified parameter, which may not always be the case. Some parameters can be difficult to estimate without proper priors, particularly when all samples come from the same point in time: for instance the ages of the phylogeny, the overall substitution rate, or the extinction or death rate. Even when there is enough information in the data to obtain precise estimates from the inference, setting vague priors will slow down the convergence of the chain, as the inference is provided with a much wider range of plausible values to explore.We can demonstrate this using our toy example from the previous section. The full details of the inference can be found in the wide_priors.Rev file. We will try two different distributions for the clock rate prior:  an exponential distribution with mean = 1 (i.e. rate = 1)    clock_morpho ~ dnExponential(1.0)        an exponential distribution with mean = 100 (i.e. rate = 0.01)    clock_morpho ~ dnExponential(0.01)      This time both distributions have the same shape, but one is much wider than the other, which we can easily check by looking again at the quantiles of the two distributions.qexponential(p = 0.975, 1.0)qexponential(p = 0.975, 0.01)This gives us a range of plausible values of [0.0 ; 3.69] for the narrow prior, and [0.0 ; 368.9] for the wide prior.Running the inference under both priors gives the results shown in . The median estimate for the clock rate is $0.018$ for the narrow prior and $0.02$ for the wide prior, so we see that setting a vague prior only had a small impact on the median estimate. However, the 95% HPD interval is much larger when we set a wide prior, which is expected since we provide less information to the inference, and so there is more uncertainty in the results. But another important thing to note is that the effective sample size (ESS) on the clock rate is almost twice as high when using a narrow prior (ESS = 834) as opposed to a wide prior (ESS = 477). The other parameters of the inference are also affected, and most show lower ESS values when using the wide prior. Thus we can see that using vague priors has a negative impact on the speed of convergence of the inference.Posterior distribution of the clock rate under a narrow prior (top) or a wide prior (bottom), visualized into Tracer.Interacting priorsOne challenge is specifying priors is that we usually expect that the prior used by the analysis will be the same as the prior that we have set. This is generally the case, unless the same component or parameter is influenced by several different priors. A common scenario where this can happen is in the case of node ages. Node ages have an implicit prior set on them by the prior used for the phylogeny itself. When using a demographic model such as a coalescent or birth-death process, node ages which are coherent with this model will have a higher probability. Similarly if a prior on branch lengths is used, then this will also impact plausible values for the node ages. But node ages can also have explicit priors set on them, for instance through the use of node calibrations defined directly by the user. Even nodes that are not calibrated will be influenced by the calibrations, since a node has to have a lower age than its ancestors and a higher age than its descendants. Thus the prior on a node age is potentially the result of the interaction of many different components.This interaction needs to be taken into account in order to correctly interpret the results of the analysis, as we will see in the following example. We use the same toy analysis from the previous examples, but this time we use a longer alignment and add node calibrations to our inference. The full details of the setup can be found in the age_calibrations.Rev script. We have defined a set of clades for our phylogeny:clade_ursus = clade(\"Ursus_arctos\",\"Ursus_maritimus\",\"Ursus_americanus\",\"Ursus_thibetanus\")clade_ursavus = clade(\"Ursavus_brevirhinus\", \"Ursavus_primaevus\")fbd_tree ~ dnConstrainedTopology(fbd_dist, constraints=v(clade_ursus, clade_ursavus))Each of these clades has an associated prior distribution on the age of their most recent common ancestor (MRCA):age_ursus := tmrca(fbd_tree, clade_ursus)age_ursus_prior ~ dnUnif(age_ursus - 28.5, age_ursus - 15.0)age_ursus_prior.setValue(0)age_ursavus := tmrca(fbd_tree, clade_ursavus)age_ursavus_prior ~ dnUnif(age_ursavus - 36.0, age_ursavus - 25.0)age_ursavus_prior.setValue(0)We also have a prior set on the origin time of the tree:origin_time ~ dnUnif(37.0, 55.0)When we run the analysis, we obtain the posterior distribution shown in  for the Ursavus clade that we have set. The first impression we might get from this result is that our inference strongly supports younger ages for this clade, because our original prior was a uniform distribution on the full range [25.0 ; 36.0] My, whereas the posterior has much higher densities for values in the first half of the range. However, this is not taking into account the interactions between priors, which may lead to an effective prior on the clade age which is not uniform at all.Posterior distribution of the age of the Ursavus clade, visualized into Tracer.We can check the effective priors for any inference by running the analysis under the prior, i.e. without taking the contributions of the data into account. There are two ways to achieve this:  we can use the .ignoreAllData() method on the model object to mark clamped nodes as ignored. CAREFUL: this will ignore all clamped nodes, including the node calibrations if they use clamping. If you want to retain clamped fossil calibrations, you can use the more selective method .ignoreData(...) to ignore some data but retain fossil calibration data.mymodel = model(variable)mymodel.ignoreAllData()  we can simply comment out all the components of the likelihood, which in the case of our analysis is only the dnPhyloCTMC component.Then we can compare the posterior distribution obtained from the complete inference to the effective prior obtained from the inference under the prior, shown in . Here we see that the effective prior for the age of the Ursavus clade is not uniform at all, but in fact also supports heavily the younger ages within the specified range. Indeed, the median of the posterior is $\\approx 26.1$My, while the median of the effective prior is $\\approx 27.8$My, so they are very close to each other. We can also see that the posterior distribution is different from the effective prior and shows support for a more narrow range of age values, so the data is also contributing to our posterior estimates. However, the support of the data for younger ages is much weaker than our initial impression, because the posterior is largely informed by our choice of priors.Distributions of the age of the Ursavus clade, visualized into Tracer. We can compare the posterior distribution obtained from a complete inference with data (blue) with the effective prior obtained from a run under the prior (green).Overall, this example illustrates why it is crucial to check the effective priors in case of unexpected interactions. Comparing the complete run and the run under the prior also allows us to evaluate how much support for our estimates is found in the data as opposed to our chosen prior, which will inform our interpretation of the results.Misspecified or conflicting priorsIn this section we see an example of what can happen when priors are misspecified, i.e. when they conflict with either the data or the other priors. We have set up a toy analysis containing a phylogeny under an FBD prior, and estimating the origin time, speciation and extinction rates. The full details of the setup can be found in the prior_conflict.Rev file, but we will focus on the priors used.The prior for the origin time is:origin_time ~ dnUnif(37.0, 55.0)The prior for the speciation rate is:speciation_rate ~ dnLognormal(2.0, 1.0)And finally the prior for the extinction rate is:extinction_rate ~ dnExponential(100)One issue with lognormal distributions is that it can be difficult to see from the parameter values alone what is the range of plausible values covered by the prior. As we have seen before, we can easily check the quantiles of the distribution using RevBayes:qLognormal(p = 0.025, 2.0, 1.0)qLognormal(p = 0.975, 2.0, 1.0)We see here that the prior expects the speciation rate to be in the interval of values [1.04 ; 52.5] events/My. However, our dataset contains only 18 species, we have set our origin time to be in the interval [37 ; 55]My and the mean of our prior on the extinction rate is set to $1/rate = 0.01$ events/My. Considering the other components of our analysis, the expectation on our speciation rate is thus unrealistically high.This is likely to affect our analysis in two important ways. First, the influence of the prior will orient the estimates of the speciation rate towards values which are higher than the values supported by the data, and this influence will need to be taken into account when interpreting the results. Second, because the prior on speciation rates is in conflict with the data and other components of the analysis, it is likely to make the likelihood surface more complex, and thus slow down the convergence.A more reasonable prior on speciation rate would expect lower values, for instance we can set:speciation_rate ~ dnLognormal(-3.0, 1.0)This distribution defines a range of plausible values in the interval [0.027 ; 0.35] events/My.The results using our original, very high prior versus the more reasonable alternative are shown in .Comparison of results obtained using the very high prior (top) or the alternative lower prior (bottom), visualized into Tracer.We can see that as expected, the estimated speciation rate is almost twice as high when using our higher prior. But we also see that the ESS for several parameters, particularly the origin time and the speciation rate, are higher when using the alternative prior, even though we ran our chain for the exact same number of generations in both cases. Thus convergence is achieved faster when using a prior which does not conflict with the data and/or the other components of the analysis.Moves/OperatorsIn order for the MCMC to work properly, every parameter that is supposed to be optimized (i.e., for which we are searching for the best estimate) has to be associated with a move or operator.The type of move and its settings determine how the parameter can be changed between MCMC steps, how much it can be changed by, and how often a change is attempted.If the move(s) for a parameter is missing, that parameter cannot be optimized, which will usually cause issues for the MCMC.To set up the code in a way to avoid missing operators is to specify them right after the priors for a parameter is set:moves = VectorMoves()All move functions in RevBayes start with the prefix mv.The exact arguments differ from move to move, but generally it includes three things:The name of the parameter the move should act on (x in the example of the sliding move), one or more arguments needed as settings for the move algorithm (e.g., the sliding window’s size parameter delta), and the move’s weight, which determines how often this move will be attempted on average during each MCMC step.?mvSlideOn top of these, moves can have the arguments tune (and tuneTarget).The former is a boolean (i.e., TRUE/FALSE) which defines whether the setting-parameters of the move (e.g., window size delta here) is allowed to be optimized during auto-tuning.We will revisit the tuning options further below.The whole set of moves in an analysis are collected in a moves vector (which can be defined upfront and be added to using .append()), so we can look at this vector before running to ensure it contains all the moves we want.movesWeightsAs mentioned, the weight of a move determines how often that move is attempted, meaning that parameters with higher weights are updated more often, which can be helpful for parameters that are harder to optimize or for which a larger parameter space needs to be covered to find the optima.For simpler parameters, lower weights are sufficient, to not make the MCMC inefficient by enforcing unnecessary move-attempts.Note that in RevBayes, where multiple moves can be performed between each iteration of the chain, the weight implies the average number of move attempts per iteration.In other implementations (e.g., MrBayes, BEAST2), where only one parameter changes per iteration, the weights imply the probabilty for this parameter to be chosen.Reconsidering weights may be a good idea if some parameters struggle more to converge than others (as can be gleaned from the ESS and traces).The weights of slower converging parameters can be increased to speed up the search for more optimal values, and those of more easily converging parameters may even be reduced to not slow down the overall convergence speed of the analysis.Example code to come soon...Step SizesWhen a proposal is made to move a parameter to a new value, this proposal dose not only depend on the nature of the move (e.g., slide, scale, …) but also the current settings of how far from the current value the new proposed one can be.This is called the step-size of the move, and is usually determined by some parameters of the move function (e.g., the window size delta of the slide move).Depending on the structure of the likelihood surface (and thus the moved parameter, the data, etc.), the appropriate step size can vary.If the step size is too small, this can mean the chain is only approaching the optimum value very slowly, thus making the analysis inefficient.If the step size is too large, its sampling may be too coarse to properly explore the likelihood, and/or it may overshoot the optimal values.Example code to come soon...Multiple Operators vs. AutotuningThe efficiency of moves is often measured in acceptance proportion – how many times when a move was proposed during the MCMC has it actually been accepted?When a move is only accepted very rarely, this may imply that it is not exploring the space in a sensible way (e.g., perhaps the step size is too large), and since the parameter is only updated rarely, it does not contribute much to the analysis.On the contrary, if a move is accepted too often, this could be because the step size is too small, and is slowly moving towards some local optimum.There are two commonly approached strategies for ensuring appropriate step sizes: using multiple moves or using autotuning.For ultiple moves, each parameter is assigned multiple moves of the same kind, covering a range of possible moving parameters.The desired effect of this is, that moves at different step sizes are available to the model, so that there is a good chance that an appropriate one can be suggested and accepted in different situations throughout the chain.Example code to come soon...Importanlty, the parameters of these moves should not be tuned during burn-in, since that would change their spacing (and likely equalize them).The other option is to make use of the auto-tuning feature.When the argument tune for a move is set to TRUE, the move’s parameters can be tuned automatically, either during the burn-in phase, the main run, or both.In this case, the acceptance ratio of the move is evaluated over a certain iterations-interval, and the move parameters are adjusted accordingly based on this past performance.For both .burnin() and .run(), the parameter tuningInterval determines after how many generations the parameters are tuned.While tuning the moves during burn-in is a good idea, we would advise caution for tuning during the main run, as it can lead to unintended behaviour.Accordingly, there is not default interval for .burnin(), but the default for .run() is 0.What is to be considered a ‘good’ acceptance ration can depend on the moves and the case, but common recommendations for desirable acceptance proportions range from around 0.2 to 0.4 – the default in RevBayes is 0.44.That value can be changed for each move individually using the argument tuneTarget.Example code to come soon...DiagnosticsAs suggested above, acceptance proportion is a common way to assess whether the MCMC is well tuned.In RevBayes, this can be checked by applying .operatorSummary() to the mcmc object once it is run.mcmc.operatorSummary()To be continued…Starting ValuesThe MCMC chain has to have a starting point from which to begin proposing new parameter values.Thus, it is ‘initialized’ with some random values (usually drawn from the priors).This makes sense, because the MCMC should be able to find the optimal parameter values from any starting point, and one would want to avoid introducing bias via the starting position (e.g. starting on top of a local optimum).However, in practice it is possible to start the chain at a ‘bad’ position, which will impact the analysis negatively.In the easiest cases, this can mean that we start at a point in parameter space that is very far away from the area where the optimum lies.This can result in the analysis runtime to be unnecessarily long (and tuning during those stretches of the chain can mis-tune the moves for when they enter the parameter space near the optima).In the trickier cases, this can mean that our starting point is at a combination of parameter values that are implausible or even conflicting, which makes it hard to calculate a likelihood, or may even crash the analysis altogether.When an analysis is likely to crash upon starting or to take a long time to get going, strategically setting some starting values to reasonable positions may be a simple way to fix it.In RevBayes, starting values can be set for any parameter after their prior distribution is specified, using the function .setValue().moves = VectorMoves()More example code to come soon...It may not be immediately clear what a ‘reasonable’ starting value for a parameter should be, and in more complex analyses it may be challenging to find out which of the many moving parts of the analysis are in conflict with one another.But revisiting what the parameters stand for, and how they relate to one another – perhaps by visualizing the chosen prior distributions and the model’s DAG – is usually a good way of getting a better sense for what might be a good way forward.A few strategies have become common:  Set the starting values to the mean value of the prior (assuming the prior is appropriate)  Set diversification rates to what would be expected given the number of species and age of the clade, e.g., using the Kendall-Moran estimate for the speciation rate ($\\lambda = ln(nTips/2) / rootAge$), and setting extinction to be an order of magnitude lower than that ($\\mu = \\lambda/10$).  Consider coordinating the starting values of parameters that are known to be dependent on each other, e.g., rates of evolution and branch lengths.  Instead of starting with a random topology, supply a starting tree that is reasonably closer to the expected result, by using the output of a faster method (e.g., maximum likelihood or neighbour-joining) or a hand-curated topology. But importantly, make sure the starting tree does not violate any of the additional settings, e.g., calibrated nodes have to be within the age-range they are being calbrated to, and taxa that are constrained to be monophyletic have to form clades in the starting tree too.But remember that we are not trying to come up with spot-on estimates for each parameter, just a value that seems reasonable enough for our data that it provides a good starting point. The primary goal is to avoid early crashes due to computational issues and auto-tuning issues, and to help speed up the analysis.In order to still try and avoid bias, it may be prudent to run several runs with different starting values (in the same logic as it is done with random starting points), and check whether they converge on the same values.Inference algorithmAn MCMC can get stuck on a local optimum, and if that happens for long enough, it could give the impression of converge, because the sampled parameter values would seem to have stabilised. There can be various causes for this to happen, but it seems particularly common when the likelihood surface is complex and many parameters are correlated. A way how this can manifest is the presence of ‘plateaus’ in the trace – parameters being stuck around the same value for a long time and then suddenly jumping to other values further away.Independent RunsA simple way to notice problems with local optima is to run several independent chains. If the resulting MCMC samples are different, this would indicate that the runs got stuck on different local optima.Example code to come soon...Note that the independent chains should be initialised at different (if possible random) starting values, to make it more likely that they would end up on different local optima.MCMCMCA more elaborate way of running the analysis on a complex likelihood surface is to use Metropolis-coupled MCMC (MCMCMC or MC$^3$). Instead of several independent (but with regards to their setup equivalent) chains, we make use of so-called ‘heated’ chains.For these additional chains, the posterior probability is raised to a power, which flattens the likelihood surface and thus lets them move faster and easier across it. This means they can explore areas in parameter space that the regular chain might not have gotten to.However, only the likelihood and parameter values of the regular chain are being recorded, but in certain intervals, its current likelihood is compared with that of the current position of the heated chains. If their likelihood is higher, the regular chain can switch places with them and continue sampling in that region of parameter space.Example code to come soon...This way, the heated chains can ‘scout’ the parameter space for different optima, while the regular chain can ‘jump’ to the highest optima that were found, instead of getting stuck at whichever it landed on first.ModelsIf there are still convergence problems after the inference setup, priors, and starting values have been checked, we have to consider whether the model is appropriate for our data. There are steps to ensure this that can be taken during setup already, but also tests and modifications that can be used afterwards, as detailed below.Helpful ToolsIt can seem hard at first to decide on appropriate models or model components, especially if it seems unclear what to base such decisions on. Luckily there exist a number of tools that can be used to make more informed decisions.  jModelTest: Standalone software that tests for best-fitting nucleotide substitution models (GitHub).  PartitionFinder: Standalone software that tests for best-fitting partitioning schemes and molecular evolution models (Website).  ClockstaR: R package that tests for appropriate relaxed clocks for molecular data (GitHub).  EvoPhylo: R package that tests for partitioning schemes for clock models for morphological data (CRAN).  PBDB tools: Lists various resources to help with assembling and processing fossil data, calculating diversification and sampling parameters, and other useful functions.Separating Joint InferencesWhen running inferences of different parameters jointly, e.g., when using different data types in the same analysis, or inferring topologies jointly with node ages, convergence issues can arise if those different inference parts are supporting different solutions. For example, if molecular and morphological data suggests different tree topologies. If the supported topologies differe too much, the joint model will be torn between them, being unable to converge (as each data source lowers the likelihood of the result the other one supports and vice versa). Similar issues can be cuased if e.g., different partitions strongly support different topologies.Example code to come soon...Model Assumptions and AdequacyBesides aforementioned conflicts, the model may also be much more fundamentally unsuitable for the data. It mighth thus be necessary to revisit the assumptions of the model and check whether the data meets those assumptions. Are sample sizes sufficient? Is variation distributed as expected? Are numbers of trait states/nucleotides severely skewed?Especially for models with many parts, the assumptions and their interactiosn can be difficult to judge. In such a case (but also in general), an option is to use model adequacy tests, which were specifically designed to find data-model-mismatches. Many use posterior predictive simulations (PPS), an approach which simulates data under the model (using the parameters inferred from the data) to simulate new datasets. The characteristics of these are then compared to those of the initial data, and if they differ significantly, that implies that the model is inadequate to describe the data.The concept of that approach is rather straightforward, and any model or analysis combining multiple models can be tested this way, as long as both inference and simulation are possible with the same model. However, the main difficulties lie in having sets of summary statistics which capture the relevant characteristics in which the model and the data need to match, and to interpret what particular data-model-mismatches the discrepant summary statistics translate into. Also, it should be noted that this approach can only be used if an MCMC chain can be run (to have a posterior to sample from to simulate new data sets), and not if it fails to run to begin with.PPS approaches have been developed for various kinds of phylogenetic models already, many of which could be used in RevBayes. The tutorial Introduction to Posterior Prediction demonstrates the concept and neccessary Rev code in more detail, while three associated tutorials exemplify the use of posterior prediction to test tree inference models using the P$^3$ approach.Reducing ComplexityAt last, we might have to consider reducing the complexity of our model. While modern approaches allow us to incorporate all kinds of processes and interactions, and to consider all kinds of patterns, such complex models will only work well if the data contains enough signal to inform this complexity. Similar to any statistical test requiring sufficient sample size to produce meaningful results, these models require certain structure and heterogeneity in the data to be able to infer parameters without too much uncertainty.For example, if we use a relaxed clock model (allowing for different rates of evolution along different branches) but only have one fossil node calibration or not much sequence variation, the MCMC may struggle to find much support for a particular configuration of branch rates. Since there is no information in the data to back up higher or lower rates in different parts of the tree the MCMC will find roughly similar support for many rate combinations, leading to estimates with large uncertainty - and in the process will take a long time to converge too. If we simplify the model by using a strict clock, we may instead converge on overall sensible rates within a reasonable time, despite missing out on some rate heterogeneity.Similarly, if we divide our sequence data into many partitions despite noth much evidence for different rate dynamics, we are thereby decreasing the sample size within each partition, perhaps making it harder to get confident rate inferences from each one than if we had concatenated some of the partitons instead.Our inferences can only provide answers as detailed and nuanced as our data can inform, meaning we have to adjust our modelling ambitions accordingly (or fill any gaps in our data).Data Quality IssuesWhile increasing the amount of data you use to generate a phylogenetic tree is generally a good thing, it can also make convergence more difficult. Increasing the amount of data increases the computational cost of estimating a phylogenetic tree, and therefore, can negatively impact inference performance. This is especially true in situations where the data are highly uncertain, have lots of missing data, or are in conflict with other sources of data. Below, we will examine a few common sources of MCMC difficulties that relate to data quality issues.ConflictConflict occurs when multiple sources of data disagree with one another about the topology, branch lengths or other parameters. Examples of this can include if two genes favor different topologies, or if morphological data and molecular data favor different trees. It is advisable to run different data sources independently to assess if this issue is at play in your dataset.SparsityIn general, assembling more data leads to more precise and more accurate inferences. Previous research has shown that total-evidence studies require $\\sim$300 morphological characters to obtain reliable estimates of tree topology and divergence times in extinct clades (missing reference).However, it is important to note that additional data is not necessarily better for the convergence of an MCMC inference. Adding more data comes with added computational costs, and thus can have a net-negative impact on the performance, especially if the added data is very uncertain or conflicts with the rest of the data or with the chosen models and priors. For instance, (missing reference) built phylogenies using either a complete alignment of nuclear markers, a supersparse matrix of $\\sim$300 genes with large amounts of missing data, or the combination of both. They found that trees obtained using the combined dataset did not significantly differ from the trees obtained using the complete alignment alone. One possible avenue for resolving convergence issues is thus to remove genes or partitions which contain low amounts of information. Software such as genesortR can be used to choose subsets of genetic datasets that provide solid phylogenetic information, thus elimiinating the need to have complete datasets.Similarly, while increasing taxon sampling has been shown to be important for phylogenetic accuracy (missing reference), increasing the number of extant or fossil samples in the tree leads to an exponential increase in the number of possible topologies. This increases compute time and decreases efficiency. For size-related convergence issues, one may choose a subset of their taxa to include. Many methods assume assume extant taxa are sampled uniformly at random; but in many cases, they are sampled sparsely by keeping only one living representative per genus or subclade.The diversified sampling scheme has been implemented in the FBD model (Zhang et al. 2016) to accommodate such a case.",
        "url": "/tutorials/mcmc_troubleshooting/",
        "index": "true"
      }
      ,
    
      "tutorials-fbd-range": {
        "title": "Macroevolutionary Analysis of Stratigraphic Range Data",
        "content": "OverviewThis tutorial demonstrates how to estimate speciation, extinction and sampling rates from stratigraphic range data in a Bayesian framework using the fossilized birth-death range model (Stadler et al. 2018).We begin with a brief description of three available models, which take different information from the fossil record as input in the , followed by a detailed example analysis in  demonstrating how to apply these models inRevBayes (Höhna et al. 2017) and use Markov chain Monte Carlo (MCMC) to estimate the posterior distribution of model parameters for a dataset of Mesozoic dinosaur occurrences downloaded from the Paleobiology Database.IntroductionSpeciation (or origination) and extinction rates are key parameters in macroevolution, and are useful for addressing a wide range of questions in evolutionary biology and paleontology.Knowledge of speciation and extinction rates can help tell us whether a clade was expanding or declining during different geological intervals and provide a starting point for linking diversification parameters to other environmental variables.Paleontological estimates of speciation and extinction rates are typically obtained from information about fossil sampling times, using a wide range of different methods.A given taxon may be represented by one or more fossil occurrences sampled from discrete geological intervals, and the duration between the first and last appearance is known as the stratigraphic range of the taxon.Of course, the true duration that a taxon existed for over geological time extends beyond the observed stratigraphic range, beginning and ending with the speciation and extinction times of the taxon.In many stratigraphic range datasets, the phylogenetic relationships between ranges may be unknown (e.g., because no morphological character data has been collected), however, it is reasonable to assume that an underlying phylogenetic brith-death process gave rise to the observed ranges.Furthermore, the distribution of stratigraphic range ages is informative about the underlying phylogenetic parameters, including the rates of speciation, extinction and fossil recovery.In this tutorial we will apply a fossilized birth-death range skyline model to infer these parameters during different geological intervals using stratigraphic ranges as input.The model is closely related to the fossilized birth-death (FBD) model (Stadler 2010) and the fossilized birth-death range (FBDR) model (Stadler et al. 2018) that can be used as a tree prior in a phylogenetic analysis incorporating character data, where the goal is to infer the topology and divergence times, along with the diversification and fossil recovery parameters (e.g., (Heath et al. 2014; Gavryushkina et al. 2017; Zhang et al. 2016)). For more information about these models see the tutorial Total-Evidence Analysis and the Fossilized Birth-Death Process.The model described in this tutorial is referred to as the FBDR Matrix model, since this model takes as input a matrix of stratigraphic range ages and no information about the underlying phylogenetic relationships (e.g., a character alignment or topological constraints).An important feature of any paleontological database is that we are typically missing an enormous amount of information, since sampling of the geological record is incomplete, highly non-uniform over time and space, and many extinct organisms are never preserved or sampled.A central advantage of applying the FBD (or related) models to the analysis of fossil data is that it explicitly incorporates incomplete species and fossil sampling, and can allow for variation in fossil recovery rates across different geological intervals.The fossilized birth-death range skyline modelModel assumming the total fossil count is knownThe fossilized birth-death range (FBDR) process described here provides a model for the distribution stratigraphic ranges, which can include extant samples, along with the total lineage duration times ().One possible realization of the fossilized birth-death range model.Sampled lineages are represented by solid black lines.Stratigraphic ranges are shown in purple. Red circles represent first and last appearances.Singletons (i.e., ranges known from a single fossil specimen) are represented by a single circle.Unsampled lineages are represented by dashed lines.We can use this model () to describe the probability of a set of ranges conditional on the birth-death parameters:$f[\\mathcal{D} \\mid \\bar\\lambda, \\bar\\mu, \\bar\\psi, \\rho]$, where$\\mathcal{D}$ denotes a summary of the fossil occurrence data and is described in more detail below.The birth-death parameters $\\lambda$ and $\\mu$ denote the speciation and extinction rates, respectively.The “fossilization rate” or “fossil recovery rate” is denoted $\\psi$ and is the rate at which fossils are sampled along lineages.The sampling probability parameter $\\rho$ represents the probability that an extant species is sampled.The FBDR skyline model allows rates to vary in a piece-wise manner across different geological intervals and we use $\\bar\\lambda$, $\\bar\\mu$ and $\\bar\\psi$ to denote the vector for each set of rates.The total number of intervals, $l$, and the length of each interval will vary depending on the available stratigraphic data (e.g., total number of fossils and/or ranges) and the biological questions of interest.A graphical model of the fossilized birth-death range model describing the generation of the stratigraphic ranges used in this tutorial. The parameters of the fossilized birth-death process are labeled in orange. The speciation,extinction and fossilization rates are stochastic nodes (circles) drawn from exponential distributions. The sampling probability is constant node (square) andequal to one. For more information on probabilistic graphical models and their notation, please see (Höhna et al. 2014).The data we observe are fossil occurrences and we use $n$ to denote the total number of sampled ranges.For a given stratigraphic range $i$, the first and last appearance are denoted $o_i$ and $y_i$, respectively.If the first and last appearance are the same (i.e., the species is a singleton), $o_i = y_i$.In addition, each lineage begins and ends at time $b_i$ and $d_i$, respectively.If a species is extant, $y_i = d_i = 0$, and if a species is only sampled at the present, $o_i = y_i = d_i = 0$.Note that for a given range, lineage duration begins at the point at which the range attaches in an incompletely sampled tree, which is not equivalent to the true origination time of the species represented by the range.In constrast, lineage duration ends at the extinction time of the speices.To account for uncertainty in the relationships among sampled ranges we marginalizeover all possible attachment points for each range (Heath et al. 2014).For range $i$, we define $\\gamma_i$, which is the number of co-existing lineages at time $b_i$.This allows us to account for topological uncertainty without explicitly sampling the underlying topology.The data summary for this first model, $\\mathcal{D} = ( \\{ k_{i,j}, b_i, d_i, o_i \\}_{i \\in 1…n, j \\in 1…l})$, includes per interval fossil counts \\(k_{i,j}\\), lineage duration times, $b_i$ and $d_i$, and first appearance time $o_i$ for each range.Typically, we do not have information about times $b_i$ and $d_i$, but we can sample these using Markov Chain Monte Carlo (MCMC), thus accounting for the uncertainty associated with these ages.Although the age of the last appearance is not included in our data summary and is not required to calculate the probability of observing our data, it is used to provide an upper limit (maximum age) for the extinction times $d_i$.Stratigraphic age uncertaintyIn most cases the age of a given occurrence will not be known precisely and instead will be known to within some stratigraphic interval.The length of this interval is highly variable but it is an important source of uncertainty in any phylogenetic analysis incorporating fossil data.In the model described above, only the age of the first appearance, $o_i$, is used in the posterior probability.We can potentially account for specimen age uncertainty in RevBayes by placing a hyperprior on the age of the first appearance, and sample the age of $o_i$ during MCMC (although this is not yet possible using this model).As noted above, the last appearance time, $y_i$, is only used to provide an upper (maximum) bound on the extinction time, $d_i$.Thus, the oldest possible age of the last appearance (i.e., the maximum stratigraphic age associated with the fossil) may be used to specify $y_i$. All other occurrences only contribute to the per-interal fossil count, $k_i$, and so need to be dated at this level of precision.Marginalising over the number of fossils within a stratigraphic rangeIn the above model, some knowledge about the total number of samples collected during each interval, $k_i$, is required. However, in some cases we may only know the age of the first and last appearance times ($o_i, y_i$), but not the number of occurrences sampled inbetween. In other cases, the number that $k_i$ represents may not be obvious. We can take care of this uncertainty by marginalizing over the total number of fossils between the first and last appearance times for each range. In this version of the model, per-interval fossil counts, $k_i$, are not required.Instead, we define ${\\kappa_i^\\prime}$, which denotes the total number of occurrences representing first and last appearances within each interval, and the data summary, $\\mathcal{D_r} = ( \\{ \\kappa_{i,j}^\\prime, b_i, d_i, o_i \\}_{i \\in 1…n, j \\in 1…l})$.Marginalising over the number of fossils within a stratigraphic intervalFor some datasets, the age and/or the number of fossil occurrences sampled during each interval may not be known very precisely.Instead we may only know whether a given range was sampled or not within a given interval, but not how many times it was sampled.We refer to this as presence-absence (1/0) sampling.To account for this type of data, we can marginalize over the total number of fossils within each interval, along with the age of first appearance, $o_i$.We use $\\kappa_{S_{i,j}}$ to indicate whether species $i$ was sampled during interval $j$ or not, such that $\\kappa_{S_{i,j}} = 1$ if $k_{i,j}&gt;0$ and $\\kappa_{S_{i,j}} = 0$ otherwise. Within each interval for which we record a species as having been sampled (i.e., $\\kappa_{S_{i,j}} = 1$) we also define $L_{S_{i,j}}$, which is the duration that the species exists within that interval. We denote the data summary, $\\mathcal{D_l} = (\\{ \\kappa_{S_{i,j}}, L_{S_i,j}, b_i, d_i \\}_{i \\in 1…n, j \\in 1…l})$.More details of the models described above are available here.ExerciseFor this exercise we will be estimating speciation, extinction and fossil recovery rates for a dataset of dinosaur fossil occurrences, under three variants of the FBDR Matrix model. The goal of this analysis is to examine broad diversification dynamics in this clade during the Mesozoic.PBDB occurrence DataDinosaur occurrences were downloaded from the Paleobiology Database.The models described in this tutorial can be computationally expensive, especially given a large number of intervals and ranges, so to ensure the analysis runs within a reasonable timeframe, available occurrence data has been subsampled.Specially, we took a random subsample of 116 species that were &gt;66 Ma, associated with &lt;5 Myr stratigraphic age uncertainty and identified at the species level.Fossil ages were also treated as known, taking the mid point between the minimum and maximum age associated with each specimen.Specimens were binned into four intervals of interest: the Early and Late Cretaceous, the Jurassic and the Triassic.Finally, we rescaled the timeline, such that the Cretaceous-Paleogene boundary = the present (i.e., 66 Ma = 0 Ma).This allows us to avoid estimating rates during any interval younger than Cretaceous, which is outwith our period of interest.The script used to generate this dataset is at the top of this page.Input filesOn your own computer, create a directory called RB_FBDRMatrix_Tutorial (or any name you like). When you execute RevBayes in this exercise, you will do so within this directory.If you are using a Unix-based operating system, we recommend that you add the RevBayes binary to your path.Download the data files dinosaur_ranges.tsv and dinosaur_fossil_counts.tsv, which you can find at the top of this page, and place them in this directory.These files contain the following data:      dinosaur_ranges.tsv: a tab-delimited table listing first and last appearance times for each range. Each row represents a seperate species. Note that for singletons the first and last appearance times will be the same. For extant ranges, including extant singletons, the last appearance time would be 0.0. Important: fossil ages have been rescaled such that 66 Ma = 0 Ma (see  for details).        dinosaur_fossil_counts.tsv: a tab-delimited table of fossil occurrence counts. Each row represents a seperate species and each column represents a seperate time interval, from youngest to oldest (that is, left to right). Each cell contains information about species sampling during each interval - this can be the absolute number of times a species was sampled during each interval (required for model 1) or a binary character indicating whether a species was sampled (= “1”) or not (= “0”) during each interval (required for model 3). This file is not required for model 2.  Setting up the analysisIn this exercise you will create a single Rev script for each of the three models described in : mcmc_FBDRMatrix_model1.Rev, mcmc_FBDRMatrix_model2.Rev and mcmc_FBDRMatrix_model3.Rev. Each file will contain all the commands needed to read the data and run the MCMC analysis.Loading the dataBegin the first Rev script (mcmc_FBDRMatrix_model1.Rev) by loading the stratigraphic ranges from the dinosaur_ranges.tsv using the readTaxonData function.taxa = readTaxonData(file = \"dinosaur_ranges.tsv\")This file contains the taxon names, along with the first and last appearance times.For the purposes of the tutorial, these times will be treated as known.Next, load the matrix of fossil counts from dinosaur_fossil_counts.tsv using the readDataDelimitedFile function.k &lt;- readDataDelimitedFile(file = \"dinosaur_fossil_counts.tsv\", header = true, rownames = true)Specifying interval agesIn this analysis, our period of interest ends at 66 Ma, which we will rescale to the present day (i.e., 66 Ma = 0 Ma).Create a vector called timeline for the maximum age of each interval, from youngest to oldest.RevBayes will assume that the age of the youngest interval = 0.timeline &lt;- v(100, 145, 201) - 66These ages represent the boundary between the Early Cretaceous, the Late Cretaceous, and the Jurassic.The oldest occurrence in our dataset is from the Triassic and we will treat the time prior to the Jurassic as a single interval (i.e., 201 Ma - infinity).Specifying the priors and moves on the FBDR model parametersThe FBDR model parameters speciation, extinction and fossil recovery rates ($\\bar\\lambda, \\bar\\mu, \\bar\\psi$) are our key parameters of interest.Here, we will use the same prior distributions for all rate parameters across all intervals and the same scale moves to sample values from the posterior distribution.Each rate during each interval is assumed to be drawn independently from a different exponential distribution.For each rate parameter, we will create an exponentially distributed stochastic node using the ~ operator and the dnExp function, with rate parameter = 10, which has an expected value (mean) of 0.1.To sample the parameters during MCMC you will assign three scaling moves (mvScale) to each stochastic node, each with a different tuning value (called lambda for mvScale).The tuning value determines the size of the proposed change and using multiple moves for a single parameter will improve the mixing of the MCMC.It may also be useful to keep track of the diversification ($\\lambda - \\mu$) and turnover ($\\mu/\\lambda$) parameters.Since these parameters can be expressed as a deterministic transformation of the speciation and extinction rates, we can also monitor these values (that is, keep track of them during MCMC, and print them to a file) by creating deterministic nodes for these parameters for each interval using the := operator.Before specifying the moves and priors on the model parameters, create a workspace variable called moves.This variable is a vector containing all of the MCMC moves used to propose new states for every stochastic node in the model graph.Similarly, we need to create a variable to hold all of our monitorsmoves    = VectorMoves()monitors = VectorMonitors()Next define a constant node representing the rate hyperparameter of the exponential prior distributions on your diversification parameters.alpha &lt;- 10Then, write a loop that specifys the priors and moves on the rates during each interval.for(i in 1:(timeline.size()+1)){    mu[i] ~ dnExp(alpha)    lambda[i] ~ dnExp(alpha)    psi[i] ~ dnExp(alpha)    div[i] := lambda[i] - mu[i]    turnover[i] := mu[i]/lambda[i]    moves.append( mvScale(mu[i], lambda = 0.01) )    moves.append( mvScale(mu[i], lambda = 0.1) )    moves.append( mvScale(mu[i], lambda = 1) )    moves.append( mvScale(lambda[i], lambda = 0.01) )    moves.append( mvScale(lambda[i], lambda = 0.1) )    moves.append( mvScale(lambda[i], lambda = 1) )    moves.append( mvScale(psi[i], lambda = 0.01) )    moves.append( mvScale(psi[i], lambda = 0.1) )    moves.append( mvScale(psi[i], lambda = 1) )}Note that this loop specifies parameters for an additional interval (timeline()+1).This is for the interval prior to the Jurassic.For this analysis we will fix the extant species sampling probability ($\\rho$) to zero to avoid having to make any assumptions about lineages that may have survived beyond the Cretaceous-Paleogene boundary.rho &lt;- 0    Because $\\rho$ is a constant node, we do not have to assign a move to this parameter.Specifying the FBDR Matrix modelAll three models described in the  can be specified using the same distribution function dnFBDRMatrix in RevBayes.The model used to calculate the likelihood will depend on the data and commands passed to this function.The dnFBDRMatrix function always takes as input the stratigraphic ranges (taxa), the FBDR model parameters (lambda, mu, psi, rho), and the vector of interval ages (timeline).The function can optionally take as input the matrix of per-interval fossil counts (k), used by model 1 and 3 only.The option binary is used to indicate whether the data in matrix k should be interpreted as absolute fossil counts (as in model 1) or presence/absence (1/0) data (as in model 3).If binary = FALSE the function will implement model 1 and interpret the data in k as absolute fossil counts, which is the default.If binary = TRUE the function will implement model 3 and use 1/0 data.If the matrix k contains cells with \\(k_{i,j} &gt; 1\\), the function will interpret this as $\\kappa_{i,j} = 1$.Model 1 requires both stratigraphic range ages and per-interval fossil counts.To use model 1 simply include the matrix k, along with the other model parameters and leave binary = FALSE (excluding this argument from the function call is equivalent to specifying the default option).bd ~ dnFBDRMatrix(taxa=taxa, lambda=lambda, mu=mu, psi=psi, rho=rho, timeline=timeline, k=k)Model 2 requires stratigraphic range ages only and no information about per-interval fossil counts.To use model 2 simply exclude the matrix k from the function call.bd ~ dnFBDRMatrix(taxa=taxa, lambda=lambda, mu=mu, psi=psi, rho=rho, timeline=timeline)Model 3 requires both stratigraphic range ages and per-interval 1/0 data, which can also be represented by k.To use model 3 simply include the matrix k, along with the other model parameters and specify binary = TRUE.bd ~ dnFBDRMatrix(taxa=taxa, lambda=lambda, mu=mu, psi=psi, rho=rho, timeline=timeline, k=k, binary=true)Add the options for model 1 to your script mcmc_FBDRMatrix_model1.Rev.# model 1bd ~ dnFBDRMatrix(taxa=taxa, lambda=lambda, mu=mu, psi=psi, rho=rho, timeline=timeline, k=k)Next specify scale moves to propose changes to the stratigraphic range start and end times (i.e., $b_i$ and $d_i$).moves.append( mvMatrixElementScale(bd, lambda = 0.01, weight=taxa.size())moves.append( mvMatrixElementScale(bd, lambda = 0.1, weight=taxa.size())moves.append( mvMatrixElementScale(bd, lambda = 1, weight=taxa.size())moves.append( mvMatrixElementSlide(bd, delta = 0.01, weight=taxa.size())moves.append( mvMatrixElementSlide(bd, delta = 0.1, weight=taxa.size())moves.append( mvMatrixElementSlide(bd, delta = 1, weight=taxa.size())Finally, specify a workspace model variable (mymodel) using the model function.mymodel = model(alpha)The object mymodel represents the entire graphical model and allows us to pass the model to the next set of functions specific to our MCMC analysis.Specifying monitors and setting up the MCMCNext, create monitors for the FBDR model parameters speciation, extinction and fossil recovery, along with diversification and turnover.monitors.append( mnScreen(lambda, mu, psi, div, turnover, printgen=100) )monitors.append( mnModel(filename=\"model1.log\", printgen=10) )The mnScreen monitor writes the parameters we specify to the screen every 100 MCMC generations.The mnFile monitor writes the parameters we specify to file every 10 MCMC generations.We can also add some additional monitors to generate output that can be used with the R package RevGadets.# monitors to print RevGagets inputmonitors.append( mnFile(filename=\"output/model1_speciation_rates.log\",lambda,printgen=10) )monitors.append( mnFile(filename=\"output/model1_speciation_times.log\",timeline,printgen=10) )monitors.append( mnFile(filename=\"output/model1_extinction_rates.log\",mu,printgen=10) )monitors.append( mnFile(filename=\"output/model1_extinction_times.log\",timeline,printgen=10) )monitors.append( mnFile(filename=\"output/model1_sampling_rates.log\",psi,printgen=10) )monitors.append( mnFile(filename=\"output/model1_sampling_times.log\",timeline,printgen=10) )To run the analysis we have to create a workspace variable that defines our MCMC run using the mcmc function. This function takes the three main analysis components as arguments and we set the move schedule to \"random\", meaning moves will be chosen at random during the analysis.mymcmc = mcmc(mymodel, moves, monitors, moveschedule=\"random\")Finally, we can execute our MCMC analysis and we will set the chain length to 30000 cycles.mymcmc.run(30000)q()Adding q() to the end of the script means the program will exit at the end of the MCMC run.You’re now ready to run the first analysis!Executing the analysisBegin by running the RevBayes executable. In Unix systems, type thefollowing in your terminal (if the RevBayes binary is in your path):rbProvided that you started RevBayes from the correct directory, you can then use the source function to feed RevBayes your script and run the analysis.source(\"mcmc_FBDRMatrix_model1.Rev\")This analysis will take around 10 minutes. While you’re waiting for this analysis to run, create the files you need to run the analysis using models 2 and 3: mcmc_FBDRMatrix_model2.Rev and mcmc_FBDRMatrix_model3.Rev.You can just copy and paste your exisiting script for model 1.Recall from Section  that to use the alternative models you only need to change the arguments passed to the distribution function dnFBDRMatrix.To use model 2 just remove the argument that passes the fossil count matrix to the function, k=k, since this model doesn’t use any information about the number of fossils sampled during different intervals.bd ~ dnFBDRMatrix(taxa=taxa, lambda=lambda, mu=mu, psi=psi, rho=rho, timeline=timeline)For this model, you also don’t need to read the maxtrix represented by k, so you can remove the line that reads this file using readDataDelimitedFile from the script.Make these modifications in mcmc_FBDRMatrix_model2.Rev, and when model 1 is done, open RevBayes and run the analysis as before.source(\"mcmc_FBDRMatrix_model2.Rev\")To use model 3, we still include the argument that passes the fossil count matrix to the function, k=k, and switch the argument binary=true to binary=false, since this model uses 1/0 sampling information, rather than absolute occurrence counts. The k matrix is still required for this model.bd ~ dnFBDRMatrix(taxa=taxa, lambda=lambda, mu=mu, psi=psi, rho=rho, timeline=timeline, k=k, binary=true)Make these modifications in mcmc_FBDRMatrix_model3.Rev, and when you’re ready, run the analysis as before.source(\"mcmc_FBDRMatrix_model3.Rev\")While you’re waiting for model 2 and 3 to run you can examine the output from model 1.Running this model under the priorTo run this model under the prior, we can’t use the conventional approach in RevBayes of passing the argument runUnderPrior=true to the mymcmc.run command.Instead, we will simply comment out the creation of the bd stochastic node and its associated moves, and instead create a workspace model variable by passing alpha to the model function.#bd ~ dnFBDRMatrix(taxa=taxa, lambda=lambda, mu=mu, psi=psi, rho=rho, timeline=timeline, k=k, binary=true)#moves.append( mvMatrixElementScale(bd, lambda = 0.01, weight=taxa.size())#moves.append( mvMatrixElementScale(bd, lambda = 0.1, weight=taxa.size())#moves.append( mvMatrixElementScale(bd, lambda = 1, weight=taxa.size())#moves.append( mvMatrixElementSlide(bd, delta = 0.01, weight=taxa.size())#moves.append( mvMatrixElementSlide(bd, delta = 0.1, weight=taxa.size())#moves.append( mvMatrixElementSlide(bd, delta = 1, weight=taxa.size())mymodel = model(alpha)This allows us to sample the prior distribution on our FBDR model parameters.Evaluating your resultsDuring the MCMC analysis RevBayes will output parameters of interest (defined using the monitors vector) to the screen and the user-specified output file (model1.log, model2.log, model3.log).TracerWe can examine the log files in the program Tracer.Once you open this program, you can open the log files using the “File &gt; Import Tracer File” option, navigate to the directory in which you ran the analysis (RB_FBDRMatrix_Tutorial) and select the relevant log file (e.g., model1.log).Or you can simply drag and drop the files into “Trace Files” (the empty white box on the upper left of the program).Take a look at the output obtained for model 1.The Estimates window. The left-hand window provides mean and ESS of the chain. The right-hand window visualizes the distribution of samples.RevBayes outputs the parameter estiamtes for each interval from youngest and to oldest.This is the same order they were specified in the vector (timeline) in , and also the order in which the intervals appear in the input file dinosaur_fossil_counts.tsv.In this analysis the youngest interval (the Cretaceous) is denoted 1 and the oldest interval is denoted 4.In Tracer, we can select multiple parameters simultaneously.If we select all the estimates  for mu, we can look at the 95% HPD intervals obtained for each interval.The Estimates window, showing 95% HPDs for multiple parameters (in this case extinction), highlighted on the lower left.The youngest interval appears on the lefthand side.We can also examine multiple log files simultaneously in Tracer and examine the overlap between estimates obtained using different models. If you load all three log files into Tracer and navigate to the “Marginal Prob Distribution” window at the top and at the bottom of the appliaction select the “Colour by &gt; Trace File” option, you can see the marginal posterior obtained for a given parameter using each model in different colours.The “Marginal Prob Distribution” window, showing estimates of the same parameter (in this case speciation during the youngest interval) obtained using different models and therefore output to different logfiles, highlighted on the upper left.You can also assess the mixing of the MCMC in Tracer, taking advantage of the “Trace” window. Note that the ESS values for some parameters are a little low. Ideally, we would run the MCMC for longer.Skyline plots using RWe can also examine and manipulate the log files using other software, such as R.At the top of the page you will find an R script skyline_plots_FBDRMatrix.R.This script contains commands that can be used to generate so-called skyline plots.These plots show the posterior rates estimated during each interval.Open R and set the working directory to the directory in which you performed the analyis, RB_FBDRMatrix_Tutorial.Run the commands in this script to produce a set plots for each model that you ran.Time series plots illustrating the rates obtained for $\\lambda, \\mu, \\psi$ between 66-252 Ma.  Discussion points            The estimates we obtained for speciation, extinction and sampling using different models are quite different. Why do you think this might be? What does this mean in terms of interpreting our results?              Note that the key FBDR model parameters - speication, extinction and fossil recovery rates ($\\lambda, \\mu, \\psi$ - appear to be elevated during interval 1 (i.e., the Cretaceous). Does this seem like a reasonable result? Are there any bio/geological reasons that we would expect to see this? How could we go about further testing this?      ",
        "url": "/tutorials/fbd_range/",
        "index": "false"
      }
      ,
    
      "tutorials-convergence": {
        "title": "Convergence assessment",
        "content": "OverviewThis tutorial covers convergence assessment of a Bayesian phylogenetic analysis using the R package Convenience.Convergence of an MCMC analysis is crucial to assure that the chain has sampled from the stationary distribution and that we have sufficiently many samples to approximate the posterior distribution. That is, the MCMC has explored the parameter space long enough to reach the true posterior distribution of the parameters and the values we are sampling belong to that distribution. Theory says that a chain that runs through an infinite time, will reach convergence. For our practical problem, we need to make a decision of when we have sampled enough to take a good estimate of the desired parameters. Here we will show how to test if we have enough samples or not.An ideal solution would be to analytically calculate the number of steps needed to reach convergence. This, however, has turned out to be unfeasible because the number of samples depends on the specific model, the specific moves applied within the MCMC simulation, and the given dataset.Since we lack a theoretical convergence assessment, what is broadly done in the MCMC field is analyze the output from the MCMC for lack of convergence.To do so, we have to keep in mind two aspects of an analysis that has reached convergence: precision and reproducibility. Precision means that if we run the chain longer, we do not change the estimates (e.g., the posterior mean estimate). While reproducibility means that if we run another independent chain, we get the same estimates.Precision can be evaluated by checking that we have sufficiently many samples because more samples lead to less variance in the estimates (e.g., the standard error of the mean). Reproducibility, on the other hand, can be evaluated by comparing independent chains run under the same model. Therefore, it’s recommended to run at least two or better four replicates when performing MCMC analyses.Another best practice is to remove the initial samples from the chain. Those initial iterations are called burn-in. By that we try to get rid of the samples that are not taken from the stationary distribution.One last concept we need to keep in mind is the Effective Sample Size (ESS), i.e., the number of independent samples generated by our MCMC sampler. The ESS takes into account the correlation between samples within a chain. Low ESS values represent high autocorrelation in the chain. If the autocorrelation is higher, then the uncertainty in our estimates is also higher.Criteria for Convergence using in ConvenienceNow that we learned about convergence, let’s take a look into the criteria in the Convenience package:The output of a phylogenetic analysis most commonly consists of two types of parameters:  Continuous parameters: the evolutionary model parameters, the tree length, clock rates, etc.;  Discrete parameters: the phylogenetic tree.To assess convergence for these parameters, the Convenience package evaluates:  The Effective Sample Size (ESS);  Comparison between windows of the same run;  Comparison between different runs.The comparison between windows of the same run works by dividing the full length of the run into 5 windows (subsets) and comparing the third and fifth window.This comparison is used to determine the size of the burn-in. A sufficient burn-in will lead to windows that sampled values from the same distribution. Finding the appropriate burn-in size is done automatically in the Convenience package. The package tests burn-in of 0, 10%, 20%, 30%, 40% and 50%. If the required burn-in is higher than 50% of the length of the MCMC, we recommend re-running the MCMC.In Figure  we can see a trace plot for the tree length from the example provided in this tutorial. The trace plot shows the sampled values over the iterations of the MCMC. The highlighted areas of the figure show the third and fifth window of the run.Trace plot of the tree length for our example analysis. The shaded areas show the third and fifth windows of the run.How do we compare windows and runs?For the continuous parameters, the comparison is made with the two-sample Kolmogorov-Smirnov (KS) test, a non-parametric statistical test for equality of probability distributions. Two samples will be equal when the KS value is below a given threshold. The KS value (D) is calculated:\\[{D}_{m,n} = \\max_{x} |{F_{1,m}(x) - G_{2,n}(x)}|\\]F(x) and G(x) are the empirical distribution functions for the samples with size m and n, respectively.The two samples will be drawn from different distributions, at level $\\alpha$, when:\\[{D}_{m,n} &gt; c(\\alpha) \\sqrt{\\frac{m + n}{m\\times n}}\\]with\\[c(\\alpha) = \\sqrt{-\\ln({\\frac{\\alpha}{2})\\times \\frac{1}{2}}}\\]The phylogenetic tree is evaluated regarding the bipartitions or splits. Therefore, the comparisons are made using the frequency of a given split between intervals of the same run or between different runs.Two example trees with tips A, B, C, D and the splits seen at each tree.ThresholdsThe current state of convergence assessment in Bayesian phylogenetics relies mainly on visual tools (e.g., Tracer) and ESS thresholds that have no clear theory to support them (Rambaut et al. 2018). The motivation for the Convenience package is to provide an easy-to-use framework with clear thresholds for each convergence criterion.We derive a minimum value for the ESS based on a normal distribution and the standard error of the mean (SEM).How much error in our estimate of the posterior mean should we find acceptable? Clearly, the mean estimate for a distribution with a large variance does not need to be as precise as the mean estimate for a distribution with a small variance. However, relative to the variance/spread of the distribution, what percentage is acceptable? We suggest to use a SEM smaller of 1% of the 95% probability interval of the distribution, which is equivalent to say that the allowed error of the mean is four times the standard deviation of the distribution. (Note that you can derive a different ESS value for any other threshold that you like.) From this SEM, we can derive the ESS with:\\[SEM = \\frac{\\sigma}{\\sqrt{ESS}}\\]\\[\\frac{\\sigma}{\\sqrt{ESS}} &lt; 1\\% \\times 4 \\times \\sigma\\]\\[ESS &gt; \\frac{1}{0.04^2}\\]\\[ESS &gt; 625\\]An ESS of 625 is therefore the default value for the convenience package.For the KS test, the threshold is the critical value for $\\alpha$ = 0.01 and the sample size is the calculated threshold for the ESS, 625.With these values, the threshold for the KS test is ${D}_{crit}$ = 0.0921.Split FrequenciesTo date, the most often test to assess convergence of split frequencies is the average standard deviation of split frequencies (ASDSF) (Nylander et al. 2008). The frequency of each split is computed for two separate MCMC runs and the difference between the two split frequency estimates is used. The ASDSF is problematic for two reasons: (1) for large trees with many splits that have posterior probabilities close to 0.0 or 1.0 will overwhelm the ASDSF and hence even a single split that is present in all samples in run 1 (thus a posterior probability of 1.0) and is never present in any sample in run 2 (thus a posterior probability of 0.0) might not be detected, and (2) the expected difference in split frequency depends on the true split frequency (see Figure ).The expected difference in split frequencies for ESS of 100, 200 and 625. The x-axis is the true value of the split frequency. The y-axis is the expected difference in split frequencies. The effect of increasing the ESS is the decrease of differences in frequency of sampled splits.Instead of the ASDSF we use the ESS of each split.We transform each split into a chain of absence and presence values; if the split was present in the i-th tree then we score the i-th value of the chain as a 1 and 0 otherwise. This sequence of absence and presence observations (0s and 1s) allows us to apply standard methods to compute ESS values and thus we can use the same ESS threshold of 625 as for our continuous parameters.With the ESS threshold for the splits, we can estimate the expected difference in splits frequencies (EDSF) and use the 95% quantile as a threshold for the split differencies. The expected difference ($ {E}[\\Delta^{sf}_{p}] $) between two samples is calculated as the ‘mean absolute difference’, with N as the ESS:\\[{E}[\\Delta^{sf}_{p}] = \\sum\\limits_{i=0}^N \\sum\\limits_{j=0}^N \\left(|\\frac{i}{N} - \\frac{j}{N}| \\times P_{binom}(i|N,p) \\times P_{binom}(j|N,p) \\right)\\]Summary provides an overview of the convergence assessment described before and implemented in the package Convenience.Overview of the workflow in the convergence assessment.ConvenienceInstallTo install Convenience, we need first to install the package devtools.In R, type the commands:  install.packages(\"devtools\") library(devtools) install_github(\"lfabreti/convenience\") library(convenience) FunctionsHere is a list of the functions the package uses to assess convergence:      checkConvergence: takes the output from a phylogenetic analysis and works through the convergence assessment pipeline. This function can take either a directory with all the output files from a single analysis or a list of files. The function has 3 arguments:          path: for when a path to a directory is provided      list_files: for when a list of files is provided      format: the software used for the phylogenetic analysis, current accepted formats are “revbayes”, “mb” for MrBayes, “beast”, “*beast”      control: calls the makeControl function            essContParam: calculates the ESS for the continuous parameters        essSplitFreq: calculates the ESS for the splits from the trees        ksTestContParam: calculates the KS test for the continuous parameters, for both the comparison between windows or runs        loadFiles: gets the MCMC output from a directory path or a list of files. This function uses the package RWTY (Warren et al. 2016) and returns a list of type rwty.chain        loadMulti: this function was modified from RWTY to include the option to pass a list of files to the function loadFiles        makeControl: a function to set the burnin size, the precision of the standard error of the mean and the continuous parameters to exclude from the assessment. Default values are burnin = 0, precision = 1%, namesToExclude = “br_lens, bl, Iteration, Likelihood, Posterior, Prior”        meanContParam: calculates the means of the continuous parameters        plotDiffSplits: plots the calculated difference in splits frequency        plotEssContinuous: plots the histogram of the ESS values for the continuous parameters        plotEssSplits: plots the histogram of the ESS values for the splits        plotKS: plots the histogram of the KS values for the combination of all runs. The MCMC must have at least 2 runs        plotKSPooled: plots the histogram of the KS values for the one-on-one comparison of runs. The MCMC must have at least 3 runs        printConvergenceDiag: a S3 method to print the class convenience.diag        printConvergenceTable: a S3 method to print the class convenience.table        printListFails: a S3 method to print the class list.fails        printTableContinuous: prints the means and the ESS of the continuous parameters, you can save the table to a csv file by passing a file name to the function        printTableSplits: prints the frequencies and ESS of the splits, you can save the table to a csv file by passing a file name to the function        readTrace: this function was modified from ‘RevGadgets’ to include the option to read only files with the continuous parameters, when the user has no tree files to assess convergence        removeBurnin: remove the initial statates from the MCMC output        splitFreq: calculates the difference in splits for the trees, for both the comparison between windows or runs  ExampleFirst, download the files listed as example output files on the top left of this page. Save them in a folder called output.These files are the output from a phylogenetic analysis performed with a dataset from bears. The nucleotide substitution model was GTR+$\\Gamma$+I and the MCMC was set to run 2 independent runs.The package also works if your analysis has only one run of the MCMC. But the part to compare runs will not be evaluated. Therefore, it is not possible to say that your MCMC result is reproducible. We strongly advise on running more than 1 run.Let’s run the checkConvergence function with our example output in a directory (this step may take a few minutes):  check_bears &lt;- checkConvergence(\"output/\") We can also list the names of the files:  check_bears &lt;- checkConvergence( list_files = c(\"bears_cytb_GTR_run_1.log\", \"bears_cytb_GTR_run_1.trees\", \"bears_cytb_GTR_run_2.log\", \"bears_cytb_GTR_run_2.trees\") ) To better understand what checkConvergence is doing, take a look at . The first step consists of the user setting up the precision in the standard error of the mean for our estimates. The thresholds for ESS, KS test and difference in split frequencies will be calculated based on the precision. Then the function reads in the MCMC output, from the files provided by the user, and computes the quantities used in the criteria for convergence. Afterwards, the function checks if the computed quantities are within the calculated thresholds. Finally, the function reports wheter the MCMC has converged or not, the computed quantities and, in case of non convergence, the parameters that failed to achieve the desired thresholds.Summary of the process in the checkConvergence function. SF is the abbreviation for split frequencies.Now, let’s see the output from checkConvergence by typing check_bears.The output message includes:      A message if convergence was achieved or not        The calculated burn-in        Lowest ESS for the splits and continuous parameters        Instructions to check further the output:    To check the calculated parameters for the continuous parameters type:    Means: output$continuous_parameters$means    ESS: output$continuous_parameters$ess    KS score: output$continuous_parameters$compare_runs   To check the calculated parameters for the splits type:    Frequencies of splits: output$tree_parameters$frequencies    ESS: output$tree_parameters$ess    Difference in frequencies: output$tree_parameters$compare_runs   To check the full summary message with splits and parameters excluded from the analysis type:    output$message_complete We can see that check_bears has 4 elements: message, message_complete, converged, continuous_parameters and tree_parameters.  message: a summary of the convergence assessment;  message_complete: the summary from message plus: (1) the splits excluded from the assessment for having frequency above 0.975 or below 0.025 (when applicable), (2) the continuous parameters excluded from the assessment for having no variation during the MCMC (when applicable);  converged: a boolean that has TRUE if the analysis converged and FALSE if it did not converge;  continuous_parameters: a list with means, ESS and KS-scores between runs;  tree_parameters: a list with frequencies, ESS and split frequencies between runs.In case the analysis has failed to converge, another element will be on the output list: failed with the parameters that failed the criteria for convergence.We can generate tables with general information for the continuous parameters the splits with the commands:  printTableContinuous(check_bears)                    means      ESS                  alpha      1.603954589      8921.945              er.1.      0.015576521      10705.048              er.2.      0.505904352      11570.794              er.3.      0.006423004      5683.824              er.4.      0.009572443      18078.730              er.5.      0.454927376      15992.299              er.6.      0.007596305      4368.023              p_inv      0.532339279      3306.411              pi.1.      0.293497210      10057.477              pi.2.      0.301726522      11016.263              pi.3.      0.135420687      9775.079              pi.4.      0.269355587      9885.677              sr.1.      0.216077996      10275.761              sr.2.      0.550663747      16053.336              sr.3.      1.015625752      11775.210              sr.4.      2.217632528      3842.496              TL      1.112491969      7677.455        printTableSplits(check_bears)                    frequencies      ESS                  Helarctos_malayanus Ursus_americanus      0.09      17127.93              Melursus_ursinus Ursus_arctos Ursus_maritimus      0.18      18074.54              Helarctos_malayanus Melursus_ursinus Ursus_americanus Ursus_thibetanus      0.32      15526.14              Helarctos_malayanus Ursus_americanus Ursus_arctos Ursus_maritimus Ursus_thibetanus      0.50      15731.22              Ursus_americanus Ursus_thibetanus      0.89      15269.03      Convenience also provides plot functions to facilitate showing that convergence has been achieved.The functions plotEssContinuous and plotEssSplits plot the histogram of the ESS values for the continuous parameters and  the splits, respectively.The function plotKS plots a histogram of the KS values for the continuous parameters.And the function plotDiffSplits yields a plot for the calculated difference in splits frequency.The plots generated with Convenience for summarizing and visualizing the results from the convergence assessment. The top-left figure shows the histogram of calculated ESS values for the model parameters (continuous parameters). The bottom-left figure shows the histogram of calculated ESS for the splits. In both histograms the grey dotted lines represents the threshold of 625. The bottom-right plot is the histogram of the Kolmogorov-Smirnov (KS) for the model parameters, the dotted grey line represents the threshold for the KS test. The bottom-left figure shows the observed difference is split frequencies in the green dots and the expected difference between split frequencies (EDSF) in the grey curve. For all plots, the grey area shows where the parameters should be for convergence to be achieved.We can see that, for all figures, all continuous parameters and splits have an ESS above the threshold. This is what we would have expected for a MCMC that has converged. The KS histogram shows that all KS values calculated for the continuous parameters are below the threshold. This means that the parameters are drawn from the same distribution for both runs of our analysis. Therefore, if convergence is achieved for a given analysis, the histogram for the KS values should be similiar to this plot.The calculated differences in split frequencies fall below the threshold curve in grey. When convergence is not achieved, some dots would be above the threshold curve.It is also possible to plot the histogram of the KS values for the comparisons one-on-one when the MCMC analysis have more than 2 runs.The function for this plot is plotKS.pooled. The following figure is an example of this plot.Histogram of the calculated KS values for the one-on-one comparison between runs. Each color represents a different comparison of runs, as shown in the legend. The x-axis is the KS values and the y-axis is the frequency. The red dotted line represents the threshold for the KS score.In this example, we observe again that all KS values are below the threshold. Which means that, for this criterion, convergence has been achieved.Now that we learned how to use the package and how to interpret the results, let’s practice with some exercises.Exercise 1Check for convergence in the output generated in the Nucleotide Substitution Models tutorial.Which analysis have converged?Convergence failureYou can see that the output is different when we have a failure in convergence. We have more information in the text output and 2 extra elements failed and failed_names.The element failed has a summary text of what failed in our analysis.The element failed_names has the specifics about the parameters that failed, like the name of the parameter or the split and if it failed in checking for the ESS, the comparison between windows or between runs. shows the failed message for the convergence assessment on the analysis from Exercise 1 with the GTR+$\\Gamma$+I nucleotide substitution model.The output from the command check_primates$failed showing a summary of the number of parameters that have failed a certain criterion.Exercise 2      Rerun the MCMC with the GTR+$\\Gamma$+I model from Nucleotide Substitution Models, but increase the number of iterations to 50000.        Check the new results for convergence.  Exercise 3  Check convergence for the output from the Estimating a Time-Calibrated Phylogeny of Fossil and Extant Taxa using Morphological Data tutorial.In this case we should check only the continuous parameters (log files). Because the trees sampled throughout the MCMC have different number of tips due to the fossil record.What to doWhen we face a convergence failure, there are a few options of what to do in our MCMC to overcome this problem. The suggestions here come from experience, rather than theoretical proofs.We can divide our MCMC that lack convergence by the number of parameters that failed:      Several parameters failing        One or few parameters failing  For the first case, we should adjust the MCMC to be more efficient. This can be done by increasing the weights on the moves of the parameters that failed, using other MCMC algorithms (such as adaptive MCMC or Metropolis-Coupled MCMC), increasing the number of iterations, etc. In the second case, we should increase the weights on the moves or even add more moves for the specific parameters that failed.",
        "url": "/tutorials/convergence/",
        "index": "true"
      }
      ,
    
      "tutorials-pomobalance": {
        "title": "Polymorphism-aware phylogenetic models with balancing selection",
        "content": "This tutorial is based on  Polymorphism-aware phylogenetic models so we recommend you to go through it first.Polymorphism-aware phylogenetic models with balancing selectionNB! Please note that the current version of the code has been tested in the development version of RevBayes built from the dev_PoMo_bs_master branch. PoMoBalance will be added to the main functionality in the next release.The polymorphism-aware phylogenetic models with balancing selection (PoMoBalance) is a natural extension of polymorphism-aware phylogenetic models (De Maio et al. 2013; De Maio et al. 2015; Schrempf et al. 2016; Borges et al. 2019; Borges et al. 2022; Borges et al. 2022) including all previous capabilities as well as detection of preferred allele frequencies and strength of balancing selection as shown in .PoMoBalance model is depicted as a Markov-chain model with boundary mutations and allelic selection ($a_i$ and $a_j$ represents alleles A, C, G and T; $a_ia_j$ represents combinations of alles without repetition).  The boundary (monomorphic) states $Na_i$ and $Na_j$ are shown as larger circles with N individuals carrying allele $a_i$ (blue circles) on the left-hand side and $a_j$ (orange circles) on the right-hand side. All the middle (polymorphic) states $(N-n)a_i$, $na_j$ are shown with smaller circles, each one sequentially carrying one less individual with a prevailing allele. In addition to the selection term, transition rates reflect balancing selection pull towards the state with preferred allele frequency $B_{a_ia_j}$ (dark red arrows) and no such pull (light red crossed arrows) if the transition happens against the preferred state.In  the transition rates from the monomorphic states are defined with mutation rates $\\mu_{a_ia_j}$ and $\\mu_{a_ja_i}$, while the transition rates from the polymorphic states are defined with \\(\\Phi_n^{a^{\\mp}_{i,j}}= \\frac{n(N-n)}{N}(1+\\sigma_{a_{i,j}})\\beta_{a_ia_j}^{\\frac{1}{2} [ |n-B_{a_ia_j}|-|n\\mp 1-B_{a_ia_j}| +1 ]}, \\label{equation1}\\tag{1}\\)where $1+\\sigma_{a_{i,j}}$ represents fitness of corresponding alleles, $B_{a_ia_j}$ is a preferred frequency and $\\beta_{a_ia_j}$ is a strength of balancing selection.PoMoBalance in addition to standard PoMos allows one to  disentangle genetic drift, mutational biases, directional selection and balancing selection;  detect preferred frequencies of polymorphisms corresponding to balancing selection;  quantify the strength of balancing selection.There are few functions implemented in RevBayes shown in .Specific functions for PoMoBalance are available in RevBayes.             Function      Description      Parameters                  fnPoMoBalanceKN      Describes the evolution of a population with $K$ alleles and $N$ individuals subjected to mutational bias, selection and balancing selection      $K$, $N$, $\\mu$, $\\phi$, $\\beta$, $B$               fnReversiblePoMoBalanceKN      Particular case of PoMoBalanceKN when mutations are considered reversible and the preferred frequency is in the middle $B=\\frac{N}{2}$.      $K$, $N$, $\\pi$, $\\rho$, $\\phi$, $\\beta$               fnPoMoBalance4N      Particular case of  fnPoMoBalanceKN where $K=4$.      $N$, $\\mu$, $\\phi$, $\\beta$, $B$               fnReversiblePoMoBalance4N      Particular case of  fnReversiblePoMoBalanceKN where $K=4$.      $N$, $\\pi$, $\\rho$, $\\phi$, $\\beta$       The DAG model representation of PoMoBalance is shown in .Graphical model representation of PoMoBalance. The model is non-reversible due to the presence of balancing selection rather than asymmetry in mutation rates, thus, we consider $\\mu_{a_ia_j}=\\rho_{a_ia_j}\\pi_{a_j}$ still leaving room for the model to accept non-symmetric mutation rates. In contrast to PoMos, we set up the node for GC-bias rate $\\sigma$ rather than for the fitness that is easily obtained from it through expression $\\phi$=[1, 1+$\\sigma$, 1+$\\sigma$, 1]. There are also two additional nodes for balancing selection $\\beta$ and $B$.Loading the dataSimilarly to PoMos, we are using count files in the same format. File great_apes_BS_10000.cf contains an example of heterozygote advantage simulation with the preferred frequency in the middle in $4$ great ape populations performed with the evolutionary simulation framework  SLiM (Haller and Messer 2019). We generated $10000$ sites, however, normally balancing selection happens in small regions containing only a few genes or around a thousand nucleotides. Thus, to improve the accuracy of the method we recommend increasing the virtual population size. In the current example, we use $N = 10$ and it can be further increased taking into account the interplay between the number of sites and the computational cost.First, we convert the allelic counts into PoMo states. Open the terminal and copy the data and script into the corresponding subfolders data and scripts of your working directory, for example, call it, PoMoBalance. Inside PoMoBalance create output folder to store the results.PoMo state-space includes fixed and polymorphic states. However, sampled fixed sites might not be necessarily fixed in the original population. We might just have been unlucky and only sampled individuals with the same allele from a locus that is polymorphic. It is typically the case that the real genetic diversity is undersampled in population genetic studies. The fewer the number of sampled individuals or the rarer are the alleles in the original population (i.e., singletons, doubletons), the more likely are we to observe fake fixed sites in the sequence alignment. The sampled-weighted method helps us to correct for such bias by attributing to each of the allelic counts an appropriate PoMo state (0-based coding). For a population size of 3 virtual individuals, we expect 16 states (coded 0-15), while for a population of 2 virtual individuals, we expected 10 states (coded 0-9).The script weighted_sampled_method.cpp is implemented in C++, and we will run it using the Rcpp package in R.  Open the counts_to_pomo_states_converter.R file and make the appropriate changes to obtain your PoMo alignments suited for PoMoBalance.name &lt;- \"great_apes_BS_10000\"                       # name of the count filecount_file &lt;- paste0(\"../data/\", name, \".cf\")       # path to the count filen_alleles  &lt;- 4                                     # the four nucleotide bases A, C, G and TN          &lt;- 10                                    # virtual population sizealignment &lt;- counts_to_pomo_states_converter(count_file,n_alleles,N) # Create the alignmentwriteLines(alignment,paste0(\"../data/\", name, \".txt\"))               # writeg the PoMo alignmentWe place the produced alignments inside the data folder. The output files follow the NaturalNumbers character type of RevBayes and can easily read by it.Open the great_apes_pomobalance.Rev file using an appropriate text editor so you can follow what each command is doing. Then run RevBayes:./rb great_apes_pomobalance.RevNote, you  may use ./rb or the parallel version ./rb-mpi to speed up the calculations.Further, let’s do through the commands in the script in more detail. We define the virtual population size and load the counts file similarly to PoMos.N &lt;- 10data &lt;- readPoMoCountFile(countFile=\"data/great_apes_BS_10000.cf\", virtualPopulationSize=N, format=\"PoMo\")Information about the alignment can be obtained by typing data. &gt;data   PoMo character matrix with 4 taxa and 10000 characters   ======================================================   Origination:                      Number of taxa:                4   Number of included taxa:       4   Number of characters:          10000   Number of included characters: 10000   Datatype:                      PoMoNext, we will specify the number of taxa, taxa names, and the number of branches.n_taxa     &lt;- data.ntaxa()n_branches &lt;- 2*n_taxa-3taxa       &lt;- data.taxa()Also variable to store moves and monitors for our analysis. You can add multiple kinds of moves into this variable and better explore the parameter space with MCMC, to avoid local minima and correlation between the moves. Monitors are for tracking MCMC analysis.moves    = VectorMoves()  monitors = VectorMonitors()Setting up the modelTwo main components are required for unrooted tree estimation with balancing selection:  the PoMo model, which in our case PoMoBalance;  the tree topology and branch lengths.Following PoMos, PoMoBalance is also defined with instantaneous-rate matrix, Q with population size N, allele frequencies pi, exchangeabilities rho (in the non-reversible case combined into mutations mu), and allele fitnesses phi. Frequencies must sum up to unity, thus, pi is initialised with Dirichlet distribution and the move is mvBetaSimplex# allele frequenciespi_prior &lt;- [0.25,0.25,0.25,0.25]pi ~ dnDirichlet(pi_prior)moves.append( mvBetaSimplex(pi, weight=2) )The rho and phi parameters must be positive real numbers and a natural choice for their prior distributions is the exponential distribution and the standard moves mvScale. Let’s add an adaptive variance multivariate-normal proposal move that uses MCMC samples to fit covariance matrix to parameters called mvAVMVN to sigma to avoid correlation between GC-bias and balancing selection coefficients# exchangeabilitiesfor (i in 1:6){  rho[i] ~ dnExponential(10.0)  moves.append(mvScale( rho[i], weight=2 ))}mu := [pi[2]*rho[1], pi[1]*rho[1], pi[3]*rho[2], pi[1]*rho[2], pi[4]*rho[3], pi[1]*rho[3], pi[3]*rho[4], pi[2]*rho[4], pi[4]*rho[5], pi[2]*rho[5], pi[4]*rho[6], pi[3]*rho[6]]# fitness coefficientssigma ~ dnExponential(1.0)moves.append(mvScale( sigma, weight=2 ))moves.append(mvAVMVN(sigma) )phi := [1.0,1.0+sigma,1.0+sigma,1.0]The strength of balancing selection beta is also exponential and for the same reason as rho combines two kinds of moves. The preferred frequency B must be a discrete positive value between 0 and N, thus, we set up variable Num with a uniform prior and two kinds of standard movesmvSlide and mvScale with high weights to enhance exploration of parameter space. We round Num on each iteration to obtain discrete B# Strengths of the balancing selectionfor (i in 1:6){  beta[i] ~ dnExponential(1.0)  moves.append( mvScale( beta[i], weight=30 ) )    # Add this move to avoid a correlation between sigma and beta  moves.append(mvAVMVN(beta[i]) )  }# The preferred frequencies of balancing selectionfor (i in 1:6){  Num[i] ~ dnUniform(0.5,9.5)  moves.append( mvSlide( Num[i], weight=10 ) )  moves.append( mvScale( Num[i], weight=10 ) )  B[i] := round(Num[i])}We will set up the virtual PoMoBalance using the function fnPoMoBalance4N. You can check the input parameters of any PoMo function by typing its name right after the question mark: ?fnPoMoBalance4N.# rate matrixQ := fnPoMoBalance4N(N,pi,rho,phi,beta,B)Note, we could also use function fnReversiblePoMoBalance4N since the preferred frequency in our example is in the middle. However, we use more general function fnPoMoBalance4N to test the estimation of preferred frequency B.The estimation of tree moves is also identical to PoMos including the nearest-neighbour interchange move mvNNI.# topologytopology ~ dnUniformTopology(taxa)moves.append( mvNNI(topology, weight=2*n_taxa) )Nest, we define  2*n_taxa−3  with standard moves.# branch lengthsfor (i in 1:n_branches) {   branch_lengths[i] ~ dnExponential(10.0)   moves.append( mvScale(branch_lengths[i]) )}Finally, we combine the tree topology and branch lengths in treeAssembly in deterministic node psipsi := treeAssembly(topology, branch_lengths)Let’s combine Q and psi into a distribution called the phylogenetic continuous-time Markov chain dnPhyloCTMCsequences ~ dnPhyloCTMC(psi,Q=Q,type=\"PoMo\")and clamp it to datasequences.clamp(data)Finally, we create the model function using any node.pomo_model = model(pi)Setting, running, and summarising the MCMC simulationLet’s set up monitors to track MCMC analysismonitors.append( mnModel(filename=\"output/great_apes_pomobalance.log\", printgen=10) )monitors.append( mnFile(filename=\"output/great_apes_pomobalance.trees\", printgen=10, psi) )monitors.append( mnScreen(printgen=10) )Run burn-in tuning the weights of the parameterspbalance_mcmc.burnin(generations=2000,tuningInterval=200)Finally, set up mcmc moves with four independent MCMC runs to ensure proper convergence and mixing.pomo_mcmc = mcmc(pomo_model, monitors, moves, nruns=4, combine=\"mixed\")and runpomo_mcmc.run( generations=10000 )Use software Tracer or the R package Convenient to assess trajectories and convergence. Look at output/great_apes_pomobalance.log in Tracer. There you see the posterior distributions of the parameters and correlations between parameters.Another way to assess the accuracy of parameter estimation is to look at the site frequency spectra (SFS) as shown in SFS of the data simulated with SLiM compared to the SFS inferred by RevBayes with PoMoBalance on the virtual population size $N=10$.To obtain the tree we need to look at the tree trace filetrace = readTreeTrace(\"output/great_apes_pomobalance.trees\", treetype=\"non-clock\", burnin= 0.2)The mapTree function will summarise the tree samples and write the maximum a posteriori (MAP) tree to the specified file. The MAP tree can be found in the output folder named great_apes_pomobalance_MAP.tree as in .mapTree(trace, file=\"output/great_apes_pomobalance_MAP.tree\" )The unrooted tree of great ape species.Please note that if PoMoBalance struggles to estimate balancing selection and species trees simultaneously, it is possible to estimate the tree with PoMos first and then run PoMoBalance with the fixed tree or the tree topology.Some questions      With  as your guide, draw the probabilistic graphical model of the reversible PoMoBalance model.        Run an MCMC analysis to estimate the posterior distribution under the reversible PoMoBalance model. Which one estimates the strengths of balancing selection better?        Compare the MAP trees estimated under the reversible and nonreversible PoMoBalance model. Are they equal, and if not, how much do they differ?  ",
        "url": "/tutorials/pomobalance/",
        "index": "true"
      }
      ,
    
      "tutorials-tikz": {
        "title": "Building Graphical Models using TikZ (incomplete tutorial)",
        "content": "IntroductionThis tutorial will demonstrate how to make clean-looking graphical models using TikZ/PGF and LaTeX. These are the toolsthat the RevBayes team use to make figures like those used in papers (Höhna et al. 2014; Höhna et al. 2016) and in many of the tutorials and documentationfor RevBayes (e.g., Nucleotide substitution models).  shows the graphicalmodel for the tutorial on the fossilized birth-death process, which was entirely generatedusing tikz. Visualizations of graphical models are incredibly useful for communicating the detailsof the model. These visualizations can be used in scientific papers and when teachingstatistical modeling.A graphical model of the fossilizedbirth-death model describing the generation of the time tree.More details about this model can be found in Combined-Evidence Analysis and the Fossilized Birth-Death Process for Stratigraphic Range DataTools for Generating TikZ FiguresBefore you can start building and generating graphs using TikZ, there are a few prerequisites.  You must have LaTeX installed. This tutorial also assumesthat you have some understanding of TeX/LaTeX.  You may find that a simple tool like LaTeXiT is the best approach for generating individual, stand-aloneimage files that you can use in different types of documents.  If you’d like to create a TikZ figure within your LaTeX document, like in an article or beamer slide-show,it is recommended that you use a LaTeX IDE or text editor. Alternatively, you can edit and compile a LaTeX documentusing online editors like Overleaf and ShareLaTeX.A Simple Stand-alone Graphical Model using LaTeXTo draw a simple graphical model, we can create a custom document in LaTeX. We will do this in a single.tex file.  Open your LaTeX IDE (either online or locally) or text editor. You can name this file simple_model_gm.tex.Now that you have an open file, we can begin by adding the different components.This TeX document will begin with the required specification of the document class.\\documentclass[12pt]{article}Then we can use the geometry package to set the dimensions of the page. This will make the pagea 5 by 5 inch square. If you create a very large model, you may have to change the page dimensions in order toaccommodate the whole graph. We will additionally make it so our document does not have any page numbers.\\usepackage[paperwidth=5in, paperheight=5in]{geometry}\\pagestyle{empty}Now that we have created the document, we can next specify TikZ-specific settings. First, we will include the tikzpackage.\\usepackage{tikz}TikZ has internal libraries that allow for some nice features. The calc TikZ library calc enables us to place elements relative to one another.\\usetikzlibrary{calc}",
        "url": "/tutorials/tikz/",
        "index": "false"
      }
      ,
    
      "tutorials-morph-tree": {
        "title": "Discrete morphology - Tree Inference",
        "content": "IntroductionWhile molecular data have become the default for building phylogenetictrees for many types of evolutionary analysis, morphological dataremains important, particularly for analyses involving fossils. The useof morphological data raises special considerations for model-basedmethods for phylogenetic inference. Morphological data are typicallycollected to maximize the number of parsimony-informative characters -that is, the characters that provide information in favor of onetopology over another. Morphological characters also do not carry commonmeanings from one character in a matrix to the next; character codingsare made arbitrarily. These two factors require extensions to ourexisting phylogenetic models. Accounting for the complexity ofmorphological characters remains challenging. This tutorial will providea discussion of modeling morphological characters, and will demonstratehow to perform Bayesian phylogenetic analysis with morphology usingRevBayes (Höhna et al. 2016).Overview of Discrete Morphology ModelsGraphical model showing the Mk model (left panel).Rev code specifying the Mk model is on the right-hand panel.Molecular data forms the basis of most phylogenetic analyses today.However, morphological characters remain relevant: Fossils often provideour only direct observation of extinct biodiversity; DNA degradation canmake it difficult or impossible to obtain sufficient molecular data fromfragile museum specimens. Using morphological data can help researchersinclude specimens in their phylogeny that might be left out of amolecular tree.To understand how morphological characters are modeled, it is importantto understand how characters are collected. Unlike in molecular data,for which homology is algorithmically determined, homology in acharacter is typically assessed by an expert. Biologists will typicallydecide what characters are homologous by looking across specimens at thesame structure in multiple taxa; they may also look at the developmentalorigin of structures in making this assessment (Phillips 2006). Oncehomology is determined, characters are broken down into states, ordifferent forms a single character can take. The state 0 commonlyrefers to absence, meaning that character is not present. In somecodings, absence will mean that character has not evolved in that group.In others, absence means that that character has not evolved in thatgroup, and/or that that character has been lost in that group(Freudenstein 2005). This type of coding is arbitrary, but bothnon-random and meaningful, and poses challenges for how we modelthe data.Historically, most phylogenetic analyses using morphological charactershave been performed using the maximum parsimony optimality criterion.Maximum parsimony analysis involves proposing trees from themorphological data. Each tree is evaluated according to how many changesit implied in the data, and the tree that requires the fewest changes ispreferred. In this way of estimating a tree, a character that does notchange, or changes only in one taxon, cannot be used to discriminatebetween trees (i.e., it does not favor a topology). Therefore, workerswith parsimony typically do not collect characters that are parsimonyuninformative.In 2001, Paul Lewis (Lewis 2001) introduced a generalization of theJukes-Cantor model of sequence evolution for use with morphologicaldata. This model, called the Mk (Markov model, assuming each characteris in one of k states) model provided a mathematical formulation thatcould be used to estimate trees from morphological data in bothlikelihood and Bayesian frameworks. While this model is a useful stepforward, as a generalization of the Jukes-Cantor, it still makes fairlysimplistic assumptions. This tutorial will guide you through estimatinga phylogeny with the Mk model, and two useful extensions to the model.The Mk ModelThe Mk model is a generalization of the Jukes-Cantor model of nucleotidesequence evolution, which we discussed in Nucleotide substitution models.The Q matrix for a two-state Mk model looks like so:\\[Q = \\begin{pmatrix} -\\mu_0 &amp; \\mu_{01} \\\\\\mu_{10} &amp; -\\mu_1  &amp;\\\\\\end{pmatrix} \\mbox{  ,}\\]This matrix can be expanded to accommodate multi-state data, as well:\\[Q = \\begin{pmatrix} -\\mu_0 &amp; \\mu_{01} &amp; \\mu_{02} &amp; \\mu_{03} \\\\\\mu_{10} &amp; -\\mu_1  &amp; \\mu_{12} &amp; \\mu_{13} \\\\\\mu_{20} &amp; \\mu_{21} &amp; -\\mu_2  &amp; \\mu_{23} \\\\\\mu_{30} &amp; \\mu_{31} &amp; \\mu_{32} &amp; -\\mu_3\\end{pmatrix} \\mbox{  ,}\\]However, the Mk model sets transitions to be equal from any state to anyother state. In that sense, our multistate matrix really looks likethis:\\[Q = \\begin{pmatrix} -(k-1)\\mu &amp; \\mu &amp; \\mu &amp; \\mu \\\\\\mu &amp; -(k-1)\\mu  &amp; \\mu &amp; \\mu \\\\\\mu &amp; \\mu &amp; -(k-1)\\mu  &amp; \\mu \\\\\\mu &amp; \\mu &amp; \\mu &amp; -(k-1)\\mu \\\\\\end{pmatrix} \\mbox{  ,}\\]Because this is a Jukes-Cantor-like model (Jukes and Cantor 1969), statefrequencies do not vary as a model parameter. These assumptions may seemunrealistic. However, all models are a compromise between reality andgeneralizability. Prior work has demonstrated that, in many conditions,the model does perform adequately (Wright and Hillis 2014). Because morphologicalcharacters do not carry common meaning across sites in a matrix in theway that nucleotide characters do, making assumptions that fit allcharacters is challenging. A visualization of this simple model can beseen in .We will first perform a phylogenetic analysis using the Mk model. Infurther sections, we will explore how to relax key assumptions of the Mkmodel.Ascertainment BiasWhen Lewis first introduced the Mk model, he observed that branchlengths on the trees were greatly inflated. The reason for this is thatwhen morphological characters are collected, characters that do notvary, or vary in a non-parsimony-informative way (such asautapomorphies) are excluded. Excluding these low-rate characters causesthe overall amount of evolution to be over-estimated. This causes aninflation in the branch lengths (Lewis 2001).Therefore, when performing a morphological phylogenetic analysis, it isimportant to correct for this bias. There are numerous statisticallyvalid ways to perform this correction (Allman and Rhodes 2008). Original correctionssimulated invariant and non-parsimony informative characters along theproposed tree. The likelihood of these characters would then becalculated and used to normalize the total likelihood value. RevBayesimplements a dynamic programming approach that calculates the samelikelihood, but does so faster.Example: Inferring a Phylogeny of Fossil Bears Using the Mk ModelIn this example, we will use morphological character data from 18 taxaof extinct bears (Abella et al. 2011). The dataset contains 62 binarycharacters, a fairly typical dataset size for morphological characters.Tutorial FormatThis tutorial follows a specific format for issuing instructions andinformation.  The boxed instructions guide you to complete tasks that are not part ofthe RevBayes syntax, but rather direct you to create directories orfiles or similar.Information describing the commands and instructions will be written inparagraph-form before or after they are issued.All command-line text, including all Rev syntax, are given inmonotype font. Furthermore, blocks of Rev code that are needed tobuild the model, specify the analysis, or execute the run are given inseparate shaded boxes. For example, we will instruct you to create aconstant node called example that is equal to 1.0 using the &lt;-operator like this:example &lt;- 1.0It is important to be aware that some PDF viewers may render somecharacters given as differently. Thus, if you copy and paste text fromthis PDF, you may introduce some incorrect characters. Because of this,we recommend that you type the instructions in this tutorial or copythem from the scripts provided.Data and Files  On your own computer, create a directory called data.In this directory download the datafiles: bears.nex.Getting Started  Create a new directory (in RB_DiscreteMorphology_Tutorial) called scripts.(If you do not have this folder, please refer to the directions insection .)When you execute RevBayes in this exercise, you will do so within themain directory you created (RB_DiscreteMorphology_Tutorial), thus,if you are using a Unix-based operating system, we recommend that youadd the RevBayes binary to your path. Alternatively make sure that you set theworking directory to, for example, RB_DiscreteMorphology_Tutorial if this isthe directory you stored the scripts and data in.Creating Rev FilesIn this exercise, you will work primarily in your text editor andcreate a set of files that will be easily managed and interchanged.In this first section, you will write the following filefrom scratch and save them in the scripts directory:  mcmc_mk.Rev: the Rev file that loads the data,specifies the model describing discrete morphologicalcharacter change (binary characters),and specifies the monitors and MCMC sampler.All of the files that you will create are also provided in theRevBayes tutorial here (see the top of this webpage).Please refer to these files to verify or troubleshoot your own scripts.  Open your text editor and create the Rev-script file called mcmc_Mk.Rev in thescripts directory.  Enter the Rev code provided in this section in the new file.In this section you will begin the file and write the Rev commands forloading in the taxon list and managing the data matrices. Then, startingin section , you will move on to specifying each ofthe model components. Once the model specifications arecomplete, you will complete the script with the instructions given in section.Load Data MatricesRevBayes uses the function readDiscreteCharacterData() to load adata matrix to the workspace from a formatted file. This function can beused for both molecular sequences and discrete morphological characters.Import the morphological character matrix and assign it to the variablemorpho.morpho = readDiscreteCharacterData(\"data/bears.nex\")Create Helper VariablesBefore we begin writing the Rev scripts for each of the models,we need to instantiate a couple “helper variables” that willbe used by downstream parts of our model specification.Create a new constant node called num_taxa that is equal to the numberof species in our analysis (18) and a constant node called num_branches representingthe number of branches in the tree. We will also create a constant node ofthe taxon names. This list will be used to initialize the tree.taxa &lt;- morpho.names()num_taxa &lt;- taxa.size()num_branches &lt;- 2 * num_taxa - 2Next, create two workspace variables called moves and monitors. These variablevectors containing all of the MCMC moves and monitors usedto propose new states for every stochastic node in the model graph.moves    = VectorMoves()monitors = VectorMonitors()One important distinction here is that moves and monitors is part of the RevBayesworkspace and not the hierarchical model. Thus, we use the workspaceassignment operator = instead of the constant node assignment &lt;-.The Mk ModelFirst, we will create a joint prior on the branch lengths and tree topology.This should be familiar from the Nucleotide substitution modelsphylogeny ~ dnUniformTopologyBranchLength(taxa, branchLengthDistribution=dnExponential(10.0))It is not necessary for this model, but often we want to monitor the tree lengthtoo. The tree length represents the number of transitions that we expect percharacter over the entire tree.tree_length := phylogeny.treeLength()We will also create several moves working on the topology and branch lengths.moves.append( mvNNI(phylogeny, weight=num_branches) )moves.append( mvSPR(phylogeny, weight=num_branches/5.0) )moves.append( mvBranchLengthScale(phylogeny, weight=num_branches) )Next, we will create a $Q$ matrix. Recall that the Mk model is simply ageneralization of the JC model. Therefore, we will create a 2x2 $Q$ matrixusing fnJC, which initializes Q-matrices with equal transitionprobabilities between all states.Q_morpho &lt;- fnJC(2)Now that we have the basics of the model specified, we will addGamma-distributed rate variation. We will use a uniform prior between $0$ and $10^8$, as recommended by Fabreti and Höhna (2023) and initialize the value with $1.0$ to improve MCMC efficiency.alpha_morpho ~ dnUniform( 0.0, 1E8 )alpha_morpho.setValue( 1.0 )rates_morpho := fnDiscretizeGamma( alpha_morpho, alpha_morpho, 4 )As usual, we add a scaling move.moves.append( mvScale(alpha_morpho,lambda=1, weight=2.0) )Lastly, we set up the CTMC. This should also be familiar from the Nucleotide substitution models.We see some familiar pieces: tree, $Q$ matrix and site_rates.We also have two new keywords: data type and coding. The data typeargument specifies the type of data - in our case, “Standard”, thespecification for morphology.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q_morpho, type=\"Standard\")phyMorpho.clamp(morpho)All of the components of the model are now specified.Complete MCMC AnalysisCreate Model ObjectWe can now create our workspace model variable with our fully specifiedmodel DAG. We will do this with the model() function and provide asingle node in the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph andallows us to pass the model to various functions that are specific toour MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our Rev-script is to specify themonitors and output file names. For this, we create a vector calledmonitors that will each sample and record or output our MCMC.The first monitor we will create will monitor every named randomvariable in our model graph. This will include every stochastic anddeterministic node using the mnModel monitor. The only parameter thatis not included in the mnModel is the tree topology. Therefore, theparameters in the file written by this monitor are all numericalparameters written to a tab-separated text file that can be opened byaccessory programs for evaluating such parameters. We will also name theoutput file for this monitor and indicate that we wish to sample ourMCMC every 10 cycles.monitors.append( mnModel(filename=\"output/mk.log\", printgen=10) )The mnFile monitor writes any parameter we specify to file. Thus, ifwe only cared about the branch lengths and nothing else (this is not atypical or recommended attitude for an analysis this complex) wewouldn’t use the mnModel monitor above and just use the mnFilemonitor to write a smaller and simpler output file. Since the treetopology is not included in the mnModel monitor (because it is notnumerical), we will use mnFile to write the tree to file by specifyingour phylogeny variable in the arguments.monitors.append( mnFile(filename=\"output/mk.trees\", printgen=10, phylogeny) )The third monitor we will add to our analysis will print information tothe screen. Like with mnFile we must tell mnScreen which parameterswe’d like to see updated on the screen.monitors.append( mnScreen(printgen=100) )Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now createthe workspace variable that defines our MCMC run. We do this using themcmc() function that simply takes the three main analysis componentsas arguments.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")The MCMC object that we named mymcmc has a member method called.run(). This will execute our analysis and we will set the chainlength to 20000 cycles using the generations option.mymcmc.run(generations=30000, tuningInterval=200)Once our Markov chain has terminated, we will want to summarize our output.First, we load in the tree trace.trace = readTreeTrace(\"output/mk.trees\", treetype=\"non-clock\")trace.setBurnin(0.25)Then, we compute the maximum a posteriori (MAP) tree and the majority rule consensus (MRC) tree.mapTree(trace, file=\"output/mk.map.tre\")consensusTree(trace, file=\"output/mk.majrule.tre\")Finally, tell the program to quit using the q() function.q()  You made it! Save all of your files.Execute the MCMC AnalysisWith all the parameters specified and all analysis components in place,you are now ready to run your analysis. The Rev scripts you justcreated will all be used by RevBayes.  Begin by running the RevBayes executable. In Unix systems, type thefollowing in your terminal (if the RevBayes binary is in your path):rbProvided that you started RevBayes from the correct directory(RB_DiscreteMorphology_Tutorial), you can then use the source()function to feed RevBayes your Rev-script file (mcmc_mk.Rev).source(\"scripts/mcmc_mk.Rev\")This will execute the analysis and you should see the following output(though not the exact same values):   Processing file \"scripts/mcmc_mk.Rev\"   Successfully read one character matrix from file 'data/bears.nex'   Running MCMC simulation   This simulation runs 2 independent replicates.   The simulator uses 4 different moves in a random move schedule with 56.4 moves per iterationIter        |      Posterior   |     Likelihood   |          Prior   |    elapsed   |        ETA   |----------------------------------------------------------------------------------------------------0           |       -680.054   |       -649.452   |       -30.6022   |   00:00:00   |   --:--:--   |100         |       -419.885   |       -414.047   |       -5.83883   |   00:00:01   |   --:--:--   |200         |       -427.028   |       -417.426   |       -9.60277   |   00:00:01   |   00:00:49   |300         |       -421.585   |        -417.96   |        -3.6253   |   00:00:02   |   00:01:04   |400         |       -431.561   |       -427.124   |       -4.43711   |   00:00:03   |   00:01:12   |500         |       -423.507   |       -422.002   |       -1.50428   |   00:00:04   |   00:01:16   |600         |       -418.061   |       -419.644   |        1.58298   |   00:00:05   |   00:01:18   |700         |       -427.552   |       -423.884   |       -3.66793   |   00:00:05   |   00:01:06   |800         |        -437.39   |       -424.302   |       -13.0876   |   00:00:06   |   00:01:09   |900         |       -418.405   |       -413.872   |        -4.5323   |   00:00:07   |   00:01:10   |1000        |       -425.641   |       -411.291   |       -14.3491   |   00:00:08   |   00:01:12   |...When the analysis is complete, RevBayes will quit and you will have anew directory called output that will contain all of the files youspecified with the monitors ().Exercises  Run the MCMC analysis in RevBayes.  Look at the resulting files mk_run_1.log and mk_run_2.log in Tracer and check for convergence.  Look at the majority rule consensus tree stored mk.majrule.tre and the MAP tree stored in mk.map.tre in FigTree.Example: Ascertainment BiasAs discussed earlier in the section , we also need tocorrect for ascertainment bias.  Create a copy of your previous Rev script, and call it mcmc_Mkv.Rev.You will need to modify the Revcode provided in this section in this file.In RevBayes it is actually very simple to add a correction for ascertainment bias.You only need to set the option coding=\"variable\" in the dnPhyloCTMC. Coding specifieswhat type of ascertainment bias is expected. We are using the variable correction,as we have no invariant character in our matrix. If we also lackedparsimony non-informative characters, we would use the coding informative.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q_morpho, type=\"Standard\", coding=\"variable\")  Remember to change all filenames for the output, e.g., from output/mk.log to output/mkv.log.That’s all you need to do! Now run this script in RevBayes.Example: Relaxing the Assumption of Equal Transition Probabilities  Make a copy of the Rev script you made earlier. Call itmcmc_mkv_dicretized.Rev. This new script willcontain the new model parameters and models.Graphical model demonstrating thediscretized Beta distribution for allowing variable state frequencies.The Mk model makes a number of assumptions, but one that may strike youas unrealistic is the assumption that characters are equally likely tochange from any one state to any other state. That means that a trait isas likely to be gained as lost. While this may hold true for sometraits, we expect that it may be untrue for many others.RevBayes has functionality to allow us to relax this assumption. We dothis by specifying a Beta prior on state frequencies. Remember from theNucleotide substitution models lesson that stationary frequencies impact how likely we areto see changes in a character. For example, it may be very likely, in acharacter, to change from 0 to 1. But if the frequency of 0 is very low,we will still seldom see this change.We can exploit the relationship between state frequencies and observedchanges to allow for variable Q matrices across characters .To do this, we generate a Beta distribution on state frequencies, and usethe state frequencies from that Beta distribution to generate a seriesof Q-matrices to use to evaluate our data (Pagel and Meade 2004).This type of model is called a mixture model. There are assumed tobe subdivisions in the data, which may require different parameters (inthis case, state frequencies). These subdivisions are not defined apriori. This model has previously been shown to be effective for arange of empirical and simulated datasets (Wright et al. 2016).Modifying the Rev-scriptAt each place in which the output files are specified in the MCMC file,change the output path so you do not overwrite the output from theprevious exercise. For example, you might call your output fileoutput/mkv_discretized.log and output/mkv_discretized.trees. Changesource statement to indicate the new model file.We will use a discretized Beta distribution to place a prior on state frequencies.The Beta distribution has two parameters, $\\alpha$ and $\\beta$. These twoparameters specify the shape of the distribution. State frequencies willbe evaluated according to this distribution, in the same way that ratevariation is evaluated according to the Gamma distribution. Thediscretized distribution is split into multiple classes, each with it’sown set of frequencies for the 0 and 1 characters. The number of classescan vary; we have chosen 5 for tractability. Note that we need to make sure that thisdiscretization results into a symmetric model, therefore we will use only one parameterfor the Beta distribution: beta_scale so that $\\alpha = \\beta$.First, we initialized the number of categories.num_cats = 5Second, we specify that our parameter of the Beta distribution.This parameter beta_scale is drawn from the uniform distribution which we specify widely, similar to the suggestion of the prior on Gamma rate categories (Fabreti and Höhna 2023).#beta_scale ~ dnUniform( 0.0, 1E4 )beta_scale ~ dnLognormal( 0.0, sd=2*0.587405 )beta_scale.setValue( 1.0 )Next, we set the categories to each represent a quadrant of the Betadistribution specified by the beta_scale.# Create the Beta distribution, according to the two parameters and the number of categories.Using these state frequencies, we will generate a new vector of Qmatrices. Because we are varying the state frequencies, we must use a Qmatrix generation function that allows for state frequencies to vary asa parameter. We will, therefore, use the fnF81 function.for (i in 1:cats.size()) {    Q[i] := fnF81(simplex(abs(1-cats[i]), cats[i]))}Additionally, in RevBayes we need to specify the probabilities that a site evolves according to oneof the $Q$-matrices. For this model the probabilities must be equal because we need to guarantee thatthe model is symmetric. This, we use a simplex function to create a vector that sums to 1.0.matrix_probs &lt;- simplex( rep(1,num_cats) )The only other specification that needs to change in the model specification isthe CTMC:phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q, type=\"Standard\", coding=\"variable\", siteMatrices=matrix_probs)phyMorpho.clamp(morpho)You will notice that we have added a command to tell the CTMC that we havemultiple site matrices that will be applied to different characters inthe matrix.Set-Up the MCMCThe MCMC chain set-up does not need to change. Run the new MCMC file,just as you ran the plain Mk file. This estimation will take longer thanthe Mk model, due to increased model complexity.Evaluate and Summarize Your ResultsChecking ConvergenceAssessing convergence using convenience (Fabreti and Höhna 2022). Here we show the difference in split frequencies. These runs have converged for us.Evaluate MCMCWe will use Tracer to evaluate the MCMC samples from ourthree estimations. Load all three of the MCMC logs into theTracer window. Highlight all three files inthe upper left-hand viewer () by right- orcommand-clicking all three files.Highlight all three files for model comparison.Once all three trace logs are loaded and highlighted, first look at theestimated likelihoods. You will notice that the Mk model, asoriginally proposed by (Lewis 2001) is improved by allowing any statefrequency heterogeneity. The discretized model and the Dirichletmodel both represent improvements, but are fairly close in likelihoodscore to each other (). Likely, we would need toperform stepping stone model assessment to truly tell if the morecomplicated model is statistically justified.This analysis is too complicated and time-consuming for this tutorial period,but you will find instructions for performing the analysis inGeneral Introduction to Model selection.Comparison of likelihood scores for all three models.Click on the Trace panel. In the lower left hand corner, you willnotice an option to color each trace by the file it came from. Choosethis option (you may need to expand the window slightly to see it). Nextto this option, you can also see an option to add a legend to your tracewindow. The results of this coloring can be seen in. When the coloring is working, you will see thatthe Mk model mixes quite well, but that mixing becomes worse as we relaxthe assumption of equal state frequencies. This is because we aregreatly increasing model complexity. Therefore, we would need to run theMCMC chains longer if we were to use these analyses in a paper.The Trace window. The traces are colored by which version of the Mk modelthey correspond to.We are interested in two aspects of the posterior distribution. First,all analyses correct for the biased sampling of variable charactersexcept for the simple analysis. Then, we expect thetree_length variable to be greater for simplethan for the remaining analyses, because our data are enriched forvariation.  shows thattree_length is approximately 30% greater forsimple than for mk_simple, which areidentical except that mk_simple corrects for samplingbias. To compare these densities, click the “Marginal Prob Distribution”tab in the upper part of the window, highlight all of the loaded TraceFiles, then select tree_length from the list of Traces.Posterior tree length estimates.Second, we are interested in characterizing the degree of heterogeneityestimated by the beta-discretized model. If the data were distributed bya single morphological rate matrix, then we would expect to see verylittle variation among the different values in cats, andvery large values for the shape and scale parameters of thediscrete-beta distribution. For example, if alpha_ofbeta =beta_ofbeta = 1000, then that would cause all discrete-betacategories to have values approaching 0.5, which approximates asymmetric Mk model.Posterior discretized state frequencies for the discrete-beta model.Posterior alpha and beta parameters for the discrete-beta model. shows that the four discrete-beta statefrequencies do not all have the exact same value. In addition, shows that the priors on thediscrete-beta distribution are small enough that we expect to seevariance among cat values. If the data contained noinformation regarding the distribution of cat values, thenthe posterior estimates for alpha_ofbeta andbeta_ofbeta would resemble the prior.Summarizing tree estimatesThe morphology trees estimated in Section  andSection  are summarized using a majority rule consensus tree(MRCT). Clades appearing in $p&gt;0.5$ of posterior samples are resolved inthe MRCT, while poorly support clades with $p \\leq 0.5$ are shown asunresolved polytomies. Poor phylogenetic resolution might be caused byhaving too few phylogenetically informative characters, or it might bedue to conflicting signals for certain species relationships. Becausephylogenetic information is generated through model choice, let’scompare our topological estimates across models.Majority rule consensus tree for the beta-discretized Mkv analysis.The MRCTs for the simple model with and without the +v correction arevery similar to that for the discretized-beta model(). Note that the scale bars forbranch lengths differ greatly, indicating that tree length estimates areinflated without the +v correction, just as we saw when comparing theposterior tree length densities. In general, it is important to assesswhether your results are sensitive to model assumptions, such as thedegree of model complexity, and any mechanistic assumptions thatmotivate the model’s design. In this case, our tree estimate appears tobe robust to model complexity.",
        "url": "/tutorials/morph_tree/",
        "index": "true"
      }
      ,
    
      "tutorials-mcmc-old": {
        "title": "Diagnosing MCMC performance",
        "content": "OverviewThis tutorial demonstrates how to diagnose the performance of MCMCsimulations, such as the output from RevBayes. You will create aphylogenetic model for the evolution of DNA sequences under a JC, HKY85,GTR, GTR+Gamma and GTR+Gamma+I substitution model. For all these modelsyou will perform an MCMC run to estimate phylogeny and other modelparameters.Requirements {#requirements .unnumbered}We assume that you have completed the following tutorials:  RB_MCMC_TutorialExercise: Assessing Performance of MCMC Simulations“You can never be absolutely certain that the MCMC is reliable,you can only identify when something has gone wrong.” AndrewGelmanModel-based inference is, after all, based on the model. Carefulresearch means being vigilant both regarding the choice of model andrigorously assessing our ability to estimate under the chosen model. Thefirst issue—model specification, which actually entails three closelyrelated issues—is critically important for the simple reason thatunbiased estimates can only be obtained under a model that provides areasonable description of the process that gave rise to our data.Model selection entails assessing the relativefit of our dataset to a pool of candidate models. Rankings arebased on model-selection methods that compare the relative fit ofcandidate modes based either on their maximum-likelihood estimates(which measures the fit of the data to the model at a single point inparameter space), or on marginal likelihood of the candidate models(which measures the average fit of the candidate models to the data).Model adequacy—an equally important but relativelyneglected issue—assesses the absolute fit of the data to the model.Model uncertainty is related to the common (and commonlyignored) scenario when multiple candidate models provide a similar fitto the data: in this scenario, conditioning on any singlemodel (even the best) will lead to biased estimates, and so modelaveraging is required to accommodate uncertainty in the choice of model.Much less concern is given to the second aspect of model-basedinference: the ability to obtain reliable estimates under the chosenmodel(s). The implicit assumption, it seems, is that if a model isimplemented correctly, and if that implementation is successfully usedto obtain an estimate from a given dataset, then we must have performedvalid inference under the model. This would be perfectly sound reasoningif inferences were based on analytical methods. Owing to the complexityof the models, however, it is not possible to estimate phylogeneticparameters analytically. Instead, parameter estimates are based onnumerical methods. In the case of maximum-likelihood estimation, theseare typically hill-climbing algorithms that attempt to search theprofile likelihood to identify the vector of point estimates of allphylogenetic model parameters that jointly maximize the likelihood ofobserving the data under the model. The reliability of these algorithmscan (and should) be assessed by comparing estimates obtained fromrepeated analyses that are initiated from random points in parameterspace. Because there is only one maximum likelihoodestimate, the terminal values estimated by replicate runs should beidentical (within the precision of computer memory).In the Bayesian statistical framework, inferences focus on the jointposterior probability density of phylogenetic model parameters, which isapproximated by Markov chain Monte Carlo (MCMC) algorithms. It may becomforting to know that, in theory, an appropriatelyconstructed and adequately run MCMC simulation isguaranteed to provide an arbitrarily precise description of the jointposterior probability density. In practice, however, even a given MCMCalgorithm that provides reliable estimates in most caseswill nevertheless fail in some cases and is notguaranteed to work for any given dataset. This raises an obviousquestion: “When do we know that an MCMC algorithm provides reliableestimates for a given empirical analyses”. The answer is simple:Never. Fortunately, this problem is not unique to the field ofphylogenetics. Much of Bayesian inference outside our field also relieson MCMC algorithms to approximate the joint posterior probabilitydensity of parameters: similar concerns regarding the reliability ofthose inferences has motivated the development of a suite of diagnostictools to assess MCMC performance. The trick is learning how to use thesetools effectively and rigorously, especially for analyses that entailcomplex phylogenetic models and/or large datasets.MCMC BasicsThe ability to rigorously diagnose MCMC performance requires familiaritywith some basic concepts from probability theory (discussed last time)and a strong intuitive understanding of the underlying mechanics—we needto know how the algorithms work in order to understand when they are notworking. In this installment we’ll briefly cover the mechanics of theMetropolis Hastings MCMC algorithm.Recall that Bayesian inference is focused on the posterior probabilitydensity of parameters. The posterior probability of the parameters can,in principle, be solved using Bayes’ theorem. However, (most)phylogenetic problems cannot be solved analytically, owing mainly to thedenominator of Bayes’ theorem—the marginal likelihood requires solvingmultiple integrals (for all of the continuous parameters, such as branchlengths, substitution rates, stationary frequencies, etc.) for eachtree, and summing over all trees.Accordingly, Bayesian inference of phylogeny typically resorts tonumerical methods that approximate the posterior probability density.There are many flavors of Markov chain Monte Carlo (MCMC)algorithms—Gibbs samplers, Metropolis-coupled and reversible-jump MCMC,etc.—we will consider the Metropolis Hastings (MH) algorithm because itis commonly used for phylogenetic problems, and because it is similar tomany other variants (which we will cover elsewhere). Note that MCMC andBayesian inference are distinct animals: they have a relationshipsimilar to that between ‘optimization algorithms’ and‘maximum-likelihood estimation.’ Some Bayesian inference can beaccomplished without MCMC algorithms, and MCMC algorithms can be used tosolve problems in non-Bayesian statistical frameworks.To introduce the MH algorithm, we will imagine a robot that is programedto explore an area. Specifically, the goal of our robot is to generate atopographic map of an unknown terrain. This terrain has a total surfacearea of one hectare. We deploy our robot by parachute at a randomlocation within the terrain. We have programmed the robot with threesimple rules:      If a proposed step will take the robot uphill, itautomatically takes the proposed step.        If a proposed step will take the robot downhill, it dividesthe elevation of the proposed location by the elevation of thecurrent location: we call this quotient $R$. It then generates auniform random number, $U[0,1]$. If $U&lt;R$, the robot takes theproposed step; otherwise, it stays put.        The distribution for proposing steps is symmetrical. That is,the probability of proposing a step (but not necessarily accepting aproposed step) from point A to point B is equal to the probabilityof proposing a step from point B to point A.  We allow our little robot to wander through the terrain following thesethree simple rules. At prescribed intervals (e.g., every$10$ proposed steps), the robot records the details of his position (hiselevation, latitude, longitude, etc.) in a log. If we allow our robot towander long enough, his log is guaranteed to provide an arbitrarilyprecise description of the topography of the terrain.Let’s consider a slightly more formal description of theMetropolis-Hastings MCMC algorithm. First some preliminaries. Thisalgorithm entails simulating a Markov chain that has a stationarydistribution that is the joint posterior probability density ofphylogenetic model parameters. The ‘state’ of the chain is a set ofparameter values that fully specify phylogenetic model:e.g., a tree topology, a vector of branch lengths, and aset of parameters specifying the stochastic model of trait change(e.g., for the GTR + $\\Gamma$ substitution model, thiscomprises a vector of values for the four stationary frequencies, avector of values for the six exchangeability parameters, and the valueof the alpha parameter describing the shape of the gamma-distributedamong-site rate variation). Under the robot analogy, the state of thechain corresponds to a unique point in the terrain. The Markov propertyof the MCMC reflects the fact that the next state of the chain onlydepends on the current state of the chain, but not on previousstates—that is, the past affects the future only through the present.The Monte Carlo aspect of the MCMC reflects the fact that it is asimulation: it is a numerical method that relies on repeated randomsampling.Now, on to the MH algorithm:      Initialize the chain with values for all parameters, includingthe tree topology, $\\tau$, the vector of branch lengths $\\nu$, andsubstitution model parameters $\\pi_i$, $q_i$, $\\alpha$. We will callthe set of model parameters $\\theta$. The initial parameter valuesmight be specified arbitrarily, or might be drawn from thecorresponding prior probability density for each parameter.        Select a single parameter (or set of parameters) to alteraccording to its proposal probability. For the moment, we won’tworry about the details of these proposal mechanisms—they basicallyinvolve different ways of ‘poking’ the current parameter value,$\\theta$, to generate a new (proposed) parameter value,$\\theta ^{\\prime}$. The important question at the moment is: Wheredo these proposal probabilities come from? The answer is:experience. We want to design the MCMC such that it invests effortin a given parameter in proportion to the difficulty ofapproximating that parameter. Note that the MCMC summarized abovewill spend $\\sim 2\\%$ of its time proposing changes to theexchangeability and stationary frequency parameters, but will invest$\\sim 40\\%$ of its time proposing changes to the topology parameter.Experience suggests that the tree topology is a more difficultparameter for the MCMC to approximate relative to theexchangeability and stationary frequency parameters (in fact, itappears that the developers of MrBayes determined from theirexperience that the topology is $\\sim 20$ times harderto approximate).        Propose a change to the selected parameter using theparameter-specific proposal mechanism. Different parameters maynaturally have different prior probability densities—for example,the stationary frequencies are proportions (ranging between $0$ and$1$), and so are conveniently described using a Dirichlet priorprobability density, whereas the alpha-shape parameter can rangebetween zero and infinity, so is better described using a uniformprior probability density. Different kinds of prior probabilitydensities will have specific proposal mechanisms—for example, thereis something called a ‘Dirichlet’ proposal mechanism that is used topropose new values for parameters described by a Dirichlet prior,and something called a ‘Slider’ proposal mechanism that is used topropose new values for parameters described by a uniform prior.(Again, we’ll go into the gory details of these proposal mechanismsanother time. The main idea for now is that the proposal mechanismsgenerate a new parameter value by poking the currentparameter value.)    The design of proposal mechanisms is something of a dark art—trialand error are used to guide the development of mechanisms that workwell. Nevertheless, there are basic criteria that the proposalmechanisms must meet. All proposal mechanisms must be:                  stochastic (new parameter values must be proposed‘randomly’)                    irreducible (all parameter values must be potentiallyaccessible by the chain)                    aperiodic (the proposal mechanism must not induceepicycles of the chain)                  Calculate the probability of accepting the proposed change, $R$:\\[\\begin{aligned}R = \\text{min}\\Bigg[1, \\underbrace{\\frac{P(X \\mid \\theta ^{\\prime})}{P(X \\mid \\theta )}}_{\\text{likelihood ratio}} \\cdot \\underbrace{\\frac{ P(\\theta ^{\\prime})}{P(\\theta)}}_\\text{prior ratio} \\cdot \\underbrace{\\frac{P(\\theta \\mid \\theta ^{\\prime})}{P(\\theta ^{\\prime} \\mid \\theta)}}_\\text{proposal ratio} \\Bigg]\\end{aligned}\\]    The acceptance probability, $R$, is either equal to one or theproduct of three ratios’ so long as that product is less than one(because $R$ is a probability, so it cannot be greater than one).So, what are these three ratios?    Likelihood ratio: the likelihood ratio is simply the likelihoodof the observations given the proposed value of the parameter,divided by the likelihood of the observations given the currentvalue of the parameter. We can calculate the likelihood for anygiven parameterization of the phylogenetic model using thepruning (Felsenstein) algorithm.    Prior ratio: in Bayesian inference, each parameter is a randomvariable, and so is described by a prior probability density.Accordingly, we can ‘look up’ (calculate) the prior probability ofany specific parameter value. The prior ratio is simply the priorprobability of the proposed and current parameter values.    Proposal ratio: the proposal ratio is the probability ofproposing the current parameter value given the proposed parametervalue, divided by the converse. This is also called the Hastingsratio. This is equivalent to the rule that forces our robot topropose steps symmetrically; i.e., that theprobability of proposing a step from point A to point B in theterrain is equal to the probability of proposing a step from point Bto point A. For inference problems in which the dimensions of themodel are static, the proposal ratio is usually equal to one (we’lldiscuss exceptions to this in the context of reversible-jump MCMC ina future post).    Note that if the proposal ratio is equal to one, the acceptanceprobability, $R$ is based only on the product of the likelihood andprior ratios. Does that ring a bell? [Hint: think of Bayes’theorem.] [Hint 2: think of the right side of Bayes’ theorem.][Hint 3: think of the numerator of the right side of Bayes’theorem.] That is, the posterior probability is proportional to theproduct of the likelihood and prior probability. Accordingly, theacceptance probability is the posterior probability of the proposedstate divided by the posterior probability of the current state.Just as we programmed our robot, if the ratio of the proposed andcurrent elevations (posterior probabilities) is greater than 1(i.e., it is an uphill step), we always accept theproposed change. When the ratio of the proposed and currentelevations is less than 1 (i.e., it is an downhillstep), we may or may not take the proposed step (stay tuned).        Generate a uniform random number between zero and one,$U[0,1]$. If $U&lt;R$, accept the proposed change; otherwise, thecurrent state of the chain becomes the next state of the chain.Downhill moves will be accepted ‘randomly’ in proportion to thedifference in elevation.        Repeat steps $2-5$ an ‘adequate’ number of times.  That’s it! Notice that the decision to accept or reject proposedsteps in the MCMC (and thus to sample from the joint posteriorprobability density) is based exclusively on the likelihood and priorprobability of the proposed and current states—two quantities that areeasy to calculate. The beautiful, fantabulous achievement of theMetropolis-Hastings algorithm is the slaying of the beastly denominatorof Bayes’ theorem. Specifically, the algorithm allows us to do inferencewhile entirely avoiding the need to calculate the (completelyintractable) marginal likelihood!Approximating the joint posterior probability density is based onsamples from the MCMC: a chain following the simple rules outlined abovewill sample parameter values in proportion to their posteriorprobability. That is, the proportion of time that the chain spends inany particular state is a valid approximation of the posteriorprobability of that state (e.g., if a clade is present in$87\\%$ of the samples drawn from the chain, then the posteriorprobability of that clade is estimated to be $0.87$).Why do we accept proposals to states with lower posterior probability?People familiar with maximum-likelihood estimation often misunderstandthe purpose of accepting proposals to states with a lower posteriorprobability. In maximum-likelihood inference, the game is to simplyclimb relentlessly and mindlessly up the likelihood surface in search ofthe very tip of the globally highest peak. Accordingly, the only reasonthese zombie hill climbers might consider a downward move would bemotivated by concerns that they were currently climbing up a local(rather than global) optimum. By contrast, Bayesians are not justinterested in the elevation of the very tip of the absolute peak of theposterior probability surface, but rather are interested in exploringand estimating the entire joint posterior probability density—we wanttopographical map of the entire posterior probability, not the elevationof the tip of the very highest peak in parameter space.This survey of the joint posterior probability density results in a morerobust inference procedure (inferences are averaged over the jointposterior probability of all model parameters), and allows us toestimate and evaluate the marginal posterior probability density foreach of the parameters (these can be viewed by querying the MCMC sampleswith respect to the parameter of interest—we will do this in futuretutorials). Accordingly, it is not sufficient for the chain to reach thepeak of the joint posterior probability density; we need the chain tomix over the entire stationary distribution, spending time at eachlocation in proportion to its posterior probability.Next to the specification of priors (which are inspired by much morephilosophical considerations), performance of the MCMC algorithm used toapproximate the joint posterior probability distribution is the greatestconcern associated with Bayesian inference (it is certainly a moregeneral concern, as it holds even when the priors are uncontroversial,and, in my opinion, is a more legitimate and practical concern). Thereare three closely related issues associated with MCMC performance: (1)convergence (is the robot sampling from the stationary distribution);(2) mixing (is the robot efficiently moving over the posteriorprobability density); and (3) adequacy (has the robot collectedsufficient samples from the target distribution to describe itadequately).Diagnosing MCMC performanceRunning Markov chain Monte Carlo Simulations &amp; Assessing OutputData &amp; ModelWe provide the data file(s) which we will use in this tutorial. You maywant to use your own data instead. In the ‘data’ folder, you will findthe following files  ‘primates_and_galeopterus_cytb.nex’: Alignment of the cytochromeb subunit from 23 primates representing 14 of the 16 families(Indriidae and Callitrichidae are missing). Note that there isone outgroup species included: Galeopterus variegatus.This is the same data and model as we used previously in the CTMCtutorial.Look at Model file &amp; MCMC file in a text editor.Running MCMCLet us now explore the behavior of our Markov chain. The aim here inthese exercises is to visually inspect the output generated byRevBayes. You should look at the trace plots using Tracer. Watch outfor badly mixing chains, poor performance of the MCMC and correlatedparameter estimates.The first analysis which we perform is specified as follows:      uniform GTR+I+G        single move per cycle        large proposal for all parameters        no-pre-burnin  You can run this analysis directly usingsource(\"scripts/mcmc_run1.Rev\")The operator summary provides you with an overview of the moves that youhave used for this MCMC run. It tells you what the weight for each movewas, the variables it has been working on, the number of times the movewas used, and how often the move was accepted. Good acceptance rates forcontinuous parameters are between 20% and 60%. For discrete characters,such as the phylogeny, there is no rule of thumb what good acceptanceis. Instead, the acceptance rate of tree topology proposal stronglydepends on the current data.Now let us open the generated output file in Tracer. The file shouldbe called output/primates_cytb_test1.log.What you may notice is that the ESS are very low and the MCMC is mixingvery poor. This is not good. We’ll try a new analysis but instead runmultiple moves per iteration. The new setting is:      uniform GTR+I+G        multiple moves per cycle        large proposal for all parameters        flat &amp; low proposal weights        no-pre-burnin  We achieve this by setting moveschedule=“random” (which is thedefault) as an argument for the MCMC algorithm. You can run thisanalysis in RevBayes usingsource(\"scripts/mcmc_run2.Rev\")Look at the output in Tracer again. You should see that the ESS valuesare still very low and the chain is still not mixing well.One issue that you see is that shape and rate parameter ‘alpha’ for theamong-site-rate-variation is updated not often enough. It is quitelikely that our move proposed too many bad new parameters. So let uschange the window size for the scaling move applied on ‘alpha’ frommoves[++mvi] = mvScale(alpha, lambda=50.0, weight=2.0)tomoves[++mvi] = mvScale(alpha, lambda=1.0, weight=2.0)This will give us now the following set-up:      uniform GTR+I+G        multiple moves per cycle        smaller proposals for ‘alpha’        flat &amp; low proposal weights        no-pre-burnin  Run the analysis file and look how often the scaling Move is nowaccepted.source(\"scripts/mcmc_run3.Rev\")Open the output and look at it in Tracer. We are now getting betterESS values and better mixing of the MCMC.That worked quite well but it seems to be cumbersome to adjust all thetuning parameters of the moves manually. Instead, let us allowRevBayes to auto-tune the moves. Just change tune=true for all moveswhere it was tune=false before. Now, also include a pre-burnin. Thepre-burnin runs if you saymymcmc.burnin(generations=1000,tuningInterval=100). This runs a chainfor 1000 iterations and updates the tuning parameters, such as thewindow size, so that you achieve a good acceptance rate. That means, ifpreviously a move accepted too few proposal it will decrease the windowsize.Here is the new model set-up:      uniform GTR+I+G        multiple moves per cycle        flat &amp; low proposal weights        pre-burnin (with scale adjusted from previous)  Run the analysis in RevBayes:source(\"scripts/mcmc_run4.Rev\")Look at the output given by the operator summary. Notice how the tuningparameters have been updated. Also look at the output in Tracer. Youwill see that the ESS values are much better because the chain is mixingbetter.Comparing your output to the priorNext, let’s compare it to the prior. Here we want to see how muchinformation we actually have from the data and how much our priorinfluenced our posterior estimates. Run now an analysis that does notuse the data. You can do this in RevBayes by calling the .ignoreAllData()on the model object to mark clamped nodes as ignored.source(\"scripts/mcmc_run_prior.Rev\")Load both the MCMC output under the prior and the posterior intoTracer. Compare the approximated distributions on each parameterbetween both outputs.Apply multiple-run diagnostics:      PSRF        ASDSF        comparetrees  Semi-automatic MCMC diagnosis using bonsaiDiagnosing MCMC performance manually can be extremely cumbersome,especially for more complex models. The R package bonsai performs astandard set of diagnostics on MCMC output and generates a report thathighlights potentially pathological MCMC behaviors. To use bonsai,download the package source fromhttps://github.com/mikeryanmay/bonsai, launch the R console, andinstall the package from source by following the instructions on thebonsai main page (making sure to install the packages that bonsai relieson as well):install.packages(packages=c('coda','tools','RColorBrewer','entropy','xtable'),dependencies=TRUE)install.packages('&lt; path to bonsai &gt;',repos=NULL,type='source')library(bonsai)Move the output files from run7 to the folder.../RB_MCMC_Tutorial/output/output_for_bonsai.Going back to the R console, provide a name for the project.project &lt;- 'Fagus run 7'Point bonsai at the directory where we placed our log files.path &lt;- '.../RB_MCMC_Tutorial/output/output_for_bonsai'Finally, we create the bonsai object and execute the runBonsai()function. It will generate a report that draws your attention topotential issues.fagus_proj &lt;- bonsai(project=project,path=path)fagus_proj$runBonsai()Check the flags generated by bonsai (Figure [fig:flags]; your resultsmay differ). Many of these flags may be innocuous; it is up to the userto decide whether a particular flag is truly problematic. For example, acorrelation between Likelihood and Posterior is not problematic;however, a significant p-value for Geweke’s diagnostic for pi[3]indicates that this parameter has not converged to its stationarydistribution.![Flags generated by bonsai.](\\ResourcePath figures/flags.pdf)   Flags generated by bonsai.There are additional bonsai reports for more complex modelsdemonstrating a wider array of MCMC pathologies in the.../RB_MCMC_Tutorial/bonsai_pre_cooked directory.",
        "url": "/tutorials/mcmc_old/",
        "index": "false"
      }
      ,
    
      "tutorials-mcmc": {
        "title": "Introduction to Markov chain Monte Carlo (MCMC) Sampling",
        "content": "OverviewThis is the very first tutorial for you in RevBayes. The goal of this set of tutorials is getting you started and familiar with the basics in RevBayes. If you have some familiarity with R or similar software, then this should be straight forward. Nevertheless, we recommendyou to work through these tutorials to learn all the specific quirks of RevBayes.Tutorials  Click on the first exercise to begin!  Installing RevBayes and first steps  Basic commands in RevBayes with Rev  Introduction to graphical models",
        "url": "/tutorials/mcmc/",
        "index": "true"
      }
      ,
    
      "tutorials-pomos": {
        "title": "Polymorphism-aware phylogenetic models",
        "content": "This tutorial uses functions implemented in the developmental branch dev_PoMo_SNP. To download and install RevBayes from source, please follow the instructions here.Polymorphism-aware phylogenetic models The polymorphism-aware phylogenetic models (PoMos) are alternative approaches to species tree estimation (De Maio et al. 2013) that add a new layer of complexity to the standard substitution models by accounting for population-level forces to describe the process of sequence evolution (De Maio et al. 2015; Schrempf et al. 2016; Borges et al. 2019). PoMos model the evolution of a population of individuals in which changes in allele content (e.g., due to mutations) and frequency (e.g., due to genetic drift or selection) are both possible ().PoMoTwo and Three state-spaces. The tetrahedron represents the PoMos state-space for the four-allelic case, which in this case are the A, C, G and T nucleotide bases. The fixed sites \\(\\{Na_i\\}\\) are represented in the vertices of the tetrahedron, while the polymorphic states \\(\\{na_i,(N −n)a_j\\}\\) are represented on its edges. The black and gray arrows distinguish mutations from frequency shifts (i.e., due to genetic drift and selection).PoMos stand out from the standard phylogenetic substitution models and other species tree methods because they:  allow to disentangle the contribution of evolutionary forces to the evolutionary process (e.g., genetic drift, mutational biases, and selection);  consider polymorphisms, thus permitting inferences with data from multiple individuals and populations;  naturally account for incomplete lineage sorting (i.e., the persistence of ancestral polymorphisms during speciation events), a known source of phylogenetic discord;  are computationally efficient by directly estimate the species tree, thus circumventing the many constraints between the species tree and the genealogical histories.Overall, PoMos constitute a full-likelihood yet computationally efficient approach to species tree inference. PoMos are designed to cope with recent radiations, including incomplete lineage sorting, and long divergence times.Polymorphism-aware phylogenetic models: the model PoMos model the evolution of a population of $N$ individuals and $K$ alleles in which changes in allele content and frequency occur. These are mediated by population forces such as mutation, genetic drift, and selection. The PoMo state-space includes fixed (or boundary) states \\(\\{Na_i\\}\\), in which all $N$ individuals have the same allele \\(i \\in \\{0,1,...,K-1\\}\\), and polymorphic states \\(\\{na_i,(N-n)a_j\\}\\), in which two alleles $a_i$ and $a_j$ are present in the population with absolute frequencies $n$ and $N-n$.  Mutations occur at a rate of $\\mu_{a_ia_j}$. Mutations govern the allele content and only occur in the fixed states:\\(q_{\\{Na_i\\} \\rightarrow \\{(N-1)a_i,1a_j\\}}=\\mu_{a_ia_j} \\label{equation1}\\tag{1}\\)Often, a reversible mutational model is considered. In this case, we break the mutations into a base composition $\\pi$ and an exchangeability parameter $\\rho$ (i.e., $\\mu_{a_ia_j}=\\rho_{a_ia_j}\\pi_{a_j}$) just like the GTR. However, in PoMos, these do not represent substitutions but mutations. Such an assumption can still model mutational biases quite well and simplifies obtaining formal quantities with PoMos. Another assumption of PoMos is that mutations can only occur in fixed states. This corresponds to the assumption that mutation rates are low, which is verified for the majority of multicellular eukaryotes.  Genetic drift is modeled according to the Moran model, in which one individual is chosen to die and one individual is chosen to reproduce at each time step. Selection acts to (dis)favor alleles by differentiated fitnesses: $\\phi_{a_i}$. Together, genetic drift and selection govern the allele frequency changes:\\(q_{\\{na_i,(N-n)a_j\\} \\rightarrow \\{(n+1)a_i,(N-n-1)a_j\\}}=\\frac{n(N-n)}{N}\\phi_{a_i} \\label{equation2}\\tag{2}\\)Like the standard substitution models, PoMos are continuous-time Markov models and are fully characterized by their rate matrices. The rates in \\ref{equation1} and \\ref{equation2} define the PoMos rate matrices. RevBayes includes the fnPoMoKN rate matrices that permit modeling population dynamics with any number of alleles, reversible mutations (i.e., $\\mu_{a_ia_j}=\\rho_{a_ia_j}\\pi_{a_j}$) and selection. You can check the input parameters of this function by typing its name right after the question mark: ?fnPoMoKN.   Arguments            K : Number of alleles             Type:       Natural, &lt;any&gt;, value         V : Number of virtual individuals             Type:       Natural, &lt;any&gt;, value         N : Number of effective individuals             Type:       RealPos, &lt;any&gt;, const reference             Default:    NULL        mu : Vector of mutation rates: mu=(mu_a0a1,mu_a1a0,mu_a0a2,mu_a2a0,...)             Type:       RealPos[], &lt;any&gt;, const reference       phi : Vector of fitness coefficients: phi=(phi_0,phi_1,...,phi_ak)             Type:       RealPos[], &lt;any&gt;, const referenceThis tutorial demonstrates how to set up and perform analyses using polymorphism-aware phylogenetic models. You will perform phylogeny inference using the virtual PoMo Three. We will do this by setting the number of virtual (and effective) individuals to 3. These models allow for very efficient species tree inferences under selection because they operate on a small state space (missing reference). You will perform a Markov chain Monte Carlo (MCMC) analysis to estimate phylogeny and other model parameters. By the end of this tutorial, we leave as an exercise to run the neutral version (PoMoTwo) and compare the resulting trees. The graphical model representation under PoMoThree is depicted in figure .Graphical model representation of PoMos. The graphical model shows the dependencies among parameters (Höhna et al. 2014). Here, the rate matrix $Q$ is a deterministic variable because it depends on the mutation rates and fitness coefficients. The same applies to the phylogenetic tree $\\Psi$, which depends on the topology and branch lengths.Count files PoMos perform inferences based on allele frequency data, which is stored in count files. These files contain two header lines. The first line indicates the number of taxa and the number of sites (or loci) in the sequence alignment. You might have noticed that NPOP stands for the number of populations, but this is not necessarily the case. PoMos can be used to infer the evolutionary history of different species or even other systematic units of interest, such as species, subspecies, communities, and so forth.The second line specifies the genomic position of each locus (chromosome and location) and the taxon names. The first two columns are not used for inference, so if you’re working with taxa for which this information is unavailable, you can input these columns with dummy values (e.g., NA or ?). The remaining lines  the other lines in the count file include allelic counts separated by commas. All elements in the count file are separated by white spaces. Here is an example of some lines from the great ape count file we will analyze in this tutorial:COUNTSFILE NPOP 3 NSITES 5CHROM POS       Gorilla_beringei_graueri Gorilla_gorilla_dielhi Gorilla_gorilla_gorillachr1  41275799  6,0,0,0                  2,0,0,0                54,0,0,0 chr2  120104878 6,0,0,0                  2,0,0,0                54,0,0,0 chr11 61364549  0,6,0,0                  0,2,0,0                0,54,0,0 chr17 44837427  6,0,0,0                  2,0,0,0                54,0,0,0 chr19 7495905   4,0,2,0                  2,0,0,0                10,0,44,0 The four allelic counts in this count file represent the allelic counts of the A, C, G, and T, respectively. Therefore, we know that the Gorilla_gorilla_gorilla has an AG polymorphism at position 7 495 905 on chromosome 19. The order of alleles in the allelic counts can vary, but it is important to remember that the vectors of mutation rates, exchangeabilities, base frequencies, and fitness coefficients all follow the order of the allele counts in the count file:  the base frequencies and the fitness vectors are in the same order as in the counts: i.e., \\(\\{a_0,a_1,\\dots,a_{K-1}\\}\\);  the mutation rate vector is \\(\\{a_0a_1, a_1a_0, a_0a_2, a_2a_0,\\dots\\}\\);  the exchangeability vector follows a similar pattern as for the mutation rates, but without the reversed mutation: i.e., \\(\\{a_0a_1, a_0a_2, \\dots\\}\\).Loading the data The first step in this tutorial is to convert the allelic counts into PoMo states. Open the terminal and navigate to your working directory, which we will call PoMos (but you can choose any name you prefer). Inside PoMos, create the usual data and output folders. Before loading the data, run RevBayes by typing ./rb (or ./rb-mpi) in the console. Open the great_apes_pomothree.Rev file using a suitable text editor so you can follow what each command is doing. Once you understand the .Rev script in detail, you can run it automatically as follows:./rb great_apes_pomothree.RevAs mentioned earlier, the PoMo state space includes both fixed and polymorphic population states. However, allele counts are typically sampled from a small number of individuals. For example, sampled fixed sites may not actually be fixed in the original population. It is possible that we only sampled individuals with the same allele from polymorphic locus, leading to an inaccurate representation of the population’s true genetic diversity. The fewer individuals sampled, or the rarer the allele in the original population (e.g., singletons or doubletons), the more likely we are to observe false fixed sites in the sequence alignment.There are methods that help us correct for this bias by attributing to each of the allelic counts an appropriate PoMo state. One such method is the weighted-method (Schrempf et al. 2016), which weights each PoMo state based on binomial sampling. In RevBayes, this is done automatically when we use the readPoMoCountFile function and set the weighting to Binomial. Alternatively, you can assign the PoMo state closest to the observed frequency. This method is called Fixed. In this tutorial, we will use Fixed. To use the readPoMoCountFile function, define the location of the counts file, set the virtual population size (which we set to 3, as we are using the virtual PoMo Three), specify the data type format PoMo and apply the Fixed correction as shown below:N &lt;- 3data &lt;- readPoMoCountFile(countFile=\"data/great_apes_1000.cf\", virtualPopulationSize=N, format=\"PoMo\", samplingCorrection=\"Fixed\")Information about the alignment can be obtained by typing data. &gt;data   PoMo character matrix with 12 taxa and 1000 characters   ======================================================   Origination:                      Number of taxa:                12   Number of included taxa:       12   Number of characters:          1000   Number of included characters: 1000   Datatype:                      PoMoIf, instead of a count file, you have a list of sequences per individual (in either fasta or nexus format), RevBayes can still convert it to PoMo data format. To do this, you need to read the sequences, provide a file with the taxon names, and perform the conversion to PoMo state space using pomoStateConvert. Please ensure that individual sequences belonging to the same taxon have the same name. Here are the commands you will need:data_char = readDiscreteCharacterData(\"data/individual_sequences.nex\")taxa = readTaxonData(\"data/taxon_names.txt\")data = pomoStateConvert(aln=data_char, k=4, virtualNe=N, taxa)Next, we define some useful variables. These include the number of taxa, taxa names, and the number of branches, which will be important for setting up our model in later steps.n_taxa     &lt;- data.ntaxa()n_branches &lt;- 2*n_taxa-3taxa       &lt;- data.taxa()Additionally, we will set up a variable that holds all the moves and monitors for our analysis. Recall that moves are algorithms used to propose new parameter values during the MCMC simulation, while monitors print the values of model parameters to the screen and/or log files during the MCMC analysis.moves    = VectorMoves()  monitors = VectorMonitors()Setting up the model Estimating an unrooted tree under the virtual PoMos requires specifying two main components:  the PoMo model, which in our case is PoMoThree;  the tree topology and branch lengths.A given PoMo model is defined by its corresponding instantaneous rate matrix, Q which depends on the virtual population size N, the mutation rates, assumed to be reversible and dependent on the allele frequencies pi, and the exchangeabilities rho. PoMoThree additionally includes allele fitnesses phi, as it accounts for selection. We will set up the virtual PoMoThree using the function fnPoMoKN. In particular, we set N to 3. Note that N is a fixed node, as we had previously defined. Since pi, rho, and gamma are stochastic variables, we must specify a move to propose updates to them. A good move for variables drawn from a Dirichlet distribution (i.e., pi) is the mvBetaSimplex move. This move randomly selects an element from the allele frequency vector pi, proposes a new value drawn from a beta distribution, and then rescales all values to sum to 1. The weight option inside the moves specifies how often the move will be applied, either on average per iteration or relative to all other moves.# allele frequenciespi_prior &lt;- [1,1,1,1]pi ~ dnDirichlet(pi_prior)moves.append( mvBetaSimplex(pi, weight=2) )The rho and phi parameters must be positive real numbers and a natural choice for their prior distributions is the exponential distribution. Again, we need to specify a move for these stochastic variables, and a simple scaling move, mvScale, typically works. In this tutorial, we want our model to capture the effect of GC-bias gene conversion. For that, we define gamma, the GC-bias rate. The allele fitnesses phi for G and C will be represented by gamma, while those for A and T by 1.0. Note that phi is a deterministic node that depends on the GC-bias rate gamma.# exchangeabilitiesfor (i in 1:6){  rho[i] ~ dnExponential(10.0)  moves.append(mvScale( rho[i], weight=2 ))}# fitness coefficientsgamma ~ dnExponential(1.0)moves.append(mvScale( gamma, weight=2 ))phi := [1.0,1.0+gamma,1.0+gamma,1.0]Because we want the mutations to be reversible, we build the mutation rate vector as a deterministic variable depending on pi and rho:# mutation ratesK &lt;- 4mu := fnPoMoReversibleMutationRates(K,pi,rho)Alternatively, if we wanted to define a nonreversible mutation rate vector, we could have set mu directly, similar to how we set rho.The function fnPoMoKN will create an instantaneous rate matrix. This function requires that an effective population size be input, but in most cases, you will not know it. Therefore, simply set it to the virtual population size.# rate matrixQ := fnPoMoKN(K,N,N,mu,phi)The tree topology and branch lengths are stochastic nodes in our phylogenetic model. We will assume that all possible labeled, unrooted tree topologies have equal probability. For an unrooted tree topology, we use the nearest-neighbor interchange move mvNNI (a subtree-prune and regrafting move mvSPR could also be used).# topologytopology ~ dnUniformTopology(taxa)moves.append( mvNNI(topology, weight=2*n_taxa) )Next, we create a stochastic node representing the length of each of the 2*n_taxa−3 branches in our tree. We can use a “for” loop to create a vector of branch lengths and assign a move to it.# branch lengthsfor (i in 1:n_branches) {   branch_lengths[i] ~ dnExponential(10.0)   moves.append( mvScale(branch_lengths[i]) )}Finally, we combine the tree topology and branch lengths using the treeAssembly function, which applies the value of the ith member of the branch_lengths vector to the branch leading to the ith node in the topology. Thus, the psi variable is a deterministic node:psi := treeAssembly(topology, branch_lengths)We have now fully specified all of the parameters of our phylogenetic model:  the tree with branch lengths psi;  the PoMo instantaneous rate matrix Q;  the type of character data: i.e., PoMo.Collectively, these parameters comprise a distribution called the phylogenetic continuous-time Markov chain, and we use the dnPhyloCTMC function to create this node. This distribution requires several input arguments:sequences ~ dnPhyloCTMC(psi,Q=Q,type=\"PoMo\")Once the PhyloCTMC model is created, we can attach our sequence data to the tip nodes of the tree. Although we assume that our sequence data are random variables, they are realizations of our phylogenetic model. For inference, we assume that the sequence data are clamped to their observed values.sequences.clamp(data)When this function is called, RevBayes sets each of the stochastic nodes representing the tree’s tips to the corresponding nucleotide sequence in the alignment, indicating that those sequences have been observed.Finally, we wrap the entire model in a single object. To do this, we simply pass model function one of the nodes previously defined.pomo_model = model(pi)Setting, running, and summarizing the MCMC simulation For our MCMC analysis, we need to set up a vector of monitors to record the states of our Markov chain. First, we will initialize the model monitor using the mnModel function. This creates a monitor variable that will output the states for all model parameters when passed into an MCMC function. We will sample every 10th generation, and the resulting file will be found in the output folder.monitors.append( mnModel(filename=\"output/great_apes_pomothree.log\", printgen=10) )The mnFile monitor will record the states for only the parameters passed as arguments. We use this monitor to specify the output for our sampled trees and branch lengths. Again, we sample every 10th generation.monitors.append( mnFile(filename=\"output/great_apes_pomothree.trees\", printgen=10, psi) )Next, we create a screen monitor that will report the states of specified variables to the screen using mnScreen. This monitor helps us track the progress of the MCMC run.monitors.append( mnScreen(printgen=10) )With a fully specified model, a set of monitors, and a set of moves, we can now set up the MCMC algorithm that will sample parameter values in proportion to their posterior probability. The mcmc function will create our MCMC object. Additionally, we will perform two independent MCMC runs to ensure proper convergence and mixing.pomo_mcmc = mcmc(pomo_model, monitors, moves, nruns=2, combine=\"mixed\")Now, we can start the MCMC run.pomo_mcmc.run( generations=100000 )Once the analysis is complete, you will find the monitored files in your output directory. Software like Tracer allows you to evaluate convergence and mixing. Look at the file output/great_apes_pomothree.log in Tracer. There, you will see the posterior distribution of the continuous parameters. Let us examine the posterior distribution of the GC-bias rate $\\gamma$. Is there any evidence of GC-bias in these great ape sequences?Left: Posterior distribution of the great apes GC-bias rate ($\\gamma$) under a PoMoThree model. Right: Trace of the GC-bias rate ($\\gamma$) samples for one MCMC run. You will also notice that the effective sample size is much larger than 200.In addition to continuous parameters, we also need to summarize the trees sampled from the posterior distribution. RevBayes can summarize the sampled trees by reading in the tree trace file:trace = readTreeTrace(\"output/great_apes_pomothree.trees\", treetype=\"non-clock\", burnin= 0.2)The mapTree function will summarize the tree samples and write the maximum a posteriori (MAP) tree to the specified file. The MAP tree can be found in the output folder.mapTree(trace, file=\"output/great_apes_pomothree_MAP.tree\" )Maximum a posteriori estimate of the great ape phylogeny under the PoMoThree model. The numbers next to each node represent posterior probabilities. We have rooted the tree using the orangutans clade. The inferred tree does not align with our expectations for the great ape species tree, and some clades show low posterior probabilities. This is due to our multiple sequence alignment containing only 1000 sites.You can look at the file output/great_apes_pomothree_MAP.tree and open it in FigTree. The maximum a posteriori estimate of the great ape phylogeny under the PoMoThree model should look like that of .We note that while visual inspection might be a good exercise to evaluate the convergence and mixing of the MCMC samples, quantitative methods exist and are recommended. These are implemented in R and ready to use; check tutorial Convergence assessment.Some questions      What is the GC-bias rate (this is the selection coefficient) for the great ape populations? Rescale it to its real value by assuming the great apes have an effective population size of about 10 000 individuals. Use the relation $(1+\\gamma’)^{N-1}=(1+\\gamma)^{N_e-1}$ to rescale $\\gamma$, where $N$ and $N_e$ represent the virtual and effective population sizes, and $\\gamma’$ and $\\gamma$ are the GC-bias rates for the virtual and effective populations.        Using  as your guide, draw the probabilistic graphical model of the neutral PoMoTwo model.        What changes are necessary in the great_apes_pomothree.Rev file to make inferences under the neutral PoMoTwo model?        Run an MCMC analysis to estimate the posterior distribution under the PoMoTwo model. Are the resulting estimates of mutation rates (base frequencies and exchangeabilities) equal? If not, how much do they differ?        Compare the MAP trees estimated under PoMoTwo and PoMoThree. Are they equal? If not, how much do they differ?  ",
        "url": "/tutorials/pomos/",
        "index": "true"
      }
      ,
    
      "tutorials-coalescent": {
        "title": "Coalescent Analyses",
        "content": "OverviewThis tutorial describes how to run a demographic analysis using the coalescent process in RevBayes.Demographic inference is about estimating population dynamics and in this tutorial, we will specifically focus on population size estimation.The coalescent process provides a flexible way of estimating population size trajectories through time.We are primarily interested in asking:(1) what the population size for our given population was?and (2) how population sizes have changed over time, e.g., increased or decreased towards the present or experienced a bottleneck.The input data are usually sequence alignments or estimated trees.In case of sequence data, trees and population size parameters will be jointly estimated.Here, we consider different types of analysis, starting with a constant demographic history and doing more complex analyses in later exercises.The first analyses will consider data from isochronous samples, i.e., data from samples that have all been collected at the same time.In the second part of the exercises, you will be asked to also analyze data from heterochronous samples, i.e., samples that have been collected at different points in time.The data for these analyes are taken from Vershinina et al. (2021).The following table provides an overview of different coalescent models. The tutorials column links directly to the respective tutorial with isochronous data. The tutorial on heterochronous data shows how to apply the models to data from different points in time.            Model      Description      Demographic function within intervals      Prior on the population sizes      Interval change-point method      Multiple loci analysis possible?      Tutorial                  Constant      A simple coalescent model with one constant population size.      constant      individually chosen      -      Yes      Constant              Linear      A coalescent model with a linear population size trajectory.      linear      individually chosen      -      Yes                     Exponential      A coalescent model with an exponential population size trajectory.      exponential      individually chosen      -      Yes                     Bayesian Skyline Plot without autocorrelation      A coalescent model with a piecewise constant population size trajectory. The different pieces (intervals) are not correlated. The times of interval change are dependent on coalescent events (“coalescent event based”).      piecewise constant      individually chosen, independent and identically distributed (iid)      coalescent event based      No, only single loci      Skyline              Bayesian Skyline Plot with autocorrelation(Drummond et al. 2005)      A coalescent model with a piecewise constant population size trajectory. The different pieces (intervals) are correlated, each population size is drawn from an exponential distribution with the mean being the previous population size. The times of interval change are dependent on coalescent events (“coalescent event based”).      piecewise constant      $\\theta_i | \\theta_{i-1}$ (exponential distribution with mean $\\theta_{i-1}$)      coalescent event based      No, only single loci      Little description included in SkylineExample script              Extended Bayesian Skyline Plot(Heled and Drummond 2008)      A coalescent model with a piecewise constant or linear population size trajectory. The different pieces are not correlated. The times of interval change are dependent on coalescent events (“coalescent event based”), they are independent and identically distributed (iid). The number of changes and thus the number of intervals is determined by stochastic variable search.      piecewise constant or linear      independent and identically distributed (iid)      coalescent event based - number and position of change points estimated      Yes      Little description included in SkylineExample script              Skyride(Minin et al. 2008)      A coalescent model with a piecewise constant population size trajectory. The different pieces are correlated via a Gaussian Markov Random Field (GMRF) Prior, the degree of smoothing is determined by a precision parameter. The times of interval change are dependent on coalescent events (“coalescent event based”).      piecewise constant      $\\theta_i | \\theta_{i-1}$ (GMRF Prior with precision parameter)      coalescent event based      No, only single loci      Little description included in SkylineExample script              Skygrid(Gill et al. 2012)      A coalescent model with a piecewise constant population size trajectory. The different pieces are correlated via a Gaussian Markov Random Field (GMRF) Prior and are equally spaced in time, the degree of smoothing is determined by a precision parameter. The times of interval change are independent from coalescent events, they are independent and identically distributed (iid).      piecewise constant      $\\theta_i | \\theta_{i-1}$ (GMRF Prior with precision parameter)      coalescent event independent - equally sized      Yes      Little description included in GMRFExample script              Gaussian Markov Random Field (GMRF) Prior with interval change points independent from coalescent events  (Faulkner et al. 2020)      A coalescent model with a piecewise constant or linear population size trajectory. The different pieces are correlated via a Gaussian Markov Random Field (GMRF) Prior. The times of interval change are independent from coalescent events, they are independent and identically distributed (iid).      piecewise constant or linear      $\\theta_i | \\theta_{i-1}$ (GMRF Prior)      coalescent event independent, user-specified - here equally sized      Yes      With sequences as input data: GMRFWith trees as input data: GMRF treebased              Horseshoe Markov Random Field (HSMRF) Prior with interval change points independent from coalescent events (Faulkner et al. 2020)      A coalescent model with a piecewise constant or linear population size trajectory. The different pieces are correlated via a Horseshoe Markov Random Field (HSMRF) Prior. The times of interval change are independent from coalescent events, they are independent and identically distributed (iid).      piecewise constant or linear      $\\theta_i | \\theta_{i-1}$ (HSMRF Prior)      coalescent event independent, user-specified - here equally sized      Yes      Little description included in GMRFScript in HSMRF              Skyfish model(similar to Opgen-Rhein et al. (2005))      A coalescent model with a piecewise constant or linear population size trajectory. The number of pieces (intervals) is not fixed. A Poisson Prior is used on the number of interval change points, additional priors need to be defined for population sizes and change points. The number of change points is determined via reversible jump MCMC (rjMCMC).      piecewise constant or linear      independent and identically distributed (iid)      coalescent event independent, number and position of change points estimated      Yes      Skyfish              Piecewise(similar to Pybus and Rambaut (2002), but with flexible combinations)      A coalescent model with user-defined demographic functions for a user-defined number of intervals. Base demographic functions included in RevBayes are constant, linear and exponential population size trajectories. These can be combined in an arbitrary way, but the most ancient one should always be constant due to computational reasons.      piecewise constant, linear, or exponential; can be different for every interval      individually chosen, independent and identically distributed (iid)      coalescent event independent, user-specified      No, only single loci      Piecewise      Preparation  In order to perform the different analyses in this tutorial, you will need to create a directory on your computer for this tutorial and download a few files.The Data  In the tutorial directory on your computer, create a subdirectory called “data” and download the data files that you can find on the left of this page.You should now have the following files in your data folder:      horses_isochronous_sequences.fasta: an alignment in FASTA format, containing sequences of 36 horse taxa, all sampled at the same time.        horses_heterochronous_sequences.fasta: an alignment in FASTA format, containing sequences of 173 horse taxa, sampled at various times.        horses_heterochronous_ages.tsv: a tab seperated table listing the heterochronous horse samples and their ages. For extant taxa, the minimum age is 0.0 (i.e. the present).  The ScriptsIt is useful to create .Rev scripts for the different analyes.We will also provide a full script in every tutorial that you can easily run for the whole analysis.  Please create a “scripts” directory in the tutorial directory.The FiguresThe R package RevGadgets provides functionality for plotting RevBayes results.In each exercise, you can plot the population size trajectories resulting from the different analyses.  If you want to plot the results from the exercises, please create a “figures” directory in the tutorial directory and install the R package RevGadgets.Exercises  You can begin by clicking on the first exercise!If you want to turn back to this page, it is listed on the left as prerequisite in all the exercises.The exercise for heterochronous data is based on the exercises for isochronous data.It aims at highlighting the changes you have to make when considering samples with different ages.It is therefore recommended to do the exercises for isochronous data first.The first five exercises work with isochronous data, the last one with heterochronous data:  The Constant coalescent model  The Skyline model  The Gaussian Markov Random Field (GMRF) model  The Skyfish model  The GMRF model with trees as input data  A piecewise model  Heterochronous dataSummaryAfter doing all the exercises, you can compare the resulting population sizes.Have a look at the summary.",
        "url": "/tutorials/coalescent/",
        "index": "true"
      }
      ,
    
      "tutorials-quick-tips": {
        "title": "Quick Tips for Analyses in RevBayes",
        "content": "Getting StartedRevBayes comes with a suite of functions and analyses, each with their own idiosyncracies and nuances. Here you will find an assortment of tips and guides that are either of general use or topics that are not explicitly covered in tutorials. Topics are listed the lefthand side under the table named Overview.More in depth tutorials for most specific models in RevBayes can be found on  the Tutorials page on the RevBayes site. If you are just beggining with RevBayes, we would recommend the Getting Started for familiarizing yourself with the syntax and language and the Intro to MCMC for the basics of creating a model and running MCMC for inference.Additionally, if you ever want to learn more about a specific function in RevBayes, you can either look at the Documentation page on the website or type ? before any function in the RevBayes terminal (e.g., ?sqrt()) to get information about the function purpose, arguments, and output.MovesGiven infinite time, any MCMC proposal scheme will converge on the posterior distribution. However, since time is finite, we need to carefully consider the moves we chose on parameters to efficiently approximate the sampling distribution. In this section we will discuss how to identify inefficient moves and poor mixing as well as the components of a move scheme that can be modulated to increase MCMC effectiveness. Further convergence diagnostics can be found in the Convergence Assessment tutorial.Choosing and Optimizing MovesIn this section we will discuss the things you will want to consider when choosing moves for your MCMC and how we can optimize those moves to efficiently sample the posterior.Choosing the Right Type of MoveFor most types of parameters there are a plethora moves implemented in RevBayes. When choosing moves it is important to consider the size and scale of the move relative to the parameter space that they operate upon. For example, let us consider the binomial coin-flipping scenario posed in Introduction to MCMC using RevBayes tutorial where we are trying to estimate the probability of a coin landing on heads, $p$. In this case we know that the parameter $p$ is bounded between $0$ and $1$ so we would want to consider moves that efficiently move around this space. The move mvScale is a valid option for our parameter $p$ but it won’t move around the space very effectively as this move multiplies the current value by some scalar which can often propose values on a different order of magnitude. Alternatively, we could chose mvSlide to propose new values within some window from the current value; this move is more better for proposing values between 0 and 1. Since moves often act in drastically different ways and on various scales, it can be useful to use multiple different types of moves on the same parameter to search space efficiently.The Size of MovesAlthough we have chosen an appropriate type of move we still need to consider the size of the move itself. In the case of mvSlide the function for the move has the parameter delta which is used to specify the size of the window around the current value $p$. In other words, the move proposes a new value $p’$ by choosing a value at random on the interval $(p-\\delta,p+\\delta)$. Large values of delta will result in proposals that often fall outside of the interval $(0,1)$ while too small of a delta will cause the MCMC to explore the parameter space slowly and inefficiently. Most moves on continous variables have parameters that control the relative size of the move, in the case of mvSlide, the lambda parameter controls the size of the scalar.We can qualitatively assess the adequacy of size parameters of moves by using TRACER to view the trace. In  we can see an example of a well-mixing MCMC, the catapillar-like appearance is a qualitative sign that the parameter is efficiently moving around the parameter space. If the move is too large the trace will look blocky, almost like a city skyline. Large moves often cause proposals to be rejected which is why we see the trace having the same value for many generations. Conversely, if we set too small of a move then we will accept most moves and the trace will appear to slowly meander about parameter values.Mixing of the mvSlide move on  sampling the probability of flipping heads $p$. The left image depicts the trace when moves are too small. The image in the center depicts moves that are too big. The image on the right depicts moves that are just right.We can directly see how often proposals on specific moves are accepted or rejected by using the operatorSummary() method on an mcmc object (see ).Output of the operatorSummary method of an mcmc object after performing an analysis. On the leftmost column we can see each move and the node that it operates on. We can also see the weight of each move, how often it was proposed, and how often it was accepted. On the far right we can see the tuning argument for each move if it has one, in the case of mvSlide this is deltaWe can see that almost every proposal was accepted for the move with the smallest window size while the largest move rejected most proposals. In general we want a move that isn’t too small such that it moves slowly but isn’t so large that it rejects most proposals, this is known as the Goldilock’s Principle. Roberts et al. (1997) found an optimal acceptance ratio of $0.234$ for a multivariate target distributions with i.i.d. components. Although being able to break the posterior into i.i.d. components is unrealistic for phylogenetic analyses, numerical studies have shown acceptance rates to be robust to this assumption and rates between $0.1$ and $0.6$ are still reasonably efficient (Roberts and Rosenthal 2001; Rosenthal 2011).Move sizes on treesThe two most common moves on tree topology are the Nearest Neighbor Interchange (NNI) move and the Subtree Pruning and Regrafting (SPR) move. The NNI move rearanges the connectivity between four subtrees while SPR moves prune a subtree and regraft it on another part of the tree . Since these moves define specific opertations on the phylogeny, they do not have arguments to adjust the size of the moves. Instead, it may be useful to think of the moves themselves as ‘big’ or ‘small’ based on their inherent qualities.One way to assess the size of these topology moves is to consider how many different topologies we can obtain by performing one move. This often speaks to how interconnected tree topologies are given a specific move. If many topologies are connected by a move then we can think of this as being able to move from one topology to another in a fewer number of moves and would be considered a large move. In this sense we would consider NNI moves to be considerably smaller than SPR moves; NNI moves on a given internal branch has only 2 other alternative topologies while an SPR move on a given internal branch can be pruned and regrafted to any other branch on the phylogeny, making it connected to more topologies.The other way we may want to evaluate the relative size of moves is by considering the diameter of the tree space of a move (St. John 2016). The diameter of a tree space for a given move is defined the maximal distance between any two topologies. Although it is NP-hard to compute the diameter of a tree space, the space of SPR moves has a tighter upper bound on the diameter than NNI moves. This means that generally we can reach trees in fewer moves with the “bigger” SPR moves than the “smaller” NNI moves.Two common tree topology moves. Left NNI. Right SPR.Tuning MovesLuckily, for moves with an adjustable size, we don’t need trial and error adjusting of that size argument to achieve a certain acceptance rate. We can tune our moves to achieve a certain acceptance ratio. When creating a move that has an adjustable size, we can set the tune argument to TRUE, this will adjust the size of the move so the acceptance rate approaches the value given in tuneTarget. Before running our MCMC analysis, we can tune our parameters by using the tuningInterval argument in  either the burnin or run methods on an mcmc object (see ). This means that every tuninginterval MCMC generations, it will try to adjust the size of the move to reach the desired tuning interval.Output of operatorSummary after tuning the moves for $100,000$ generations. The moves in this image started with the same size values as .  We can notice that the value delta has is different from when it started and the acceptance rate for each move approaches 0.44 which is the default tuning targetCreating a Move SchemeAfter we’ve chosen the moves we want, we need to specify how often those moves get called and how they are scheduled. First, for any function that creates a move, there is an argument called weight. Although the specific details vary between which move scheduler is used, the weights correspond to how often the move gets used.Given finite resources, we may want to upweight or downweight certain nodes to focus our resources. Analyses may contain nuisance parameters, or parameters we aren’t particularly interested in estimating; we could downweight these to spend more time ensuring our parameters of interest are well sampled. Additionally, we may want to upweight parameters that are complex or difficult to sample, this is often done for moves regarding the tree topology. We know that the number tree topologies grow for a given number of taxa, specifically for $n$ taxa there are $(2n-3)!!$ different rooted topologies (NOTE: we are using the double factorial function, not the factorial function used twice). Since the space of tree topologies grows dramatically with the number of taxa, we may want to upweight moves on the tree topology accordingly. In practice we usually set the weight for topology moves to be proportional to the number of tips on the tree, though even this is a conservative scaling for the weight relative to the size of tree topology spaceWe can set up a move schedule that determines the order of moves with the moveschedule argument of the mcmc function. There are $3$ different options for moveschedule:  sequential: Each MCMC cycle moves get performed in the order that they are entered into the move vector. We perform each move a number of times that corresponds to the weight of that move. For example, if we specify a weight of $4.35$ on a move then that move will be performed $4$ times garuanteed and then there is $0.35$ probability of the move being performed a $5^{th}$ time. After that, the scheduler moves on to the next move in the moves vector  single: This scheduler only considers one move each MCMC cycle. A move gets picked at random based on the weights. Moves with higher weights relative to the other moves in the move vector will be picked more often.  random: This scheduler is similar to the single move scheduler except that multiple moves get picked at random each cycle. The number of moves each cycle is the sum of the wieghts for all the moves in the move vector.Multiple Independent RunsThere are a variety of scenarios where it may be helpful to run an analysis multiple times independently. First, one way of assessing whether our MCMC converged on the posterior distribution is by looking at multiple independent runs and asking how similar they are to one another. The Convergence Assessment tutorial, in particular, makes use of multiple runs.  It can also be helpful to have multiple runs for the purposes of good mixing and reducing autocorrelation between samples.We can run multiple analyses by adjusting the nruns argument of an mcmc() function. If we only adjust the nruns argument and perform an analysis with the run() method then will notice our MCMC generates multiple output log files. We can see that each run in our multiple run log outputs get saved in the style MCMCoutput_run_x.log.###We assume we already created these objects from the Binomial MCMC tutorialmy_model \t##The modelmy_moves \t##The vector of moves for the parameters we are inferrringmy_monitors ##The vector of monitors used to record our MCMC analysis##create an mcmc objectmy_mcmc = mcmc(my_model, my_moves, my_monitors,nruns=2)Files generated from running the code above with multiple runs.Alternatively, if we want to save our MCMC outputs in one file, we can change the combine argument in the mcmc() function. If the combine argument is used then each run will save in the same file but will generate an additional column titled Replicate_ID that denotes which run created each observation. combine can take the values \"sequential\" or \"mixed\".Metropolis Coupled MCMCOne common concern with Bayesian phylogenetic inference is being entrapped in a local optimum. Posterior distributions can be multimodal and efficiently moving between optima during MCMC is a difficult task, even with a good proposal scheme.Metropolis Coupled Markov Chain Monte Carlo is a common technique in phylogenetic analysis to efficiently move around a parameter space with local optima (Altekar et al. 2004). Briefly, this technique has mutiple MCMC chains running in parallel, with one ‘cold’ chain that records operates as a normal MCMC,recording parameter values at prespecified intervals, and multiple ‘hot’ chains that effectively have a flattened posterior distribution. These hot chains are less likely to reject move proposals, making it easier for them to move into low posterior density valleys to move between local optima. Periodically, the chains will swap their ‘heating’ and current parameter values. The idea here is that the ‘hot’ chains can efficiently move around large swathes of parameter space and ocassionally swap values with the ‘cold’ chain as it comes across different optima. Formally, a heating value refers to the posterior distribution being raised to some power $\\beta$, where lower values of $\\beta$ effectively flatten out the posterior distribution.We can use the mcmcmc() function in RevBayes to perform Metropolis Coupled Markov Chain Monte Carlo. This function is very similar to the mcmc() function in terms of arguments and associated methods but contains a few extra arguments that focus around the multiple parallel chains.Those additional arguments are:  nchains: The number of chains to run in parallel.      swapInterval: This is how often chains attempt to swap their heats    deltaHeat: a numeric that specifies the heat of each chain. If there are $i$ chains then the heat of the $i^{th}$ chain is denoted as $\\frac{1}{1+deltaHeat*i}$. Here we can see that the heat $\\beta$ decreases with each subsequent chain, making them ‘hotter’.  heats: A vector that explicity defines the heat of each chain. The first element must be $1.0$ (it is the cold chain) and the length of the vector must match nchains.NOTE: Only one of deltaHeat or heats are required for mcmcmc(). These arguments are used to specify the heat $\\beta$ of each chain. For example, for a chain with nchains=4, setting either deltaHeat=0.5 or heats=[1, 2/3, 1/2, 2/5] would be equivalent.After making our mcmcmc object, we can use the operatorSummary() method to inspect the heats of each chain ().Partial output of the operatorSummary() method of mcmcmc objects. We can see each chain with their respective heats and proposed moves.  tuneHeat: A boolean specifying whether the heats $\\beta$’s should be tuned during burnin.  tuneHeatTarget: The acceptance probability of adjacent chain swaps targeted by heats auto-tuning.NOTE: Much like how moves can be tuned to achieve ideal acceptance proportions, the heats can be adjusted so swapping between chains at some ideal proportion.Specifically, if tuneHeat is TRUE then during burnin the heats will tune such that the proportion of successfully swapping the heats between any two adjacent chains is tuneHeatTarget. we can use the operatorSummary() method on the mcmcmc ().Partial output of the operatorSummary() method of mcmcmc objects. Each potential chain swap is listed along with the number of times each type of swap was proposed and accepted.  SwapMethod: The method at which chain swaps are proposed. Valid options are:          \"neighbor\": Swaps for $i^{th}$ chain will be proposed to the $i\\pm 1$ chains.      \"random\": Swaps between chains will be proposed at random      \"both\": both \"neighbor\" and \"random\" type chain swaps will be proposed.        swapInterval2: This argument is only used if swapMethod=\"both\". In that case then swapInterval denotes how often chains attempt swaps using the neighbor method while swapInterval2 denotes how often chains attempt swaps using the random method. If no argument is provided for swapInterval2 then both methods will use the same interval provided in swapInterval.  swapMode: A string of either \"single\" or \"multiple\", that denotes how many chain swaps are proposed at each interval. If swapMode=\"multiple\" then $(nchains-1)$ swaps will be proposed for nieghbor type chain swaps and ${nchains \\choose 2}$ swaps for random type chain swaps.###We assume we already created these objectsmy_model \t##The modelmy_moves \t##The vector of moves for the parameters we are inferrringmy_monitors ##The vector of monitors used to record our MCMC analysis# Create an mcmcmc objectmyMcmcmcObject = mcmcmc( mymodel, monitors, moves, nchains=4, deltaHeat=5)# print the summary of the operators (now tuned)##In particular take note of the chain heats before tuningmyMcmcmcObject.operatorSummary()# Tune heatsmyMcmcmcObject.burnin( generations = 40000, tuningInterval = 100)# print the summary of the operators (now tuned). heats should have changedmyMcmcmcObject.operatorSummary()MCMC under the PriorBefore we perform our phylogenetic analysis using our data, we may first want to run the MCMC under the prior. Running our analysis under the prior ignores all of our data and likelihood computations associated with our data and uses prior distributions to copmute the sampled posterior. Since we are not performing any likelihood calculations the MCMC should sample relatively quickly, making this a good way to assess the efficiency of our move scheme. This can also be a good way to ensure the model was set up correctly and gauge how long we will need to run our MCMC. As a result of not using any data when running an analysis under the prior, our sampled posterior should match the distributions we used for our prior beliefs. When running an analysis in this way, if our posterior looks nothing like our prior we can either say that we haven’t ran the analysis long enough or the model may be set up incorrectly.Assuming that we’ve already set up our model, moves, and monitors in RevBayes, we can run an analysis under the prior by calling my_model.ignoreAllData() to mark all clamped data as ignored:###We assume we already created these objectsmy_model \t##The modelmy_moves \t##The vector of moves for the parameters we are inferrringmy_monitors ##The vector of monitors used to record our MCMC analysis##Mark data in the model as ignoredmy_model.ignoreAllData()##create an mcmc objectmy_mcmc = mcmc(my_model, my_moves, my_monitors) ##Perform an analysis (this will sample from the prior)my_mcmc.run(generations = 10000)",
        "url": "/tutorials/quick_tips/",
        "index": "false"
      }
      ,
    
      "tutorials-stairwayplot": {
        "title": "StairwayPlot Analyses",
        "content": "  For your info  This tutorial and the included exercises are currently under construction.If you want to run the analyses, please compile RevBayes from the dev_stairwayplot branch as described here (for the development branch). The code should hopefully be available in the next release (RevBayes v1.2.6).OverviewThis tutorial describes how to run a demographic analysis using the StairwayPlot approach (Liu and Fu 2015; Liu and Fu 2015) in RevBayes (Höhna et al. 2016).The StairwayPlot approach assumes an underlying coalescent for a single panmictic population and an infinite sites model.Population sizes are allowed to vary between coalescent events but assumed to be constant in between, thus producing the characteristic stairway shape.As data, the StairwayPlot approach uses the site frequency spectrum (SFS), or allele frequency spectrum, and assumes that SNPs are unlinked, i.e., have their own coalescent history.For a detailed description we refer the reader to Höhna and Catalán (2025).The data for these analyses are taken from (missing reference).More specifically, the data consists of a folded site frequency spectrum of the Munich population from the big European firefly Lampyris noctiluca.Preparation  In order to perform the different analyses in this tutorial, you will need to create a directory on your computer for this tutorial and download a few files.The ScriptsIt is useful to create .Rev scripts for the different analyses.We will also provide a full script in every tutorial that you can easily run for the whole analysis.  Please create a “scripts” directory in the tutorial directory.The DataThe data for a StairwayPlot analysis are the site frequency spectrum.For this tutorial, we assume that the site frequency spectrum was already computed.We also refer to our own tutorial (currently in preparation) for obtaining an SFS from a VCF file.In general, it is best to compute first the SFS so that it can be used efficiently in several analyses without extracting it each time from the VCF.The SFS is simply specified as a vector of counts:obs_sfs = [ 405017549, 1393598, 922300, 682542, 530181, 421015, 360546, 303107, 257549, 218569, 197952, 172446, 161453, 145816, 136546, 123085, 121036, 114119, 116561, 113349, 65829 ]We obtained the data of the Munich population from the big European firefly Lampyris noctiluca from (missing reference), and you can see it plotted in .Here we use the folded site frequency spectrum from 20 diploid individuals, thus the size of the folded site frequency spectrum is 21 and the first entry represents the fixed sites.Unfolded and folded site frequency spectrum of the big European firefly, Lampyris noctiluca. These represent 20 sampled diploid individuals. Data are obtained from (missing reference).We will also use some variables from the data, the number of haploid individuals N_IND and the total sequence length N_SITESN_IND   = 2*abs(obs_sfs.size()-1)N_SITES = round(sum(obs_sfs))Folding the SFSIf you have a full SFS but need to fold it, you can also easily do this in RevBayes:tmp_sfs[1] &lt;- obs_sfs[1] + obs_sfs[N_IND+1]for (i in 1:ceil(N_IND/2)) {  if ( i != N_IND/2 ) {    tmp_sfs[i+1] &lt;- obs_sfs[i+1] + obs_sfs[N_IND-i+1]  } else {    tmp_sfs[i+1] &lt;- obs_sfs[i+1]  }}obs_sfs &lt;- tmp_sfsA Bayesian StairwayPlot AnalysisInitializing Global VariablesIn our RevBayes scripts we need to use some global variables, specifically a vector for ‘moves’ and ‘monitors’.Remember that moves are the operators/algorithms that propose and change the stochastic variables in a model based on the Metropolis-Hastings algorithm (Metropolis et al. 1953; Hastings 1970).Monitors are used to either store samples of variables in files or print them to the screen.The monitors give you the flexibility which variables to store into which file for specific post-processing.moves    = VectorMoves()monitors = VectorMonitors()Demographic Model (Prior on Population Sizes)The different priors on the population sizes are the main feature of the Bayesian StairwayPlot compared with the Maximum Likelihood implementation (Liu and Fu 2020; Höhna and Catalán 2025).The StairwayPlot approach allows to specify one population size parameter for each interval between coalescent events, that is, for a dataset with n individuals (more specifically bins in the unfolded SFS, thus assuming haploid individuals) you can specify n-1 population size parameters.There are various different ways how to specify priors on population size parameters, and we will primarily focus here on the independent and identically distributed (iid) prior.Other options, such as the Gaussian Markov Random Field (GMRF) and the Horseshoe Markov Random Field (HSMRF) are provided as additional extensions.The very first step is to initialize a multivariate-normal move (Baele et al. 2017).This move proposes new population sizes for the entire vector (all instead of a single parameter) based on a multivariate normal distribution.Since posterior samples of the population size parameters are likely to be correlated, either positively or negatively, this multivariate normal distribution learns first the correlation matrix in an initial learning phase by taking samples from the current MCMC.avmvn = mvAVMVN(weight=50)Next, we will specify each independent population size parameter.Remember that the StairwayPlot approach actually uses the parameters $\\theta_{k} = 4\\times N_{k}\\times \\mu$.Thus, we need to specify prior distributions on $\\theta_{k}$ instead of $N_{k}$, and we will transform the $\\theta$ later into effective population sizes $N$.In the simplest model, we will assume that each $\\theta_k$ has a uniform prior distribution, $\\theta_k \\sim \\text{Unif}(0,0.1)$, but any other prior that is defined for positive real numbers, such as the Exponential, Lognormal, and Gamma distribution.# now specify a different theta per intervalfor (k in 1:(N_IND-1)) {  # draw theta from a uniform prior distribution  theta[k] ~ dnUnif( 0.0, 0.1 )  # set a scaling move for this variable  moves.append( mvScale(theta[k], weight=5) )  avmvn.addVariable(theta[k])}In this for loop you saw how we specified the iid-uniform prior.In some cases, it also might be helpful to specify starting values, for example with theta[i].setValue( 0.05 ).You also should have noticed that we applied a scaling move (mvScale) for each $\\theta_k$ parameter with a weight of 3, thus proposing on average 3-times a new value per MCMC iteration/cycle.We also added each $\\theta_k$ to our multivariate normal move, which we finally add to our vector of moves.moves.append( avmvn )As alternative prior distributions on the population size parameters, we provide several examples here.You can simply replace the part above with alternative priors.Gaussian Markov Random Field (GMRF, first and second order)The Gaussian Markov Random Field (GMRF) priors specify normal distributions as shrinkage priors on the population size parameters.The population size of the next interval is drawn from a normal distribution with mean being equal to the population size in the previous interval.Note that the variation depends on a global standard deviation parameter.The entire code block including moves is given by:avmvn = mvAVMVN(weight=100)log_theta_at_present ~ dnUniform(-20,2)log_theta_at_present.setValue( ln( 0.001 ) )moves.append( mvSlideBactrian(log_theta_at_present,weight=5) )avmvn.addVariable(log_theta_at_present)## Global shrinkage parameterglobal_scale_hyperprior     &lt;- 0.021# Global-scaled variances for hierarchical horseshoeglobal_scale ~ dnHalfCauchy(0,1)# Make sure values initialize to something reasonableglobal_scale.setValue(runif(1,0.005,0.1)[1])# moves on the global scalemoves.append( mvScaleBactrian(global_scale,weight=5.0) )# now specify a different theta per intervalfor (i in 1:(N_IND-2)) {  # non-centralized parameterization of horseshoe  delta[i] ~ dnNormal( mean=0, sd=global_scale*global_scale_hyperprior )  # Make sure values initialize to something reasonable  delta[i].setValue(runif(1,-0.1,0.1)[1])  # set a sliding move for this variable  moves.append( mvSlideBactrian(delta[i], weight=5) )  avmvn.addVariable(delta[i])  if ( i &gt; 1 ) {    move_up_down_delta[i-1] = mvUpDownSlide(weight=3)    move_up_down_delta[i-1].addVariable( delta[i], up=TRUE )    move_up_down_delta[i-1].addVariable( delta[i-1], up=FALSE )    moves.append( move_up_down_delta[i-1] )  }}# Assemble first-order differences and speciation_rate at present into the random fieldtheta := fnassembleContinuousMRF(log_theta_at_present, delta, initialValueIsLogScale=TRUE, order=1)# joint sliding moves of all vector elementsmoves.append( mvVectorSlide(delta, weight=10) )moves.append( avmvn )Note that you can switch between first and second order GMRF simply by specifying order=1 or order=2 in fnassembleContinuousMRF(log_theta_at_present, delta, initialValueIsLogScale=TRUE, order=1)Horseshoe Markov Random Field (HSMRF, first and second order)The Horseshoe Markov Random Field (HSMRF) priors specify normal distributions as shrinkage priors on the population size parameters similar to the GMRF.However, instead of using a global standard deviation parameter, each interval receives its own standard deviation parameter.The entire code block including moves is given by:avmvn = mvAVMVN(weight=100)log_theta_at_present ~ dnUniform(-20,2)log_theta_at_present.setValue( ln( 0.001 ) )moves.append( mvSlideBactrian(log_theta_at_present,weight=5) )avmvn.addVariable(log_theta_at_present)## Global shrinkage parameterglobal_scale_hyperprior     &lt;- 0.021# Global-scaled variances for hierarchical horseshoeglobal_scale ~ dnHalfCauchy(0,1)# Make sure values initialize to something reasonableglobal_scale.setValue(runif(1,0.005,0.1)[1])# moves on the global scalemoves.append( mvScaleBactrian(global_scale,weight=5.0) )# now specify a different theta per intervalfor (i in 1:(N_IND-2)) {  # non-centralized parameterization of horseshoe  delta[i] ~ dnNormal( mean=0, sd=global_scale*global_scale_hyperprior )  # Make sure values initialize to something reasonable  delta[i].setValue(runif(1,-0.1,0.1)[1])  # set a sliding move for this variable  moves.append( mvSlideBactrian(delta[i], weight=5) )  avmvn.addVariable(delta[i])  if ( i &gt; 1 ) {    move_up_down_delta[i-1] = mvUpDownSlide(weight=3)    move_up_down_delta[i-1].addVariable( delta[i], up=TRUE )    move_up_down_delta[i-1].addVariable( delta[i-1], up=FALSE )    moves.append( move_up_down_delta[i-1] )  }}# Assemble first-order differences and speciation_rate at present into the random fieldtheta := fnassembleContinuousMRF(log_theta_at_present, delta, initialValueIsLogScale=TRUE, order=1)# joint sliding moves of all vector elementsmoves.append( mvVectorSlide(delta, weight=10) )moves.append( avmvn )Note that you can switch between first and second order HSMRF simply by specifying order=1 or order=2 in fnassembleContinuousMRF(log_theta_at_present, delta, initialValueIsLogScale=TRUE, order=1)StairwayPlot DistributionNow comes the main component of the Bayesian StairwayPlot analysis: the StairwayPlot distribution.We implemented this distribution in simple but flexible way (Höhna and Catalán 2025) (note that we refer here to the likelihood in Höhna and Catalán (2025) to match exactly our implementation although the likelihood is, except for conditioning, the same as in Liu and Fu (2015)).Our StairwayPlot distribution (dnStairwayPlot), which specifies the StairwayPlot likelihood (Höhna and Catalán 2025), comes with the following parameters:  theta: The vector of $\\theta$ parameters. This must be a vector of length n-1.  numSites: The total sequence length, which should be the sum of the SFS. This is used for simulation/initialization and to compute the normalizing constant of the likelihood (Höhna and Catalán 2025).  numIndividuals: The number of haploid individuals $n$. This again is used primarily for simulation/initialization and needs to match the size of the observed data.  folded: Whether the data are folded or not (TRUE or FALSE).  monomorphicProbability: Whether to compute the probability of monomorphic sites based on the rest of the probability (rest) or on the tree length (treelength)?  coding: Whether we condition on have all sites observed, or if we didn’t observe monomorphic sites (no-monomorphic), or if we also excluded singletons (no-singletons), i.e., singletons are merged together with monomorphic sites.Several of these parameters have default values.You can see some information using ?dnStairwayPlot.Let us specify a variable sfs drawn from the dnStairwayPlot:sfs ~ dnStairwayPlot( theta, numSites=N_SITES, numIndividuals=N_IND, folded=TRUE, coding=\"all\" )and attach/clamp our observed data to this variablesfs.clamp( obs_sfs )For post-processing and plotting purposes, we need some additional variables, for example the times when the population sizes changed and the effective population size.We obtain the changepoint times as the expected coalescent times from our StairwayPlot distribution, thus using a member function of our variable sfstimes := sfs.getTimes()Note that this variable is a deterministic variable and not a stochastic variable.Next, we transform these times into actual times and actual effective population sizes.Therefore, we need to assume some mutation rate $\\mu$ and some generation time.mu        = 2.8e-09GEN_TIME  = 2Here we assume a mutation rate of $2.8e-09$ (taken from Heliconius melpomene (Keightley et al. 2015)) and a generation time of 2 years (missing reference).Now we can transform the thetaordered_Ne    := rev(theta) / 4.0 / muordered_times := rev(times) / mu * GEN_TIMEWe also want to store the expected site frequency from our model, to compute later the leave-one-out cross-validation score (Lewis et al. 2014; Lartillot 2023)esfs := sfs.getExpectedAlleleFrequencies()Putting it All TogetherWe have fully specified all of the parameters of our Bayesian StairwayPlot analysis.Finally, we wrap the entire model in a single object to provide convenient access to the DAG.To do this, we only need to give the model() function a single node.With this node, the model() function can find all of the other nodes by following the arrows in the graphical model:my_model = model(sfs)Specifying Monitors and Output FilesThe next step for our RevBayes analysis is to specify which variables to monitor and to which files to write them.We want the following monitors:  One monitor that stores all variables of the model into a log-file (mnModel).  One monitor that stores the vector of effective population sizes into a file (mnFile).  One monitor that stores the change points of population sizes into a file (mnFile).  One monitor that stores the expected SFS into a file (mnFile).  One monitor that keeps us informed about the progress on the screen (mnScreen).Overall, we specify that our monitors that write into our files will sample every 10 iterations, thus thinning the MCMC.We could also instead increase the weights of our moves.    monitors.append( mnModel(filename=\"output/StairwayPlot_iid.log\", printgen=10) )monitors.append( mnFile(ordered_Ne, filename=\"output/StairwayPlot_iid_ordered_Ne.log\", printgen=10) )monitors.append( mnFile(ordered_times, filename=\"output/StairwayPlot_iid_ordered_times.log\", printgen=10) )monitors.append( mnFile(esfs, filename=\"output/StairwayPlot_iid_esfs.log\", printgen=10) )monitors.append( mnScreen(printgen=1000) )      Initializing and Running the MCMC SimulationNow we are ready to run our MCMC simulation.The first step is to create the MCMC object.There, we need our model object, our moves vector and our monitors vector.Additionally, we need to specify how many MCMC replicates we perform.Here we perform 4 replicates to check for convergence and reproducibility.We also need to specify whether and how to combine the log-files of the separate replicates.my_mcmc = mcmc(my_model, monitors, moves, nruns=4, combine=\"mixed\")Next, we will run both a pre-burnin phase of 10,000 MCMC iterations, and the actual MCMC simulation with 100,000 iterations.During the pre-burnin, our monitors will not sample value to the files.This is mostly done to tune the moves (we tune them here automatically every 100 iterations) and to move from random or fixed starting values to (hopefully) random samples from the stationary posterior distribution.my_mcmc.burnin( generations=10000, tuningInterval=100 )my_mcmc.run( generations=100000, tuningInterval=200 )After the MCMC is finished, we can check the acceptance rates of our moves to inspect the MCMC performance.my_mcmc.operatorSummary()Model Testing: Leave-one-out cross-validationThe next step of our analysis is to compute the leave-one-out cross-validation score (Lartillot 2023).The LOO-CV gives the probability of observing the data under the model after the parameters have been trained to the data.That is, the LOO-CV gives a prediction of the overall model fit to the data and is more powerful than conventional Bayes factors (Lartillot 2023).We compute the LOO-CV from the conditional posterior ordinates (Lewis et al. 2014; Lartillot 2023).Thus, we create a ConditionalPosteriorOrdinate object with the expected SFS read from our log files for each of the 4 MCMC replicates.Then, we compute the LOO-CV given the actual observed SFS.for (i in 1:4) {   cpo = ConditionalPosteriorOrdinate( filename=\"output/StairwayPlot_iid_esfs_run_\"+i+\".log\" )   pred_prob = cpo.predictiveProbability( obs_sfs, log=FALSE )   \"predictive probability\"   pred_prob}  Finish your RevBayes script and run this Bayesian StairwayPlot analysis with the iid prior on population sizes.The FiguresThe R package RevGadgets provides functionality for plotting RevBayes results.In each exercise, you can plot the population size trajectories resulting from the different analyses.  If you want to plot the results from the exercises, please create a “figures” directory in the tutorial directory and install the R package RevGadgets.Here is a code example how to plot the demographic history for each replicate separately.Since RevGadgets returns ggplot2 objects, you can easily modify the output to your liking.library(RevGadgets)library(ggplot2)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"x.lim &lt;- c(1E2,1E7)y.lim &lt;- c(1E3,1E7)num_grid_points = 500for (run in 1:4) {  population_size_log = paste0(\"output/StairwayPlot_iid_ordered_Ne_run_\",run,\".log\")  interval_change_points_log = paste0(\"output/StairwayPlot_iid_ordered_times_run_\",run,\".log\")  df &lt;- processPopSizes(population_size_log, interval_change_points_log, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points)  p &lt;- plotPopSizes(df) +       ggtitle( paste0(\"Demography of Lampyris noctiluca (Munich) - Rep \", run ) + theme(plot.title = element_text(hjust = 0.5, size=10, face=\"bold.italic\")) +       xlab(\"Time before present (in years)\") + theme(axis.title.x = element_text(size=10, face=\"bold\")) +       scale_x_continuous(trans='log10', limits = x.lim, breaks=c(1E2,1E3,1E4,1E5,1E6,1E7), labels=c(\"1E2\",\"1E3\",\"1E4\",\"1E5\",\"1E6\",\"1E7\")) +       ylab(\"Effective Population Size\") + theme(axis.title.y = element_text(size=10, face=\"bold\")) +       scale_y_continuous(trans='log10', limits = y.lim, breaks=c(1E2,1E3,1E4,1E5,1E6,1E7), labels=c(\"1E2\",\"1E3\",\"1E4\",\"1E5\",\"1E6\",\"1E7\"))  ggplot2::ggsave(filename=paste0(\"figures/StairwayPlot_iid_\",run,\".pdf\"), plot=p, width=4, height=4)}Deomgraphy of the big European firefly, Lampyris noctiluca (missing reference).Here we show 4 MCMC replicates to demonstrate the MCMC convergence.The prior on population sizes was the iid-uniform distribution.Exercises  As an exercise, try different models, such as the GMRF, GMRF2, HSMRF and HSMRF2.",
        "url": "/tutorials/stairwayplot/",
        "index": "true"
      }
      ,
    
      "tutorials-geosse-orig": {
        "title": "State-dependent diversification with GeoSSE",
        "content": "IntroductionIn the previous examples, we used a Cladogenetic State change Speciation and Extinction (ClaSSE) model (Goldberg and Igić 2012) to investigate the evolution of primates. ClaSSE jointly models character evolution and the birth-death process, incorporating both anagenetic and cladogenetic state changes. The GeoSSE model (Goldberg et al. 2011) is a specific type of ClaSSE model that is explicitely designed for geographic range evolution, with particular model assumptions related to the ways that species spread and split. This tutorial gives a step-by-step explanation of how to perform a GeoSSE analysis in RevBayes. We will model the evolution and biogeography of the South American lizard genus Liolaemus using two regions: Andean, and non-Andean (Esquerré et al. 2019).NOTE: Although this tutorial is written for a two-region biogeographic analysis, it is designed to be applicable to analyses involving more regions. In general, we anticipate it should perform well for as many as eight regions (255 distinct ranges) or more with additional optimizations.The GeoSSE modelAn example tree showing GeoSSE event types: within-region speciation (w), extinction (e), between-region speciation (b), and dispersal (d).In the GeoSSE model, lineage “states” represent possible geographic ranges, comprised of one or more discrete regions. For example, in a two-region scenario, there are three possible ranges: A, B, and AB. Lineages split and transition between these states according to four core processes: within-region speciation, local extinction (extirpation), between-region speciation, and dispersal. The model is constrained such that a lineage can only experience a single event at any instant in time.Within- and between-region speciation are cladogenetic processes that create new phylogenetic lineages, which may inherit ranges that differ from the ancestral species. In a within-region speciation event, one daughter lineage inherits the entire ancestral range (which might consist of one or more regions), and the other daughter inherits a single region from the ancestral range. In a between-region speciation event, the widespread ancestral range (of two or more regions) is subdivided and inherited by two new daughter lineages. Between-region speciation rates are always symmetric (separation between A and B is the same as separation between B and A). The standard GeoSSE model does not allow for other kinds of speciation events. For example, an ancestor with a widespread range (of two or more regions) cannot produce daughters that both possess the entire ancestral range (a widespread sympatry scenario).Extinction and dispersal are anagenetic processes, occurring along the branches of an evolutionary tree. Extinction occurs locally within a single region; there is no separate parameter for global extinction, and lineages can only experience one local extirpation event at a time. Therefore, a widespread lineage can only go extinct by losing each of its regions individually until one remains, then losing that last region. Lineages also disperse by adding individual regions to their ranges, and dispersal rates into a new region are the sum of pairwise dispersal rates from each starting region into the new region.The GeoSSE model allows each region or region pair to possess its own rate for each process. For example, the within-region speciation rate for region A may not necessary equal the within-region speciation rate for region B. Similarly, the dispersal rate from A to B does not necessarily equal the dispersal rate from B to A. When constructing the GeoSSE model, each rate will be represented with its own parameter. We will represent these rates with the following vectors and matrices: $r_w$ for the vector of within-region speciation rates, $r_e$ for  the vector of extinction rates, $r_b$ for the matrix of between-region speciation rates, and $r_d$ for the matrix of dispersal rates.Transition diagram for the GeoSSE model with two regions, based on Figure 1 from (Goldberg et al. 2011). Anagenetic processes are represented with dashed arrows, while cladogenetic processes are represented with solid arrows.Setup  Important version info!!  This tutorial is the first in a series of lessons explaining how to build increasingly powerful but computationally demanding GeoSSE-type models for biogeographic analyses. Inference under these models is powered by the Tensorphylo plugin for RevBayes, located here: bitbucket.org/mrmay/tensorphylo/src/master (May and Meyer).This tutorial, and following tutorials for GeoSSE-type models, will also require a development version of RevBayes built from the hawaii_fix branch (this message will be removed when the branch is merged).As an alternative to building the development version of RevBayes and installing Tensorphylo, you can instead use the RevBayes Docker image, which comes pre-configured with Tensorphylo enabled. The RevBayes Docker tutorial is located here: revbayes.github.io/tutorials/docker.Running a GeoSSE analysis in RevBayes requires two important data files: a file representing the time-calibrated phylogeny and a biogeographic data matrix describing the ranges for each species. In this tutorial, tree.mcc.tre is a time-calibrated phylogeny of Liolaemus. ranges.nex assigns ranges to each species for a two-region system: an Andean region and a non-Andean region in South America. For each species (row) and region (column), the file reports if the species is present (1) or absent (0) in that region.If you prefer to run a single script instead of entering each command manually, the RevBayes script called geosse.Rev contains all of the commands that are used in the tutorial. There is also an R script for plotting the analysis results. The data and script can be found in the Data files and scripts box in the left sidebar of the tutorial page. Somewhere on your computer, you should create a directory (folder) for this tutorial. Inside the tutorial directory, you should create a scripts directory. This is the directory where you will run RevBayes commands, or where you will put the geosse.Rev and geosse.R scripts. Then, you should create a data directory inside the tutorial directory, and download the two datafiles to this directory.GeoSSE in RevBayesGetting startedAfter starting up RevBayes from within your local scripts directory, you can load the TensorPhylo plugin. You will need to know where you downloaded the plugin. For example, if you cloned the TensorPhylo directory into your home directory at ~/tensorphylo, you would use the following command to load the plugin:loadPlugin(\"TensorPhylo\", \"~/tensorphylo/build/installer/lib\")Note that if you’re using the RevBayes Docker image, then the Tensorphylo plugin is installed in the / (root) directory:loadPlugin(\"TensorPhylo\", \"/tensorphylo/build/installer/lib\")It is also a good idea to set a seed. If you want to exactly replicate the results of the tutorial, you should use the seed 1.seed(1)We also want to tell RevBayes where to find our data (and where to save our output later). If you have set up your tutorial directory in a different way than suggested, you will need to modify the filepaths.fp          = \"../\"dat_fp      = fp + \"data/\"out_fp      = fp + \"output/\"bg_fn       = dat_fp + \"ranges.nex\"phy_fn      = dat_fp + \"tree.mcc.tre\"DataNext, we will read in the data. Let’s start with the phylogenetic tree.phy &lt;- readTrees(phy_fn)[1]In order to set up our analysis, we will want to know some information about this tree: the root age, the taxa and their names, and the number of taxa.tree_height &lt;- phy.rootAge()taxa = phy.taxa()num_taxa = taxa.size()We also want to read in the range data.bg_01 = readDiscreteCharacterData(bg_fn)We want to get some information about this range data: how many regions there are, how many ranges can be constructed from these regions, and how many region pairs there are.num_regions = bg_01.nchar()num_ranges = abs(2^num_regions - 1)num_pairs = num_regions^2 - num_regionsFinally, we want to format the range data to be used in a GeoSSE analysis. This will take the binary range data and output integer states. Note that the integers used to represent ranges are first sorted by range size, then sorted by range patterns given each size-class, following general format of the table in the Introduction to Phylogenetic Models of Discrete Biogeography tutorial.bg_dat = formatDiscreteCharacterData(bg_01, format=\"GeoSSE\", numStates=num_ranges)Model setupIn the GeoSSE model, there are four processes: within-region speciation, extinction, between-region speciation, and dispersal. For each process, each possible event its own event rate that depends on the involved regions or region pairs. This will result in two rate vectors r_w and r_e with lengths equal to the number of regions, and two square rate matrices r_b and r_d with a number of entries equal to the number of region pairs. We will construct the event rates by multiplying the region- or pair-specific relative rate parameters in m_x for each event class $x \\in { w, e, b, d}$ against the appropriate base rate parameter rho_x to produce the absolute rates r_x. All rho_x parameters will be drawn from the exponential distribution dnExp(1). We will use Dirichlet distributions to generate relative rates.We will set up within-region speciation rates first.rho_w ~ dnExp(1)m_w_simplex ~ dnDirichlet(rep(1,num_regions))m_w := m_w_simplex * num_regionsr_w := rho_w * m_wTo obtain our vector of relative rates, m_w, we first create the simplex m_w_simplex which is a vector containing num_regions random values that will be estimated, where each value is between 0 and 1 and all values sum to 1. The Dirichlet(1) distribution assigns equal probability to any combination of values in the simplex, making it a “flat prior”. Setting the alpha value to be large sets higher prior probability on relative rates being similar to one another. We design the model in this way so that users can better control how relative rates of within-region speciation are distributed among regions. We then multiply m_w_simplex by num_regions to produce the mean relative rate value of 1 for any region represented in the resulting relative rate vector, m_w. Lastly, we multiply these relative rates by the absolute scaling factor, rho_w, to obtain our vector of absolute rates, r_w.Extinction rates are set up similarly. The same general logic applies as before. However, these rates are applied only to extinction and not to within-region speciation.rho_e ~ dnExp(1)m_e_simplex ~ dnDirichlet(rep(1,num_regions))m_e := m_e_simplex * num_regionsr_e := rho_e * m_eFrom these extinction rates (which are actually single-region extinction rates), we will set up global extinction rates for each possible range in the state space. In the GeoSSE model, lineage-level extincion events occur when a species goes globally extinct (i.e. it loses the last region from its range). Therefore, we will assign all multi-region ranges an extinction rate of 0, and we will assign all single-region ranges an extinction rate equal to the local extirpation rate. Note, ranges are numbered such that indices 1, 2, through num_regions correspond to ranges that respectively contain only region 1, region 2, up through the last region in the system.for (i in 1:num_ranges) {    mu[i] &lt;- 0.0    if (i &lt;= num_regions) {        mu[i] := r_e[i]    }}For between-region speciation, we want to assign rates to each region pair. However, we want these rates to be symmetric, so we only want num_pairs/2 unique values. The same value will be assigned to m_b[i][j] as m_b[j][i]. We can do this by creating an initial simplex from a Dirichlet distribution, and assigning successive values from this simplex as we traverse the m_b matrix.rho_b ~ dnExp(1)m_b_simplex ~ dnDirichlet(rep(1,num_pairs/2))m_b_idx = 1for (i in 1:num_regions) {    m_b[i][i] &lt;- 0.0    for (j in 1:num_regions) {        if (i &lt; j) {            m_b[i][j] := abs(m_b_simplex[m_b_idx] * num_pairs)            m_b[j][i] := abs(m_b_simplex[m_b_idx] * num_pairs)            m_b_idx += 1        }        r_b[i][j] := rho_b * m_b[i][j]    }}For a two-region system with just one pair of regions, m_b_simplex will contain only a single relative-rate factor with the value of 1. That means the value of r_b for between-region speciation is driven entirely by rho_b. However, when the code is used for analyses with num_regions &gt; 2, the simplex m_b_simplex will contain different values. By allowing these values to vary, we allow widespread ranges to split into daughter ranges at different rates depending on the resulting split. These rates are computed using a range-split score (Landis et al. 2022), which we will not cover in this tutorial (RevBayes will complete this calculation automatically).Finally, for dispersal rates, we want to assign rates to each region pair. These rates are allowed to be asymmetric, so we need num_pairs unique values.rho_d ~ dnExp(1)m_d_simplex ~ dnDirichlet(rep(1,num_pairs))m_d_idx = 1for (i in 1:num_regions) {    m_d[i][i] &lt;- 0.0    for (j in 1:num_regions) {        if (i != j) {            m_d[i][j] := abs(m_d_simplex[m_d_idx++] * num_pairs)        }        r_d[i][j] := rho_d * m_d[i][j]    }}From these rates, we can use RevBayes functions to construct the rate matrices used by the analysis. The first is an anagenetic rate matrix, which gives rates of anagenetic processes. We are not restricting the number of regions that a species can live in at any given time, so we set the maxRangeSize equal to the number of regions. Settings maxRangeSize may be used to reduce the number of range patterns in the model, particularly when num_regions is large.Q_bg := fnBiogeographyRateMatrix(    dispersalRates=r_d,    extirpationRates=r_e,    maxRangeSize=num_regions)We also construct a cladogenetic event matrix, describing the absolute rates of different cladogenetic events. We are not restricting the sizes of ‘split’ subranges following between-region speciation, so we set the max_subrange_split_size equal to the number of regions. From this matrix, we can obtain the total speciation rates per state, as well as a cladogenetic probability matrix.clado_map := fnBiogeographyCladoEventsBD(    speciation_rates=[rho_w,rho_b],    within_region_features=m_w,    between_region_features=m_b,    max_range_size=num_regions,    max_subrange_split_size=num_regions)lambda := clado_map.getSpeciationRateSumPerState()omega := clado_map.getCladogeneticProbabilityMatrix()Lastly, we need to assign a probability distribution to range of the most recent common ancestor of all species, prior to the first speciation event. In this analysis, we will assume all ranges were equally likely for that ancestor.pi_base &lt;- rep(1,num_ranges)pi &lt;- simplex(pi_base)With all of the rates constructed, we can create a stochastic variable drawn from this GeoSSE model with state-dependent birth, death, and speciation processes. This establishes how the various processes interact to generate a tree with a topology, divergence times, and terminal taxon states (ranges). Then we can clamp the variable with the fixed tree and present-day range states, allowing us to infer model parameters based on our observed data.We will use the dnGLHBDSP distribution that interfaces with the Tenorsphylo plugin to model a Generalized Lineage Heterogeneous Birth Death Sampling Process, which is a generalized model (as the name suggests) that can express simpler models, such as GeoSSE models.Although most of the model variable arguments provided to construct the timetree variable have been described above, we pass a few additional arguments to define how we compute the model likelihood. First, we instruct the model to condition on the process evolving for tree_height units of time by setting condition=\"time\". Alternatively, condition can be used to condition on the process e.g. producing a given number of taxa or surviving until the present (producing &gt;2 taxa). Second, we permit Tensorphylo to use four processors with nProc=4 to speed up computation.timetree ~ dnGLHBDSP(    rootAge     = tree_height,    lambda      = lambda,    mu          = mu,    eta         = Q_bg,    omega       = omega,    pi          = pi,    condition   = \"time\",    taxa        = taxa,    nStates     = num_ranges,    nProc       = 4)timetree.clamp(phy)timetree.clampCharData(bg_dat)MCMCFor this analysis, we will perform a short MCMC of 1000 generations, with 100 generations of hyperparameter-tuning burnin. An analysis of this length may not achieve convergence, so these settings should only be used for testing purposes. You can alter this MCMC by changing the number of iterations, the length of the burnin period, or the move schedule. We will also set up the MCMC to record every 10 iterations.n_gen = 1000n_burn = n_gen/10printgen = 10We want MCMC to update all of the base rate rho parameters, as well as the relative rate Dirichlet simplexes. We will use a scaling move for the base rates, since they should always have positive values. These moves will each be performed once per iteration. Simplexes have a unique kind of move in RevBayes. Instead of performing one simplex move per generation, we will make the number of moves per iteration equal to the number of elements in the simplex.mvi = 1mv[mvi++] = mvScale(rho_w, weight=1)mv[mvi++] = mvScale(rho_e, weight=1)mv[mvi++] = mvScale(rho_b, weight=1)mv[mvi++] = mvScale(rho_d, weight=1)mv[mvi++] = mvSimplex(m_e_simplex, weight=m_e.size())mv[mvi++] = mvSimplex(m_w_simplex, weight=m_w.size())mv[mvi++] = mvSimplex(m_b_simplex, weight=m_b_simplex.size())mv[mvi++] = mvSimplex(m_d_simplex, weight=m_d_simplex.size())We also want MCMC to keep track of certain things while it runs. We want it to print some output to the screen so we can see how it is running (mnScreen). We also want it to save model parameters to a file (mnModel). Finally, if we want to use the output for ancestral state reconstruction, we want to save states and stochastic character mappings (mnJointConditionalAncestralStates and mnStochasticCharacterMap). All of the output files will be saved in the output directory so that it can be accessed later.mni = 1mn[mni++] = mnScreen(printgen=printgen)mn[mni++] = mnModel(printgen=printgen, filename=out_fp+\"model.log\")mn[mni++] = mnJointConditionalAncestralState(glhbdsp=timetree, tree=timetree, printgen=printgen, filename=out_fp+\"states.log\", withTips=true, withStartStates=true, type=\"NaturalNumbers\")mn[mni++] = mnStochasticCharacterMap(glhbdsp=timetree, printgen=printgen, filename=out_fp+\"stoch.log\")Then we can start up the MCMC. It doesn’t matter which model parameter you use to initialize the model, so we will use m_w. RevBayes will find all the other parameters that are connected to m_w and include them in the model as well. Then we create an MCMC object with the moves, monitors, and model, add burnin, and run the MCMC.mdl = model(m_w)ch = mcmc(mv, mn, mdl)ch.burnin(n_burn, tuningInterval=10)ch.run(n_gen)After the MCMC analysis has concluded, we can summarize the ancestral states we obtained, creating an ancestral state tree. This tree will be written to the file ase.tre. It may take a little while.f_burn = 0.2x_stoch = readAncestralStateTrace(file=out_fp+\"stoch.log\")x_states = readAncestralStateTrace(file=out_fp+\"states.log\")summarizeCharacterMaps(x_stoch,timetree,file=out_fp+\"events.tsv\",burnin=f_burn)state_tree = ancestralStateTree(tree=timetree,                   ancestral_state_trace_vector=x_states,                   include_start_states=true,                   file=out_fp+\"ase.tre\",                   summary_statistic=\"MAP\",                   reconstruction=\"marginal\",                   burnin=f_burn,                   nStates=3,                   site=1)writeNexus(state_tree,filename=out_fp+\"ase.tre\")OutputOne interesting thing we can do with the output of the GeoSSE analysis is plot ancestral states. This can be done using RevGadgets, an R packages that processes RevBayes output. You can use R to generate a tree with ancestral states by running the geosse.R script, or by executing the following code in R. You can also examine the output files, like model.log, to assess the relative rates of different processes occurring in different regions.library(RevGadgets)library(ggplot2)tree_file = \"../output/ase.tre\"output_file = \"../output/states.png\"states &lt;- processAncStates(tree_file, state_labels=c(\"0\"=\"Andean\", \"1\"=\"Non-Andean\", \"2\"=\"Both\"))plotAncStatesMAP(t=states,                 tree_layout=\"circular\",                 node_size=1.5,                 node_size_as=NULL) +                 ggplot2::theme(legend.position=\"bottom\",                                legend.title=element_blank())ggsave(output_file, width = 9, height = 9)Ancestral state reconstruction of Liolaemus.",
        "url": "/tutorials/geosse_orig/",
        "index": "false"
      }
      ,
    
      "tutorials-selection": {
        "title": "Inferring Selection through Codon Models",
        "content": "Overview",
        "url": "/tutorials/selection/",
        "index": "false"
      }
      ,
    
      "tutorials-revscripter": {
        "title": "Using RevScripter to Build an Analysis Script",
        "content": "OverviewThis tutorial will guide you through creating a RevBayes script using RevScripter.RevScripter is a web-based graphical tool intended to introduce new users to the Rev language and standard analysis methods in RevBayes.RevScripter: An Introductory Tool for Creating RevBayes Analysis ScriptsRevBayes is extremely flexible and the Rev language and graphical model framework enable researchers to apply a very rich set of complex evolutionary models. The development of RevScripter was motivated by a need to provide a familiar introductory interface for RevBayes and the Rev language. It will not be possible to maintain a menu-driven graphical user interface (GUI) that encompasses the wide range of models and methods available in RevBayes. Thus, this tool will be useful for those new to RevBayes to set up standard analyses and easily see how elements of the model are reflected in the generated Rev language script.Design and Source CodeThe design of RevScripter is inspired by the BEAUti tool in the BEAST packages (Drummond et al. 2012; Bouckaert et al. 2014), as well as the graphical user interface for the program BPP (missing reference). All of these tools are menu-based user interfacesfor creating complex analysis scripts for Bayesian phylogenetic inferenceprograms. These interfaces are dynamic and the available options react to thechoices made by the user. In many ways, these menu-driven GUIs are also modeled off of PAUP* (missing reference), the phylogenetic analysis program that is one of the most commonly used tools in the field. Thus, creating a GUI that uses menus and options for the user to choose is a familiar approach among statistical phylogenetics software tools and this type of interface will hopefully make it easier for those new toanalysis in RevBayes to get started. With the scripts generated by RevScripter,you will be able to directly link the choices you made in the graphical toolto the Rev language syntax that builds the model and specifies the execution of the analysis.RevScripter is written in Javascript and uses CSS for the layout and design.The source code for RevScripter is hosted on GitHub: https://github.com/revbayes/revscripter.A Graphical Interface for RevBayesRevScripter is intended to provide a gateway for new users interested in building scripts for standard analyses and accessing RevBayes through a familiarinterface. Ultimately, however, the menu-based design of RevScripter will alwaysbe limited to just a subset of the available models and methods that are possiblein RevBayes. Instead, a much more flexible and modular graphical user interfaceis needed for a program like RevBayes. John Huselsenbeck and Sveta Krasikova are currently developing a GUI that is integrated with the RevBayes core, which willenable users to access the wide range of possible models and methods availablein RevBayes with a graphical tool.Building Your Model and Analysis with RevScripterExercise: Unrooted phylogenetic analysis under the Jukes-Cantor substitution model",
        "url": "/tutorials/revscripter/",
        "index": "false"
      }
      ,
    
      "tutorials-multifig-orig": {
        "title": "Feature-informed diversification with the MultiFIG model",
        "content": "IntroductionIn the previous examples, we used a GeoSSE model (Goldberg et al. 2011) to investigate the evolution of the South American lizard genus Liolaemus. The GeoSSE model allows us to estimate rates of within-region speciation, extinction, between-region speciation, and dispersal that differ among regions. Biologically, we expect that these different rates are informed by features of the regions where the species are evolving. For example, we might expect that species disperse at a lower rate between more distant regions, or go extinct at a higher rate in smaller regions. The FIG model (Landis et al. 2022) and the Multiple Feature-Informed GeoSSE (MultiFIG) model (Swiston and Landis 2023) address this. Rather than giving each region its own evolutionary rate parameters, it uses functions to link features of those regions to evolutionary rates. This allows us to test hypotheses about the importance of certain environmental features on evolutionary processes. It also has the benefit of reducing the number of parameters that need to be estimated. The number of parameters in the MultiFIG model is constant with respect to the number of regions, so we can investigate systems with more regions. In this tutorial, we will model the evolution and biogeography of Liolaemus using six regions (Esquerré et al. 2019) and eight regional features.Six regions of southern South America used in the analysis of Liolaemus with abbreviations: Aa (Altiplanic Andes), Ca (Central Andes), Pa (Patagonia), Cc (Central Chile), Ad (Atacama Desert), El (Eastern Lowlands).The MultiFIG modelMuch like the GeoSSE model, MultiFIG uses four core processes: within-region speciation, extinction, between-region speciation, and dispersal. However, instead of assigning each region or region pair its own rate for each process, MultiFIG uses regional feature data and a series of strength parameters and functions to construct rates.Graphical model of MultiFIG. Square nodes represent constant values (data). Circle nodes with solid lines represent stochastic variables (model parameters, and the phylogeny, which is fixed in this analysis). Circle nodes with dotted lines represent deterministic variables (functions). Large rectangles indicate iterative plates.The FIG model incorporates geographical features with two value types as model variables: quantitative features and categorical features. Quantitative features have continuous real values while categorical features have discrete values. MultiFIG also separates data by dimensionality type, incorporating one-dimensional within-region data and two-dimensional between-region data. We use four containers to store this data: $w_c$, $w_q$, $b_c$, and $b_q$.Each regional feature is assigned a “feature effect” parameter that measures the strength and direction of the effect of a particular feature on a particular process. Note that “effect” refers to a mathematical relationship here, but does not indicate causality. These strength parameters are referred to as $\\sigma$ and $\\phi$, representing the effects of categorical and quantitative features respectively. There is one $\\sigma$ or $\\phi$ parameter per feature per process. For example, $\\phi_w^{Altitude}$ would represent the relationship between region altitude and within-region speciation.For each process, the categorical and quantitative feature effects (with feature data modified by strength parameters) are gathered into $c$ and $q$ vectors, then ultimately combined into an $m$ vector. The $m$ vector represents the total effects of all regional features on a particular process, with entries representing each region (or region pair for between-region processes). The $m$ vector represents relative rates among regions, but to obtain absolute rates, the $m$ vector for each process is multiplied by a process-specific base rate parameter $\\rho$. This constructs the $r$ vectors that are analogous to GeoSSE rates: $r_w$ for within-region speciation rates, $r_e$ for extinction rates, and $r_d$ for dispersal rates (calculating $r_b$ for between-region speciation rates also requires the use of a range split score, as in (Landis et al. 2022)). This tutorial will not describe the details of these intermediate functions, but they can be found in (Swiston and Landis 2023).In this analysis, we are examining 8 regional features. 4 are quantitative: area, mean altitude, distance, and difference in mean altitude. The other 4 are categorical: size, height, adjacency, and height sameness. While the categorical features are closely related to the quantitative features in this analysis, this is not a requirement of the model. Because each within-region feature acts on 2 processes and each between-region feature acts on 2 processes, this creates a total of 16 parameters. Adding one $\\rho$ parameter for each process results in a total of 20 model parameters to be estimated. We will use a time-calibrated phylogeny and present-day ranges for Liolaemus to estimate these parameters, and use those estimates to determine which regional features are most strongly related to particular processes.The 8 regional features investigated in this analysis and the 16 associated parameters relating these features to core biogeographic processes.Setup  Important version info!!  This tutorial is the one of a series of lessons explaining how to build increasingly powerful but computationally demanding GeoSSE-type models for biogeographic analyses. Inference under these models is powered by the Tensorphylo plugin for RevBayes, located here: bitbucket.org/mrmay/tensorphylo/src/master (May and Meyer).This tutorial, and following tutorials for GeoSSE-type models, will also require a development version of RevBayes built from the hawaii_fix branch (this message will be removed when the branch is merged).As an alternative to building the development version of RevBayes and installing Tensorphylo, you can instead use the RevBayes Docker image, which comes pre-configured with Tensorphylo enabled. The RevBayes Docker tutorial is located here: revbayes.github.io/tutorials/docker.Running a MultiFIG analysis in RevBayes requires several important data files, including a file representing the time-calibrated phylogeny and a biogeographic data matrix describing the ranges for each species. tree.mcc.tre is a time-calibrated phylogeny of Liolaemus. full_ranges.nex assigns ranges to each species for a six-region system: Altiplanic Andes, Central Andes, Patagonia, Central Chile, Atacama Desert, and Eastern Lowlands. For each species (row) and region (column), the file reports if the species is present (1) or absent (0) in that region. There are also feature files that contain regional feature data, and a feature_summary.csv file that describes all the regional feature files (where they are found and what kind of data they contain).If you prefer to run a single script instead of entering each command manually, the RevBayes script called multifig.Rev contains all of the commands that are used in the tutorial. There is also an R script for plotting the analysis results. The data and script can be found in the Data files and scripts box in the left sidebar of the tutorial page. Somewhere on your computer, you should create a directory (folder) for this tutorial. Inside the tutorial directory, you should create a scripts directory. This is the directory where you will run RevBayes commands, or where you will put the multifig.Rev and multifig.R scripts. Then, you should create a data directory inside the tutorial directory. The scripts/commands for the tutorial expect that the primary data files (tree.mcc.tre, full_ranges.nex, and feature_summary.csv) will be in this directory, while the feature files (the data, not the summary file) will be in a subdirectory called features. However, you can always modify the filepaths to locate the data wherever you choose to download it.MultiFIG in RevBayesGetting startedAfter starting up RevBayes from within your local scripts directory, you can load the TensorPhylo plugin. You will need to know where you downloaded the plugin. For example, if you cloned the TensorPhylo directory into your home directory at ~/tensorphylo, you would use the following command to load the plugin:loadPlugin(\"TensorPhylo\", \"~/tensorphylo/build/installer/lib\")Note that if you’re using the RevBayes Docker image, then the Tensorphylo plugin is installed in the / (root) directory:loadPlugin(\"TensorPhylo\", \"/tensorphylo/build/installer/lib\")It is also a good idea to set a seed. If you want to exactly replicate the results of the tutorial, you should use the seed 1.seed(1)We also want to tell RevBayes where to find our data (and where to save our output later). If you have set up your tutorial directory in a different way than suggested, you will need to modify the filepaths.fp          = \"../\"dat_fp      = fp + \"data/\"out_fp      = fp + \"output/\"bg_fn       = dat_fp + \"full_ranges.nex\"phy_fn      = dat_fp + \"tree.mcc.tre\"feature_fn  = dat_fp + \"feature_summary.csv\"DataNext, we will read in the data. Let’s start with the phylogenetic tree.phy &lt;- readTrees(phy_fn)[1]In order to set up our analysis, we will want to know some information about this tree: the root age, the taxa, and the number of taxa.tree_height &lt;- phy.rootAge()taxa = phy.taxa()num_taxa = taxa.size()We also want to read in the range data.bg_01 = readDiscreteCharacterData(bg_fn)We want to get some information about this range data: how many regions there are, how many ranges can be constructed from these regions, and how many region pairs there are.num_regions = bg_01.nchar()num_ranges = abs(2^num_regions - 1)num_pairs = num_regions^2 - num_regionsWe want to format the range data to be used in a GeoSSE-type analysis. This will take the binary range data and output integer states.bg_dat = formatDiscreteCharacterData(bg_01, format=\"GeoSSE\", numStates=num_ranges)We also want to get our feature data. Using the RevBayes function readRegionalFeatures, we can look at the feature_summary.csv file and automatically look for feature data. The feature_summary.csv file is specially formated to be read by RevBayes, consisting of 5 columns. The first column is time_index. More advanced analyses, like TimeFIG, may involve time-heterogenous region features, in which case we would need to index our feature data by time slices. However, in this analysis, we are only using present-day data, so all of our features will have a time_index of 1. The second column is feature_index. Each feature type (within-region categorical, within-region quantitative, between-region categorical, and between-region quantitative) has a container that can contain several features, so we want to index the features within those containers. For example, in this analysis, each container has two features, indexed 1 and 2. Keep in mind that Feature 1 in one container does not have to be related to Feature 1 in another container. It’s important to keep track of these indices so you know which output corresponds to which feature. For this analysis, we’ve listed the features in order on the feature table – for example, Quantitative Between-Region Feature 1 is Mean Distance Between Points (km). The third column is feature_relationship. This column is for indicating whether the feature is a within-region feature or a between-region feature, with options ‘within’ or ‘between’. The fourth column is feature_type, for indicating whether the feature is quantitative of categorical. Finally, the fifth column is feature_path, which gives a filepath for the actual file containing the data for that feature.geo_features &lt;- readRegionalFeatures(feature_fn, delimiter=\",\",nonexistent_region_token=\"nan\")Next, we transform the feature data into feature layers, a RevBayes object that we will use later for informing our biogeographic rates. First, we normalize the features (important for scaling reasons). Then we pull each feature type out of our geo_features object. Finally, we create the layers.geo_features.normalize(\"within\")geo_features.normalize(\"between\")feature_CW &lt;- geo_features.get(\"within\",\"categorical\",1)feature_QW &lt;- geo_features.get(\"within\",\"quantitative\",1)feature_CB &lt;- geo_features.get(\"between\",\"categorical\",1)feature_QB &lt;- geo_features.get(\"between\",\"quantitative\",1)for (i in 1:feature_CW.size()) {layer_CW[i] &lt;- feature_CW[i].get()}for (i in 1:feature_QW.size()) {layer_QW[i] &lt;- feature_QW[i].get()}for (i in 1:feature_CB.size()) {layer_CB[i] &lt;- feature_CB[i].get()}for (i in 1:feature_QB.size()) {layer_QB[i] &lt;- feature_QB[i].get()}Model setupIn the MultiFIG model, there are four processes: within-region speciation, extinction, between-region speciation, and dispersal. Rates per region or region pair are calculated using feature data, feature effect parameters, and base rate parameters. We will set the prior on base rate parameters to the exponential distribution dnExp(1). We will set the prior on feature effect parameters to the normal distribution dnNormal(0,1). Then we will use the RevBayes function fnFeatureInformedRates to combine the feature data and feature effect parameters to create $m$ vectors/matrices, representing relative rates per region or region pair. Finally, we will multiply $m$ by the base rate parameter to get model rates $r_w$, $r_e$, $r_b$, and $r_d$.Let’s start by creating distributions that we will use for all $\\rho$, $\\phi$, and $\\sigma$ parameters.sigma_dist  = dnNormal(0,1)phi_dist    = dnNormal(0,1)rho_dist    = dnExp(1)Now we will set up our rates for the four core processes. We will set up within-region speciation rates first. We won’t worry about multiplying $m_w$ by the base rate yet, because the fnBiogeographyCladoEventsBD function will do this later.rho_w ~ rho_distfor (i in 1:feature_CW.size()) sigma_w[i] ~ sigma_distfor (i in 1:feature_QW.size()) phi_w[i] ~ phi_distm_w := fnFeatureInformedRates(layer_CW, layer_QW, sigma_w, phi_w, null_rate=0)Extinction rates are set up similarly, and we will incorporate $\\rho$ this time. From these extinction rates (which are actually single-region extinction rates), we will set up global extinction rates for each possible range in the state space. In the MultiFIG model, lineage-level extincion events occur when a species goes globally extinct (i.e. it loses the last region from its range). Therefore, we will assign all multi-region ranges an extinction rate of 0, and we will assign all single-region ranges an extinction rate equal to the local extirpation rate. Note, ranges are numbered such that indices 1, 2, through num_regions correspond to ranges that respectively contain only region 1, region 2, up through the last region in the system.rho_e ~ rho_distfor (i in 1:feature_CW.size()) sigma_e[i] ~ sigma_distfor (i in 1:feature_QW.size()) phi_e[i] ~ phi_distm_e := fnFeatureInformedRates(layer_CW, layer_QW, sigma_e, phi_e, null_rate=1e3)r_e := rho_e * m_e[1]for (i in 1:num_ranges) {    mu[i] &lt;- abs(0)    if (i &lt;= num_regions) {        mu[i] := r_e[i]    }}Between-region speciation rates are set up similarly. Again, we do not need to incorporate $\\rho$ yet. We also don’t have to worry about incorporating range split score; RevBayes will do this automatically when we create the cladogenetic probability matrix.rho_b ~ rho_distfor (i in 1:feature_CB.size()) sigma_b[i] ~ sigma_distfor (i in 1:feature_QB.size()) phi_b[i] ~ phi_distm_b := fnFeatureInformedRates(layer_CB, layer_QB, sigma_b, phi_b, null_rate=1)Finally, for dispersal rates, we will set up dispersal rates.rho_d ~ rho_distfor (i in 1:feature_CB.size()) sigma_d[i] ~ sigma_distfor (i in 1:feature_QB.size()) phi_d[i] ~ phi_distm_d := fnFeatureInformedRates(layer_CB, layer_QB, sigma_d, phi_d, null_rate=0)for (i in 1:num_regions) {r_d[i] := rho_d * m_d[i]}From these rates, we can use RevBayes functions to construct the rate matrices used by the analysis. The first is an anagenetic rate matrix, which gives rates of anagenetic processes. We are not restricting the number of regions that a species can live in at any given time, so we set the maxRangeSize equal to the number of regions. Settings maxRangeSize may be used to reduce the number of range patterns in the model, particularly when num_regions is large.Q_bg := fnBiogeographyRateMatrix(    dispersalRates=r_d,    extirpationRates=r_e,    maxRangeSize=num_regions)We also construct a cladogenetic event matrix, describing the absolute rates of different cladogenetic events. We are not restricting the sizes of ‘split’ subranges following between-region speciation, so we set the max_subrange_split_size equal to the number of regions. From this matrix, we can obtain the total speciation rates per state, as well as a cladogenetic probability matrix.clado_map := fnBiogeographyCladoEventsBD(    speciation_rates=[ rho_w, rho_b ],    within_region_features=m_w[1],    between_region_features=m_b,    max_range_size=num_regions,    max_subrange_split_size=num_regions)lambda := clado_map.getSpeciationRateSumPerState()omega  := clado_map.getCladogeneticProbabilityMatrix()Lastly, we need to assign a probability distribution to range of the most recent common ancestor of all species, prior to the first speciation event. In this analysis, we will assume all ranges were equally likely for that ancestor.pi_base &lt;- rep(1,num_ranges)pi &lt;- simplex(pi_base)With all of the rates constructed, we can create a stochastic variable drawn from this MultiFIG model with state-dependent birth, death, and speciation processes. This establishes how the various processes interact to generate a tree with a topology, divergence times, and terminal taxon states (ranges). Then we can clamp the variable with the fixed tree and present-day range states, allowing us to infer model parameters based on our observed data.timetree ~ dnGLHBDSP(    rootAge     = tree_height,    lambda      = lambda,    mu          = mu,    eta         = Q_bg,    omega       = omega,    pi          = pi,    rho         = 1,    condition   = \"time\",    taxa        = taxa,    nStates     = num_ranges,    nProc       = 4)timetree.clamp(phy)timetree.clampCharData(bg_dat)MCMCFor this analysis, we will perform a short MCMC of 1000 generations, with 100 generations of hyperparameter-tuning burnin. An analysis of this length may not achieve convergence, so these settings should only be used for testing purposes. You can alter this MCMC by changing the number of iterations, the length of the burnin period, or the move schedule. We will also set up the MCMC to record every 10 iterations.n_gen = 1000n_burn = n_gen/10printgen = 10We want MCMC to update all of the base rate $\\rho$ parameters, as well as the $\\sigma$ and $\\phi$ parameters. We will use a scaling move for the base rates, since they should always have positive values. These moves will each be performed once per iteration. We will use a sliding move for the feature effect parameters, since they can have positive or negative values. These moves will also be performed once per iteration.mvi = 1mv[mvi++] = mvScale(rho_w, weight=1)mv[mvi++] = mvScale(rho_e, weight=1)mv[mvi++] = mvScale(rho_b, weight=1)mv[mvi++] = mvScale(rho_d, weight=1)for (i in 1:sigma_d.size()) {mv[mvi++] = mvSlide(sigma_d[i])}for (i in 1:sigma_b.size()) {mv[mvi++] = mvSlide(sigma_b[i])}for (i in 1:sigma_e.size()) {mv[mvi++] = mvSlide(sigma_e[i])}for (i in 1:sigma_w.size()) {mv[mvi++] = mvSlide(sigma_w[i])}for (i in 1:phi_d.size()) {mv[mvi++] = mvSlide(phi_d[i])}for (i in 1:phi_b.size()) {mv[mvi++] = mvSlide(phi_b[i])}for (i in 1:phi_e.size()) {mv[mvi++] = mvSlide(phi_e[i])}for (i in 1:phi_w.size()) {mv[mvi++] = mvSlide(phi_w[i])}We also want MCMC to keep track of certain things while it runs. We want it to print some output to the screen so we can see how it is running (mnScreen). We also want it to save model parameters to a file (mnModel). Finally, if we want to use the output for ancestral state reconstruction, we want to save states and stochastic character mappings (mnJointConditionalAncestralStates and mnStochasticCharacterMap). All of the output files will be saved in the output directory so that it can be accessed later.mni = 1mn[mni++] = mnScreen(printgen=printgen)mn[mni++] = mnModel(printgen=printgen, filename=out_fp+\"model.log\")mn[mni++] = mnJointConditionalAncestralState(glhbdsp=timetree, tree=timetree, printgen=printgen, filename=out_fp+\"states.log\", withTips=true, withStartStates=true, type=\"NaturalNumbers\")mn[mni++] = mnStochasticCharacterMap(glhbdsp=timetree, printgen=printgen, filename=out_fp+\"stoch.log\")Then we can start up the MCMC. It doesn’t matter which model parameter you use to initialize the model, so we will use m_w. RevBayes will find all the other parameters that are connected to m_w and include them in the model as well. Then we create an MCMC object with the moves, monitors, and model, add burnin, and run the MCMC.mdl = model(m_w)ch = mcmc(mv, mn, mdl)ch.burnin(n_burn, tuningInterval=10)ch.run(n_gen)After the MCMC analysis has concluded, we can summarize the ancestral states we obtained, creating an ancestral state tree. This tree will be written to the file ase.tre . It may take a little while.f_burn = 0.2x_stoch = readAncestralStateTrace(file=out_fp+\"stoch.log\")x_states = readAncestralStateTrace(file=out_fp+\"states.log\")summarizeCharacterMaps(x_stoch,timetree,file=out_fp+\"events.tsv\",burnin=f_burn)state_tree = ancestralStateTree(    tree=timetree,    ancestral_state_trace_vector=x_states,    include_start_states=true,    file=out_fp+\"ase.tre\",    summary_statistic=\"MAP\",    reconstruction=\"marginal\",    burnin=f_burn,    nStates=3,    site=1)writeNexus(state_tree,filename=out_fp+\"ase.tre\")OutputOne interesting thing we can do with the output of the MultiFIG analysis is plot ancestral states. This can be done using RevGadgets, an R packages that processes RevBayes output. You can use R to generate a tree with ancestral states by running the multifig.R script, or by executing the following code in R. Before plotting the ancestral state tree, we create two vectors. The vector labels is useful because it maps actual region labels onto state numbers, so the legend can be easily interpreted. If you used your own data, you would have to provide your own state labels. The vector colors is necessary in this case because RevGadgets does not automatically generate color schemes with more than 12 colors, and there are more than 12 states on the reconstructed tree. If you use your own data, you may need more colors or fewer colors (the colors vector is allowed to have more colors than states).labels &lt;- c(\"0\" = \"Aa\",\"1\" = \"Ca\",\"2\" = \"Pa\",\"3\" = \"Cc\",\"4\" = \"Ad\",\"5\" = \"El\",\"6\" = \"AaCa\",\"7\" = \"AaPa\",\"8\" = \"CaPa\",\"9\" = \"AaCc\",\"10\" = \"CaCc\",\"11\" = \"PaCc\",\"12\" = \"AaAd\",\"13\" = \"CaAd\",\"14\" = \"PaAd\",\"15\" = \"CcAd\",\"16\" = \"AaEl\",\"17\" = \"CaEl\",\"18\" = \"PaEl\",\"19\" = \"CcEl\",\"20\" = \"AdEl\",\"21\" = \"AaCaPa\",\"22\" = \"AaCaCc\",\"23\" = \"AaPaCc\",\"24\" = \"CaPaCc\",\"25\" = \"AaCaAd\",\"26\" = \"AaPaAd\",\"27\" = \"CaPaAd\",\"28\" = \"AaCcAd\",\"29\" = \"CaCcAd\",\"30\" = \"PaCcAd\",\"31\" = \"AaCaEl\",\"32\" = \"AaPaEl\",\"33\" = \"CaPaEl\",\"34\" = \"AaCcEl\",\"35\" = \"CaCcEl\",\"36\" = \"PaCcEl\",\"37\" = \"AaAdEl\",\"38\" = \"CaAdEl\",\"39\" = \"PaAdEl\",\"40\" = \"CcAdEl\",\"41\" = \"AaCaPaCc\",\"42\" = \"AaCaPaAd\",\"43\" = \"AaCaCcAd\",\"44\" = \"AaPaCcAd\",\"45\" = \"CaPaCcAd\",\"46\" = \"AaCaPaEl\",\"47\" = \"AaCaCcEl\",\"48\" = \"AaPaCcEl\",\"49\" = \"CaPaCcEl\",\"50\" = \"AaCaAdEl\",\"51\" = \"AaPaAdEl\",\"52\" = \"CaPaAdEl\",\"53\" = \"AaCcAdEl\",\"54\" = \"CaCcAdEl\",\"55\" = \"PaCcAdEl\",\"56\" = \"AaCaPaCcAd\",\"57\" = \"AaCaPaCcEl\",\"58\" = \"AaCaPaAdEl\",\"59\" = \"AaCaCcAdEl\",\"60\" = \"AaPaCcAdEl\",\"61\" = \"CaPaCcAdEl\",\"62\" = \"AaCaPaCcAdEl\")colors &lt;- c(\"#FD3216\", \"#00FE35\", \"#6A76FC\", \"#FED4C4\", \"#FE00CE\", \"#0DF9FF\", \"#F6F926\", \"#FF9616\", \"#479B55\", \"#EEA6FB\", \"#DC587D\", \"#D626FF\", \"#6E899C\", \"#00B5F7\", \"#B68E00\", \"#C9FBE5\", \"#FF0092\", \"#22FFA7\", \"#E3EE9E\", \"#86CE00\", \"#BC7196\", \"#7E7DCD\", \"#FC6955\", \"#E48F72\")library(RevGadgets)library(ggplot2)tree_file = \"../output/ase.tre\"output_file = \"../output/states.png\"states &lt;- processAncStates(tree_file, state_labels=labels)plotAncStatesMAP(t=states,                 tree_layout=\"circular\",                 node_size=1.5,                 node_color_as=\"state\",                 node_color=colors,                 node_size_as=NULL) +                 ggplot2::theme(legend.position=\"bottom\",                                legend.title=element_blank())ggsave(output_file, width = 9, height = 9)Ancestral state reconstruction of Liolaemus.You can also examine the output files, like model.log, to assess the relationships between regional features and biogeographic processes, or to compare relative rates of different event types. This can also be done using RevGadgets. Here, we show a posterior plot for a single parameter ($\\phi_d^{Distance}$) as an example.trace &lt;- readTrace(path=log_file)plotTrace(trace, vars=c(\"phi_d[1]\"))ggsave(posterior_file, width = 9, height = 9)Posterior for the parameter $\\phi_d^{Distance}$, which relates quantitative distances between regions to the process of dispersal. The 95% Highest Posterior Density (HPD) interval is shaded in dark blue. Note that 0 (a null effect) falls outside this interval. Therefore, distance is likely related to dispersal. A negative value indicates a negative relationship; more distance between regions reduces dispersal.",
        "url": "/tutorials/multifig_orig/",
        "index": "false"
      }
      ,
    
      "tutorials-fbd-simple": {
        "title": "Estimating a Time-Calibrated Phylogeny of Fossil and Extant Taxa using Morphological Data",
        "content": "This tutorial comes with a recorded video walkthrough. The video corresponding to each section of the exercise is linked next to the section title. The full playlist is available here: Overview This tutorial provides a guide to using RevBayes to perform a simple phylogenetic analysis of extant and fossil bear species (family Ursidae), using morphological data as well as the occurrence times of lineages from the fossil record.A version of this tutorial is published as: Barido-Sottani et al. (2020) “Estimating a time-calibrated phylogeny of fossil and extant taxa using RevBayes”. In Scornavacca, C., Delsuc, F., and Galtier, N., editors, Phylogenetics in the Genomic Era, chapter No. 5.2, pp. 5.2:2–5.2:22. No commercial publisher | Authors open access book.IntroductionTo get an overview of the model, it is useful to think of the model as a generating process for our data. Suppose we would like to simulate our fossil and morphological data; we would consider two components ():  Time tree model: This is the diversification process that describes how a phylogeny is generated, as well as when fossils are sampled along each lineage on the phylogeny.  This component generates the phylogeny, divergence times, and the fossil occurrence data. The tree topology and node ages are parameters of the model that generates our morphological characters.  Discrete morphological character change model: This model describes how discrete morphological character states change over time on the phylogeny. The generation of observed morphological character states is governed by other model components including the substitution process and variation among characters in our matrix and among branches on the tree.These two components, or modules, form the backbone of the inference model and reflect our prior beliefs on how the tree, fossil data, and morphological trait data are generated. We will provide a brief overview of the specific models used within each component while pointing to other tutorials that implement alternative models.Modular components of the graphical model used in theanalysis described in this tutorial. The gray boxes indicate the observed data: fossil ages and discrete morphological characters. The white boxes represent the models that generated the data.Time Tree Model: The Fossilized Birth-Death ProcessThe fossilized birth death (FBD) process provides a joint distribution on the divergence times of living and extinct species, the tree topology, and the sampling of fossils (Stadler 2010; Heath et al. 2014). The FBD model can be broken into two sub-processes, the birth-death process and the fossilization process.Birth-Death ProcessThe birth-death process is a branching process that provides a distribution for the tree topology and divergence times on the tree. We will consider a constant-rate birth-death process (Kendall 1948; Thompson 1975). Specifically, we will assume every lineage has the same constant rate of speciation $\\lambda$ and rate of extinction $\\mu$ at any moment in time (Nee et al. 1994; Höhna 2015). Speciation and extinction events occur with rate parameters $\\lambda$ and $\\mu$ respectively, whereby the waiting time between events is exponentially distributed with parameter ($\\lambda+\\mu$). Then, given an event occurred, the probability of the event being a speciation is ($\\lambda$ / ($\\lambda+\\mu$)) while the probability of the event being an extinction is ($\\mu$ / ($\\lambda+\\mu$)).The birth-death process depends on two other parameters as well, the origin time and the sampling probability. The origin time, denoted $\\phi$, represents the starting time of the stem lineage, which is the age of the entire process. The sampling probability, denoted $\\rho$, gives the probability that an extant species is sampled.The assumption that, at any given time, each lineage has the same speciation rate and extinction rate may not be realistic or valid in some systems. Several models are currently implemented in RevBayes that relax the assumption of constant rates, including:  episodic diversification rates (Höhna 2015)  environment-dependent diversification rates (Condamine et al. 2018)  branch-specific diversification rates (Höhna et al. 2019)  diversification rates tied to a species trait (Maddison et al. 2007; Freyman and Höhna 2018; Freyman and Höhna 2019)Fossilization ProcessGiven a phylogeny, in this case a phylogeny generated by a birth-death process, the fossilization process provides a distribution for sampling fossilized occurrences of lineages in the tree (Heath et al. 2014). Much like speciation and extinction, fossil sampling is modeled according to a Poisson process with rate parameter $\\psi$. This means that each lineage has the same constant rate of producing a fossil. As a result, along a given lineage, the time between fossilization events is exponentially distributed with rate $\\psi$.One key assumption of the FBD model is that each fossil represents a distinct fossil specimen. However, if certain taxa persist through time and fossilize particularly well, then the same taxon may be sampled at different stratigraphic ages. These fossil data are commonly represented by only the first and last appearances of a fossil morphospecies. In this case one might want to consider the fossilized birth-death range process (Stadler et al. 2018) to model the stratigraphic ranges of fossil occurrences.Accounting for Fossil Age UncertaintyOften, there is uncertainty around the age of each fossil, which is typically represented as an interval of the minimum and maximum possible ages. Moreover, a recent study demonstrated using simulated data  that ignoring uncertainty in fossil occurrence dates can lead to biased estimates of divergence times (Barido-Sottani et al. 2019).RevBayes allows fossil occurrence time uncertainty to be modeled by directly treating it as part of the likelihood of the fossil data given the time tree. We model this by assuming the likelihood of a particular fossil occurrence $\\mathcal{F}_i$ is zero if the inferred age $t_i$ occurs outside the time interval $(a_i,b_i)$ and some non-zero likelihood when the fossil is placed within the interval. Specifically, we will assume the fossil could occur anywhere within the observed interval with uniform probability, this means that the likelihood is equal to one if the inferred fossil age is consistent with the observed fossil interval:\\[f[\\mathcal{F}_i \\mid a_i, b_i, t_i] = \\begin{cases}1 &amp; \\text{if } a_i &lt; t_i &lt; b_i\\\\0 &amp; \\text{otherwise}\\end{cases}\\]The incorporation of uncertainty around the fossil occurrence data is shown graphically as a part of our model in .A graphical model of thefossil age likelihood model used in this tutorial. The likelihood offossil observation $\\mathcal{F}_i$ is uniform and non-zero when theinferred fossil age $t_i$ falls within the observed time interval$(a_i,b_i)$.Modeling Discrete Morphological Character ChangeGiven a phylogeny, the discrete morphological character change model will describe how traits change along each lineage, resulting in the observed character states of fossils and living species. In our case, the phylogeny and fossil occurrences are generated from the FBD process and we will be modeling the evolution of discrete morphological characters with two states. There are three main components to consider with modeling discrete morphological traits (as shown in ): the substitution model, the branch rate model, and the site rate model.Substitution ModelThe substitution model describes how discrete morphological characters evolve over time. We will be using the Mk model (Lewis 2001), a generalization of the Jukes-Cantor (Jukes and Cantor 1969) model described for nucleotide substitutions. The Mk model assumes that all transitions from one state to another occur at the same rate, for all $k$ states. Since the characters used in this tutorial all have two states, we will specifically be using a model where $k=2$. Thus, a transition from state 0 to state 1 is equally as likely as a transition from state 1 to state 0. For this tutorial, we focus on binary (2-state) characters for simplicity, but it is important to note that RevBayes can also accommodate multistate characters as well.The evolution of discrete morphological characters is thought to occur at a very slow rate.Moreover, once some characters transition to a certain state, they rarely transition back, which means that the assumption of symmetric rates is likely violated my many empirical datasets (Wright et al. 2016; Wright 2019). We can accommodate asymmetric transition rates for each state using alternative models in RevBayes. Additionally, if some characters change symmetrically while others change asymmetrically, it is possible to partition the character matrix to account for model heterogeneity in the matrix.Branch-Rate ModelThe branch-rate model describes how rates of morphological state transitions vary among branches in the tree. Each lineage in the phylogeny is assigned a value that acts as a scalar for the rate of character evolution. In our case we assume each branch has the same rate of evolution, this is a strict morphological clock (Zuckerkandl and Pauling 1962), which is analogous to a strict molecular clock.It is also possible to account for variation in rates among branches. These “relaxed-clock” models are commonly applied to molecular datasets and are currently implemented in RevBayes (see the Clocks and Time Trees tutorial).Site-Rate ModelThe rate of character evolution can often vary from site to site, i.e. from one column in the matrix to another. Under the site-rate model, a scalar is applied to each character to account for variation in relative rates. In our case we will assume that each character belongs to one of four rate categories from the discretized gamma distribution (Yang 1994), which is parameterized by shape parameter $\\alpha$ and number of rate categories $n$. Normally a gamma distribution requires shape $\\alpha$ and rate $\\beta$ parameters, however, we set our site rates to have a mean of one, which results in the constraint $\\alpha=\\beta$, thus eliminating the second parameter. The parameter $n$ breaks the gamma distribution into $n$ equiprobable bins where the rate value of each bin is equal to its mean or median.Putting Together the Complete Phylogenetic ModelWe have outlined the specific components forming the processes that govern the generation of the time tree and morphological character data; and together these modules make up the complete phylogenetic model.  shows the complete probabilistic graphical model that includes all of the parameters we will use in this tutorial (see Höhna et al. (2014) for more on graphical models for statistical phylogenetics).The complete graphical model used in theanalysis described in this tutorial. This explicit representation of the model expands on the modular version depicted in . The model components are defined in the box on the right. To simplify the model, we do not represent the components accounting for fossil age uncertainty illustrated in .The parameters represented as stochastic nodes (solid white circles) in  are unknown random variables that are estimated in our analysis.For each of these parameters, we assume a prior distribution that describes our uncertainty in that parameter’s value. For example, we apply an exponential distribution with a rate of 10 as a prior on the mutation rate: $\\mu\\sim$ Exponential(10).The parameters represented as constant nodes (white boxes) are fixed to “known” or asserted values in the analysis.Alternative Models and AnalysesThe model choices and analysis in this tutorial focus on a simple example.Importantly, the modular design of RevBayes allows for many model choices to be swapped with more complex or biologically relevant processes for a given system.Analyses of a wide range of data types are also implemented in RevBayes (e.g. nucleotide sequences, historical biogeographic ranges).Moreover, it is possible to fully integrate models describing the generation of data from different sources like in the “combined-evidence” approach (Ronquist et al. 2012; Zhang et al. 2016; Gavryushkina et al. 2017) in a single, hierarchical Bayesian model.Some researchers may wish to perform analyses with node calibrationsUltimately, for any statistical analysis of empirical data, it is important to consider the processes governing the generation of those data and how they can be represented in a hierarchical model.Exercise: Phylogenetic Inference under the Fossilized Birth-Death ProcessIn this exercise, we will create a script in Rev, the interpreted programming language used by RevBayes, that defines the model outlined above and specifies the details of the MCMC simulation. This script can be executed in RevBayes to complete the full analysis.We conclude the exercise by evaluating the performance of the MCMC and summarizing the results.Data and Files   On your own computer or your remote machine, create a directory called RB_FBD_Tutorial(or any name you like).  Then, navigate to the folder you created and make a new one called data.  In the data folder, add the following files (click on the hyperlinked file names to download):      bears_taxa.tsv: a tab-separated table listing the 18 bear species in our analysis (both fossil and extant) and their occurrence age ranges (minimum and maximum ages). For extant taxa, the minimum age is 0.0 (i.e. the present).    bears_morphology.nex: a matrix of 62 discrete, binary (coded 0 or 1) morphological characters for our 18 species of fossil and extant bears.  Now you can create a separate file for the Rev script.  In the RB_FBD_Tutorial directory created above, create a blank file called FBD_tutorial.Rev and open it in a text editor.  It is also possible to execute this entire tutorial in the RevBayes console.The file FBD_tutorial.Rev will contain all of the instructions required to load the data, assemble the different model components used in the analysis, and configure and run the Markov chain Monte Carlo (MCMC) analysis.Once you finish writing this file, you can compare your script with the example provide with this tutorial (see links above).Importing Data into RevBayes We will begin our Rev script by loading in the two data files that were downloaded and saved to the data directory.In RevBayes, we use functions to read the contents of files and assign them to variables in our workspace.First, we will create a variable called taxa that will contain the data from bears_taxa.tsv.taxa &lt;- readTaxonData(\"data/bears_taxa.tsv\")The file bears_taxa.tsv contains a table with all of the fossil and extant bear species names in the first column, their minimum age in the second column and their maximum age in the third column.We use the function readTaxonData to load this table into the workspace.Next, we will import the morphological character matrix from bears_morphology.nex and assign it to the variable morpho.In this exercise, we are using a NEXUS-formatted data file, but it is worth noting that several other file-types are acceptable depending on the kind of data (e.g., FASTA for molecular data).morpho &lt;- readDiscreteCharacterData(\"data/bears_morphology.nex\")Here, we use the function readDiscreteCharacterData to load a data matrix to the workspace from a formatted file. This function can be used for discrete morphological data as well as molecular sequence data (e.g., nucleotides, amino acids).Helper Variables Before we begin specifying the hierarchical model, it is useful to instantiate some “helper variables” that will be used in our model and MCMC specification throughout our script.First, we will create a new constant node called n_taxa that is equal to the number of species in our analysis (18).n_taxa &lt;- taxa.size()Next, we will create a workspace variable called moves, which is a vector that will contain all of the MCMC moves used to propose new states for every stochastic node in the model graph. Each time a new stochastic node is created in the model, we can append the corresponding moves to this vector.moves = VectorMoves()One important distinction here is that moves is part of the RevBayes workspace and not the hierarchical model. Thus, we use the workspace assignment operator = instead of the constant node assignment &lt;-.The Fossilized Birth-Death Process Speciation and Extinction RatesTwo key parameters of the FBD process are the speciation rate (the rate at which lineages are added to the tree, denoted by $\\lambda$ in ) and the extinction rate (the rate at which lineages are removed from the tree, $\\mu$ in ).We will place exponential priors on both of these values, meaning we assume each parameter is drawn independently from a different exponential distribution, where each distribution has a rate parameter equal to 10.Note that an exponential distribution with a rate of 10 has an expected value (mean) of 1/10.Create the exponentially distributed stochastic nodes for the speciation_rate and extinction_rate using the ~ stochastic assignment operator.speciation_rate ~ dnExponential(10)extinction_rate ~ dnExponential(10)The ~ operator in Rev instantiates a stochastic node in the model (i.e., a solid circle in ). Every stochastic node must be defined by a distribution. In this case, we use the Exponential.In the Rev language, every distribution has the prefix dn to make it easier to locate the various distributions in the Rev language documentation (https://revbayes.com/documentation).When a stochastic node is created in the model, the distribution function assigns it an initial value by drawing a random value from the prior distribution and assigns the node to the named variable.For every stochastic node we declare, we must also specify proposal algorithms (called moves) to sample the value of the parameter in proportion to its posterior probability.If a move is not specified for a stochastic node, then it will not be estimated, but fixed to its initial value.The extinction rate and speciation rate are both positive, real numbers (i.e., non-negative floating point variables).For both of these nodes, we will use a scaling move (mvScale), which proposes multiplicative changes to a parameter.moves.append( mvScale(speciation_rate, weight=1) )moves.append( mvScale(extinction_rate, weight=1) )You will also notice that each move has a specified weight.This option indicates the frequency a given move will be performed in each MCMC cycle.In RevBayes, the MCMC is executed by default with a schedule of moves at each step of the chain, instead of just one move per step, as is done in MrBayes (Ronquist and Huelsenbeck 2003) or BEAST (Drummond et al. 2012; Bouckaert et al. 2014).Here, if we were to run our MCMC with our current vector of 2 moves each with a weight of 1, then our move schedule would perform 2 moves in each cycle. Within a cycle, an individual move is chosen from the move list in proportion to its weight. Therefore, with both moves assigned weight=1, each has an equal probability of being executed and will be performed on average one time per MCMC cycle.For more information on moves and how they are performed in RevBayes, please refer to the Introduction to Markov chain Monte Carlo (MCMC) Sampling and Nucleotide substitution models tutorials.In addition to the speciation ($\\lambda$) and extinction ($\\mu$) rates, we may also be interested in inferring the net diversification rate ($\\lambda - \\mu$) and the turnover ($\\mu/\\lambda$).Since these parameters can each be expressed as a deterministic transformation of the speciation and extinction rates, we can monitor their values (i.e., track their values and print them to a file) by creating two deterministic nodes using the := deterministic assignment operator.diversification := speciation_rate - extinction_rateturnover := extinction_rate/speciation_rateDeterministic nodes are represented by circles with dotted borders in a probabilistic graphical model. To maintain the simplicity of the model in , the diversification rate and turnover are not shown.Extant Sampling ProbabilityEvery extant bear species is represented in this dataset. Therefore, we will fix the probability of sampling an extant lineage ($\\rho$ in ) to 1. The parameter rho will be specified as a constant node (new values for rho will not be sampled in the MCMC) using the &lt;- constant assignment operator.rho &lt;- 1.0Because $\\rho$ is a constant node, we do not have to assign a move to this parameter because we assume the value is known and fixed.Fossil Sampling RateSince our data set includes serially sampled lineages, we must also account for the rate of sampling through time. This is the fossil sampling (or recovery) rate ($\\psi$ in ), which we will instantiate as a stochastic node named psi.As with the speciation and extinction rates (see ), we will use an exponential prior on this parameter and apply a scale move to sample values from the posterior distribution.psi ~ dnExponential(10) moves.append( mvScale(psi, weight=1) )Origin TimeThe FBD process is conditioned on the origin time ($\\phi$ in ), which requires specification of a node representing the age of the clade.We will set a uniform distribution on the origin age, with the lower bound set at the age of the oldest bear fossil (37 My) and the higher bound of 55 My set to the age of the most recent common ancestor of crown Carnivora estimated by recent studies (dos Reis et al. 2012).For the move, we will use a sliding window move (mvSlide), which samples a parameter uniformly within an interval (defined by the half-width delta, which is set to 1 by default). Sliding window moves can be problematic for small values, as the window may overlap zero. However, our prior on the origin age excludes values $\\leq 37.0$, so this is not an issue.origin_time ~ dnUnif(37.0, 55.0)moves.append( mvSlide(origin_time, weight=1.0) )The FBD Tree Now that we have specified all of the parameters of the FBD process ($\\lambda$, $\\mu$, $\\phi$, $\\psi$), we will use these parameters to instantiate the stochastic node representing the time-calibrated tree that we will call fbd_tree.The fbd_tree ($\\mathcal{T}$ in ) is generated by a fossilized birth-death distribution and is conditionally dependent on $\\lambda$, $\\mu$, $\\phi$, and $\\psi$.The FBD distribution function fnFBDP takes the FBD parameters as arguments as well as the taxa variable which specifies the number of terminal taxa as well as the taxon labels.fbd_tree ~ dnFBDP(origin=origin_time, lambda=speciation_rate, mu=extinction_rate, psi=psi, rho=rho, taxa=taxa)Next, in order to sample from the posterior distribution of trees, we need to specify moves that propose changes to the topology (e.g. mvFNPR) and node times (e.g. mvNodeTimeSlideUniform). We also include a proposal that will collapse or expand a fossil branch (mvCollapseExpandFossilBranch), thus sampling trees where a given fossil is either a sampled ancestor or a sampled tip.In addition, when conditioning on the origin time, we also need to explicitly sample the root age (mvRootTimeSlideUniform).moves.append( mvFNPR(fbd_tree, weight=15.0) )moves.append( mvCollapseExpandFossilBranch(fbd_tree, origin_time, weight=6.0) )moves.append( mvNodeTimeSlideUniform(fbd_tree, weight=40.0) )moves.append( mvRootTimeSlideUniform(fbd_tree, origin_time, weight=5.0) )Note that we specified a higher move weight for each of the proposals operating on fbd_tree than we did for the previous stochastic nodes.This means that our move schedule will propose fifteen times as many new topologies via the mvFNPR move as it will new values of speciation_rate using mvScale, for example.By increasing the number of times new values are proposed for a parameter, we are increasing the sampling intensity for that parameter.Typically, we do this for parameters that we are particularly interested in or for parameters that tend to induce long mixing times.A node like $\\mathcal{T}$ in our graphical model () represents a complex set of variables: the tree topology and all divergence times.Moreover, the likelihoods of our fossil occurrence data and the morphological character data are both conditionally dependent on the time tree.Such complex variables require more extensive sampling than other nodes.Sampling Fossil Occurrence Times We need to account for uncertainty in the age estimates of our fossils using the observed minimum and maximum stratigraphic ages that are provided in the file bears_taxa.tsv.We can represent the fossil likelihood using any uniform distribution that is non-zero when the likelihood is equal to one (see ).For example, if $t_i$ is the inferred fossil age and $(a_i,b_i)$ is the stratigraphic age uncertainty interval, we know the likelihood is equal to one when $a_i &lt; t_i &lt; b_i$, or equivalently $t_i - b_i &lt; 0 &lt; t_i - a_i$.So we can represent this likelihood using a uniform random variable,  uniformly distributed in $(t_i - b_i, t_i - a_i)$ and clamped at zero.To do this, we will get all the fossils from the tree and use a for loop to iterate over them. For each fossil observation, we will create a uniform random variable representing the likelihood, based on the minimum and maximum ages specified in the file bears_taxa.tsv.fossils = fbd_tree.getFossils()for(i in 1:fossils.size()){    t[i] := tmrca(fbd_tree, clade(fossils[i]))    a_i = fossils[i].getMinAge()    b_i = fossils[i].getMaxAge()    F[i] ~ dnUniform(t[i] - b_i, t[i] - a_i)    F[i].clamp( 0 )}Finally, we will add a move that samples the ages of all the fossils on the tree.moves.append( mvFossilTimeSlideUniform(fbd_tree, origin_time, weight=5.0) )Monitoring Parameters of Interest There are additional parameters that may be of particular interest to us that are not directly sampled as part of the graphical model defined thus far.As with the diversification and turnover nodes specified in , we can create deterministic nodes to sample the posterior distributions of these parameters.Here we will create a deterministic node called num_samp_anc that will compute the number of sampled ancestors in our fbd_tree.num_samp_anc := fbd_tree.numSampledAncestors();We are also interested in the age of the most-recent-common ancestor (MRCA) of all living bears.To monitor this age in our MCMC sample, we must use the clade function to identify the node corresponding to the MRCA.Once this clade is defined we can instantiate a deterministic node called age_extant that will record the age of the MRCA of all living bears, using the tmrca() function.clade_extant = clade(\"Ailuropoda_melanoleuca\",\"Tremarctos_ornatus\",\"Melursus_ursinus\",                    \"Ursus_arctos\",\"Ursus_maritimus\",\"Helarctos_malayanus\",                    \"Ursus_americanus\",\"Ursus_thibetanus\")age_extant := tmrca(fbd_tree, clade_extant)In the same way we monitored the MRCA of the extant bears, we can also monitor the age of a fossil taxon that we may be interested in recording. We will monitor the marginal distribution of the age of Kretzoiarctos beatrix, which is sampled between 11.2–11.8 My.age_Kretzoiarctos_beatrix := tmrca(fbd_tree, clade(\"Kretzoiarctos_beatrix\"))Modeling the Evolution of Binary Morphological Characters The next part of the graphical model we will define specifies the model of morphological character evolution.This component includes the substitution model, the model of rate variation among characters, and the model ofrate variation among branches ().As stated in the  section, we willuse the Mk model to model our data.Because the Mk model is a generalization ofthe Jukes-Cantor model (Jukes and Cantor 1969), we will initialize our instantaneous rate matrix from a Jukes-Cantormatrix.The constant node Q_morpho corresponds to the two-state rate matrix $Q$ in .Q_morpho &lt;- fnJC(2)We will assume that rates vary among characters in our data matrix according to a discretized gamma distribution (described in the section on ).For this model, we create a vector of rates named rates_morpho which is the productof a function fnDiscretizeGamma() thatdivides up a gamma distribution into a set of equal-probability bins ($\\mathbf{R}$ in Figure ). Here, our only stochastic node is alpha_morpho ($\\alpha$ in (missing reference), which is the shapeparameter of the discretized gamma distribution.alpha_morpho ~ dnExponential( 1.0 )rates_morpho := fnDiscretizeGamma( alpha_morpho, alpha_morpho, 4 )moves.append( mvScale(alpha_morpho, weight=5.0) )The phylogenetic model also assumes that each branch has a rate ofmorphological character change.For simplicity, we will assume a strictmorphological clock — meaning that every branch has the same rate rate represented by the stochastic node clock_morpho ($c$ in ), which is drawn froman exponential distribution (see the  section).clock_morpho ~ dnExponential(1.0)moves.append( mvScale(clock_morpho, weight=4.0) )The Phylogenetic CTMCIf you refer to , you will see that we have defined almost all of the components of thecomplete model except for the observed node representingour morphological character data ($\\mathcal{M}$).The character matrix is a clamped stochastic node thatis generated by a phylogenetic continuous-time Markov chain (CTMC) distribution.This node is conditionally dependent on the time tree ($\\mathcal{T}$: fbd_tree), clock rate ($c$: clock_morpho), site rates ($\\mathbf{R}$: rates_morpho), and the two-state Mk rate matrix ($Q$: Q_morpho).With all of these nodes instantiated in the graphical model,we can now connect the components by defining thenode representing our observed morphological data.There are some uniqueaspects to specifying a phylogenetic CTMC for morphologicaldata.You will notice that we have an option called coding.This optionallows us to condition on biases in the way the morphological data werecollected (i.e., ascertainment bias).By setting coding=variable we can correct for coding only variable characters (discussed in (Lewis 2001)).phyMorpho ~ dnPhyloCTMC(tree=fbd_tree, siteRates=rates_morpho, branchRates=clock_morpho, Q=Q_morpho, type=\"Standard\", coding=\"variable\")phyMorpho.clamp(morpho)Now that we have defined our complete model, we can create a workspace variable that packages the entire model graph.This makes it easy to pass the whole model to functions thatwill set up our MCMC analysis.This variable is created using the model() function, whichtakes only a single node in the graph.We will use the fbd_tree node, but you can try this with an alternative node (e.g., clock_morpho, rho, etc.).As long as you have established all of the connections among the model parameters, the model() function will find everyother node by traversing the edges of the graph ().mymodel = model(fbd_tree)Monitoring Variables We have defined the full probabilistic graphical modelshown in  and we are now ready tospecify the details of our MCMC analysis.The first step in setting up the analysis is tocreate monitors that will record the values ofeach parameter in our model for every sampled cycle ofthe MCMC.The sampled values are saved to file (or printed to screen) and can be summarized when our MCMC simulation is complete.Let’s create three different monitor objects for this analysis.To manage the monitors in RevBayes, we create anotherworkspace variable called monitors that is a vector containing the three monitor variables.monitors = VectorMonitors()We will append our first monitor to the monitors vector.This will create a file called bears.log in a directory called output (if this directory does not already exist, RevBayes will create it).The function mnModel() initializes a monitor that saves all of the numerical parameters in the model to a tab-delineated file.This file is useful for summarizing marginal posteriors in statistical plotting tools like Tracer (Rambaut et al. 2018) or R (R Core Team 2020). We will exclude the F vector from logging, as it is purely used as an auxiliary variable for estimating fossil ages, and is clamped to 0.Additionally, we also specify how frequently we sample our Markov chain by setting the printgen option.We will sample every 10 cycles of our MCMC.Importantly, we also specify how frequently we sample our Markov chain by setting the printgen option.We will sample every 10 cycles of our MCMC.monitors.append( mnModel(filename=\"output/bears.log\", printgen=10, exclude = [\"F\"]) )You may think that sampling every 10 generations may be too frequent to avoid correlation between samples in our MCMC.However, recall that a single “generation” in RevBayes performs a schedule of moves that is determined by the number of moves in the moves vector and the weights assigned to those moves (see the  section).Thus, a single generation in this analysis will involve 26 moves, so if we record every 10 generations, there will be260 moves between each sample.We want to create a separate file containing samples ofthe tree and branch lengths since these will not besaved by the monitor defined above.To save the tree parameter, we can use themnFile() function that saves specific parametersto a file.We indicate the parameters by including them in the function’s options.monitors.append( mnFile(filename=\"output/bears.trees\", printgen=10, fbd_tree) )The final monitor will print updates of our MCMC to the screen.The screen monitor function, mnScreen() allows us to addparameters in our model that will be displayed along witha few default values (including the current iteration, posterior, likelihood, and prior).We will monitor the age of the MRCA of the living bears, the number of sampled ancestors, and the origin time.monitors.append( mnScreen(printgen=10, age_extant, num_samp_anc, origin_time) )Setting up and Running the MCMC Sampler Our Rev script specifies the three major parts of ourMCMC analysis: a model (mymodel), a list of MCMC proposals (moves), and a way to save the values sampled by our Markov chain (monitors).With these three components, we can set up our analysis using the mcmc() function.This function creates a workspace variable that we can useto execute the MCMC simulation.mymcmc = mcmc(mymodel, monitors, moves)Using our variable mymcmc, we can execute the run()member method to start our MCMC sampler.mymcmc.run(generations=10000)Finally, since we are going to save this analysis in a script file and run it in RevBayes, it is useful to include a statement that will quit the program when the run is complete.q()  Your script is now complete!  Save the FBD_tutorial.Rev file in the RB_FBD_Tutorial directory.Execute the Analysis Script in RevBayes With your script complete and data files in the proper locations, you can execute the FBD_tutorial.Rev script in RevBayes.  Run the RevByes executable.  On Unix systems, if the RevBayes is in your path, you simply need to navigate to the RB_FBD_Tutorial directory and type rb.  If the RevBayes executable is not in your path, you can execute it and then change your working directory using the setwd() function which takes the absolute path to your directory as an argument.  setwd(\"&lt;path to&gt;/RB_FBD_Tutorial\")  Once RevBayes is in the correct directory (RB_FBD_Tutorial), you can then use the source() function to feed RevBayes your master script file (FBD_tutorial.Rev).source(\"FBD_tutorial.Rev\")   Processing file \"FBD_tutorial.Rev\"   Successfully read one character matrix from file 'data/bears_morphology.nex'   Running MCMC simulation   This simulation runs 1 independent replicate.   The simulator uses 11 different moves in a random move schedule with 26 moves per iteration   ...This will execute the analysis and you should see the various parameters—specified when you initialized the screen monitor—printed to the screen every 10 generations.When the analysis is complete, RevBayes will quit and you will have a new directory called output that will contain all of the files you specified with the monitors.Results Two files are created by the monitors in the  section. These files, located in the output directorycontain the record of values sampled for the various parameters of the model over the course of the MCMC. In the following sections, we will assess the performance of our MCMC sampler and summarize the marginal posterior distributions of numerical parameters (in the file bears.log) and the time-calibrated phylogeny (in the file bears.trees).Evaluating the MCMC SamplerThe first step when analyzing the output of an MCMC run is to check whether the chain has converged on the stationary distribution and sampled effectively (i.e., achieved “good mixing”). This can be done by loading the parameter log, in our case the file bears.log, in a program such as Tracer (Rambaut et al. 2018), shown in .Analysis in Tracer of the parameter estimates obtained on the bears dataset.On the left side is a panel summarizing all the parameters appearing in the log, with their mean estimate and ESS value (effective sample size). The ESS of a parameter determines whether the chain has adequately sampled the associated variable: values above 200 are considered “good”, whereas values below 200, highlighted by Tracer in yellow or red, indicate poor mixing.Explicitly, the ESS measures the degree of independence between samples and parameters with signatures of autocorrelation between samples are indicative of an inadequate sampler.Here we can see that the chain has mixed well for some parameters, but not others. In particular, we see low ESS values for the origin time (origin_time) and the ages of some fossil tips (t[1], t[9], and t[10]).This may indicate that the MCMC sampler hasnot converged on the stationary distribution for these parameters, which are associated with the FBD tree. What this assessment reveals is that we did not perform enough proposals for these parameters.Thus, it will be important to run the MCMC for more generations (specified in the  section) and/orincrease the weights of moves applied to thesestochastic nodes (e.g., the mvSlide applied to origin_time in the  section). For more details on diagnosing convergenceof MCMC samples under the FBD model, please see the tutorial on combined-evidence analysis in RevBayes.Summarizing the TreeOnce we are certain that our MCMC has effectivelysampled the joint posterior distribution of our model parameters, we cansummarize the tree topology, branch times, and fossil ages that were saved to output/bears.trees using some built-in RevBayes functions.  Run the RevByes executable, making sure that the working directory is RB_FBD_Tutorial.The file bears.trees contains the trees and associated parameters that were sampled every 10 generations by our monitor. In RevBayes, we often refer to a set of samples from our MCMCas a “trace”.Begin by loading the tree trace into RevBayes from the bears.trees file.trace = readTreeTrace(\"output/bears.trees\")   Processing file \"&lt;path to&gt;/RB_FBD_Tutorial/output/bears.trees\"Progress:0---------------25---------------50---------------75--------------100********************************************************************By default, a burn-in of 25% is used when reading in the tree trace (250 trees in our case). Note that this is different from Tracer, which uses a burn-in fraction of 10% by default. You can specify a different burn-in fraction, say 50%, by typing the command trace.setBurnin(500).Now we will use the mccTree() function to return a maximum clade credibility (MCC) tree. The MCC tree is the tree with the maximum product of the posterior clade probabilities. When considering trees with sampled ancestors, we refer to the maximum sampled ancestor clade credibility (MSACC) tree (Gavryushkina et al. 2017).mccTree(trace, file=\"output/bears.mcc.tre\" )When there are sampled ancestors present, visualizing the tree can be fairly difficult in traditional tree viewers. When there are sampled ancestors present, visualizing the tree can be fairly difficult in traditional tree viewers. We will make use of a browser-based tree visualization tool called IcyTree (Vaughan 2017), which can be accessed at https://icytree.org. IcyTree has many unique options for visualizing phylogenetic trees and can produce publication-quality vector image files (i.e. SVG). Additionally, it correctly represents sampled ancestors on the tree as nodes, each with only one descendant ().  Navigate to https://icytree.org and open the file output/bears.mcc.tre in IcyTree.  Try to replicate the tree in  (Hint: Style &gt; Mark Singletons)  ⇨ Why might a node with a sampled ancestor bereferred to as a singleton?  ⇨ How can you see the names of the fossils that are putative sampled ancestors?  ⇨ What is theposterior probability that Zaragocyon daamsi is a sampled ancestor?Maximum sampled ancestor clade credibility (MSACC) tree of bear species used in this tutorial.SummaryIn this tutorial, we have introduced core information about how morphological and age information are modeled for use with the FBD model in RevBayes. We have also discussed important aspects of executing and summarizing MCMC analysis.This exercise uses a simplified data set and set of models for analysis of fossil and extant data.Most researchers working on living taxa have access to molecular (including genomic) data andmay be interested in applying these methodsto much larger datasets and more complex problems.Note that the goal of this tutorial is to provide a concise introduction to the framework for analysis of paleontological and neontological data in RevBayes. For more information on how to apply RevBayesdatasets combining morphological and molecularcharacters, please refer to the following tutorial:  Combined-Evidence Analysis and the Fossilized Birth-Death Process for Analysis of Extant Taxa and Fossil Specimens",
        "url": "/tutorials/fbd_simple/",
        "index": "false"
      }
      ,
    
      "tutorials-partition": {
        "title": "Partitioned data analysis",
        "content": "This tutorial comes with a recorded video walkthrough, available here: . The video corresponding to each section of the exercise is linked next to the section title.OverviewThis tutorial provides the second protocol from our recent publication(Höhna et al. 2017). The first protocol is described in the Nucleotide substitution modelsand the third protocol is described in the Model selection of common substitution models for one locus.This tutorial demonstrates how to accommodate variation in thesubstitution process across sites of an alignment. In the precedingtutorials, we assumed that all sites in an alignment evolved under anidentical substitution process. This assumption is likely to be violatedbiologically, since different nucleotide sites are subject to differentselection pressures, such as depending on which gene or codon positionthe site belongs to. Here, we will demonstrate how to specify—and selectamong—alternative data partition schemes using ‘RevBayes‘. This iscommonly referred to as partitioned-data analysis, where two or moresubsets of sites in our alignment are assumed to evolve under distinctprocesses.This tutorial will construct three multi-gene models. The first model,Partition_uniform, assumes all genes evolve under the same processparameters. The second model, Partition_gene, assumes all genes evolveaccording to the same process, but each gene has its own set of processparameters. The third model, Partition_codon, partitions the data notonly by gene, but also by codon position. Each analysis will generate amaximum a posteriori tree to summarize the inferred phylogeny. Anadvanced exercise introduces how to compute Bayes factors to selectacross various partitioning schemes.All of the files for this analysis are provided for you and you can runthese without significant effort using the ‘source()‘ function in the‘RevBayes‘ console, e.g.,source(\"scripts/mcmc_Partition_uniform.Rev\")If everything loaded properly, then you should see the program beginrunning the Markov chain Monte Carlo analysis needed for estimating theposterior distribution. If you continue to let this run, you willsee it output the states of the Markov chain once the MCMC analysisbegins.Overview Introduction &amp; BackgroundVariation in the evolutionary process across the sites of nucleotidesequence alignments is well established, and is an increasinglypervasive feature of datasets composed of gene regions sampled frommultiple loci and/or different genomes. Inference of phylogeny fromthese data demands that we adequately model the underlying processheterogeneity; failure to do so can lead to biased estimates ofphylogeny and other parameters (Brown and Lemmon 2007).Accounting for process heterogeneity involves adopting a partitioneddata approach (sometimes also called a ‘mixed-model’ approach(Ronquist and Huelsenbeck 2003)), in which the sequence alignment is first parsed into anumber of data subsets that are intended to capture plausible processheterogeneity within the data. The determination of the partitioningscheme is guided by biological considerations regarding the dataset athand. For example, we might wish to evaluate possible variation in theevolutionary process within a single gene region(e.g., between stem and loop regions ofribosomal sequences), or among gene regions in a concatenated alignment(e.g., comprising multiple nuclear lociand/or gene regions sampled from different genomes). The choice ofpartitioning scheme is up to the investigator and many possiblepartitions might be considered for a typical dataset.In this exercise, we assume that each data subset evolved under anindependent general-time reversible model with gamma-distributed ratesacross sites (GTR+$\\Gamma$). Under this model the observed data areconditionally dependent on the exchangeability rates ($\\theta$),stationary base frequencies ($\\pi$), and the degree of gamma-distributedamong-site rate variation ($\\alpha$), as well as the rooted tree($\\Psi$) and branch lengths. When we assume different GTR+$\\Gamma$models for each data subset, this results in a composite model, in whichall sites are assumed to share a common, rooted tree topology andproportional branch lengths, but subsets of sites are assumed to haveindependent substitution model parameters. Finally, we perform aseparate MCMC simulation to approximate the joint posterior probabilitydensity of the phylogeny and other parameters.For most sequence alignments, several (possibly many) partition schemesof varying complexity are plausible a priori, which therefore requiresa way to objectively identify the partition scheme that balancesestimation bias and error variance associated with under- andover-parameterized mixed models, respectively. Increasingly,partition-model selection is based on Bayes factors[e.g., (Suchard et al. 2001)], which involves firstcalculating the marginal likelihood under each candidate partitionscheme and then comparing the ratio of the marginal likelihoods for theset of candidate partition schemes(missing reference). The analysis pipeline that we will use in this tutorial is depicted in Figure [fig:pipeline].   The analysis pipeline forExercise 1. We will explore three partition schemes for the primatesdataset. The first model (the ‘uniform model’, $M_0$) assumes that allsites evolved under a common GTR+$\\Gamma$ substitution model. The secondmodel (the ‘moderately partitioned’ model, $M_1$) invokes two datasubsets corresponding to the two gene regions (cytB and cox2), andassumes each subset of sites evolved under an independent GTR+$\\Gamma$model. The final partition model (the ‘highly partitioned’ model, $M_2$)invokes four data subsets—the first two subsets corresponds to the cytBgene region, where the first and second codon position sites arecombined into one subset distinct from the third codon position sites,and the cox2 gene has two subsets of its own, partitioned by codonpositions in the same way—and each data subset is assumed evolved underan independent GTR+$\\Gamma$ substitution model. Note that we assume thatall sites share a common tree topology, $\\Psi$, and branch-lengthproportions, for each of the candidate partition schemes.We perform twoseparate sets of analyses for each partition model—a MCMC simulation toapproximate the joint posterior probability density of thepartition-model parameters, and a ‘power-posterior’ MCMC simulation toapproximate the marginal likelihood for each mixed model. The resultingmarginal-likelihood estimates are then evaluated using Bayes factors toassess the fit of the data to the three candidate partition models.Concatenated, Non-partitionedOur first exercise is to construct a multi-gene analysis where all genesevolve under the same process and parameters.Setting up the modelLoading and preparing the dataTo begin, load in the sequences using the ‘readDiscreteCharacterData()‘function.data_cox2 = readDiscreteCharacterData(\"data/primates_and_galeopterus_cox2.nex\")data_cytb = readDiscreteCharacterData(\"data/primates_and_galeopterus_cytb.nex\")Since the first step in this exercise is to assume a single model acrossgenes, we need to combine the two datasets using‘concatenate()‘data = concatenate( data_cox2, data_cytb )Typing ‘data‘ reports the dimensions of the concatenated matrix, thisprovides information about the alignment:   DNA character matrix with 23 taxa and 1837 characters   =====================================================   Origination:                   \"primates_and_galeopterus_cox2.nex\"   Number of taxa:                23   Number of included taxa:       23   Number of characters:          1837   Number of included characters: 1837   Datatype:                      DNAFor later use, we will store the taxon information (‘taxa‘) and thenumber of taxa and branches.n_taxa &lt;- data.ntaxa()num_branches &lt;- 2 * n_taxa - 3taxa &lt;- data.taxa()Additionally, we will create our move and monitor vectors.moves    = VectorMoves()monitors = VectorMonitors()Substitution modelNow we can proceed with building our GTR$+\\Gamma$ model. First, we willdefine and specify a prior on the exchangeability rates of the GTR modeler_prior &lt;- v(1,1,1,1,1,1) er ~ dnDirichlet( er_prior )and assign its movesmoves.append( mvBetaSimplex(er, alpha=10, tune=true, weight=3) )moves.append( mvDirichletSimplex(er, alpha=10.0, tune=true, weight=1.0) )We can use the same type of distribution as a prior on the 4 stationaryfrequencies ($\\pi_A, \\pi_C, \\pi_G, \\pi_T$) since these parameters alsorepresent proportions. Specify a flat Dirichlet prior density on thebase frequencies:pi_prior &lt;- v(1,1,1,1) pi ~ dnDirichlet( pi_prior )Now add the simplex scale move on the stationary frequencies to themoves vectormoves.append( mvBetaSimplex(pi, alpha=10, tune=true, weight=2) )moves.append( mvDirichletSimplex(pi, alpha=10.0, tune=true, weight=1.0) )We can finish setting up this part of the model by creating adeterministic node for the GTR rate matrix ‘Q‘. The ‘fnGTR()‘ functiontakes a set of exchangeability rates and a set of base frequencies tocompute the rate matrix used when calculating the likelihood of ourmodel.Q := fnGTR(er,pi)Among site rate variationWe will also assume that the substitution rates vary among sitesaccording to an one-parametric gamma distribution,i.e., where the shape equals the rate($\\alpha=\\beta$) and thus with mean 1.0 (Yang 1994). Since we do nothave good prior knowledge about the variance in site rates, we apply a uniform distribution between $1$ and $10^8$.Then create a stochastic node called ‘alpha‘ with a uniform prior:alpha ~ dnUniform( 0, 1E8 )The way the ASRV model is implemented involves discretizing the mean-onegamma distribution into a set number of rate categories. Thus, we cananalytically marginalize over the uncertainty in the rate at each site.To do this, we need a deterministic node that is a vector of ratescalculated from the gamma distribution and the number of ratecategories. The ‘fnDiscretizeGamma()‘ function returns thisdeterministic node and takes three arguments: the shape and rate of thegamma distribution and the number of categories. Since we want todiscretize a mean-one gamma distribution, we can pass in ‘alpha‘ forboth the shape and rate.Initialize the ‘gamma_rates‘ deterministic node vector using the‘fnDiscretizeGamma()‘ function with ‘4‘ bins:gamma_rates := fnDiscretizeGamma( alpha, alpha, 4, false )The random variable that controls the rate variation is the stochasticnode ‘alpha‘. This variable is a single, real positive value (‘RevType =RealPos‘). We will apply a simple scale move to this parameter. Thescale move’s tuning parameter is called ‘lambda‘ and this value dictatesthe size of the proposal.moves.append( mvScale(alpha, lambda=0.1, tune=false, weight=4.0) )Invariant sitesInvariant sites (sites that remain fixed throughout their evolutionaryhistory) may be seen as an extreme case of among-site rate variation. Incontrast to $+ \\Gamma$ models, the $+I$ model allows site someprobability of having substitution rate equal to zero. Here, we give theprobability of a site being invariant with ‘pinvar‘pinvar ~ dnBeta(1,1)moves.append( mvBetaProbability(pinvar, delta=10.0, tune=true, weight=2.0) )Tree priorThe tree topology and branch lengths are also stochastic nodes in ourmodel. For simplicity, we will use the same prior distribution on thetree topology, a uniform topology prior, and branch lengths, independentexponential prior distributions, as done in the Nucleotide substitution models.We will assume that all possible labeled, unrooted tree topologies haveequal probability. This is the ‘dnUniformTopology()‘ distribution in‘RevBayes‘. Note that in ‘RevBayes‘ it is advisable to specify theoutgroup for your study system if you use an unrooted tree prior,whereas other software, e.g.,MrBayes uses the firsttaxon in the data matrix file as the outgroup. Specify the ‘topology‘stochastic node by passing in the tip labels ‘names‘ to the‘dnUniformTopology()‘ distribution:out_group = clade(\"Galeopterus_variegatus\")topology ~ dnUniformTopology(taxa, outgroup=out_group)To update the unrooted tree topology, we can use both a nearest-neighborinterchange move (‘mvNNI‘) and a subtree-prune and regrafting move(‘mvSPR‘). These moves do not have tuning parameters associated withthem, thus you only need to pass in the ‘topology‘ node and proposal‘weight‘.moves.append( mvNNI(topology, weight=n_taxa/2.0) )moves.append( mvSPR(topology, weight=n_taxa/10.0) )The weight specifies how often the move will be applied either onaverage per iteration or relative to all other moves. Have a look at theMCMC Diagnosistutorialfor more details about moves and MCMC strategies.Next we have to create a stochastic node for each of the $2N-3$ branchesin our tree (where $N=$ ‘n_species‘). We can do this using a ‘for‘ loop— this is a plate in our graphical model. In this loop, we can createeach of the branch-length nodes and assign each move. Copy this entireblock of ‘Rev‘ code into the console:for (i in 1:num_branches) {    br_lens[i] ~ dnExponential(10.0)    moves.append( mvScale(br_lens[i]) )}It is convenient for monitoring purposes to add the tree length asdeterministic variable. The tree length is simply the sum of all branchlengths. . Accordingly, the tree length can be computed using the‘sum()‘ function, which calculates the sum of any vector of values.TL := sum(br_lens)Finally, we can create a phylogram (a phylogeny in which the branchlengths are proportional to the expected number of substitutions/site)by combining the tree topology and branch lengths. We do this using the‘treeAssembly()‘ function, which applies the value of the $i^{th}$member of the ‘br_lens‘ vector to the branch leading to the $i^{th}$node in ‘topology‘. Thus, the ‘psi‘ variable is a deterministic node:psi := fnTreeAssembly(topology, br_lens)Putting it All TogetherWe now have all the parameters needed to model the phylogeneticmolecular substitution processseq ~ dnPhyloCTMC(tree=psi, Q=Q,  siteRates=gamma_rates, pInv=pinvar, type=\"DNA\")To compute the likelihood, we condition the process on the data observedat the tips of the treeseq.clamp(data)Since the model is now specified, we wrap the components in aModel object.my_model = model(Q)Specifying MonitorsFor our MCMC analysis we need to set up a vector of monitors to savethe states of our Markov chain. The monitor functions are all called‘mn*‘, where ‘*‘ is the wildcard representing the monitor type. First,we will initialize the model monitor using the ‘mnModel‘ function. Thiscreates a new monitor variable that will output the states for all modelparameters when passed into a MCMC function.monitors.append( mnModel(filename=\"output/PS_uniform.log\",printgen=10) )The ‘mnFile‘ monitor will record the states for only the parameterspassed in as arguments. We use this monitor to specify the output forour sampled trees and branch lengths.monitors.append( mnFile(psi, filename=\"output/PS_uniform.trees\", printgen=10) )Finally, create a screen monitor that will report the states ofspecified variables to the screen with ‘mnScreen‘:monitors.append( mnScreen(alpha, pinvar, TL, printgen=1000) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, wecan now set up the MCMC algorithm that will sample parameter values inproportion to their posterior probability. The ‘mcmc()‘ function willcreate our MCMC object:mymcmc = mcmc(my_model, monitors, moves, nruns=2, combine=\"mixed\")Note that this will automatically run two independent replicated MCMCsimulations because we specified ‘nruns=2‘.Now, run the MCMC:mymcmc.run(generations=30000, tuningInterval=200)When the analysis is complete, you will have the monitor files in youroutput directory.‘RevBayes‘ can also summarize the tree samples by reading in thetree-trace file:treetrace = readTreeTrace(\"output/PS_uniform.trees\", treetype=\"non-clock\")treetrace.summarize()The ‘mapTree()‘ function will summarize the tree samples and write themaximum a posteriori tree to file:map_tree = mapTree(treetrace,\"output/PS_uniform_map.tre\")This completes the uniform partition analysis. The next two sectionswill implement more complex partitioning schemes in a similar manner.Partitioning by Gene RegionThe uniform model used in the previous section assumes that all sites inthe alignment evolved under the same process described by a shared tree,branch length proportions, and parameters of the GTR+$\\Gamma$substitution model. However, our alignment contains two distinct generegions—cytB and cox2—so we may wish to explore the possibility that thesubstitution process differs between these two gene regions. Thisrequires that we first specify the data partitions corresponding tothese two genes, then define an independent substitution model for eachdata partition.First, we’ll clear the workspace of all declared variablesclear()Since we wish to avoid individually specifying each parameter of theGTR+$\\Gamma$ model for each of our data partitions, we can loop overour datasets and create vectors of nodes. To do this, we begin bycreating a vector of data file names:filenames &lt;- v(\"data/primates_and_galeopterus_cox2.nex\", \"data/primates_and_galeopterus_cytb.nex\")Set a variable for the number of partitions:n_data_subsets &lt;- filenames.size()Next we’ll create a vector of data matrices called ‘data‘, and a corresponding vector recording the number of sites in each partition:for (i in 1:n_data_subsets){    data[i] = readDiscreteCharacterData(filenames[i])    num_sites[i] = data[i].nchar()}Now we can initialize some important variables. This does require,however, that both of our alignments have the same number of species andmatching tip names.taxa &lt;- data[1].taxa()n_taxa &lt;- data[1].ntaxa()num_branches &lt;- 2 * n_taxa - 3moves    = VectorMoves()monitors = VectorMonitors()Specify the Parameters by Looping Over PartitionsWe can avoid creating unique names for every node in our model if we usea ‘for‘ loop to iterate over our partitions. Thus, we will only have totype in our entire GTR+$\\Gamma$ model parameters once. This will producea vector for each of the unlinked parameters—e.g., there will be a vector of ‘alpha‘nodes where the stochastic node for the first partition (cytB) will be‘alpha[1]‘ and the stochastic node for the second partition (cox2)will be called ‘alpha[2]‘.The script for the model,RevBayes_scripts/mcmc_Partition_gene.Rev, creates themodel parameters for each partition in one large loop. Here, we willsplit the loop into smaller parts to achieve the same end.First, we will create the GTR rate matrix for partition $i$ by firstcreating exchangeability ratesfor (i in 1:n_data_subsets) {    er_prior[i] &lt;- v(1,1,1,1,1,1)    er[i] ~ dnDirichlet(er_prior[i])    moves.append( mvBetaSimplex(er[i], alpha=10, tune=true, weight=3) )}and stationary frequenciesfor (i in 1:n_data_subsets) {    pi_prior[i] &lt;- v(1,1,1,1)    pi[i] ~ dnDirichlet(pi_prior[i])    moves.append( mvBetaSimplex(pi[i], alpha=10, tune=true, weight=2) )}then passing those parameters into a rate matrix functionfor (i in 1:n_data_subsets) {    Q[i] := fnGTR(er[i],pi[i]) }which states the rate matrix (Q[i]) for partition $i$ isdetermined by the exchangeability rates (er[i]) andstationary frequencies (pi[i]) also defined for partition$i$. Following this format, we construct the remaining partitionparameters: the $+\\Gamma$ mixture modelfor (i in 1:n_data_subsets) {    alpha[i] ~ dnUniform( 0.0, 1E8 )    gamma_rates[i] := fnDiscretizeGamma( alpha[i], alpha[i], 4, false )    moves.append( mvScale(alpha[i],weight=2) )}the $+I$ invariant sites modelfor (i in 1:n_data_subsets) {    pinvar[i] ~ dnBeta(1,1)    moves.append( mvBetaProbability(pinvar[i], delta=10.0, tune=true, weight=2.0) )}and the per-partition substitution rate multipliers# specify a rate multiplier for each partitionpart_rate_mult ~ dnDirichlet( rep(10.0, n_data_subsets) )moves.append( mvBetaSimplex(part_rate_mult, alpha=1.0, tune=true, weight=n_data_subsets) )moves.append( mvDirichletSimplex(part_rate_mult, alpha=1.0, tune=true, weight=2.0) )# Note that here we are dividing two vectors element-wise, i.e., # each element of part_rate_mult gets divided by the corresponding# element of num_sites. Then we multiply the result by sum(num_sites),# which is just a scalar. This operation ensures that the weighted mean# of partition-specific branch lengths, weighted by the number of sites# in each partition, stays equal to the branch lengths we are # actually sampling.part_rate := (part_rate_mult / num_sites) * sum(num_sites)Different Substitution Models for each GeneAlternatively, we might be interested in applying different substitutionmodels for each gene independently instead of assuming the samesubstitution albeit with different parameters for each gene. In this twogene case this is rather simple to do by specifying the substitutionmodel for each gene independently. For many genes this might becomelengthy and you might want to write a script to generate this section(note: we may provide such scripts soon).For simplicity and sake of demonstration, we assume that the cytochromeb region evolves under a Jukes-Cantor substitution model and the COX-IIgene under an HKY substitution model. We begin with the cytochrome bgene and the Jukes-Cantor substitution model:# specify the JC rate matrixQ[1] &lt;- fnJC(4)Second, we specify the HKY substitution model for the COX-II gene:pi_prior &lt;- v(1,1,1,1) pi ~ dnDirichlet(pi_prior)# specify a move to propose updates to on pimoves.append( mvBetaSimplex(pi, weight=2) )moves.append( mvDirichletSimplex(pi, weight=1) )# specify a lognormal distribution as the prior distribution on kappakappa ~ dnLognormal(0.0,1.25)# a simple scaling move to update kappamoves.append( mvScale(kappa) )# Finally, create the HKY rate matrixQ[2] := fnHKY(kappa,pi)Note that we specified manually in this way our vector of rate matrices‘Q‘. We can thus specify any substitution model manually for a givengene. We hope that this brief example conveys the idea how to specifygene-specific substitution models. You can add rate-variation amongsites and/or probabilities for a site being invariant for each gene too.Finally, you can then either loop over all genes to create the‘dnPhyloCTMC‘ distribution (see below) if the structure of the modelallows it (i.e.,if all models have avariable for site-rate-variation and probabilities for invariant site),or you efficiently set these variables to default values(e.g.,‘pinvar[i]=0.0‘ if there is noprobability for a site being invariant for this gene), or you create the‘seq[i] $\\sim$ dnPhyloCTMC(…)‘ manually outside a loop as well.Tree priorWe assume that both genes evolve along the same tree. Hence, we need tospecify a random variable for our tree parameter which is the same aswas specified for mcmc_Partition_uniform.Rev.out_group = clade(\"Galeopterus_variegatus\")# Prior distribution on the tree topologytopology ~ dnUniformTopology(taxa, outgroup=out_group)moves.append( mvNNI(topology, weight=n_taxa/2.0) )moves.append( mvSPR(topology, weight=n_taxa/10.0) )# Branch length priorfor (i in 1:num_branches) {    bl[i] ~ dnExponential(10.0)    moves.append( mvScale(bl[i]) )}TL := sum(bl)psi := treeAssembly(topology, bl)Putting it all togetherSince we have a rate matrix and a site-rate model for each partition, wemust create a phylogenetic CTMC for each gene. Additionally, we must fixthe values of these nodes by attaching their respective data matrices.These two nodes are linked by the ‘psi‘ node and their log-likelihoodsare added to get the likelihood of the whole DAG.for (i in 1:n_data_subsets) {    seq[i] ~ dnPhyloCTMC(tree=psi, Q=Q[i], branchRates=part_rate[i], siteRates=gamma_rates[i], pInv=pinvar[i], type=\"DNA\")    seq[i].clamp(data[i])}The remaining steps should be familiar: wrap the model components in amodel objectmy_model = model(psi)Create monitorscreate the monitorsmonitors.append( mnModel(filename=\"output/PS_gene.log\",printgen=10) )monitors.append( mnFile(psi, filename=\"output/PS_gene.trees\", printgen=100) )monitors.append( mnScreen(TL, printgen=1000) )configure and run the MCMC analysismymcmc = mcmc(my_model, moves, monitors, nruns=2, combine=\"mixed\")mymcmc.run(30000,tuningInterval=200)and summarize the posterior density of trees with a MAP treetreetrace = readTreeTrace(\"output/PS_gene.trees\", treetype=\"non-clock\")treetrace.summarize()mapTree(treetrace,\"output/PS_gene_MAP.tre\")Partitioning by Codon Position and by GeneBecause of the genetic code, we often find that different positionswithin a codon (first, second, and third) evolve at different rates.Thus, using our knowledge of biological data, we can devise a thirdapproach that further partitions our alignment. For this exercise, wewill partition sites within the cytB and cox2 gene by codon position.clear()data_cox2 &lt;- readDiscreteCharacterData(\"data/primates_and_galeopterus_cox2.nex\")data_cytb &lt;- readDiscreteCharacterData(\"data/primates_and_galeopterus_cytb.nex\")We must now add our codon-partitions to the ‘data‘ vector. The first andsecond elements in the data vector will describe cytB data,and the third and fourth elements will describe cox2 data. Moreover, thefirst and third elements will describe the evolutionary process for thefirst and second codon position sites, while the second and fourthelements describe the process for the third codon position sites alone.We can create this by calling the helper function ‘setCodonPartition()‘,which is a member function of the data matrix. We are assuming that thegene is in frame, meaning the first column in your alignment is afirst codon position. The ‘setCodonPartition()‘ function takes a singleargument, the position of the alignment you wish to extract. It thenreturns every third column, starting at the index provided as anargument.Before we can use the use the ‘setCodonPartition()‘ function, we mustfirst populate the position in the ‘data‘ matrix with some sequences.Then we call the member function of ‘data[1]‘ to exclude all but the1$^{st}$ and 2$^{nd}$ positions for cox2.data[1] &lt;- data_cox2data[1].setCodonPartition( v(1,2) )Assign the 3$^{rd}$ codon positions for cox2 to ‘data[2]‘:data[2] &lt;- data_cox2data[2].setCodonPartition( 3 )Then repeat for cytB, being careful to store the subsetted data toelements 3 and 4:data[3] &lt;- data_cytbdata[3].setCodonPartition( v(1,2) )data[4] &lt;- data_cytbdata[4].setCodonPartition( 3 )Now we have a data vector containing each subset. We can then specifythe independent substitution models per data subset. The remaining partsof the model are identical to the previous exercise where we partitionedby gene.Don’t forget to rename the output files!Exercises      Reviewing posterior estimates. Open thePS_codon.log file in Tracer. Rememberthat data subsets 1 and 2 are for cox2, partitions 3 and 4 are forcytB, subsets 1 and 3 are for sites in the first and second codonpositions (per gene), and subsets 2 and 4 are for sites in the thirdand fourth codon positions (per gene).    Aside from the tree topology and branch lengths, each data subset ismodeled to have its own set of parameters. However, the posteriorestimates for some parameters appear quite similar between somepairs of subsets yet different between other pairs of subsets. Forexample, part_rate is the per-subset substitutionrate. This clock is approximately one order of magnitude faster forpartitions 2 and 4 (third codon position sites) than it is forsubsets 1 and 3 (non-third codon position sites).    Identify other parameter-subset relationships like this in theposterior. Under this model, would you consider the gene or thecodon site position to hold greater influence over the site’sevolutionary mode?        Comparison of MAP trees. Open the three inferredMAP trees in FigTree. Check to enable “Node Labels”,click “Display” and select “posterior” from the dropdown menu.Internal nodes now report the probability of the clade appearing inthe posterior density of sampled trees. Do different models yielddifferent tree topologies? Generally, do complex models providehigher or lower clade support?        Partitioned model selection. Bayes factors arecomputed as the ratio of marginal likelihoods (see General Introduction to Model selectionfor more details). Rather than constructing the analysis with anmcmc object, marginal likelihood computations rely onoutput from a powerPosterior object.    Copy mcmc_Partition_uniform.Rev toml_Partition_uniform.Rev. Inml_Partition_uniform.Rev, delete all lines after themodel function is called, so the MCMC is never run andthe MAP tree is never computed.    Instead, configure and run a power posterior analysis    pow_p = powerPosterior(mymodel, moves, monitors, \"output/model_uniform.out\", cats=127)pow_p.burnin(generations=5000,tuningInterval=200)pow_p.run(generations=2000)        then compute the marginal likelihood using the stepping stonesampler    ss = steppingStoneSampler(file=\"output/model_uniform.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")ss.marginal()        and again using the path sampler    ps = pathSampler(file=\"model_uniform.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")ps.marginal()      ",
        "url": "/tutorials/partition/",
        "index": "true"
      }
      ,
    
      "tutorials-dating": {
        "title": "Dating trees",
        "content": "OverviewThis tutorial aims to guide you through different options for calibrating species divergences to time using RevBayes. The exercises are based on a dataset of bears (family Ursidae) for which we have molecular sequence data for extant species, morphological data for extant and fossil species, and information about fossil sampling times. The material used in this tutorial is directly taken from three others that explore some of the topics in more detail.  Relaxed Clocks &amp; Time Trees written by Tracy Heath  Divergence Time Calibration written by Tracy Heath and Sebastian Höhna  Estimating a Time-Calibrated Phylogeny of Fossil and Extant Taxa using Morphological Data written by Joëlle Barido-Sottani, Joshua Justison, April M. Wright, Rachel C. M. Warnock, Walker Pett, and Tracy A. HeathIn exercises 1 and 2 we’ll use the molecular sequence data to infer the relationships among living species and transform the branch lengths assuming a strict or relaxed clock model. Since these analyses don’t incorporate any information from the geological record, this approach can only be used to infer the relative age of speciation events.In exercise 3 we’ll use information from the fossil record to calibrate the tree to absolute time using node-dating. Finally, in exercise 4 we’ll also incorporate the morphological data to infer the relationships and divergence times among living and fossil bear species (total-evidence dating).The data  Create a directory on your computer for this tutorial.In this directory, create a subdirectory called data, and download the data files that you can find on the left of this page.In the data folder, you should now have the following files:      bears_cytb.nex: an alignment in NEXUS format of 1,000 bp ofcytochrome b sequences for 8 living bear species.        bears_morphology.nex: a matrix of 62 discrete, binary (coded 0or 1) morphological characters for 18 species of fossil andextant bears.        bears_taxa.tsv: a tab-separated table listing every bear species(both fossil and extant) and their stratigraphic age ranges. For extant taxa, the minimum age is 0.0 (i.e. the present).  ScriptsFor more complex models and analyses, it’s useful to create separate Rev scripts that contain all the model parameters, moves, and functions for different model components (e.g. the substitution model and the clock model).  Create another subdirectory called scripts.In this tutorial, you will work primarily in your text editor and create a set of modular files that can be easily managed and interchanged. Examples of all the commands used to perform each analysis are also provided at the top of this page under Scripts but try to write the complete scripts yourself from the beginning to ensure you understand all the steps involved and the differences between setting up each analysis.Exercises  Click on the first exercise to begin!  The global molecular clock model  The uncorrelated exponential relaxed clock model  Estimating speciation times using node dating  Estimating speciation times using the fossilized birth-death process",
        "url": "/tutorials/dating/",
        "index": "true"
      }
      ,
    
      "tutorials-relative-time-constraints": {
        "title": "Dating with Relative Constraints",
        "content": "IntroductionIn phylogenetics, the term dating denotes the inference of node ages ofphylogenies. For example, dating a species tree involves inference of the timeswhen the species split. Dating phylogenies is central to understanding theevolution of life on Earth. However, evolutionary sequences evolve withdifferent rates, an observation that has been termed relaxed molecularclock. In general, it is difficult to map a phylogeny obtained from a multiplesequence alignment, and with branch lengths measured in average number ofsubstitutions per site, to a phylogeny with branch lengths measured in actualunits of time. For this reason, the molecular clocks are calibrated withinformation gained from fossils, which can be accurately dated. Incorporatingfossil information enables calibration of node ages of a phylogeny. Pleasealso see the tutorial on fossil calibrations.In this tutorial, we explore another possibility to improve on the accuracy ofdating a phylogeny. Namely, horizontal gene transfers and ancient evolutionaryrelationships such as symbioses are informative about the order of nodes of aphylogeny of interest. That is, horizontal gene transfers define relative nodeconstraints. Relative node constraints can be particularly helpful when thegeological record is sparse, for example, for microorganisms, which representthe vast majority of extant and extinct biodiversity. Combination of nodecalibrations with relative node constraints can significantly improve bothaccuracy and resolution of molecular clock estimates (Szöllősi et al. 2021).DefinitionsAlignment: Multiple sequence alignment.Timetree: Phylogeny with branch lengths measured in units of time. If theleaves have been sampled at the same time, for example, at the present, atime-tree is ultrametric.Branch-length tree: Phylogeny with branch lengths measured in expectednumbers of substitutions. Usually, branch-length trees are obtained from aphylogenetic analysis of an alignment.Calibration: Absolute node calibration; an estimate of the age of a node ofa timetree. This is usually associated with a prior describing the uncertaintyassociated to this node age.Constraint: Relative node order constraint, which specifies the relativeorder in time of two nodes of a tree (e.g. node A is older than node B).Getting StartedThis tutorial was tested against the development branch,available at commit 06c1cac.Similar to other RevBayes tutorials, we suggest using the followingdirectory structure:.├── data├── scripts└── outputThe following data files are available:data/├── alignment.fasta    -- Alignment file simulated with the Jukes-Cantor (JC) model.├── constraints.txt    -- File containing constraints.├── substitution.tree  -- Tree with branch length in expected number of substitutions used to simulate the alignment.└── time.tree          -- Original timetree to be recovered.The following RevBayes script files are available:scripts/├── 1a_mcmc_jc.rev                   -- Infer branch-length trees using the JC model.├── 1b_summarize_branch_lengths.rev  -- Extract posterior means and variances of branch lengths.└── 2_mcmc_dating.rev                -- Date with calibrations and constraints.Simulated data will be used, so that the inferred timetrees can be tested foraccuracy against the correct timetree stored in the file time.tree. Inferencewill be done twice: (1) using calibrations only, and (2) using calibrations andconstraints. We will not perform inference of the topology of the tree, only ofthe age of its nodes. We will therefore work with the correct, fixed treetopology obtained from the file substitution.tree. On empirical data, onecould use the maximum-likelihood (ML) or the maximum a posteriori (MAP) topologyobtained from a phylogenetic reconstruction based on the alignment as in e.g.tutorial Phylogenetic inference of nucleotide data using RevBayes.Statement of the ProblemWe are interested in inferring the following timetree using calibrationsand constraints:Timetree with 25 leaves (T0 to T24) to be inferred. The treewas simulated using a birth process (Yule process) with a birth rate of 1.0 perunit of time. The tree was conditioned on having height 0.3, and 25 leaves.However, the only information we get is an alignment simulated along thefollowing branch-length tree:Branch-length tree obtained from the timetree by multiplying the branchlengths with randomly chosen rates. The rates are sampled fromtwo log-normal distributions introducing frequent minor, and sparse major ratemodifications, respectively. The alignment used for inference was simulatedalong this tree using the JC model.For reasons of computational efficiency, the phylogenetic likelihood isapproximated using a branch-wise composition of normal distributions. For adetailed description of this two-step procedure please see (Szöllősi et al. 2021). In the first step, the posterior distributions of branch lengths for a fixedunrooted topology is inferred using the JC model (1a_mcmc_jc.rev). Then, theinferred posterior distributions of branch lengths are summarized into theposterior means and variances of the branch lengths(1b_summarize_branch_lengths.rev). In the second step, these posterior meansand variances of the branch lengths, as well as the calibrations and constraintsare used to date the phylogeny (2_mcmc_dating.rev).The approximation of the phylogenetic likelihood using posterior means andvariances is optional but recommended for reasons of computational efficiency.It is not necessary to use this approximation when dealing with relative timeconstraints, but the benefit here is clear: the first step computesbranch-length tree distributions, which is costly and takes a long time. Oncethis has been done, in step two, several different dating models can be runefficiently without the need to rerun the analysis performed in the first step.Step 1a: Inference of the Posterior Distributions of the Branch LengthsPlease execute the following command to perform Bayesian inference of theposterior distributions of branch-length trees using a Jukes-Cantor substitutionmodel on a single alignment with a fixed tree topology. The Markov chain MonteCarlo (MCMC) algorithm runs for 30000 iterations. It uses the alignmentdata/alignment.fasta, and the branch-length tree data/substitution.tree. Theoutput is a file output/alignment.fasta.trees containing the 30000branch-length trees from the MCMC chain. It will be used to calculate theposterior means and variances of the branch lengths. rb ./scripts/1a_mcmc_jc.revFor more detailed explanations of the script file, please consult thecontinuous time Markov chain tutorial.Step 1b: Summarizing Branch Length Distributions by Means and VariancesIn this step, we compute the means and variances of the posterior distributionsof branch lengths using the 30000 trees obtained in step 1a. Please have a lookat the script file scripts/1b_summarize_branch_lengths.rev.First, we specify the name of the file containing the trees, and the amount ofthinning we apply (thinning = 5 means that we take every fifth tree):# File including trees obtained in step 1a.tree_file=\"alignment.fasta.trees\"# Only use every nth tree to calculate the posterior means and variances.thinning = 5Then, the trees are loaded and thinned:tre = readBranchLengthTrees(outdir+file_trees_only)print(\"Number of trees before thinning: \")print(tre.size())# Perform thinning.index = 1for (i in 1:tre.size()) {  if (i % thinning == 0) {    trees[index] = tre[i]    index = index + 1  }}The posterior means and squared means (which will be used to calculatevariances) are then computed:print(\"Compute the posterior means and squared means of the branch lengths.\")num_branches = trees[1].nnodes()-1bl_means &lt;- rep(0.0,num_branches)bl_squaredmeans &lt;- rep(0.0,num_branches)# Extract the posterior branch lengths. The index `i` is traversing the trees,# the index `j` is traversing the branches.for (i in 1:(trees.size())) {  for (j in 1:num_branches ) {    bl_means[j] &lt;- bl_means[j] + trees[i].branchLength(j)    bl_squaredmeans[j] &lt;- bl_squaredmeans[j] + trees[i].branchLength(j)^2    }}# Compute the posterior means and squared means.for (j in 1:num_branches ) {  bl_means[j] &lt;- bl_means[j] / (trees.size())  bl_squaredmeans[j] &lt;- bl_squaredmeans[j] / (trees.size())}Finally, the posterior means and variances are stored in two trees having thesame topology as the one used for inference. In this way we ensure that theposterior means and variances for the specific branches are tracked in a correctway.The posterior variances are calculated using the standard formula \\(Var(X) =E(X^2) - E(X)^2.\\)Here we save the posterior means in a separate tree so we can use them later.print(\"Compute the tree with posterior means as branch lengths.\")meanTree = readBranchLengthTrees(outdir+file_trees_only)[1]print(\"Original mean tree before changing branch lengths\")print(meanTree)for (j in 1:num_branches ) {  meanTree.setBranchLength(j, bl_means[j])}print(\"Tree with posterior means as branch lengths.\")print(meanTree)writeNexus(outdir+tree_file+\"_meanBL.nex\", meanTree)print(\"The tree with posterior mean branch lengths has been saved.\")We also save the posterior varainces in a separate tree so we can use them later.print (\"Compute tree with posterior variances as branch lengths.\")varTree = readBranchLengthTrees(outdir+file_trees_only)[1]print(\"Original variance tree before changing branch lengths\")print(varTree)# Here, the posterior variances are calculated using the well known formula# Var(x) =E[X^2] - E[X]^2.for (j in 1:num_branches ) {  varTree.setBranchLength(j, abs ( bl_squaredmeans[j] - bl_means[j]^2) )}print(\"Tree with posterior variances as branch lengths.\")print(varTree)writeNexus(outdir+tree_file+\"_varBL.nex\", varTree)print(\"The tree with posterior variance branch lengths has been saved.\")To run the script, please execute rb ./scripts/1b_summarize_branch_lengths.revStep 2: Dating Using Calibrations and ConstraintsFinally, we date the phylogeny using a relaxed clock model. Please have a lookat the script 2_mcmc_dating.rev. Important parts will be explained in thefollowing.OptionsFirst, let’s define some file names and options:time_tree_file=\"data/time.tree\"constraints_file=\"data/constraints.txt\"mean_tree_file=\"output/alignment.fasta.trees_meanBL.nex\"var_tree_file=\"output/alignment.fasta.trees_varBL.nex\"out_bn=\"alignment.fasta.approx.dating\"# Show debug output?debug = true# Use relative constraints?constrain = false# Length of Markov chain.mcmc_length = 50000mcmc_burnin = 100# The posterior branch length distribution of some branches of the first run has# zero variance. In this case, set the variance to the following value.var_min = 1e-6Root AgeThen, the root age is calibrated using an interval around the true root age:root_age &lt;- tree.rootAge()root_age_delta &lt;- root_age / 5root_age_min &lt;- root_age - root_age_deltaroot_age_max &lt;- root_age + root_age_deltaroot_time_real ~ dnUniform(root_age_min, root_age_max)root_time_real.setValue(tree.rootAge())root_time := abs( root_time_real )The correct root age is obtained from the variable tree which was initializedto store the correct timetree. By doing this, we are benefiting from the factthat we know the true timetree.When analyzing an empirical data set for which the timetree is not known, oneneeds to find other ways to come upwith a reasonable prior distribution for the root age. We set auniform prior on the age of the root.Setting up ConstraintsIf specified, we also load the constraints from the given file contraints.txt:if (constrain) {  out_bn = out_bn + \"_cons\"  constraints &lt;- readRelativeNodeAgeConstraints(file=constraints_file)}The file constraints.txt stores the constraints obtained from 3 hypotheticalgene transfer events.-- Contents of file `data/constraints.txt`T0\tT1\tT14\tT15T7\tT8\tT19\tT20T3\tT4\tT8\tT9The first line T0\\tT1\\tT14\\tT15 tells RevBayes that the most recent commonancestor (MRCA) of T0 and T1 has to be older than the MRCA of T14 andT15, and so on for the following lines. You can look for the respective nodeson the timetree given above, and check that the constraints are actually valid.In fact, the constraints may be helpful in that they resolve the order ofnodes having similar ages.Yule Tree ModelNext, we use a birth process with an unknown birth rate as a prior for thetimetree psi, and define some proposals for the MCMC sampler. Theproposals change the root age, scale the branches of psi, and slide the nodes.################################################################################# Tree model.birth_rate ~ dnExp(1)moves.append(mvScale(birth_rate, lambda=1.0, tune=true, weight=3.0))if (!constrain) psi ~ dnBDP(lambda=birth_rate, mu=0.0, rho=1.0, rootAge=root_time, samplingStrategy=\"uniform\", condition=\"survival\", taxa=taxa)if (constrain) psi ~  dnConstrainedNodeOrder(dnBDP(lambda=birth_rate, mu=0.0, rho=1.0, rootAge=root_time, samplingStrategy=\"uniform\", condition=\"survival\", taxa=taxa), constraints)psi.setValue(tree)if (debug == true) {  print(\"The original time tree:\")  print(tree)  print(\"The tree used in the Markov chain:\")  print(psi)}moves.append(mvScale(root_time_real, weight=1.0, lambda=0.1))moves.append(mvSubtreeScale(psi, weight=1.0*n_branches))moves.append(mvNodeTimeSlideUniform(psi, weight=1.0*n_branches))moves.append(mvLayeredScaleProposal(tree=psi, lambda=0.1, tune=true, weight=1.0*n_branches))Adding Node CalibrationsNow, two calibrations are added. The MRCAs of T1 and T2, as well as T14and T15 are fixed to be within intervals centered around their true ages(initially, psi stores the true tree). The distributiondnSoftBoundUniformNormal is uniform between the specified boundaries anddecreases normally out of the boundaries with the given standard deviation(sd).################################################################################# Node calibrations provide information about the ages of the MRCA.# The MRCAs of the following clades are calibrated.clade_0 = clade(\"T1\",\"T2\")clade_1 = clade(\"T14\",\"T15\")# Clade 0.tmrca_clade_0 := tmrca(psi, clade_0)age_clade_0_mean &lt;- tmrca(psi, clade_0)age_clade_0_delta &lt;- age_clade_0_mean / 5age_clade_0_prior ~ dnSoftBoundUniformNormal(min=age_clade_0_mean-age_clade_0_delta, max=age_clade_0_mean+age_clade_0_delta, sd=2.5, p=0.95)age_clade_0_prior.clamp(age_clade_0_mean)# Clade 1.tmrca_clade_1 := tmrca(psi, clade_1)age_clade_1_mean &lt;- tmrca(psi, clade_1)age_clade_1_delta &lt;- age_clade_1_mean / 5age_clade_1_prior ~ dnSoftBoundUniformNormal(min=age_clade_1_mean-age_clade_1_delta, max=age_clade_1_mean+age_clade_1_delta, sd=2.5, p=0.95)age_clade_1_prior.clamp(age_clade_1_mean)Posterior Means and VariancesFinally, we read in the posterior means and variances of the branch lengthsprepared in the step 1b.mean_tree &lt;- readTrees(mean_tree_file)[1]var_tree &lt;- readTrees(var_tree_file)[1]We re-root the timetrees and ensure that they are bifurcating. These steps arenecessary to get the correct mapping between branches, and their posterior meansand variances. Internally, it is ensured that the indices are shared correctly.# Reroot and make bifurcating.rootId &lt;- tree.getRootIndex()outgroup &lt;- tree.getDescendantTaxa(rootId)mean_tree.reroot(clade=clade(outgroup), make_bifurcating=TRUE)var_tree.reroot(clade=clade(outgroup), make_bifurcating=TRUE)# Renumber nodes.mean_tree.renumberNodes(tree)var_tree.renumberNodes(tree)Further, this step harbors a complication. During the first step, the reversibleJC model was used to estimate the posterior means and variances of the branchlengths. Hence, the estimated branch-length tree is unrooted. Now, we estimate arooted time tree. It follows that the two branches leading to the root of thetimetree correspond to a single branch of the unrooted tree from the first step.We have to take this into account when approximating the phylogenetic likelihood.# Get indices of left child and right child of root.i_left &lt;- tree.child(tree.nnodes(),1)i_right &lt;- tree.child(tree.nnodes(),2)# Get posterior means and variances of all branches except the branches leading# to the root.for(i in 1:n_branches) {  if(i != i_left &amp;&amp; i != i_right) {    posterior_mean_bl[i] &lt;- mean_tree.branchLength(i)    posterior_var_bl[i] := var_tree.branchLength(i)    if (posterior_var_bl[i]&lt;var_min) posterior_var_bl[i]:=var_min  }}# Get the index of the root in the time tree.if (i_left&lt;i_right)  i_root &lt;- i_leftif (i_left&gt;=i_right) i_root &lt;- i_right# Get the mean and variance of the branch containing the root.posterior_mean_bl_root &lt;- mean_tree.branchLength(i_root)posterior_var_bl_root &lt;- var_tree.branchLength(i_root)if (posterior_var_bl_root&lt;var_min) posterior_var_bl_root:=var_minPlease also note that we set a minimum variance. In some extreme cases, theposterior distribution of a branch length may be so condensed, that the variance istoo low to be used for calculating the approximate phylogenetic likelihood.Relaxed Molecular Clock ModelIn the next step, we define the relaxed molecular clock model. Several of themare available in RevBayes. Here, we decided to use an uncorrelated model. Weseparate between a global rate (global_rate_mean) and the relative branch-wiserates (rel_branch_rates). We use an uncorrelated gamma model(UGAM). In the UGAM model, the relative branch rates are distributed accordingto a gamma distribution. We use a hyper parameter sigma on the shape and scaleparameters of the gamma distribution:mean_tree_root_age := mean_tree.rootAge()global_rate_mean ~ dnExp(1)global_rate_mean.setValue(mean_tree_root_age/tree.rootAge());sigma ~ dnExp(10.0)first_gamma_param := 1/sigmasecond_gamma_param := 1/sigmamoves.append(mvScaleBactrian(global_rate_mean, lambda=0.5, weight=10.0))moves.append(mvScaleBactrian(sigma, lambda=0.5, weight=10.0))# Use a Gamma distribution on rates.for (i in n_branches:1) {  times[i]=psi.branchLength(i)  rel_branch_rates[i] ~ dnGamma(first_gamma_param, second_gamma_param)  # Exclude the branches leading to the root (see above).  if(i != i_left &amp;&amp; i != i_right) {    rel_branch_rates[i].setValue(posterior_mean_bl[i]/times[i]/global_rate_mean)  } else {    # And set them to half of the branch length in the unrooted tree.    rel_branch_rates[i].setValue(posterior_mean_bl_root/2/times[i]/global_rate_mean)  }  moves.append(mvScale(rel_branch_rates[i], lambda=0.5, weight=1.0,tune=true))}for (i in n_branches:1) {  branch_rates[i] := global_rate_mean * rel_branch_rates[i]}LikelihoodFinally, we calculate the approximate phylogenetic likelihood. In detail, foreach branch, the length measured in substitutions is the product of the lengthmeasured in time, and the evolutionary rate at that branch:mean_bl[i] := times[i]*branch_rates[i]The branch lengths are then distributed according to normal distributions withthe posterior means and variances obtained in the previous step:bls[i] ~ dnNormal(mean_bl[i] ,sqrt(posterior_var_bl[i]))Again, the two branches leading to the root are handled in a special way. Intotal, the approximate phylogenetic likelihood is calculated like so:times[i_left] := psi.branchLength(i_left)times[i_right] := psi.branchLength(i_right)for(i in 1:n_branches) {  if(i != i_left &amp;&amp; i != i_right) {    times[i] := psi.branchLength(i)    mean_bl[i] := times[i]*branch_rates[i]    bls[i] ~ dnNormal(mean_bl[i] ,sqrt(posterior_var_bl[i]))    bls[i].clamp(posterior_mean_bl[i])  }}# See above.mean_bl_root := times[i_left]*branch_rates[i_left] + times[i_right]*branch_rates[i_right]bls[i_root] ~ dnNormal(mean_bl_root, sqrt(posterior_var_bl_root))bls[i_root].clamp(posterior_mean_bl_root)Monitors and MCMCThe last part of the script defines the monitors, executes the MCMC chain, andsaves the results.Define the monitors:for(i in 1:n_branches){  ages_psi[i] := psi.nodeAge(i)  ages_true[i] := tree.nodeAge(i)}if (debug == true) {  print(\"True node ages:\")  print(ages_true)  print(\"Node ages of state of Markov chain:\")  print(ages_psi)}trees_base_name = \"output/\" + out_bntrees_file_name = trees_base_name + \".trees\"monitors.append(mnModel(filename=\"output/\"+out_bn+\".log\",printgen=10, separator = TAB))monitors.append(mnStochasticVariable(filename=\"output/\"+out_bn+\"_Stoch.log\",printgen=10))monitors.append(mnExtNewick(filename=trees_file_name, isNodeParameter=FALSE, printgen=10, separator = TAB, tree=psi, branch_rates))# Add some random age monitors.monitors.append(mnScreen(printgen=100, root_time, ages_psi[20], ages_psi[25], ages_psi[27]))Use the Metropolis-coupled Markov chain Monte Carlo algorithm:# mymodel = model(branch_rates)mymodel = model(bls)mymcmc = mcmcmc(mymodel, monitors, moves, nruns=1, nchains=4, tuneHeat=TRUE)mymcmc.burnin(generations=mcmc_burnin,tuningInterval=mcmc_burnin/10)mymcmc.operatorSummary()mymcmc.run(generations=mcmc_length)mymcmc.operatorSummary()To perform the dating analysis, please execute twice (oncewith constrain = true, and once with constrain = false):rb ./scripts/2_mcmc_dating.revThe MAP trees for the analysis with calibrations only, and with calibrations andconstraints are stored in the files output/alignment.fasta.approx.dating.tree,and output/alignment.fasta.approx.dating_cons.tree, respectively.Analysis of resultsThe following figures show the inferred timetrees.Inferred timetree using calibrations only.Inferred timetree using calibrations and constraints.We see that the constraints help improve the accuracy, as well as reduce theconfidence intervals of the node ages. Further, we can measure the distancebetween the inferred timetrees, and the original timetree used for simulatingthe alignment. We use the branch score distance\\[d(T_1, T_2) = \\sqrt{\\sum_{i} \\Delta_i^2},\\]where $T_1$ and $T_2$ are two trees, $i$ traverses the branches, and $\\Delta_i$is the difference of the lengths of the branch in $T_1$ and $T_2$. If a branchis only in one tree, the whole length is chosen (although this is not the casehere). The branch score distances can be calculated with any phylogeneticsoftware package. Here, we used ELynx. Thebranch score distances between the original timetree, and the inferred timetreesare:$ tlynx distance -d branch-score -f RevBayes -m data/time.tree output/alignment.fasta.approx.dating.tree output/alignment.fasta.approx.dating_cons.tree=== tlynxCompare, examine, and simulate phylogenetic trees.ELynx Suite version 0.4.0.Developed by Dominik Schrempf.Compiled on September 19, 2020, at 13:41 pm, UTC.Start time: October 9, 2020, at 11:29 am, UTC.Command line: tlynx distance -d branch-score -f RevBayes -m data/time.tree output/alignment.fasta.approx.dating.tree output/alignment.fasta.approx.dating_cons.treeWrite results to standard output.Read master tree from file: data/time.tree.Compute distances between all trees and master tree.Read trees from files.Trees are named according to their file names.Use branch score distance.Summary statistics of Branch Score Distance:Mean:     0.151Median:   0.159Variance: 0.000Tree 1                                          Tree 2                                            Branch Scoredata/time.tree                                  output/alignment.fasta.approx.dating.tree                0.159data/time.tree                                  output/alignment.fasta.approx.dating_cons.tree           0.142No output file given --- skip writing ELynx file for reproducible runs.=== End time: October 9, 2020, at 11:29 am, UTC.That is, using constraints helped improve the branch score distance by 11percent.Concluding remarksWhen analyzing real data a more appropriate substitution model should be used.Also, when the tree topology is unknown, it has to be inferred. This has to be donebeforehand. Finally, calibrations and constraints have to be obtained manuallyeither based on the literature or based on original research.The simulations were done using a custom simulator called ELynx. If you areinterested, please have a look at the source code ofELynx and the simulation scripts for thistutorial.",
        "url": "/tutorials/relative_time_constraints/",
        "index": "true"
      }
      ,
    
      "tutorials-timefig-simple": {
        "title": "Time-heterogeneous FIG (TimeFIG) model",
        "content": "OverviewIn the previous tutorial, we saw how the MultiFIG model (Swiston and Landis 2023) allows us to test hypotheses about the relationships between certain environmental features and evolutionary processes using feature effect rates, as well as infer biogeographic event parameters and ancestral areas. We used MultiFIG to investigate the Hawaiian radiation of the plant genus Kadua based on species ranges, present-day features of islands, and a time-calibrated phylogeny.However, we know that regional features change over time, and this may impact our ancestral state reconstructions and estimates of feature/process relationships. The TimeFIG model addresses the time-heterogeneity of regional features using “time slices” (discrete time periods), allowing regional features to have different values during each time slice, while assuming the relationships between features and processes remain constant.For example, in the previous tutorial, the within-region speciation rate for region $i$ was defined as\\[r_w(i) = \\rho_w \\times m_w(i)\\]which is the base rate for the process, $\\rho_w$, times the relative rate multiplier for region $i$, $m_w(i)$. This formulation assume the rate is constant across time. In this tutorial, we instead define piecewise constant within-speciation rates that depend on time as\\[r_w(i,t) = \\rho_w \\times m_w(i,t)\\]where the key difference is that $m_w(i,t)$ is now itself a function of time, $t$. We can retain the notation and interpretation given in the MultiFIG tutorial, while adapting it to be a function of time, yielding\\[m_w(i,t) = \\underbrace{ \\prod_{k} q^{k}_w(i,t) }_{\\text{quantitative effects}} \\times \\underbrace{\\prod_{\\ell} c^{\\ell}_w(i,t)}_{\\text{categorical effects}}\\]where now we define the feature effect variables that control relative within-region speciation rates\\[q^{k}_w(i,t) = \\text{exp} \\left\\{ \\phi_w^{k} \\times w_q^{k}(i,t) \\right\\}\\]and\\[c^{\\ell}_w(i,t) = \\text{exp} \\left\\{ \\sigma_w^{\\ell} \\times w_c^{\\ell}(i,t) \\right\\}\\]as functions of both a region, $i$, but now also a time, $t$.What is important to stress is that all $\\phi$ and $\\sigma$ parameters are independent of time and region. They only modulate the sign and strength of effect between a particular regional feature layer ($k$ or $\\ell$) and a biogeographic process (in this case $w$ for within-region speciation). However, the quantitative ($w^{(k)}_q(i,t)$) and categorical features ($w^{(k)}_c(i,t)$ may themselves change with time. In fact, they are the only variables on the right hand side that are functions of time, meaning the time-varying features drive any dynamical changes for relative rates in $m_w(i,t)$, while the feature effect parameters mostly control the sign and magnitude of those changes.In this tutorial, we will model the evolution and biogeography of Kadua using seven regions, eight regional features, and seven time slices. We summarize some relevant details below.The Hawaiian Hot Spot ArchipelagoThe Hawaiian archipelago is a system in which phylogenetic models of historical biogeography will produce much more accurate reconstructions if they incorporate change over time in paleogeography than if change in island feature is ignored. In this tutorial, we apply a TimeFIG model to the Hawaiian radiation of Kadua (26 spp. including non-Hawaiian outgroups) to infer paleogeographically-informed parameter estimates for biogeographic event rates, effect rates of regional features, and ancestral areas.The introduction to this tutorial series describes the complex palegeological history of the Hawaiian Archipelago (link). Briefly, each Hawaiian island formed in the soutwest through seamount vulcanism and then drifted to the northwest with tectonic movements of the Pacific Plate. This creates a “conveyor belt” system, where taller but younger islands reside in the southeast while flatter but older islands extend to the northwest. This tutorial makes use of various paleogeographic measurements, such as island age, and estimates, such as paleoaltitude, to help shape biogeographic rates over time. For example, we might expect that terrestrial plants have higher extinction rates the oldest (barren) islands or that the plants cannot colonize islands below sea level (impossible).We will use the same features for the TimeFIG analysis that we used in the MultiFIG analysis. For reference, here are the 8 regional features and the 16 associated parameters relating these features to core biogeographic processes.The analysis utilizes 7 different time slices, numbered starting from the present. These time slices are delimited by 6 historical time points: T1, T2, T3, etc. Distributions may be assigned to these time points to account for uncertainty.The paleogeographic information is organized in ./data/hawaii. To view the different time slices within a command line prompt:$ # from command line$ cat ./data/hawaii/age_summary.csvindex,mean_age,start_age,end_age,feature_dir1,1.20,1.30,1.10,time12,2.55,3.00,2.10,time23,4.135,4.34,3.93,time34,6.15,6.30,6.00,time45,8,8.15,7.75,time56,18,18.15,17.75,time6Paleogeographic features are further organized into subdirectories named time1, time2, etc. For example, to view quantitative within-region features for layer 1 (max. altitude) at time slice 4, type:$ # from command line$ cat ./data/hawaii/time4/qw_feature1.csvG,N,K,O,M,H,Z50,500,3787,nan,nan,nan,1500Below is a visualization of maximum island altitude across regions and epoch, made by summarizing the feature files across the different time directories:Maximum altitude values for seven regions across seven epochs. This quantitative within-region feature may influence within-region speciation and extinction rates through the sign and magnitude of the estimated $\\phi_w^{(1)}$ and $\\phi_e^{(1)}$ parameters. Light colors are small values, dark colors are large values, and gray indicates the region was absent (missing feature).Setup  Important dataset and script!  Tutorial files: This tutorial requires numerous input files and scripts to run properly. Download and unzip the timefig_simple_project.zip archive to create a project directory named timefig_simple_project. Enter the new directory to run the tutorial exercises. Analysis scripts are available on the sidebar for easy access.  Important version info!  Note: This tutorial currently requires specific versions of RevBayes and TensorPhylo to run properly (see linked branches and commits).  We recommend that you complete the tutorial using a PhyloDocker container, which is pre-configured with the above versions of RevBayes and TensorPhylo. Instructions to install and use PhyloDocker are here: link.Running a TimeFIG analysis in RevBayes requires several important data files, including a file representing the time-calibrated phylogeny and a biogeographic data matrix describing the ranges for each species. kadua.tre is a time-calibrated phylogeny of Kadua. kadua_range_n7.nex assigns ranges to each species for a seven-region system: G (Gardner), N (Necker), K (Kauaii), O (Oahu), M (Maui Nui Complex), H (Hawaii), and Z (mainland). For each species (row) and region (column), the file reports if the species is present (1) or absent (0) in that region. There are also feature files that contain regional feature data, a feature_summary.csv file that describes all the regional feature files (where they are found and what kind of data they contain), and an age_summary.csv file that tells us what ages delimit the time slices for our analysis.If you prefer to run a single script instead of entering each command manually, the RevBayes script called timefig.Rev contains all of the commands that are used in the tutorial. The data and script can be found in the Data files and scripts box in the left sidebar of the tutorial page. Note that for this tutorial, the data files are not individually visible (there are a LOT of them). Instead, a timefig_simple_project.zip file is provided. Downloading and unzipping this file will give you a directory containing all of the scripts and data files for the tutorial, set up the way the tutorial expects. This main directory is where you will run RevBayes commands.Inside the tutorial directory, there will be a scripts directory. This is the directory where the timefig.Rev script lives. There is also a data directory inside the tutorial directory. Within data, there will be two more directories: hawaii, and kadua. The data files related to Kadua (kadua.tre and kadua_range_n7.nex) are in the kadua directory. The data related to Hawaii, including the feature_summary.csv file, the age_summary.csv file, the feature_description.csv file, and all feature-related *_feature*.csv files are in the hawaii directory. However, you can always modify the filepaths to locate the data wherever you choose to download it.TimeFIG in RevBayesGetting startedAfter starting up RevBayes from within your main tutorial directory, you can load the TensorPhylo plugin. You will need to know where you downloaded the plugin. For example, if you cloned the TensorPhylo directory into your home directory at ~/tensorphylo, you would use the following command to load the plugin:loadPlugin(\"TensorPhylo\", \"~/tensorphylo/build/installer/lib\")Note that if you’re using the PhyloDocker image, then the Tensorphylo plugin is installed in /.plugins, where RevBayes is able to find it without including a filepath:loadPlugin(\"TensorPhylo\")Next, we want to tell RevBayes where to find our data (and where to save our output later). If you have set up your tutorial directory in a different way than suggested, you will need to modify the filepaths.# filesystemanalysis      = \"simple_timefig\"dat_fp        = \"./data/kadua/\"phy_fn        = dat_fp + \"kadua.tre\"bg_fn         = dat_fp + \"kadua_range_n7.nex\"label_fn      = dat_fp + \"kadua_range_label.csv\"geo_fp        = \"./data/hawaii/\"feature_fn    = geo_fp + \"feature_summary.csv\"times_fn      = geo_fp + \"age_summary.csv\"out_fn        = \"./output/\" + analysisSimilar to the MultiFIG analysis, we will set up containers moves and monitors at the beginning of the script, and choose some MCMC settings for later: the number of computer processors to use, the number of generations we want to run the analysis for, and how often we want RevBayes to record output.# MCMC variablesnum_proc  = 6num_gen   = 500          # set num_gen = 5000 for full analysisprint_gen = 1moves     = VectorMoves()monitors  = VectorMonitors()DataNow, we will start reading in data and constructing the TimeFIG model. Let’s start by loading the phylogenetic tree.phy &lt;- readTrees(phy_fn)[1]In order to set up our analysis, we will want to know some information about this tree: the taxa, the number of taxa, and the number of branches.taxa         = phy.taxa()num_taxa     = taxa.size()num_branches = 2 * num_taxa - 2We also want to read in the range data. This is the same data from the MultiFIG example.dat_01 = readDiscreteCharacterData(bg_fn)Once again, we want to get some information about this range data: how many regions there are, and how many ranges can be constructed from these regions. We will still set our maximum range size to 4.num_regions    = dat_01.nchar()max_range_size = 4num_ranges     = 0for (k in 1:max_range_size) {    num_ranges += choose(num_regions, k)}Again, we want to format the range data to be used in a GeoSSE-type analysis. This will take the binary range data and output integer states.dat_nn         = formatDiscreteCharacterData(dat_01, format=\"GeoSSE\", numStates=num_ranges)desc           = dat_nn.getStateDescriptions()write(\"index,range\\n\", filename=label_fn)for (i in 1:desc.size()) {    write((i-1) + \",\" + desc[i] + \"\\n\", filename=label_fn, append=true)}We also want to read in the biogeographic data. First, we’ll read the age file that tells us how many time slices to include and what times delimit those slices. Note that for $n$ times, there will be $n+1$ time slices. The age_summary.csv file also includes information that would help establish a uniform prior on each of these times (start_age and end_age), but we will be using the mean_age without setting a prior (no uncertainty).times_table = readDataDelimitedFile(times_fn, delimiter=\",\", header=true)num_times &lt;- times_table.size() + 1for (i in 1:(num_times-1)) { times[i] &lt;- times_table[i][2] }We also want to get our feature data. Using the RevBayes function readRegionalFeatures, we can look at the feature_summary.csv file and automatically look for feature data. The feature_summary.csv file is specially formated to be read by RevBayes, consisting of 5 columns. The first column is time_index, telling us which time slice the feature data corresponds to. Time slices are numbered from the present starting with 1. The second column is feature_index. Each feature type (within-region categorical, within-region quantitative, between-region categorical, and between-region quantitative) has a container that can contain several features, so we want to index the features within those containers. In this analysis, we will only have one feature of each type, so the index will always be 1. The third column is feature_relationship. This column is for indicating whether the feature is a within-region feature or a between-region feature, with options ‘within’ or ‘between’. The fourth column is feature_type, for indicating whether the feature is quantitative of categorical. Finally, the fifth column is feature_path, which gives a filepath for the actual file containing the data for that feature.geo_features &lt;- readRegionalFeatures(feature_fn, delimiter=\",\", nonexistent_region_token=\"nan\")Next, we transform the feature data into feature layers, a RevBayes object that we will use later for informing our biogeographic rates. First, we normalize the features (important for scaling reasons). Then, for each time slice [i], we pull each feature type out of our geo_features object and create the layers.geo_features.normalize(\"within\")geo_features.normalize(\"between\")# get feature-sets for each measurement-type, process-type, and timeslicefor (i in 1:num_times) {    feature_CW[i] &lt;- geo_features.get(\"within\",\"categorical\",i)    feature_QW[i] &lt;- geo_features.get(\"within\",\"quantitative\",i)    feature_CB[i] &lt;- geo_features.get(\"between\",\"categorical\",i)    feature_QB[i] &lt;- geo_features.get(\"between\",\"quantitative\",i)    for (j in 1:feature_CW[i].size()) {        layer_CW[i][j] &lt;- feature_CW[i][j].get()    }    for (j in 1:feature_QW[i].size()) {        layer_QW[i][j] &lt;- feature_QW[i][j].get()    }    for (j in 1:feature_CB[i].size()) {        layer_CB[i][j] &lt;- feature_CB[i][j].get()    }    for (j in 1:feature_QB[i].size()) {        layer_QB[i][j] &lt;- feature_QB[i][j].get()    }}Model setupIn the TimeFIG model, we use the same four processes as earlier models: within-region speciation, extinction, between-region speciation, and dispersal. Rates per region or region pair are calculated the same way as in the MultiFIG model using feature data, feature effect parameters, and base rate parameters. However, unlike the MultiFIG model, our feature values change from epoch to epoch! Therefore, while we will use the same set of base rates and feature effect parameters for each time slice, our relative rates will end up being different between time slices.First, we will set priors for the feature effect parameters. Then we will use the RevBayes function fnFeatureInformedRates to combine the feature data and feature effect parameters for each time slice to create $m$ vectors/matrices that are specific to the time slices, representing relative rates of a particular process per region or region pair during that time slice. Finally, we will multiply the $m$ for each process in each time slice by base rate parameters to get model rates $r_w$, $r_e$, $r_b$, and $r_d$ for each time slice.Let’s start by creating distributions that we will use for all $\\phi$ and $\\sigma$ parameters. We will use reversible jump distributions again. We also have to assign an rj_prob to the reversible jump distribution, which is the prior probability of RJMCMC using the fixed value 0 instead of the continuous distribution.We will use the same bounds on our continuous distributions that we did for the MultiFIG model.# set up priors for feature effectsrj_null_value &lt;- 0.0          # fixed \"off-value\" for RJMCMCrj_prob       &lt;- 0.5          # prob. of RJMCMC taking \"off-value\"# prior of \"on-value\" for RJMCMCbound &lt;- 2rj_base_sym_dist = dnUniform(-bound, bound)rj_base_neg_dist = dnUniform(-bound, 0)     # negative only (e.g. distance on dispersal)rj_base_pos_dist = dnUniform(0, bound)      # positive only (e.g. distance on betw.-reg. speciation)rj_sym_dist = dnRJMixture(rj_null_value, rj_base_sym_dist, p=rj_prob)rj_neg_dist = dnRJMixture(rj_null_value, rj_base_neg_dist, p=rj_prob)rj_pos_dist = dnRJMixture(rj_null_value, rj_base_pos_dist, p=rj_prob)# categorical feature effectsfor (i in 1:feature_CW[1].size()) sigma_w[i] ~ rj_sym_distfor (i in 1:feature_CW[1].size()) sigma_e[i] ~ rj_sym_distfor (i in 1:feature_CB[1].size()) sigma_d[i] ~ rj_sym_distfor (i in 1:feature_CB[1].size()) sigma_b[i] ~ rj_sym_dist# quantitative feature effectsfor (i in 1:feature_QW[1].size()) phi_w[i] ~ rj_sym_distfor (i in 1:feature_QW[1].size()) phi_e[i] ~ rj_sym_distfor (i in 1:feature_QB[1].size()) phi_d[i] ~ rj_sym_distfor (i in 1:feature_QB[1].size()) phi_b[i] ~ rj_sym_dist# force signed relationships between region features and rates# (overrides existing distribution assignments)phi_b[1]   ~ rj_pos_dist   # Distance (km) results in faster speciationphi_b[2]   ~ rj_pos_dist   # Log-distance (km) results in faster speciationsigma_b[1] ~ rj_pos_dist   # LDD (1) results in faster speciationsigma_w[1] ~ rj_pos_dist   # High Islands (1) drives faster speciation phi_d[1]   ~ rj_neg_dist   # Distance (km) results in slower dispersalphi_d[2]   ~ rj_neg_dist   # Log-distance (km) results in slower dispersalsigma_d[1] ~ rj_neg_dist   # LDD (1) results in slower dispersalsigma_e[1] ~ rj_neg_dist   # High Islands (1) drives slower extinctionNow we can create the relative rates of each process. These $m$ containers hold the per-region or per-region-pair relative rates for each time slice. We will turn these into actual rates (incorporating a base rate parameter) later. Each of these relative rate containers also has a null_rate argument, which tells RevBayes what to do with missing regions. This is important because some regions did not exist during some times!# regional rate factorsfor (t in 1:num_times) {    # NOTE: do not index [1] in RHS of assignment to drop \"dummy\" dimension for m_W and m_E!    m_w[t] := fnFeatureInformedRates(layer_CW[t], layer_QW[t], sigma_w, phi_w, null_rate=0)    m_e[t] := fnFeatureInformedRates(layer_CW[t], layer_QW[t], sigma_e, phi_e, null_rate=1e3)    m_d[t] := fnFeatureInformedRates(layer_CB[t], layer_QB[t], sigma_d, phi_d, null_rate=0)    m_b[t] := fnFeatureInformedRates(layer_CB[t], layer_QB[t], sigma_b, phi_b, null_rate=1)}Because we are going to do an MCMC analysis later in the tutorial, we want MCMC to update all of the $\\sigma$ and $\\phi$ parameters. Once again, we will add MCMC moves on these parameters. We may also want to initialize the MCMC to reasonable values for these feature effect parameters. We will set the values of our distributions to be (temporarily) equal to those initial values to start the MCMC.First, we will address the categorical feature effects for each process (w, e, d, and b). These are our $\\sigma$ parameters. The logic is the same for each process. First, we find the container of features which impact that process (within-region features for within-region speciation and extinction, between-region features for between-region speciation and dispersal). Then we loop over the different features inside that container. For each feature, we initialize the value of the parameter, and add appropriate moves for the MCMC. We also include a use_ line that allows us to turn off certain features if we want to perform analyses without them. Note that we do not loop over the time slices here, because the feature effects are shared across times!# initialize categorical feature effects, create moves, add monitor variablesfor (i in 1:feature_CW[1].size()) {    sigma_w[i].setValue(0)    moves.append( mvScale(sigma_w[i], weight=2) )    moves.append( mvSlide(sigma_w[i], weight=2) )    moves.append( mvRJSwitch(sigma_w[i], weight=3) )    use_sigma_w[i] := ifelse(sigma_w[i] == 0.0, 0, 1)}for (i in 1:feature_CW[1].size()) {    sigma_e[i].setValue(0)    moves.append( mvScale(sigma_e[i], weight=2) )    moves.append( mvSlide(sigma_e[i], weight=2) )    moves.append( mvRJSwitch(sigma_e[i], weight=3) )    use_sigma_e[i] := ifelse(sigma_e[i] == 0.0, 0, 1)}for (i in 1:feature_CB[1].size()) {    sigma_d[i].setValue(0)    moves.append( mvScale(sigma_d[i], weight=2) )    moves.append( mvSlide(sigma_d[i], weight=2) )    moves.append( mvRJSwitch(sigma_d[i], weight=3) )    use_sigma_d[i] := ifelse(sigma_d[i] == 0.0, 0, 1)}for (i in 1:feature_CB[1].size()) {    sigma_b[i].setValue(0)    moves.append( mvScale(sigma_b[i], weight=2) )    moves.append( mvSlide(sigma_b[i], weight=2) )    moves.append( mvRJSwitch(sigma_b[i], weight=3) )    use_sigma_b[i] := ifelse(sigma_b[i] == 0.0, 0, 1)}Similarly, we will address the quantitative features for each process. These are our \\phi parameters.# initialize quantitative feature effects, create moves, add monitor variablesfor (i in 1:feature_QW[1].size()) {    phi_w[i].setValue(0)    moves.append( mvScale(phi_w[i], weight=2) )    moves.append( mvSlide(phi_w[i], weight=2) )    moves.append( mvRJSwitch(phi_w[i], weight=3) )    use_phi_w[i] := ifelse(phi_w[i] == 0.0, 0, 1)}for (i in 1:feature_QW[1].size()) {    phi_e[i].setValue(0)    moves.append( mvScale(phi_e[i], weight=2) )    moves.append( mvSlide(phi_e[i], weight=2) )    moves.append( mvRJSwitch(phi_e[i], weight=3) )    use_phi_e[i] := ifelse(phi_e[i] == 0.0, 0, 1)}for (i in 1:feature_QB[1].size()) {    phi_d[i].setValue(0)    moves.append( mvScale(phi_d[i], weight=2) )    moves.append( mvSlide(phi_d[i], weight=2) )    moves.append( mvRJSwitch(phi_d[i], weight=3) )    use_phi_d[i] := ifelse(phi_d[i] == 0.0, 0, 1)}for (i in 1:feature_QB[1].size()) {    phi_b[i].setValue(0)    moves.append( mvScale(phi_b[i], weight=2) )    moves.append( mvSlide(phi_b[i], weight=2) )    moves.append( mvRJSwitch(phi_b[i], weight=3) )    use_phi_b[i] := ifelse(phi_b[i] == 0.0, 0, 1)}Now we will set up our rates for the four core processes, and put together our tree object. First, we will assign distributions to our base process rates, $\\rho$. These rates are shared amongst all regions and all time slices, and are combined with relative rates $m$ to get true process rates in each region or pair for each time slice, $r$. We will use exponential distributions with rate 30 for each base rate parameter. Once again, we will initialize these values so MCMC will start in a reasonable place, and append the appropriate moves. We can also calculate the total speciation rate from the base rates of each type of speciation event.# base rate parametersrho_d ~ dnExp(40)rho_e ~ dnExp(40)rho_w ~ dnExp(40)rho_b ~ dnExp(40)rho_d.setValue(0.1)rho_e.setValue(0.1)rho_w.setValue(0.1)rho_b.setValue(0.1)moves.append( mvScale(rho_d, weight=5) )moves.append( mvScale(rho_e, weight=5) )moves.append( mvScale(rho_w, weight=5) )moves.append( mvScale(rho_b, weight=5) )# summarize base ratesspeciation_rates := [ rho_w, rho_b ]total_speciation := sum( speciation_rates )Next, we will construct the total rates for each anagenetic process. Note how we loop over each time slice here; this is because total model rates, and the rate matrices obtained from them, are different during each time slice. Otherwise, this part is identical to the MultiFIG model.for (k in 1:num_times) {        # dispersal rate (region gain)    for (i in 1:num_regions) {        r_d[k][i] := rho_d * m_d[k][i]    }    # extirpation rate (region loss)    r_e[k] := rho_e * m_e[k][1]    # dispersal-extirpation rate matrix    # - states are discrete ranges    # - elements are rates of range expansion/contraction    Q_bg[k] := fnBiogeographyRateMatrix(dispersalRates=r_d[k],                                  extirpationRates=r_e[k],                                  maxRangeSize=max_range_size)}We also construct a cladogenetic event matrix, describing the absolute rates of different cladogenetic events. From this matrix, we can obtain the total speciation rates per state, as well as a cladogenetic probability matrix. Once again, we loop over each time slice, because the total model rates and rate matrices obtained from them are different during each time slice. Otherwise, this part is identical to the MultiFIG model.# speciation rate matrixfor (k in 1:num_times) {    clado_map[k] := fnBiogeographyCladoEventsBD(speciation_rates=speciation_rates,                                             within_region_features=m_w[k][1],                                             between_region_features=m_b[k],                                             max_range_size=max_range_size,                                             normalize_split_score=false)    # clado_map    # speciation rates for each range    lambda[k] := clado_map[k].getSpeciationRateSumPerState()    # probabilities of speciation outcomes for each range    omega[k] := clado_map[k].getCladogeneticProbabilityMatrix()    # monitor variables for absolute speciation rates    r_w[k] := rho_w * m_w[k][1]    # NOTE: this rate only represents species with range size 2    #       i.e., the inverse sum of inverse edge weights    #       (relative rates in m_b[i][j]) is equal to the edge weight    #       of a 2-region range    for (i in 1:num_regions) {        r_b[k][i] := rho_b * m_b[k][i]    }}We may also want to monitor the absolute extinction rates. Because only lineages with a range of size 1 can go extinct, we will assign larger ranges an absolute extinction rate of 0. Again, we loop over the time slices.for (k in 1:num_times) {    # extinction rates (lineage death)    for (i in 1:num_ranges) {        if (i &lt;= num_regions) {            # species with range-size 1 can go extinct            mu[k][i] := r_e[k][i]        } else {            # widespread species cannot            mu[k][i] &lt;- abs(0)        }    }}Next, we need to assign a probability distribution to range of the most recent common ancestor of all species, prior to the first speciation event. This will be a distribution (simplex) of possible range states that the ancestor might have had. Because some of the ranges are not possible (not all regions exist) when the lineage begins, we will assume that the ancestor started in the mainland.# base frequenciespi_bg_base &lt;- rep(0, num_ranges)# assume that the integer equal to \"num_regions\" is the# range-integer for a species that occurs only in the# mainland region, for base-indexing of 1 (Rev script).# For example, region 7 is the non-Hawaiian region,# set the range-integer for the range {7} to 1.pi_allowed_ranges &lt;- [ num_regions ]for (i in 1:pi_allowed_ranges.size()) {    j = pi_allowed_ranges[i]    pi_bg_base[j] &lt;- 1}pi_bg &lt;- simplex(pi_bg_base)We also need to set up the tip sampling probabilities based on state. In this analysis, the Hawaiian (ingroup) Kadua have been thoroughly sampled. However, we have only included 3 mainland (outgroup) samples, so we have to account for the low sampling here. Also, we will assign rho_times the value of 0, because we only sampled at the present (age =0)n_total           &lt;- 29 + 2 + 1n_total_ingroup   &lt;- 22 + 2n_total_outgroup  &lt;- n_total - n_total_ingroupn_sample_ingroup  &lt;- 24n_sample_outgroup &lt;- 3rho_ingroup       &lt;- Probability(n_sample_ingroup/n_total_ingroup) rho_outgroup      &lt;- Probability(n_sample_outgroup/n_total_outgroup)rho_poorly_sampled_ranges &lt;- [ 7 ]for (i in 1:num_ranges) {    rho_sample[1][i] &lt;- rho_ingroup}for (i in rho_poorly_sampled_ranges) {    rho_sample[1][i] &lt;- rho_outgroup}rho_times &lt;- [ 0.0 ]Before getting to the tree object, we want to make the root age of the tree object equal to the height of the input phylogeny. When we run future analyses that do not use a fixed tree, we can actually estimate this instead.# fixed root ageroot_age &lt;- phy.rootAge()With all of the rates constructed, we can create a stochastic variable drawn from this MultiFIG model with state-dependent birth, death, and speciation processes. This establishes how the various processes interact to generate a tree with a topology, divergence times, and terminal taxon states (ranges). Note how this model differs from the MultiFIG model. Here, we provide containers with different time slices inside of them (like lambda, mu, eta, omega). Therefore, we also have to tell the model what times delimit those slices using specific arguments (lambdaTimes, muTimes, etaTimes, omegaTimes). Fortunately, we already constructed a vector of times for this purpose earlier in the tutorial.# use Time/Multi FIG setuptimetree ~ dnGLHBDSP( rootAge      = root_age,                      lambda       = lambda,                      mu           = mu,                      eta          = Q_bg,                      omega        = omega,                      lambdaTimes  = times,                      muTimes      = times,                      etaTimes     = times,                      omegaTimes   = times,                      rhoTimes     = rho_times,                      pi           = pi_bg,                      rho          = rho_sample,                      condition    = \"time\",                      taxa         = taxa,                      nStates      = num_ranges,                      nProc        = num_proc)Then we can clamp the variable with the fixed tree and present-day range states, allowing us to infer model parameters based on our observed data.timetree.clamp(phy)timetree.clampCharData(dat_nn)MCMCFor this analysis, we will perform an MCMC of 10000 generations. This may seem like a low number of generations (compared to other programs), but this is because RevBayes performs multiple moves per iteration under the random move scheduler (a setting from the start of the tutorial). You can alter this MCMC by changing the number of iterations, the move schedule, or how frequently the MCMC prints output. You can even add a period of burnin that tunes hyperparameters for moves. We have already created all of our moves for this MCMC, so we can move on to monitors. Note that there are separate file monitors for each time slice, which will make the output much easier to read.# screen monitor, so you don't get boredmonitors.append( mnScreen(rho_d, rho_e, rho_w, rho_b, printgen=print_gen) )# file monitor for all simple model variablesmonitors.append( mnModel(printgen=print_gen, file=out_fn+\".model.txt\") )# file monitor for treemonitors.append( mnFile(timetree, printgen=print_gen, file=out_fn + \".tre\") )# monitor ancestral ranges at internal nodesmonitors.append( mnJointConditionalAncestralState(    tree=timetree, glhbdsp=timetree, printgen=print_gen,    filename=out_fn+\".states.txt\",    withTips=true, withStartStates=true, type=\"NaturalNumbers\") )# file monitor for biogeographic ratesfor (k in 1:num_times) {    bg_mon_fn = out_fn + \".time\" + k + \".bg.txt\"    monitors.append( mnFile( filename = bg_mon_fn, printgen=print_gen,                             rho_e, rho_w, rho_d, rho_b,                             r_e[k], r_w[k],                             r_d[k][1], r_d[k][2], r_d[k][3], r_d[k][4],                             r_d[k][5], r_d[k][6], r_d[k][7],                             r_b[k][1], r_b[k][2], r_b[k][3], r_b[k][4],                             r_b[k][5], r_b[k][6], r_b[k][7],                             m_e[k][1], m_w[k][1],                             m_d[k][1], m_d[k][2], m_d[k][3], m_d[k][4],                             m_d[k][5], m_d[k][6], m_d[k][7],                             m_b[k][1], m_b[k][2], m_b[k][3], m_b[k][4],                             m_b[k][5], m_b[k][6], m_b[k][7] ) )}# monitor stochastic mappings along branches of tree# NOTE: uncomment if needed, but can cause performance issues# monitors.append( mnStochasticCharacterMap(#    glhbdsp=timetree, printgen=print_gen*10,#    filename=out_fn+\".stoch.txt\",#    use_simmap_default=false) )Then we can start up the MCMC. It doesn’t matter which model parameter you use to initialize the model, so we will use the timetree. RevBayes will find all the other parameters that are connected to the timetree and include them in the model as well. Then we create an MCMC object with the moves, monitors, and model. Finally, we can run that MCMC!# create model objectmymodel = model(timetree)# create MCMC objectmymcmc = mcmc(mymodel, moves, monitors, moveschedule=\"single\")       # set moveschedule=\"random\" for full analysis# run MCMCmymcmc.run(num_gen)After the MCMC analysis has concluded, we can summarize the ancestral states we obtained, creating an ancestral state tree. This tree will be written to the file ase.tre . It may take a little while.f_burn = 0.2x_stoch = readAncestralStateTrace(file=\"output/\" + analysis + \".stoch.txt\")x_states = readAncestralStateTrace(file=\"output/\" + analysis + \".states.txt\")summarizeCharacterMaps(x_stoch,timetree,file=\"output/\" + analysis + \".events.txt\",burnin=f_burn)state_tree = ancestralStateTree(    tree=timetree,    ancestral_state_trace_vector=x_states,    include_start_states=true,    file=\"output/\" + analysis + \".ase.tre\",    summary_statistic=\"MAP\",    reconstruction=\"marginal\",    burnin=f_burn,    nStates=num_ranges,    site=1)writeNexus(state_tree,filename=\"output/\" + analysis + \".ase.tre\")Output  Example output  Note: Complete FIG analyses can take several hours to run. To exploreFIG analysis output as part of a workshop, we recommend that youdownload precomputed “example output” from the top left menu on thispage. Save these files into your local output directory and view resultsand/or run the following plotting code.This section shows how generate plots for FIG analysis results using the FIG Tools repository, which primarily uses R, RevGadgets, ggplot, and igraph for visualization.NOTE: Your output may look slightly different than the output shown below. If you want to exactly replicate the results of the tutorial, you must set a seed at the beginning of the kadua_geosse.Rev script by adding the RevBayes command seed(1).To proceed, we’ll exit RevBayes and work from the command line prompt in shell. We assume that ./timefig_simple is a subdirectory from your current location. To generate the images below, first save a copy of FIG tools to your filesystem:# Download .zip file (open in browser our save in command line)wget https://github.com/hawaiian-plant-biogeography/fig_tools/archive/refs/heads/main.zip# Unzip file as \"fig_tools-main\"unzip main.zip# Rename directorymv fig_tools-main fig_toolsNext, copy the files in ./fig_tools/scripts into your TimeFIG project directory as ./timefig_simple/plot:# copycp -R ./fig_tools/scripts ./timefig_simple/plotThese scripts assume you are in the base of your analysis directory:cd ./timefig_simpleNow we can generate plots using FIG tools. First, we generate a tree with ancestral range estimates using these commands:# prepare tree and state output for plottingrb --args ./output/simple_timefig.tre ./output/simple_timefig.states.txt --file ./plot/make_tree.Rev# make ancestral tree plotRscript ./plot/plot_states_tree.R ./output/out.states.tre ./output/out.mcc.tre ./data/kadua/kadua_range_label.csv GNKOMHZAncestral state reconstruction of Kadua. Pie chart colors indicate the three most probable ancestral ranges for an ancestral branch or node. Range labels represent the following set of regions: G=Gardner, N=Necker, K=Kauai, O=Oahu, M=Maui Nui, H=Hawaii, Z=Remaining non-Hawaiian regions.To generate the plot of the inputted paleogeographically varying features displayed at the start of this tutorial (), enter this code:# make region feature vs. time plotsRscript ./plot/plot_features_vs_time_grid.R ./data/hawaii/feature_summary.csv ./data/hawaii/age_summary.csv ./data/hawaii/feature_description.csv GNKOMHZIn addition, we generate a plot of within-region speciation rates, $r_w(i,t)$, for each region $i$ at time $t$, which shows elevated speciation in islands soon after emergence. The code for this is:# make region rate vs. time plotsRscript ./plot/plot_rates_vs_time_grid.R ./output/simple_timefig ./data/hawaii/feature_summary.csv ./data/hawaii/age_summary.csv ./data/hawaii/feature_description.csv GNKOMHZWithin-region speciation rate estimates for Kadua. Dark colors are high rates, light colors are low rates, and gray indicates the region did not exist during that interval (missing feature). Range labels represent the following set of regions: G=Gardner, N=Necker, K=Kauai, O=Oahu, M=Maui Nui, H=Hawaii, Z=Remaining non-Hawaiian regions.Lastly, this script will plot a network that summarizes relationships between regional features, feature effect parameters, and core biogeographic processes:# make feature vs. rate network plotRscript ./plot/plot_feature_rate_network.R ./output/simple_timefig.model.txt ./data/hawaii/feature_description.csvNetwork diagram displaying the relationships between regional features (gold), feature effect parameters ($\\phi$ and $\\sigma$ in green), and rate modifier functions ($m$ in cyan). Edges colors indicate positive (blue) versus negative (red) relationships and widths indicate weak (thin) versus (strong) interactions.",
        "url": "/tutorials/timefig_simple/",
        "index": "true"
      }
      ,
    
      "tutorials-tutorial-structure": {
        "title": "Setting Up RevBayes",
        "content": "OverviewThis tutorial covers how to set up files and directories to work effectively in RevBayes (Höhna 2014; Höhna et al. 2016).This workshop assumes no familiarity with the command line, or programming in general.The themes of good directory structuring shown in this tutorial will be used in many other RevBayes tutorials.Using your computer’s graphical user interfaceDownloading and storing data and scriptsMany of the RevBayes tutorials will as you to download data and/or scripts.Let us begin by downloading the data and scripts associated with this tutorial.For each tutorial that you do, you should create a directory, sometimes called a folder, somewhere logical on your computer.Since we are doing the tutorial_structure tutorial, please title your directory tutorial_structure.In this new directory, create two more directories, one called data and one called scripts.You will note a box called Data files and scripts in the upper left-hand corner of this webpage.Please download the files listed in this directory.Drag and drop example_file.nex into your data directory.Then, move test.Rev into your scripts directory.Having a directory of data that contains all your data for a project, and a directory of scripts is a good practice.This allows you to stay organized and avoid misplacing crucial scripts in your data analysis pipeline.Spaces In FilenamesMost scientific programming languages and software does not deal well with spaces in file names.If you will be doing much scientific computing, it will be best to get in the habit of not using spaces in file and folder names.Preparing to Run RevBayesIn this section of the tutorial, we will focus on running RevBayes from your computer’s graphical interface.First, note down the location of your tutorial directory.For example, mine is in my computer’s user home, in a directory called Tutorials.A graphic of this is shown in Fig. 1.Computers do not understand the visual information of the directory structure.Instead, we will translate this information into text.This is an example of the Macintosh File Viewer. In this instance, I have a directoy, Tutorials with a subdirectory for this specific tutorial.MacintoshThe home directory on a computer on a Mac is labeled with a ~/.Each contained directory is separated by a / character.The above directory structure would be written out like so:~/tutorials/tutorial_structure/Note down where you have your tutorial stored.Next, we will launch RevBayes.Do this by double-clicking on rb.We will now set your working directory.This ensures that RevBayes is aware of where in your computer your code and data are stored.By default, RevBayes assumes you are in your user home on your computer.Therefore, we can leave off the ~/, as RevBayes will assume its presence.In RevBayes, use the setwd() command in conjunction with your path to your tutorial to set your working directory.For example, my command will look like so:setwd(\"Tutorials/tutorial_structure/\")Finally, test your working directory like so: source(\"scripts/test.Rev\")If everything has suceeded, you will see the following output:   Processing file \"scripts/test.Rev\"   Hi there! Welcome to RevBayes! I am now going to read in some test data.   Successfully read one character matrix from file 'data/primates_and_galeopterus_cytb.nex'   Congratulations, you set up your scripts and code directories correctly.If something goes wrongIf you were not able to successfully execute the script, the most common culprit is that RevBayes is not executing from where you think. Try running getwd() and making sure that your starting working directory is what you think it is.WindowsThe home directory on a computer on a Windows is labeled as c:.Each contained directory is separated by a \\\\ character.The above directory structure would be written out like so:\"c:\\\\april\\\\tutorials\\\\tutorial_structure\"Note down where you have your tutorial stored.Next, we will launch RevBayes.Do this by double-clicking on rb.We will now set your working directory.This ensures that RevBayes is aware of where in your computer your code and data are stored.In RevBayes, use the setwd() command in conjunction with your path to your tutorial to set your working directory.For example, my command will look like so:setwd(\"c:\\\\april\\\\tutorials\\\\tutorial_structure\")Finally, test your working directory like so: source(\"scripts/test.Rev\")If everything has suceeded, you will see the following output:   Processing file \"scripts/test.Rev\"   Hi there! Welcome to RevBayes! I am now going to read in some test data.   Successfully read one character matrix from file 'data/primates_and_galeopterus_cytb.nex'   Congratulations, you set up your scripts and code directories correctly.LinuxLinux users must access RevBayes via the command line interface.Using your computer’s terminal interfaceMany RevBayes users may want to use RevBayes from the command-line interface.This technology allows users to interact directly with the computer’s file system.It is the predominant way many remote servers and supercomputers are used.Command Line on WindowsLinux and Macintosh users have a command line interface by default on their machines. Windows users will have to install one. A common command line interface is Git For Windows.Downloading and storing data and scriptsMany of the RevBayes tutorials will as you to download data and/or scripts.Let us begin by downloading the data and scripts associated with this tutorial.For each tutorial that you do, you should create a directory, sometimes called a folder, somewhere logical on your computer.Since we are doing the tutorial_structure tutorial, please title your directory tutorial_structure.In this new directory, create two more directories, one called data and one called scripts.Spaces in filenamesMost scientific programming languages and software does not deal well with spaces in file names.If you will be doing much scientific computing, it will be best to get in the habit of not using spaces in file and folder names.You will note a box called Data files and scripts in the upper left-hand corner of this webapge.Please download the files listed in this directory.Drag and drop primates_and_galeopterus_cytb.nex into your data directory.Then, move test.Rev into your scripts directory.Having a directory of data that contains all your data for a project, and a directory of scripts is a good practice.This allows you to stay organized and avoid misplacing crucial scripts in your data analysis pipeline.Preparing to Run RevBayesIn this section of the tutorial, we will focus on running RevBayes from your computer’s graphical interface.First, note down the location of your tutorial directory.For example, mine is in my laptop’s shared drive, in a directory called tutorials.This can be seen in Computers do not understand the visual information of the directory structure.Instead, we will translate this information into text.This is an example of the Macintosh File Viewer. In this instance, I have a directoy, tutorials with a subdirectory for this specific tutorial.Macintosh and LinuxThe home directory on a computer on in a terminal is labeled with a ~/.Each contained directory is separated by a / character.The above directory structure would be written out like so:~/Tutorials/tutorial_structure/Open your terminal application. Change directories into the tutorial directory with  the terminal’s cd command. For example, my command will be:cd ~/Tutorials/tutorial_structure/Next, we will launch RevBayes. First, note where RevBayes is stored on your computer.For example, my copy of RevBayes is stored a software directory in my user home.Therefore, to launch my RevBayes, I will type:~/software/rbFinally, test your working directory like so: source(\"scripts/test.Rev\")If everything has suceeded, you will see the following output:   Processing file \"scripts/test.Rev\"   Hi there! Welcome to RevBayes! I am now going to read in some test data.   Successfully read one character matrix from file 'data/primates_and_galeopterus_cytb.nex'   Congratulations, you set up your scripts and code directories correctly.The System PathThe System Path tells your computer default locations to look for pieces of software.If a piece of software is added to the path, it can be found and launched from anywhere on the computer.This is beyond the scope of this tutorial, but information is readily available on doing this from other sources. If RevBayes is in your path, it can be executed by simply typing rb on Windows or ./rb on Mac or Linux.WindowsWe will first launch RevBayes.Open your terminal application.See the aside Command Line on Windows for more information on this point.Note where RevBayes is stored on your computer.For example, my copy of RevBayes is stored a software directory in my user home.Therefore, to launch my RevBayes, I will type:~/software/rbNext, we will set our working directory.Windows will not pick up file paths from the environment in the same way Mac and Linux will.Therefore, we will need to write out our directories, separated by \\\\ characters.My tutorial directory, as shown in Figure 1, will be written out as:setwd(\"c:\\\\april\\\\tutorials\\\\tutorial_structure\")Finally, test your working directory like so: source(\"scripts/test.Rev\")If everything has succeeded, you will see the following output:   Processing file \"scripts/test.Rev\"   Hi there! Welcome to RevBayes! I am now going to read in some test data.   Successfully read one character matrix from file 'data/primates_and_galeopterus_cytb.nex'   Congratulations, you set up your scripts and code directories correctly.Using RevBayes with R and RStudioR is a fairly common computing language in biology.RevBayes users may want to use RevBayes through RStudio (missing reference), a popular graphical interface for R.In this section of the tutorial, we will focus on running RevBayes from RStudio. Once you’ve followed the RStudio instructions on the interfaces page, you can run use Rev language as you would in a standard RMarkown document.InstallationRevticulate can be installed in two ways.Both will assume a working installation of R and (optionally) RStudio.The first is via CRAN, using the default install.packages function in R:install.packages(\"Revticulate\")The second is via the remotes package, a lightweight package enabling installation from GitHub repositories.remotes::install_github(\"revbayes/Revticulate\")The GitHub repository for Revticulate contains cutting-edge features and may contain bugfixes, but the CRAN is known to be stable for everyday use.To begin using your Revticulate package in R, in an RMarkdown chunk, type: library(Revticulate) knitRev()When you execute this chunk, Revticulate will run a package check.This check searches for and .Renviron file that contains a RevBayes path. If the package doesn’t find this file, or finds it without the path, the package prompts the user to use usethis::edit_r_environ(). This opens the .Renviron file, and the user will enter rb={absolute path to revbayes}. This can be edited at any time if there are multiple installs on the system, or if you recompile RevBayes and want to use a new version.First, we will test that RevBayes is accessible to us. RevBayes can be used in a KnitR chunk by changing the header to rb instead of r. If you are unsure how to edit a chunk to have the proper type, the test.Rmd file in this tutorial provides examples. In the below chunk, we create an object called example and use the assignment operator to give it the value 1. Then we print it. This will fail if RevBayes cannot be found. If this is the case, check that the path in your Renviron goes to RevBayes, and restart.```{rb}variable &lt;- \"Hi there! Welcome to RevBayes! I am now going to read in some test data.\"variable```Next, we will attempt to read in some data. Use the setwd() command to ensure that you are in the directory for this tutorial.```{rb}molecular_data &lt;- readDiscreteCharacterData(\"data/example_file.nex\")molecular_data```If you get an error such as rb not found, make sure you executed this block:```{r}library(Revticulate)knitRev()```Using JupyterJupyter (missing reference) is a popular interface for programming in python, though it implements numerous other languages. Once you have completed the instructions on the interfaces page, you will be able to choose RevBayes as a possible language. An example of this is below.This is a Jupyter notebook with the language set to Rev.Once the language is set to Rev, all cells in the notebook will execute using RevBayes. Example notebooks can be found in the notebooks repository.",
        "url": "/tutorials/tutorial_structure/",
        "index": "true"
      }
      ,
    
      "": {
        "title": "",
        "content": "    RevBayes  Bayesian phylogenetic inference using probabilistic graphical models and an interpreted languageAboutRevBayes provides an interactive environment for statistical computation in phylogenetics. It is primarily intended for modeling, simulation, and Bayesian inference in evolutionary biology, particularly phylogenetics. However, the environment is quite general and can be useful for many complex modeling tasks.RevBayes uses its own language, Rev, which is a probabilistic programming language like JAGS, STAN, Edward, PyMC3, and related software. However, phylogenetic models require inference machinery and distributions that are unavailable in these other tools.The Rev language is similar to the language used in R. Like the R language, Rev is designed to support interactive analysis. It supports both functional and procedural programming models, and makes a clear distinction between the two. Rev is also more strongly typed than R.RevBayes is a collaboratively developed software project.",
        "url": "/",
        "index": ""
      }
      ,
    
      "interfaces": {
        "title": "Interfaces",
        "content": "The Rev LanguageIn practice, most researchers will primarily work with RevBayes by writing scripts in the Rev language and executing these directly. Most of the tutorials demonstrate this approach to RevBayes analysis. In the Documentation, you will find the Rev Language Reference that will allow you to browse the various components of Rev and their associated documentation. If you use RevBayes interactively in your terminal, you can also access Rev language documentation using the ? object. For example, typing ?dnPoisson will output a description and usage details of the Poisson distribution function. This is the same information provided on the Poisson distribution help page: https://revbayes.github.io/documentation/dnPoisson.html.Graphical User InterfacesWithin the RevBayes project we are developing alternative interfaces for specifying RevBayes models and analyses. These are intended to provide more interactive workflows using different interface styles. Note that some of these projects are still under active development and may not be fully functional.  : specify and execute analyses using the Rev language in a notebook interface  : create Rev scripts for execution in RevBayes using R and RStudio  : an introductory, menu-driven GUI for generating Rev scripts for a subset of analyses (in development)  : an interactive macOS GUI for creating analysis workflows and models for RevBayes (in development)Jupyter NotebookGet Python3 and Jupyter NotebookFirst, download and install Python 3 and the Jupyter Notebook.Installation instructions are available from the Jupyter Development team here.Get RevBayesRevBayes pre-built executables are designed to work with Jupyter. You may want to ensures that rb executable can be found using the which command or be located using the environment variable, REVBAYES_JUPYTER_EXECUTABLE. For instance, you can set the environment variable usingexport REVBAYES_JUPYTER_EXECUTABLE=&lt;revbayes_path&gt;/rbConnect RevBayes and JupyterFinally, clone the RevBayes Jupyter Kernel.git clone https://github.com/revbayes/revbayes_kernel.gitChange directories into the revbayes_kernel directory and usesudo python3 setup.py installpython3 -m revbayes_kernel.installpip3 install metakernelto deploy the kernel. Now, when launching a Jupyter Notebook, RevBayes should be an available language when starting a notebook. You can check the installation by executing the revbayes_mcmc_demo.ipynb found in the revbayes_kernel directory.Examples of RevNotebooks can be found in the RevNotebook repository.RStudioCompile or download RevBayes as appropriate for your system above. Note where on your computer it is stored.Download R and RStudio. Once these are downloaded, start RStudio. Install the remotes package. We will also install the package usethis to aid with installation:install.packages(\"remotes\")install.packages(\"usethis\")RStudio does not run RevBayes by default, as RevBayes is a separate piece of software from R or RStudio. Use remotes to install Revticulate, an R package for using RevBayes within R and RStudio environments:remotes::install_github(\"revbayes/Revticulate\")Once installation is complete, type into an RMarkdown cell:library(Revticulate)knitRev()When you execute the above, Revticulate will run a package check.This check searches for and .Renviron file that contains a RevBayes path. If the package doesn’t find this file, or finds it without the path, the package prompts the user to use usethis::edit_r_environ(). This opens the .Renviron file, and the user will enter rb={absolute path to revbayes}. This can be edited at any time if there are multiple installs on the system, or if you recompile RevBayes and want to use a new version.Now, you may use RevBayes in either KnitR or console. For examples of RevBayes used via Revticulate, see the Revticulate website and our tutorial on setting up RevBayes.RevScripterRevScripter is a web-based tool that enables researchers to create scripts describing phylogenetic models and analyses in the Rev language. These script files then can be executed in the program RevBayes. RevScripter is intended to serve as an introductory tool that guides new users through the setup for a subset of phylogenetic analyses.RevScripterCurrently, the options available in RevScripter are limited and the tool is still very much in development. The tool can create scripts for running an unrooted analysis using nucleotide data under standard substitution models. For more details about the available models and analyses, please see the README file in the RevScripter source repository: https://github.com/revbayes/revscripter.RevBayes macOS GUIThe RevBayes macOS GUI is an interactive graphical user interface for RevBayes written in Swift with the use of Cocoa libraries that runs on the macOS platform.The complexity of the RevBayes program and the Rev language can be a significant barrier to the scientists wanting to use the program. It is the goal of the RevBayes GUI project to overcome that barrier. RevBayes GUI serves to simplify the use of RevBayes while at the same time expose its full functionality.RevBayes GUI is a document-based application, with a collection of phylogenetic analyses seen as a document. The user can create an analysis by dragging the necessary components from a tool palette and dropping them on the analysis canvas. Adopting a convention from the field of graphical models in computer science, we allow a biologist specify their analysis as a graph. We use a tool metaphor to expose the functionality of the program. Each node in the graph is a tool that carries out a single task, such as reading and visualizing data from a file, setting the phylogenetic model graphically, aligning DNA sequences, etc. Tool Model, often the core of an analysis, allows the user to specify their model assumptions. Like with the analysis, RevBayes GUI exposes a model as a graph. The user can build up models graphically, by bringing together different mathematical components of a model just as a child would build a house out of Lego bricks.Development of RevBayes GUI is funded by an NSF grant. It is an open-source project on GitHub. The GitHub repository can be freely cloned.If you would like to contribute to the project or report issues please contact the lead developer to request access.",
        "url": "/interfaces",
        "index": ""
      }
      ,
    
      "workshops-intro-phylogenomics-2020-html": {
        "title": "Introduction to Phylogenomics",
        "content": "Course Website and Registration  Introduction to Phylogenomics (Transmitting Science)",
        "url": "/workshops/intro_phylogenomics_2020.html",
        "index": ""
      }
      ,
    
      "jobs": {
        "title": "Jobs",
        "content": "Throughout the year, the members of the RevBayes development team and our collaborators advertise jobs in our groups related to development in RevBayes.Application DateJob TitleLocationPI\t\t\t\tAugust 31, 2019\t\tPhD position&#58; Species Tree Estimation\t\tGeoBio-Center, Ludwig-Maximilians-Universität, München, Germany\t\tSebastian Höhna\t\t\t\t\t\t\tAugust 31, 2019\t\tPhD position&#58; Modeling Species Diversification\t\tGeoBio-Center, Ludwig-Maximilians-Universität, München, Germany\t\tSebastian Höhna\t\t\t\t\t\t\tOctober 31, 2018\t\tProgrammer position&#58; Supporting development of RevBayes\t\tGeoBio-Center, Ludwig-Maximilians-Universität, München, Germany\t\tSebastian Höhna\t\t\t\t\t\t\tOctober 31, 2018\t\tPostdoc position&#58; Developing methods and models for gene-tree species-tree estimation\t\tGeoBio-Center, Ludwig-Maximilians-Universität, München, Germany\t\tSebastian Höhna\t\t\t\t\t\t\tOctober 31, 2018\t\tPhD position&#58; Developing new methods and models for robust gene-tree estimation\t\tGeoBio-Center, Ludwig-Maximilians-Universität, München, Germany\t\tSebastian Höhna\t\t\t",
        "url": "/jobs/",
        "index": ""
      }
      ,
    
      "license": {
        "title": "License",
        "content": "RevBayes SoftwareRevBayes and all associated software developed by the RevBayes Team are distributed for free and open-source. These products are licensed under version 3 of the GNU General Public License (GPL). The source code is available on GitHub.If you use RevBayes, please be sure to properly cite the software and materials. See the Citation page for a list of the RevBayes papers.RevBayes Website and TutorialsThe RevBayes website is generated using Jekyll. All of the content on this site, except where otherwise noted, is licensed under a Creative Commons Attribution 4.0 license.",
        "url": "/license",
        "index": ""
      }
      ,
    
      "workshops-mad-phylo2018-html": {
        "title": "MadPhylo&amp;#58; Madrid Workshop on Phylogenetics",
        "content": "",
        "url": "/workshops/mad_phylo2018.html",
        "index": ""
      }
      ,
    
      "workshops-mad-phylo2019-html": {
        "title": "MadPhylo&amp;#58; Madrid Workshop on Phylogenetics",
        "content": "",
        "url": "/workshops/mad_phylo2019.html",
        "index": ""
      }
      ,
    
      "assets-css-main-css": {
        "title": "",
        "content": ".maintitle {  font-family: \"Raleway\";  font-size: 2.5em;}.not_found {  margin-top: 4em;  margin-bottom: 8em;  text-align: center;  font-family: \"Raleway\";  font-size: 2em;}:not(pre) > code {  background-color: #eff0f1;  border-radius: 0;  padding: 1px 5px;}.default pre {  font-size: 14px;  background-color: #eff0f1;  color: #000000;}.default pre .line::before {  content: \"\" \" \";}.default pre .secondary::before {  content: \"\" \" \";}.Rev pre {  font-size: 14px;  background-color: #b7cfed;  color: #000000;}.Rev pre .line::before {  content: \">\" \" \";}.Rev pre .secondary::before {  content: \"+\" \" \";}.rev pre {  font-size: 14px;  background-color: #b7cfed;  color: #000000;}.rev pre .line::before {  content: \">\" \" \";}.rev pre .secondary::before {  content: \"+\" \" \";}.rev-output pre {  font-size: 14px;  background-color: #b7cfed;  color: #808080;}.rev-output pre .line::before {  content: \"\" \" \";}.rev-output pre .secondary::before {  content: \"\" \" \";}.Rev-output pre {  font-size: 14px;  background-color: #b7cfed;  color: #808080;}.Rev-output pre .line::before {  content: \"\" \" \";}.Rev-output pre .secondary::before {  content: \"\" \" \";}.bash pre {  font-size: 14px;  background-color: #f0f0f0;  color: #000000;}.bash pre .line::before {  content: \"$\" \" \";}.bash pre .secondary::before {  content: \">\" \" \";}.instruction {  border: 1px solid;  border-color: #ddd;  display: table;}.tutorial_files {  padding-left: 5px;  padding-top: 0;  padding-bottom: 0;  padding-right: 0;  border: 1px solid;  border-color: #f4fd9c;  padding-bottom: 5px;  margin-top: 0;}.tutorial_files > h2:first-child {  padding-top: 5px;  padding-bottom: 5px;  font-size: 20px;  background: linear-gradient(to bottom, #f4fd9c, #f5fda6);  border-color: #f4fd9c;  margin-top: 0px;  margin-left: -5px;}.tutorial_files > h2:first-child:before {  font-family: \"Glyphicons Halflings\";  content: \"\";  float: left;  padding-left: 5px;  padding-right: 5px;  display: inline-block;  margin-top: 0px;  -webkit-font-smoothing: antialiased;}.overview {  padding-left: 5px;  padding-top: 0;  padding-bottom: 0;  padding-right: 0;  border: 1px solid;  border-color: #b7cfed;  padding-bottom: 5px;  margin-top: 0;}.overview > h2:first-child {  padding-top: 5px;  padding-bottom: 5px;  font-size: 20px;  background: linear-gradient(to bottom, #b7cfed, #bed4ef);  border-color: #b7cfed;  margin-top: 0px;  margin-left: -5px;}.overview > h2:first-child:before {  font-family: \"Glyphicons Halflings\";  content: \"\\e105\";  float: left;  padding-left: 5px;  padding-right: 5px;  display: inline-block;  margin-top: 0px;  -webkit-font-smoothing: antialiased;}.info {  padding-left: 5px;  padding-top: 0;  padding-bottom: 0;  padding-right: 0;  border: 1px solid;  border-color: #eec275;  padding-bottom: 5px;  margin-top: 0;}.info > h2:first-child {  padding-top: 5px;  padding-bottom: 5px;  font-size: 20px;  background: linear-gradient(to bottom, #eec275, #f0c883);  border-color: #eec275;  margin-top: 0px;  margin-left: -5px;}.info > h2:first-child:before {  font-family: \"Glyphicons Halflings\";  content: \"\\e086\";  float: left;  padding-left: 5px;  padding-right: 5px;  display: inline-block;  margin-top: 0px;  -webkit-font-smoothing: antialiased;}.aside {  padding-left: 5px;  padding-top: 0;  padding-bottom: 0;  padding-right: 0;  border: 1px solid;  border-color: #eee0e5;  padding-bottom: 5px;  margin-top: 0;}.aside > h2:first-child {  padding-top: 5px;  padding-bottom: 5px;  font-size: 20px;  background: linear-gradient(to bottom, #eee0e5, #f0e3e8);  border-color: #eee0e5;  margin-top: 0px;  margin-left: -5px;}.aside > h2:first-child:before {  font-family: \"Glyphicons Halflings\";  content: \"\";  float: left;  padding-left: 5px;  padding-right: 5px;  display: inline-block;  margin-top: 0px;  -webkit-font-smoothing: antialiased;}.aside {  margin-top: 20px;}.aside span.fold-unfold:first-child {  cursor: pointer;  float: left;  padding-left: 5px;  padding-right: 5px;  display: inline-block;  -webkit-font-smoothing: antialiased;}.download_files {  cursor: pointer;  float: left;  padding-left: 5px;  padding-right: 5px;  display: inline-block;  -webkit-font-smoothing: antialiased;}.titlebar {  margin-bottom: 2em;  text-align: left;  font-family: \"Raleway\";}.titlebar .subtitle {  font-size: 1.8em;  font-style: italic;}.titlebar .authors {  font-size: 1em;}h2.section, h2.references {  margin-top: 30pt;  margin-bottom: 0pt;  font-size: 1.85em;}hr.section, hr.references {  margin: 5pt 0pt 5pt 0pt;}h3.subsection {  font-size: 1.6em;}hr.subsection {  display: none;}h4.subsubsection {  margin-top: 15pt;  font-size: 1.2em;}hr.subsubsection {  display: none;}.sidebar {  width: 22em;  float: left;  font-size: 0.9em;  margin-right: 1em;  margin-top: 0em;  padding: 8px;}.MathJax_Display {  pointer-events: none;}.tutorialbox {  display: flex;  flex-wrap: wrap;}.tutorial {  width: \"30%\";  background-color: #fafafa;  margin: 0.2em;  padding: 0.6em;  max-width: 13.5em;  text-align: left;}.tutorial .title {  font-size: 1.2em;}.tutorial .subtitle {  font-style: italic;}figure {  padding: 10px 20px;  counter-increment: figures;  text-align: center;  margin-left: auto;  margin-right: auto;  margin-top: 0.9em;  margin-bottom: 0.5em;  border-left: 5px solid #eee;  border-right: 5px solid #eee;  display: table;}figure img {  margin-bottom: 0.5em;}figure.table {  padding: 2px 4px;  counter-increment: tables;  align-items: center;  margin-left: auto;  margin-right: auto;  margin-top: 0.9em;  margin-bottom: 0.5em;  border-left: 5px solid #eee;  border-right: 5px solid #eee;  display: table;}figure.table figcaption.table {  padding: 10px 20px;  text-align: center;  margin-left: auto;  margin-right: auto;  margin-top: 0.9em;  margin-bottom: 0.5em;}figure.table table.table {  padding: 10px 20px;  text-align: left;  margin-left: auto;  margin-right: auto;  margin-top: 0.9em;  margin-bottom: 0.5em;}figure:not(.table) figcaption:before {  content: \"Figure \" counter(figures) \". \";  font-weight: bold;}figure.table figcaption:before {  content: \"Table \" counter(tables) \". \";  font-weight: bold;}@media print {  figure {    border: 1px solid #999;    page-break-inside: avoid;  }  figure .table {    border: 1px solid #999;    page-break-inside: avoid;  }}* {  box-sizing: border-box;}body {  font: 16px Arial;}.autocomplete {  /*the container must be positioned relative:*/  position: relative;  display: inline-block;  float: left;}.btn-clear {  margin-left: 10px;}input {  border: 1px solid transparent;  background-color: #f1f1f1;  padding: 10px;  font-size: 16px;}input[type=text] {  background-color: #f1f1f1;  width: 80%;}input[type=submit] {  color: #fff;  width: 15%;}.autocomplete-items {  position: absolute;  border: 1px solid #d4d4d4;  border-bottom: none;  border-top: none;  z-index: 99;  /*position the autocomplete items to be the same width as the container:*/  top: 100%;  left: 0;  right: 0;}.autocomplete-items div {  padding: 10px;  cursor: pointer;  background-color: #fff;  border-bottom: 1px solid #d4d4d4;}.autocomplete-items div:hover {  /*when hovering an item:*/  background-color: #438ac7;  color: #ffffff;}.autocomplete-active {  /*when navigating through the items using the arrow keys:*/  background-color: #438ac7 !important;  color: #ffffff;}.btn-keyword {  margin: 3px;  background-color: #438ac7;  color: #ffffff;}ol.bibliography {  padding-left: 0px;}ol.bibliography li {  list-style-type: none;  padding-bottom: 10px;}.sidebar_nav {  overflow: auto;  height: 75vh;}.help {  overflow: auto;  height: 75vh;}.help-title {  padding-bottom: 2pt;}.help-description {  padding-top: 5pt;  padding-bottom: 2pt;  font-size: 13pt;}.help-section-header.arguments {  margin-bottom: 0px;}.help-section-body {  padding-left: 30px;}.table.arguments > tbody > tr > td,.table.arguments > tbody > tr > th {  border-top: none;}table.arguments .lhs {  text-align: right;  padding-top: 0px;  padding-bottom: 0px;}table.arguments .rhs {  text-align: left;  padding-top: 0px;  padding-bottom: 0px;}table.arguments .lhs.argument, table.arguments .rhs.argument {  padding-top: 10px;}.methods > li {  padding-bottom: 4px;}body {  display: none;  font-family: HelveticaNeue, \"Helvetica Neue\", Helvetica, Arial, \"Lucida Grande\", sans-serif;  font-size: 16px;  counter-reset: figures;  counter-reset: tables;  padding-top: 70px;}html {  overflow: scroll;}@media print {  .no-print, .no-print * {    display: none !important;  }}.footertext {  text-align: center;}img.navbar-logo {  height: 40px;  padding-top: 5px;  padding-right: 10px;  padding-left: 40px;}ul,ol {  padding-left: 1em;}blockquote div:not(:last-child) {  margin-bottom: 10px;}code {  padding: 0 0;  color: inherit;  background-color: inherit;}img {  max-width: 100%;}.navbar .navbar-collapse {  height: 50px;  text-align: right;}.navbar-nav {  display: flex;}.navbar-nav > li:last-child {  margin-left: auto;  flex: 1;}.navbar-nav > li:last-child > * {  width: 100%;}.header-search-form {  margin: 8px 0px;}.pull-left {  float: left;  margin-left: 10px;}.navbar-toggle {  float: right;}/*# sourceMappingURL=main.css.map */",
        "url": "/assets/css/main.css",
        "index": ""
      }
      ,
    
      "developer-make-help-file-html": {
        "title": "Adding a RevBayes help file",
        "content": "  Overview  Set up GitHub Access  Ask someone to send you an invite to the revbayes organization:          Ben Redelings, Tracy Heath, Michael Landis, Sebastian Hoehna, and Joelle Barido-Sotani can do this here.        Generate an ssh public/private key if you don’t have one.  Add the public key to your github account.  Clone the revbayes repo:    git clone git@github.com:revbayes/revbayes.git        Alternatively, if you already cloned the repo, then you may need to change the origin URL for your repo from one that starts with https: to one that starts with git@.    git remote -v              # check if the URL for origin starts with https: or git@git remote rm origingit remote add origin git@github.com:revbayes/revbayes.gitgit remote -v              # check that the origin URL now starts with git@      The development branch is “protected”RevBayes does not allow pushing to the development branch directly.Instead, you should create a new branch off of development and add your changes there.In order to merge these changes into the development branch you will first create a “pull request” (PR) based on your new branch.The pull request is a request to merge the changes into the development branch.If automatic tests pass for the the new branch, and the pull request is approved by someone, then you can merge the changes into development.Push a branchFirst, make sure you are currently on the development branch so that your new branch starts from the right place.git checkout developmentgit branchgit logThen make a new branch:git branch my-new-branch-namegit checkout my-new-branch-namegit branchFinally, let’s commit some changes.  Let’s suppose that you are modifying the help file help/md/something.md.  So edit this file and save it.  Before you do this, you might want to change the default editor.git status                          # check that the file you changed is shown as modifiedgit add help/md/something.mdgit status                          # check that your modified file is stagedgit commitgit push --set-upstream=origin my-new-branch-nameThe changes that you committed will then be publicly available from github.They do not need to be perfect: making them public allows others to comment on your work, review it, and make suggestions.Make a Pull Request (PR)  Go to https://github.com/revbayes/revbayes/pulls.  If you just pushed a branch, then there should be a green button to make a PR for that branch.  Otherwise, click the green button New pull request.  Change the left branch button (the base) from master to development.  The right branch button should be the branch with the changes that you want to merge.Update a help file  Choose a help file in help/md/ to modify.  Look at help/md/dnNorm.md for an example of what the fields mean, and what information should be included.  Q: How much information?  A: Try to include the essential information without letting the file get too long.  The description should be about 1 line.  Longer information goes in the details.  General background information (e.g. “what is a prior”?) that can be found on wikipedia should not go in the help files.  You need to run projects/generate_help.sh to copy information from the markdown files in help/md into the C++ code.",
        "url": "/developer/make-help-file.html",
        "index": ""
      }
      ,
    
      "workshops-montana2018-html": {
        "title": "RevBayes Demonstration and Mini Workshop",
        "content": "Additional InformationPlease be sure to download and install RevBayes. Go to the Software page for more information.SlidesHere are some lecture slides that are similar to the ones presented in the workshop. These slides cover the basics of phylogenetic inference and analysis using RevBayes.   Introduction to Bayesian Phylogenetics  from Tracy Heath    Bayesian Divergence Time Estimation – Workshop Lecture  from Tracy Heath ",
        "url": "/workshops/montana2018.html",
        "index": ""
      }
      ,
    
      "workshops-montpellier2018-html": {
        "title": "Analysing Macroevolutionary Processes using RevBayes",
        "content": "",
        "url": "/workshops/montpellier2018.html",
        "index": ""
      }
      ,
    
      "tutorials-morph-morph-dec2018-html": {
        "title": "Discrete morphology - Tree Inference",
        "content": "IntroductionWhile molecular data have become the default for building phylogenetictrees for many types of evolutionary analysis, morphological dataremains important, particularly for analyses involving fossils. The useof morphological data raises special considerations for model-basedmethods for phylogenetic inference. Morphological data are typicallycollected to maximize the number of parsimony-informative characters -that is, the characters that may provide information in favor of onetopology over another. Morphological characters also do not carry commonmeanings from one character in a matrix to the next; character codingsare made arbitrarily. These two factors require extensions to ourexisting phylogenetic models. Accounting for the complexity ofmorphological characters remains challenging. This tutorial will providea discussion of modeling morphological characters, and will demonstratehow to perform Bayesian phylogenetic analysis with morphology usingRevBayes (Höhna et al. 2016).Graphical ModelsRevBayes uses a graphical model framework inwhich all probabilistic models, including phylogenetic models,are comprised of modular components that can be assembled in a myriad of ways.The statistics literature has developed a rich visual representation for graphical models.Visually representing graphical models can be useful for communication model assumptions.The notation used in the visual representation of these models is briefly explained in ,and enourage users to see Höhna et al. (2014) for more details.Representing graphical models in computer code (using the Rev language) is useful in developing an understanding of phylogenetic models. For more information about this topic see the tutorial Introduction to Graphical Models.The symbols for a visual representation of a graphicalmodel. a) Solid squares represent constant nodes, which specify fixed-valued variables. b) Stochastic nodes are represented by solid circles.These variables correspond to random variables and may depend onother variables. c) Deterministic nodes (dotted circles) indicate variablesthat are determined by a specific function applied to another variable.They can be thought of as variable transformations. d) Observed statesare placed in clamped stochastic nodes, represented by gray-shadedcircles. e) Replication over a set of variables is indicated by enclosingthe replicated nodes in a plate (dashed rectangle). f) Tree plates represent the different classes of nodes in a phylogeny. The tree topology orders the nodes in the tree plate andmay be a constant node (as in this example) or a stochastic node (if thetopology node is a solid circle).Image and text modified from Höhna et al. (2014)Overview of Discrete Morphology ModelsGraphical model showing the Mk model (left panel). Rev code specifying the Mk model is on the right-hand panel.Molecular data forms the basis of most phylogenetic analyses today.However, morphological characters remain relevant. Fossils often provideour only direct observation of extinct biodiversity. DNA degradation canmake it difficult or impossible to obtain sufficient molecular data fromfragile museum specimens. Using morphological data can help researchersinclude specimens in their phylogeny that might be left out of amolecular tree.To understand how morphological characters are modeled, it is importantto understand how characters are collected. Unlike in molecular data,for which homology is algorithmically determined, homology in a morphological character is typically assessed by an expert. Biologists will typically decide what characters are homologous by looking across specimens at the same structure in multiple taxa; they may also look at the developmental origin of structures in making this assessment (Phillips 2006). Once homology is determined, characters are broken down into states, or different forms a single character can take. The state 0 commonly refers to absence, meaning that character is not present. In somecodings, absence will mean that character has not evolved in that group.In others, absence means that that character has not evolved in thatgroup, and/or that that character has been lost in that group(Freudenstein 2005). This type of coding is arbitrary, but bothnon-random and meaningful, and poses challenges for how we modelthe data.Historically, most phylogenetic analyses using morphological charactershave been performed using the maximum parsimony optimality criterion.Maximum parsimony analysis involves proposing trees from themorphological data. Each tree is evaluated according to how many changesit implied in the data, and the tree that requires the fewest changes ispreferred. In this way of estimating a tree, a character that does notchange, or changes only in one taxon, cannot be used to discriminatebetween trees (i.e., it does not favor a topology). Therefore, workerswith parsimony typically do not collect characters that are parsimonyuninformative.In 2001, Paul Lewis (Lewis 2001) introduced a generalization of theJukes-Cantor model of sequence evolution for use with morphologicaldata. This model, called the Mk (Markov model, assuming each characteris in one of k states) model provided a mathematical formulation thatcould be used to estimate trees from morphological data in bothlikelihood and Bayesian frameworks. While this model is a useful stepforward, as a generalization of the Jukes-Cantor, it still makes fairlysimplistic assumptions. This tutorial will guide you through estimatinga phylogeny with the Mk model, and two useful extensions to the model.The Mk ModelThe Mk model is a generalization of the Jukes-Cantor model of nucleotidesequence evolution, which we discussed in Nucleotide substitution models. The Q-matrix for a two-state Mk model looks like so:\\[Q = \\begin{pmatrix} -\\mu_0 &amp; \\mu_{01} \\\\\\mu_{10} &amp; -\\mu_1  &amp;\\\\\\end{pmatrix} \\mbox{  ,}\\]This matrix can be expanded to accommodate multi-state data, as well:\\[Q = \\begin{pmatrix} -\\mu_0 &amp; \\mu_{01} &amp; \\mu_{02} &amp; \\mu_{03} \\\\\\mu_{10} &amp; -\\mu_1  &amp; \\mu_{12} &amp; \\mu_{13} \\\\\\mu_{20} &amp; \\mu_{21} &amp; -\\mu_2  &amp; \\mu_{23} \\\\\\mu_{30} &amp; \\mu_{31} &amp; \\mu_{32} &amp; -\\mu_3 \\end{pmatrix} \\mbox{  ,}\\]However, the Mk model sets transitions to be equal from any state to anyother state. In that sense, our multistate matrix really looks likethis:\\[Q = \\begin{pmatrix} -(k-1)\\mu &amp; \\mu &amp; \\mu &amp; \\mu \\\\\\mu &amp; -(k-1)\\mu  &amp; \\mu &amp; \\mu \\\\\\mu &amp; \\mu &amp; -(k-1)\\mu  &amp; \\mu \\\\\\mu &amp; \\mu &amp; \\mu &amp; -(k-1)\\mu \\\\\\end{pmatrix} \\mbox{  ,}\\]Because this is a Jukes-Cantor-like model (Jukes and Cantor 1969), statefrequencies do not vary as a model parameter. These assumptions may seemunrealistic. However, all models are a compromise between reality andgeneralizability. Prior work has demonstrated that, in many conditions,the model does perform adequately (Wright and Hillis 2014). Because morphologicalcharacters do not carry common meaning across sites in a matrix in theway that nucleotide characters do, making assumptions that fit allcharacters is challenging. A visualization of this simple model can beseen in .We will first perform a phylogenetic analysis using the Mk model. Infurther sections, we will explore how to relax key assumptions of the Mkmodel.Example: Inferring a Phylogeny of Fossil Bears Using the Mk ModelIn this example, we will use morphological character data from 18 taxaof extinct bears (Abella et al. 2011). The dataset contains 62 binarycharacters, a fairly typical dataset size for morphological characters.Tutorial FormatThis tutorial follows a specific format for issuing instructions andinformation.  The boxed instructions guide you to complete tasks that are not part ofthe RevBayes syntax, but rather direct you to create directories orfiles or similar.Information describing the commands and instructions will be written inparagraph-form before or after they are issued.All command-line text, including all Rev syntax, are given inmonotype font. Furthermore, blocks of Rev code that are needed tobuild the model, specify the analysis, or execute the run are given inseparate shaded boxes. For example, we will instruct you to create aconstant node called example that is equal to 1.0 using the &lt;-operator like this:example &lt;- 1.0If you copy and paste text from the browser, beware of introducing incorrect characters!Data and Files  On your own computer, create a directory for the tutorial called RB_DiscreteMorphology_Tutorial (or whatever you want) and within this directory create a subdirectory called data.In this directory download the data files: bears.nex.Getting Started  Create a new subdirectory (in RB_DiscreteMorphology_Tutorial) called scripts.When you execute RevBayes in this exercise, you will do so within themain directory you created (RB_DiscreteMorphology_Tutorial), thus,if you are using a Unix-based operating system, we recommend that youadd the RevBayes binary to your path. Alternatively make sure that you set the working directory to, for example, RB_DiscreteMorphology_Tutorial if this is the directory you stored the scripts and data in.Creating Rev FilesIn this exercise, you will work primarily in your text editor andcreate a set of files that will be easily managed and interchanged. In this first section, you will write the following filefrom scratch and save them in the scripts directory:  mcmc_mk.Rev: the Rev file that loads the data, specifies the model describing discrete morphologicalcharacter change (binary characters), and specifies the monitors and MCMC sampler.All of the files that you will create are also provided in theRevBayes tutorial here (see the top of this webpage). Please refer to these files to verify or troubleshoot your own scripts.  Open your text editor and create the Rev-script file called mcmc_Mk.Rev in thescripts directory.  Enter the Rev code provided in this section in the new file.In this section you will begin the file and write the Rev commands forloading in the taxon list and managing the data matrices. Then, startingin section , you will move on to specifying each of the model components. Once the model specifications arecomplete, you will complete the script with the instructions given in section.Load Data MatrixRevBayes uses the function readDiscreteCharacterData() to load adata matrix to the workspace from a formatted file. This function can beused for both molecular sequences and discrete morphological characters.Import the morphological character matrix and assign it to the variablemorpho.morpho &lt;- readDiscreteCharacterData(\"data/bears.nex\")Create Helper VariablesBefore we begin writing the Rev scripts for each of the models, we need to instantiate a couple “helper variables” that willbe used by downstream parts of our model specification.Create a new constant node called num_taxa that is equal to the numberof species in our analysis (18) and a constant node called num_branches representingthe number of branches in the tree. We will also create a constant node ofthe taxon names. This list will be used to initialize the tree.taxa &lt;- morpho.names()num_taxa &lt;- morpho.size() num_branches &lt;- 2 * num_taxa - 2Next, create two workspace variables called moves and monitors. These variables are vectors containing all of the MCMC moves and monitors, respectively.moves    = VectorMoves()monitors = VectorMonitors()One important distinction here is that moves and monitors are part of the RevBayesworkspace and not the hierarchical model. Thus, we use the workspaceassignment operator = instead of the constant node assignment &lt;-.The Mk ModelFirst, we will create a joint prior on the branch lengths and tree topology. This should be familiar from the Nucleotide substitution models. In the first step we will specify a stochastic node for the mean of theSome types of stochastic nodes can be updated by a number of alternative moves. Different moves may explore parameter space in different ways, and it is possible to use multiple different moves for a given parameter to improve mixing (the efficiency of the MCMC simulation). In the case of our unrooted tree topology, for example, we can use both a nearest-neighbor interchange move (mvNNI) and a subtree-prune and regrafting move (mvSPR). These moves do not have tuning parameters associated with them, thus you only need to pass in the topology node and proposal weight.br_len_lambda ~ dnExp(0.2)moves.append( mvScale(br_len_lambda, weight=2) )phylogeny ~ dnUniformTopologyBranchLength(taxa, branchLengthDistribution=dnExp(br_len_lambda))moves.append( mvNNI(phylogeny, weight=num_branches/2.0) )moves.append( mvSPR(phylogeny, weight=num_branches/10.0) )moves.append( mvBranchLengthScale(phylogeny, weight=num_branches) )    tree_length := phylogeny.treeLength()Next, we will create a Q-matrix. Recall that the Mk model is simply ageneralization of the JC model. Therefore, we will create a 2x2 Q-matrixusing fnJC, which initializes Q-matrices with equal transitionprobabilities between all states.Q_morpho &lt;- fnJC(2)Now that we have the basics of the model specified, we will addgamma-distributed rate variation and specify moves on the parameter ofthe gamma distribution, alpha_morpho.alpha_morpho ~ dnUniform( 0, 1E8 )rates_morpho := fnDiscretizeGamma( alpha_morpho, alpha_morpho, 4 )# Moves on the parameters of the gamma distributionmoves.append( mvScale(alpha_morpho, lambda=1, weight=2.0)Lastly, we set up the CTMC. This should also be familiar from the Nucleotide substitution models. We see some familiar pieces: the tree, Q-matrix and site rates.We also have two new keywords: data type and coding. The data typeargument specifies the type of data - in our case, “Standard”, thespecification for morphology. (The role of the coding argument will be described in the next section .)phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q_morpho, type=\"Standard\")phyMorpho.clamp(morpho)All of the components of the model are now specified.Complete MCMC AnalysisCreate Model ObjectWe can now create our workspace model variable with our fully specifiedmodel DAG. We will do this with the model() function and provide asingle node from the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph andallows us to pass the model to various functions that are specific toour MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our Rev-script is to specify themonitors and output file names. For this, we create a vector calledmonitors that will each sample and record or output our MCMC.The first monitor we will create will monitor every named randomvariable in our model graph. This will include every stochastic anddeterministic node using the mnModel monitor. The only parameter thatis not included in the mnModel is the tree topology. Therefore, theparameters in the file written by this monitor are all numericalparameters. This file will be a tab-separated text file that can be opened by accessory programs for evaluating such parameters. We will also name the output file for this monitor and indicate that we wish to sample ourMCMC every 10 cycles.monitors.append( mnModel(filename=\"output/mk.log\", printgen=10) )The mnFile monitor writes any parameter we specify to file. Thus, ifwe only cared about the branch lengths and nothing else (this is not atypical or recommended attitude for an analysis this complex) wewouldn’t use the mnModel monitor above and just use the mnFilemonitor to write a smaller and simpler output file. Since the treetopology is not included in the mnModel monitor (because it is notnumerical), we will use mnFile to write the tree to file by specifyingour phylogeny variable in the arguments.monitors.append( mnFile(filename=\"output/mk.trees\", printgen=10, phylogeny) )The third monitor we will add to our analysis will print information tothe screen.monitors.append( mnScreen(printgen=100) )Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now createthe workspace variable that defines our MCMC run. We do this using themcmc() function that takes the three main analysis componentsas arguments.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")The MCMC object that we named mymcmc has a member method called.run(). This will execute our analysis and we will set the chainlength to 20000 cycles using the generations option.mymcmc.run(generations=20000, tuningInterval=200)Once our Markov chain has terminated, we will want RevBayes to close.Tell the program to quit using the q() function.q()  You made it! Save all of your files.Execute the MCMC AnalysisWith all the parameters specified and all analysis components in place,you are now ready to run your analysis. The Rev scripts you justcreated will all be used by RevBayes.  Begin by running the RevBayes executable. In Unix systems, type thefollowing in your terminal (if the RevBayes binary is in your path):rbProvided that you started RevBayes from the correct directory(RB_DiscreteMorphology_Tutorial), you can then use the source()function to feed RevBayes your Rev-script file (mcmc_mk.Rev).    source(\"scripts/mcmc_mk.Rev\")This will execute the analysis and you should see the following output(though not the exact same values):   Processing file \"scripts/mcmc_mk.Rev\"   Successfully read one character matrix from file 'data/bears.nex'   Running MCMC simulation   This simulation runs 2 independent replicates.   The simulator uses 5 different moves in a random move schedule with 58.4 moves per iterationIter        |      Posterior   |     Likelihood   |          Prior   |    elapsed   |        ETA   |----------------------------------------------------------------------------------------------------0           |       -680.054   |       -649.452   |       -30.6022   |   00:00:00   |   --:--:--   |100         |       -419.885   |       -414.047   |       -5.83883   |   00:00:01   |   --:--:--   |200         |       -427.028   |       -417.426   |       -9.60277   |   00:00:01   |   00:00:49   |300         |       -421.585   |        -417.96   |        -3.6253   |   00:00:02   |   00:01:04   |400         |       -431.561   |       -427.124   |       -4.43711   |   00:00:03   |   00:01:12   |500         |       -423.507   |       -422.002   |       -1.50428   |   00:00:04   |   00:01:16   |600         |       -418.061   |       -419.644   |        1.58298   |   00:00:05   |   00:01:18   |700         |       -427.552   |       -423.884   |       -3.66793   |   00:00:05   |   00:01:06   |800         |        -437.39   |       -424.302   |       -13.0876   |   00:00:06   |   00:01:09   |900         |       -418.405   |       -413.872   |        -4.5323   |   00:00:07   |   00:01:10   |1000        |       -425.641   |       -411.291   |       -14.3491   |   00:00:08   |   00:01:12   |...When the analysis is complete, RevBayes will quit and you will have anew directory called output that will contain all of the files youspecified with the monitors ().Exercises  Run the MCMC analysis in RevBayes.  Look at the resulting files mk_run_1.log and mk_run_2.log in Tracer and check for convergence.  Look at the majority rule consensus tree stored in mk.majrule.tre and the MAP tree stored in mk.map.tre in FigTree.Example: Correcting for Ascertainment BiasWhen Lewis first introduced the Mk model, he observed that branchlengths on the trees were greatly inflated. The reason for this is thatwhen morphological characters are collected, characters that do notvary, or vary in a non-parsimony-informative way (such asautapomorphies) are excluded. Excluding these low-rate characters causesthe overall amount of evolution to be over-estimated. This causes aninflation in the branch lengths (Lewis 2001).Therefore, when performing a morphological phylogenetic analysis, it isimportant to correct for this bias. There are numerous statisticallyvalid ways to perform this correction (Allman and Rhodes 2008). Original corrections simulated invariant and non-parsimony informative characters along the proposed tree. The likelihood of these characters would then becalculated and used to normalize the total likelihood value. RevBayesimplements a dynamic programming approach that calculates the samelikelihood, but does so faster.Modifying the Rev-script  Create a copy of your previous Rev script, and call it mcmc_Mkv.Rev. You will need to modify the Rev.code provided in this section in this file.In RevBayes it is actually very simple to add a correction for ascertainment bias. You only need to set the option coding=\"variable\" in the dnPhyloCTMC. Coding specifies what type of ascertainment bias is expected. We are using the variable correction, as we have no invariant character in our matrix. If we also lacked parsimony non-informative characters, we would use the coding informative (there are actually 4 parsimony non-informative characters in our matrix).phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q_morpho, type=\"Standard\", coding=\"variable\")  Remember to change all filenames for the output, e.g., from output/mk.log to output/mkv.log.  That’s all you need to do! Now run this script in RevBayes.Example: Relaxing the Assumption of Equal Transition ProbabilitiesThe Mk model makes a number of assumptions, but one that may strike youas particularly unrealistic is the assumption that characters are equally likely to change from any one state to any other. That means that a trait is as likely to be gained as lost. While this may hold true for sometraits, we expect that it may be untrue for many others.RevBayes has functionality to allow us to relax this assumption. We dothis by specifying a beta prior on state frequencies. Remember from theNucleotide substitution models lesson that stationary frequencies impact how likely we are to see changes in a character. For example, it may be very likely, in a character, to change from 0 to 1. But if the frequency of 0 is very low, we will still seldom see this change.We can exploit the relationship between state frequencies and observedchanges to allow for variable Q-matrices across characters (). To do this, we generate a beta distribution on state frequencies, and use the state frequencies from that distribution to generate a series of Q-matrices used to evaluate our data (Pagel and Meade 2004).This type of model is called a mixture model. There are assumed tobe subdivisions in the data, which may require different parameters (inthis case, state frequencies). These subdivisions are not defined apriori. This model has previously been shown to be effective for arange of empirical and simulated datasets (Wright et al. 2016).Graphical model demonstrating thediscretized beta distribution for allowing variable state frequencies.Modifying the Rev-script  Make a copy of the Rev script you made earlier. Call itmcmc_mk_dicretized.Rev. This new script willcontain the new model parameters and models.We will use a discretized beta distribution to place a prior on the state frequencies. The beta distribution has two parameters, $\\alpha$ and $\\beta$. These twoparameters specify the shape of the distribution. State frequencies willbe evaluated according to this distribution, in the same way that ratevariation is evaluated according to the gamma distribution. Thediscretized distribution is split into multiple classes, each with it’sown set of frequencies for the 0 and 1 characters. The number of classescan vary; we have chosen 4 for tractability. Note that we need to make sure that this discretization results in a symmetric model, therefore we will use only one parameter for the beta distribution: beta_scale such that $\\alpha = \\beta$.num_cats = 4beta_scale ~ dnLognormal( 0.0, sd=2*0.587405 )moves.append( mvScale(beta_scale, lambda=1, weight=5.0 ) )Above, we initialized the number of categories, the parameters of thebeta distribution, and the moves on these parameters.Next, we set the categories to each represent a quadrant of the betadistribution specified by beta_scale.cats := fnDiscretizeBeta(beta_scale, beta_scale, num_cats)If you were to print the cats variable, you would see a list of statefrequencies like so:[ 0.011, 0.236, 0.764, 0.989 ]Using these state frequencies, we will generate a new vector of Q-matrices. Because we are varying the state frequencies, we must use a Q-matrix generation function that allows for state frequencies to vary asa parameter. We will, therefore, use the fnF81 function.for (i in 1:cats.size()){    Q[i] := fnF81(simplex(abs(1-cats[i]), cats[i]))}Additionally, in RevBayes we need to specify the probabilities that a site evolves according to oneof the Q-matrices. For this model the probabilities must be equal because we need to guarantee thatthe model is symmetric. Thus, we use a simplex function to create a vector that sums to 1.0.matrix_probs &lt;- simplex( rep(1,num_cats) )The only other specification that needs to change in the model specification isthe CTMC:phyMorpho ~ dnPhyloCTMC(tree=phylogeny, siteRates=rates_morpho, Q=Q, type=\"Standard\", coding=\"variable\", siteMatrices=matrix_probs)You will notice that we have added a command to tell the CTMC that we havemultiple site matrices that will be applied to different characters inthe matrix.  Remember to change all filenames for the output, e.g., from output/mkv.log to output/mkv_discretized.log.  The MCMC chain set-up does not need to change. Run the new MCMC file,just as you ran the previous MCMC analyses. This estimation will take longer than the Mk model, due to increased model complexity.Evaluate and Summarize Your ResultsEvaluating the MCMCWe will use Tracerto evaluate the MCMC samples from ourthree estimations. Load all three of the MCMC logs into theTracer window. Highlight all three files inthe upper left-hand viewer () by right- orcommand-clicking all three files.Highlight all three files for model comparison.Once all three trace logs are loaded and highlighted, first look at theestimated likelihoods. You will notice that the Mk model (mk.log), asoriginally proposed by (Lewis 2001) is improved by adding the correction for ascertainment bias.The mkv and the mkv_discretized analyses both represent improvements, but are fairly close in likelihoodscore to each other (). Likely, we would need to perform stepping stone model assessment to truly tell if the more complicated model is statistically justified. This analysis is too complicated and time-consuming for this tutorial period, but you will find instructions for performing the analysis inGeneral Introduction to Model selection.Comparison of likelihood scores for all three models.Next, click on the Trace tab. In the lower left hand corner, you willnotice an option to color each trace by the file it came from. Choosethis option (you may need to expand the window slightly to see it). Nextto this option, you can also see an option to add a legend to your tracewindow. The results of this coloring can be seen in. You will see that the models are mixing quite well, but we may want to run the MCMC chains longer if we were to use these analyses in a paper.The Trace window. The traces are colored by which version of the Mk model they correspond to.We are interested in two aspects of the posterior distribution. First,the mkv and mkv_discretized analyses incorporated a correction for the biased sampling of variable characters, while the mk analysis does not. In this case, we expect the tree_length variable to be greater for the mk analysis, because in reality our data are enriched for variation (i.e. the model overestimates variation). Click on the “Marginal Density” tab and select the tree_length parameter. shows that tree_length is approximately 20% greater for the mk.Posterior tree length estimates.Second, we are interested in characterizing the degree of heterogeneityestimated by the beta discretized model. If the data were distributed bya single morphological rate matrix, then we would expect to see verylittle variation among the different cats value estimates, anda very large value for the beta_scale parameter of thediscretized beta distribution. For example, if beta_scale =1000, then that would cause all discretized beta categories to have values approaching 0.5, which approximates a symmetric Mk model.Examine the output for beta_scale in Tracer ().Beta scale parameter of the discretized beta model. shows that the four discretized beta statefrequencies do not all have the exact same value. To replicate this figure in Tracer select the cats[i] estimates, click on the Marginal Density tab and select Colour by: “Trace and Trace File” and Display: “Box and whisker”.Posterior discretized state frequencies for the discretized beta model.Summarizing TreesThe trees estimated in the above sections are summarized using a majority rule consensus tree (MRCT). Clades appearing in $p&gt;0.5$ of posterior samples are resolved in the MRCT, while poorly support clades with $p \\leq 0.5$ are shown as unresolved polytomies. Poor phylogenetic resolution might be caused by having too few phylogenetically informative characters, or conflicting signals for certain species relationships. Let’s compare our topological estimates across models.Majority rule consensus tree for the beta-discretized Mkv analysis.The MRCTs for the Mk model with and without the +v correction arevery similar to that for the discretized-beta model(). Click on “Branch Labels” to show the branch lengths. Note that the branch lengths differ - the tree length is inflated without the +v correction, just as we saw when comparing theposterior tree length densities. In general, it is important to assesswhether your results are sensitive to model assumptions, such as thedegree of model complexity, and any mechanistic assumptions thatmotivate the model’s design. In this case, our tree estimate appears tobe relatively robust to model complexity.",
        "url": "/tutorials/morph/morph_dec2018.html",
        "index": "false"
      }
      ,
    
      "tutorials-morph-morph-more-html": {
        "title": "Discrete morphology - Ancestral State Estimation",
        "content": "IntroductionDiscrete morphological models are not only useful for tree estimation,as was done in Tutorial Discrete morphology - Tree Inference, but also to ask specific questions aboutthe evolution of the morphological character of interest.Specifically, there are two types of analyses that we might be interest in.First, we can test different model of morphological evolution,such as reversible and irreversible models, and estimate rates under these models.Using an irreversible model of evolution, we can test, for example,for Dollo’s law of a complex character that can be lost but not gained again (Goldberg and Igić 2008).Additionally, we might be interest in ancestral state estimation,or mapping transition on the phylogeny.Commonly the central problem in statistical phylogenetics concerns marginalizing overall unobserved character histories that evolved along the branches of a givenphylogenetic tree according to some model, $M$, under some parameters, $\\theta$.This marginalization yields the probability of observing the tip states, $X_\\text{tip}$,given the model and its parameters,$P( X_\\text{tip} | \\theta, M ) = \\sum_{X_\\text{internal}} P( X_\\text{internal}, X_\\text{tip} \\mid \\theta, M )$.One might also wish to find the probability distribution of ancestral stateconfigurations that are consistent with the tip state distribution,$P( X_\\text{internal} \\mid X_\\text{tip}, \\theta, M )$, and to sampleancestral states from that distribution.This procedure is known as ancestral state estimation.This tutorial will provide a discussion of modeling morphological charactersand ancestral state estimation, and will demonstrate how to perform suchBayesian phylogenetic analysis using RevBayes (Höhna et al. 2016).Overview of Discrete Morphology Models: Unequal rate models and irreversible modelsThe instantaneous rate matrix encodes the transition rates between all pairs of evolutionary states.It is important to emphasize that all rate matrices are assertions about how morphological evolution operates.Depending on how one populates the rate matrix elements, different evolutionary hypotheses may be expressed.When we model the evolution of morphological data, unlike nucleotide data, each change may require a sequence of intermediate changes.Getting to one state may require going through another.In short, it is probably not likely that one single model describes all characters well.Symmetric unorderedThe standard Mk model of character evolution, where M denotes it is a Markov model and $K$ denotes the number of states for the character.The lineage may transition directly from state 1 to state 4 without going through states 2 and 3, which is representative of a character withunordered states.In addition, all transition rates are equal as they are in the Jukes-Cantor rate matrix (Jukes and Cantor 1969).Here is an example of a symmetric unordered Mk model for $k=3$.\\[Q = \\begin{pmatrix}- &amp; r &amp; r \\\\r &amp; - &amp; r \\\\r &amp; r &amp; -\\end{pmatrix}\\]Define the single shared rate parametermu &lt;- 1.0Define the ratesrates := [ [0.0,  mu,  mu],           [ mu, 0.0,  mu],           [ mu,  mu, 0.0] ]Create the rate matrixQ := fnFreeK(rates)Q   [ [ -1.0000, 0.5000, 0.5000 ] ,     0.5000, -1.0000, 0.5000 ] ,     0.5000, 0.5000, -1.0000 ] ]Now, let’s compute the transition probability matrix for a branch length of 0.1.P &lt;- Q.getTransitionProbabilities(0.1)P[ [ 0.907, 0.046, 0.046 ],  [ 0.046, 0.907, 0.046 ],  [ 0.046, 0.046, 0.907 ] ]If we believed that, for example, for a couple of states, some transitions are stronglymore likely, we could add a multiplier between those two states:rates := [ [  0.0,  mu,   mu],           [ 2*mu, 0.0, 2*mu],           [   mu,  mu,   mu] ]Q := fnFreeK(rates)Q   [ [ -0.8333, 0.4167, 0.4167 ] ,     0.8333, -1.6667, 0.8333 ] ,     0.4167, 0.4167, -0.8333 ] ] P &lt;- Q.getTransitionProbabilities(0.1) P   [ [ 0.922, 0.038, 0.040 ],     [ 0.075, 0.850, 0.075 ],     [ 0.040, 0.038, 0.922 ] ]If we then get our transition probabilities, we see that this changes our probability of observing the not simply our two state transitions that we changed the rate for, but others as well.This type of approach is common in parsimony, where it is one of several things referred to as weighted parsimony.This approach to using different matrices requires you to, a priori specify your matrix.But in reality, there is often fairly little guidance or information by which we decide if a weight applied to a transition is appropriate.Biologists still use models of this type - the Dollo model, in which a character is assumed not to be able to re-evolve once lost, is an extreme version of penalizing one change.Because the transition rates between all differing pairs of states are the same, so are the transition probabilities.Similarly, the probability of remaining in any given state is also equal across states.Asymmetric orderedCharacter states that are ordered imply that evolutionary transitions occur in particular sequences.For example, the number of digits on a foot might vary by gaining and losing single digits, meaning the transition from three to five digits cannot occur without going through the evolutionary state of possessing four digits.Below, we assume that gain events ( $n \\rightarrow n+1$ ) occur at rate $\\lambda$ and loss events ($n \\rightarrow n-1$) occur at rate $\\mu$.The zeroes indicate that there is no immediate evolutionary path between states $1$ and $4$: states $2$ and $3$ must be used to reach $4$ from $1$.\\[Q = \\begin{pmatrix}- &amp; \\lambda &amp; 0 &amp; 0 \\\\\\mu &amp; -   &amp; \\lambda &amp; 0 \\\\0 &amp; \\mu &amp; -   &amp; \\lambda \\\\0 &amp; 0 &amp; \\mu &amp; -\\end{pmatrix}\\]Create two rate parameters, $\\lambda$ for gain and $\\mu$ loss events.lambda ~ dnExponential( 1 )mu ~ dnExponential( 1 )lambda.setValue( 3 )mu.setValue( 1 )Create a tridiagonal matrix of transition rates, meaning state $i$ may only transition to states $i-1$ and $i+1$diag_rates := [ [  0.0, lambda,    0.0,    0.0],                [   mu,    0.0, lambda,    0.0],                [  0.0,     mu,    0.0, lambda],                [  0.0,    0.0,     mu,    0.0] ]Create the rate matrixQ := fnFreeK(diag_rates)Q[ [ -1.5385, 1.5385, 0.0000, 0.0000 ] ,     0.5128, -2.0513, 1.5385, 0.0000 ] ,     0.0000, 0.5128, -2.0513, 1.5385 ] ,     0.0000, 0.0000, 0.5128, -0.5128 ] ]Compute the transition probability matrix for a branch length of 0.1.P &lt;- Q.getTransitionProbabilities(rate=0.1)P[ [ 0.861, 0.129, 0.010, 0.001],  [ 0.043, 0.821, 0.126, 0.010],  [ 0.001, 0.042, 0.821, 0.136],  [ 0.000, 0.001, 0.045, 0.954]]Note that $P[1][2] &gt; P[1][3] &gt; P[1][4]$, primarily because those transitions require a minimum of one, two, and three events, respectively.In addition, note that assigning asymmetric transition rates causes $P[1][2] &gt; P[2][1]$ because $rates[1][2] &gt; rates[2][1]$.  In RevBayes we have also the specific rate matrix for this model: fnOrderedRateMatrix(maxState, lambda, mu).Correlated binary charactersTwo characters do not necessarily evolve independently of one another.Take two characters in plants: the presence or absence of toothed leaf margins (character X) is thought to be ecologically correlated with the presence of absence of leaf lobing (character Y).For a single binary character, there are two states (0 and 1), but there are four states for a pair of non-independent binary characters (00, 10, 01, and 11).(missing reference) introduced a general framework for modeling the evolution of joint sets of characters.These models require that only one evolutionary event can occur in a moment of time, which is enforced with the 0 terms.In addition, the transition rate that, say, character X goes from 0 to 1 depends on the current value of character Y.Seven free parametersIn the first case, all possible transitions might be assigned their own parameter.Here, we’ll assign a simplex over all rates, leaving seven free parameters (plus an eighth parameter that scales the rate matrix).\\[Q = \\begin{pmatrix}                      - &amp; \\mu_{00 \\rightarrow 10} &amp; \\mu_{00 \\rightarrow 01} &amp;                       0 \\\\\\mu_{10 \\rightarrow 00} &amp;                       - &amp;                       0 &amp; \\mu_{10 \\rightarrow 11} \\\\\\mu_{01 \\rightarrow 00} &amp;                       0 &amp;                       - &amp; \\mu_{01 \\rightarrow 11} \\\\                      0 &amp; \\mu_{11 \\rightarrow 10} &amp; \\mu_{11 \\rightarrow 01} &amp;                       - \\\\\\end{pmatrix}\\]r ~ dnDirichlet( [1,1,1,1,1,1,1,1] )r.setValue( simplex(1,1,3,3,3,3,1,1) )Create an array of zeroes for the four states (00, 10, 01, 11)for (i in 1:4) {    for (j in 1:4) {        rates[i][j] &lt;- 0.0    }}Populate the elements of ratesrates[1][2] := r[1] # 00-&gt;10rates[1][3] := r[2] # 00-&gt;01rates[2][1] := r[3] # 10-&gt;00rates[2][4] := r[4] # 10-&gt;11rates[3][1] := r[5] # 01-&gt;00rates[3][4] := r[6] # 01-&gt;11rates[4][2] := r[7] # 11-&gt;10rates[4][3] := r[8] # 11-&gt;01Create the rate matrixQ := fnFreeK(rates)Q[ [ -0.6667, 0.3333, 0.3333, 0.0000 ] ,     1.0000, -2.0000, 0.0000, 1.0000 ] ,     1.0000, 0.0000, -2.0000, 1.0000 ] ,     0.0000, 0.3333, 0.3333, -0.6667 ] ]Compute the transition probability matrix for a branch length of 0.1.P &lt;- Q.getTransitionProbabilities(rate=0.1)P[ [ 0.938, 0.029, 0.029, 0.003],  [ 0.088, 0.822, 0.003, 0.088],  [ 0.088, 0.003, 0.822, 0.088],  [ 0.003, 0.029, 0.029, 0.938] ]Note that the probability of remaining in state 10 or state 01 is less than the probability of remaining in state 00 or state 11.In this toy example, these probabilities reflect that states 10 an 01 are less evolutionarily stable than states 00 and 11.Four free parametersAlternatively, characters X and Y might share state frequencies, $\\pi_j$, and transition rates $\\mu_{ij}^{(k)}$, where $i$ is the starting state for the character undergoing change, $j$ is the ending state, and $k$ is the state of the other character.This results in two stationary frequencies (one free parameter), four transition rates for $0 \\rightarrow 1$ and $1 \\rightarrow 0$ given that the other character is in state 0 or state 1 (three free parameters), plus one free parameter to scale the rate matrix.\\[Q = \\begin{pmatrix}- &amp; \\mu_{01}^{(0)} \\pi_1 \\pi_0 &amp; \\mu_{01}^{(0)} \\pi_0 \\pi_1 &amp; 0 \\\\\\mu_{10}^{(0)} \\pi_0 \\pi_0 &amp; -   &amp; 0 &amp; \\mu_{01}^{(1)} \\pi_1 \\pi_1 \\\\\\mu_{10}^{(0)} \\pi_0 \\pi_0 &amp; 0   &amp; - &amp; \\mu_{01}^{(1)} \\pi_1 \\pi_1 \\\\0 &amp; \\mu_{10}^{(1)} \\pi_1 \\pi_0 &amp; \\mu_{10}^{(1)} \\pi_0 \\pi_1 &amp; - \\\\\\end{pmatrix}\\]Assign the stationary frequencies of being in state 0 or state 1 shared by both characters X and Y.pi ~ dnDirichlet([1,1])pi.setValue( simplex(1,3) )Assign the relative transition rates for gain and loss provided that the other character is in state 0 or 1.r ~ dnDirichlet( [1,1,1,1] )r.setValue( simplex(1,3,3,1) )Create an array of zeroes for the four states (00, 10, 01, 11)for (i in 1:4) {    for (j in 1:4) {        rates[i][j] &lt;- 0.0    }}Populate the elements of {\\tt rates}rates[1][2] := r[1] * pi[2] * pi[1] # 00-&gt;10rates[1][3] := r[1] * pi[1] * pi[2] # 00-&gt;01rates[2][1] := r[3] * pi[1] * pi[1] # 10-&gt;00rates[2][4] := r[2] * pi[2] * pi[2] # 10-&gt;11rates[3][1] := r[2] * pi[1] * pi[1] # 01-&gt;00rates[3][4] := r[3] * pi[2] * pi[2] # 01-&gt;11rates[4][2] := r[4] * pi[2] * pi[1] # 11-&gt;10rates[4][3] := r[4] * pi[1] * pi[2] # 11-&gt;01Create the rate matrixQ := fnFreeK(rates)Q[ [ -0.6333, 0.3167, 0.3167, 0.0000 ] ,     0.4750, -2.3750, 0.0000, 1.9000 ] ,     0.4750, 0.0000, -2.3750, 1.9000 ] ,     0.0000, 0.3167, 0.3167, -0.6333 ] ]Compute the transition probability matrix for a branch length of 0.1.P &lt;- Q.getTransitionProbabilities(0.1)P[ [ 0.940, 0.027, 0.027, 0.005],  [ 0.041, 0.792, 0.003, 0.164],  [ 0.041, 0.003, 0.792, 0.164],  [ 0.001, 0.027, 0.027, 0.944] ]Note that this model has a tendency towards state 11.P_10 &lt;- Q.getTransitionProbabilities(10.0)P_10[ [ 0.159, 0.105, 0.105, 0.630],  [ 0.158, 0.105, 0.105, 0.632],  [ 0.158, 0.105, 0.105, 0.632],  [ 0.158, 0.105, 0.105, 0.632] ]CovarionCovarion models (Tuffley and Steel 1998) capture the possibility that a ``hidden’’ (unobserved or unmeasurable) states cause evolutionary processes to vary in tempo and modes.For example, phylogenetically local clusters of plant lineages appear to transition between herbaceous and woody habits at relatively high rates, so one might want to quantify where these bursts occur (Beaulieu et al. 2013).While similar in structure to the correlated character model of (Pagel 1994), covarion models do not observe the hidden state that induce the mode-shifts.Instead, covarion models expand the character’s state space by a factor of $K$, and observe the character once for each of the $K$ categories.For example, take a binary character modeled with $K=2$ hidden state classes.The model would treat a character that is observed as being in state 0 as possibly being in either of the $K=2$ classes (0,1) and (0,2).In practice, this is done by setting the likelihood of observing those $0k$ states to equal 1.The expanded structure of a simple covarion rate matrix with $K=2$ is\\[Q = \\left(\\begin{array}{cc|cc}- &amp; r_1 q_{01}^{(1)} &amp; s_{12} &amp; 0 \\\\r_1 q_{10}^{(1)} &amp; - &amp; 0 &amp; s_{12} \\\\\\hlines_{21} &amp; 0 &amp; - &amp; r_2 q_{01}^{(2)} \\\\0 &amp; s_{21} &amp; r_2 q_{10}^{(2)} &amp; -  \\\\\\end{array}\\right)\\]This form can be reduced to a simpler block-matrix representation\\(Q = \\left(\\begin{array}{c|c}r_1 Q^{(1)} &amp; s_{12} I  \\\\\\hlines_{21} I &amp; r_2 Q^{(2)} \\\\\\end{array}\\right)\\)where $Q^{(i)}$ is the rate matrix for the $i$th class, $r_i \\in r$ is the clock rate for the $i$th class, and $S$ is the rate matrix to switch between classes.sr ~ dnDirichlet([1,1])sr.setValue( simplex(1,2) )switch_rates := [ [   0.0, sr[1] ],                  [ sr[2],   0.0 ] ]Q_switch := fnFreeK(switch_rates)Create an array of zeroes for the four states (00, 10, 01, 11)cr[1] ~ dnExp(1)cr[2] ~ dnExp(1)cr[1].setValue(3)cr[2].setValue(1)Populate the elements of ratesQ_class[1] := fnJC(2)bf ~ dnDirichlet( [1,1] )bf.setValue( simplex(1,3) )Q_class[2] := fnF81( bf )Create the rate matrixQ := fnCovarionRateMatrix(Q=Q_class, switch_rates=Q_switch, clock_rates=cr)Q[ [ -1.1126, 0.8901, 0.2225, 0.0000 ] ,     0.8901, -1.1126, 0.0000, 0.2225 ] ,     0.4451, 0.0000, -1.0385, 0.5934 ] ,     0.0000, 0.4451, 0.1978, -0.6429 ] ]Compute the transition probability matrix for a branch length of 0.1.P &lt;- Q.getTransitionProbabilities(0.1)P &lt;- Q.getTransitionProbabilities(1)P[ [ 0.899, 0.080, 0.020, 0.002],  [ 0.080, 0.899, 0.001, 0.020],  [ 0.040, 0.003, 0.902, 0.055],  [ 0.002, 0.041, 0.018, 0.939] ]The rows and columns correspond to (in order): state 0 evolving by $r_1 Q^{(1)}$, state 1 evolving by $r_1 Q^{(1)}$, state 1 evolving by $r_2 Q^{(2)}$, and state 2 evolving by $r_2 Q^{(2)}$.Note that $P[1][2] &gt; P[3][4]$, which is largely due to the fact that $r_1 &gt; r_2$.Example: Ancestral state estimation with a simple equal rates modelIn this example we will look at the evolution of morphological character placenta type in placental mammals.We have three different types of placenta: Epitheliochorial(1), Endotheliochorial(2), Hemochorial(3) ( (missing reference)).We are interested, in general, in the evolution of placenta type in placental mammals.Specifically, we want to know what the ancestral state of all placental mammals is.Furthermore, we want to know if placenta type is evolving under an equal-rates model, an unequal rates model, an irreversible model, or any other type of transition model.Visualization of different placenta types. Reproduced from (missing reference).Specifying the Mk ModelWe will start this tutorial with the simple Mk model with three states, $k=3$ (Lewis 2001).Thus, we will follow the Discrete morphology - Tree Inference Tutorial very closely and refer you to that tutorial for more information.Let us start with defining the rate matrix $Q$ for this 3-state model:\\(Q = \\begin{pmatrix} -\\mu_1 &amp; \\mu_{12} &amp; \\mu_{13} \\\\\\mu_{21} &amp; -\\mu_2  &amp; \\mu_{23} \\\\\\mu_{31} &amp; \\mu_{32} &amp; -\\mu_3\\end{pmatrix} \\mbox{  .}\\)Remember, the Mk model sets transitions to be equal from any state to any other state.In that sense, our 3-state matrix really looks like this:\\(Q = \\begin{pmatrix} -(k-1)\\mu &amp; \\mu &amp; \\mu \\\\\\mu &amp; -(k-1)\\mu  &amp; \\mu \\\\\\mu &amp; \\mu &amp; -(k-1)\\mu \\\\\\end{pmatrix} \\mbox{  .}\\)Because this is a Jukes-Cantor-like model (Jukes and Cantor 1969), state frequencies do not vary as a model parameter.A visualization of this simple model can be seen in .Graphical model showing the Mk model (left panel). Rev code specifying the Mk model is on the right-hand panel.We will first perform a phylogenetic analysis using the Mk model.In further sections, we will explore how to relax key assumptions of the Mk model.Example: Ancestral State Estimation Using the Mk ModelIn this example, we will use the placenta type data applied to a thinned phylogeny of placental mammals.We have thinned the phylogeny only so that it will run considerably fast for this tutorial.Our actual analysis uses the full dataset of $\\sim 5000$ taxa.Data and Files  Create a directory called RB_DiscreteMorphology_RateASE_Tutorial (or any name you like).  Make sure that you have the data files copied into a subdirectory called data: .Getting Started  Create a new directory (in RB_DiscreteMorphology_RateASE_Tutorial) called scripts`.(If you do not have this folder, please refer to the directions in section .)When you execute RevBayes in this exercise, you will do so within the main directory you created (RB_DiscreteMorphology_RateASE_Tutorial).Creating Rev FilesFor complex models and analyses, it is best to create Rev script files that will contain all of the model parameters, moves, and functions.In this exercise, you will work primarily in your text editor and create a set of files that will be easily managed and interchanged.In this first section, you will write the following files from scratch and save them in the scripts directory:  mcmc_ase_mk.Rev: the Rev-script file that loads the data, specifies the model describing discrete morphological character change (binary characters), and specifies the monitors and MCMC sampler.All of the files that you will create are also provided in the this RevBayes tutorial.Please refer to these files to verify or troubleshoot your own scripts.  Open your text editor and create the master Rev file called mcmc_ase_Mk.Rev in the scripts directory.  Enter the Rev code provided in this section in the new model file.The file you will begin in this section will be the one you load into RevBayes when you have completed all of the components of the analysis.In this section you will begin the file and write the Rev commands for loading in the taxon list and managing the data matrices.Then, starting in section , you will move on to writing module files for each of the model components.Once the model files are complete, you will return to editing mcmc_ase_Mk.Rev and complete the Rev script with the instructions given in section .Load Data MatricesRevBayes uses the function readDiscreteCharacterData() to load a data matrix to the workspace from a formatted file.This function can be used for both molecular sequences and discrete morphological characters.Import the morphological character matrix and assign it to the variable morpho.morpho &lt;- readDiscreteCharacterData(\"data/mammals_thinned_placenta_type.nex\")Create Helper VariablesBefore we begin writing the Rev scripts for each of the model components, we need to instantiate a couple ``helper variables’’ that will be used by downstream parts of our model specification files.Create vectors of moves and monitorsmoves = VectorMoves()monitors = VectorMonitors()The Mk ModelFirst, we read in the tree topology:phylogeny &lt;- readTrees(\"data/mammals_thinned.tree\")[1]Next, we will create a Q matrix.Recall that the Mk model is simply a generalization of the JC model.Therefore, we will create a 3x3 Q matrix using fnJC, which initializes $Q$-matrices with equal transition probabilities between all states.Q_morpho &lt;- fnJC(3)Now that we have the basics of the model specified, we will specify the only parameter of the model, $\\mu$.The parameter specifies all the rates of morphological evolution:mu_morpho ~ dnExponential( 1.0 )Since $\\mu$ is a rate parameter, we will apply a scaling move to update it.moves.append( mvScale(mu_morpho,lambda=1, weight=2.0) )Lastly, we set up the CTMC.This should be familiar from the Nucleotide substitution models tutorial.We see some familiar pieces: tree and Q matrix.We also have two new keywords: data type and coding.The data type argument specifies the type of data - in our case, “Standard”, the specification for morphology.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, branchRates=mu_morpho, Q=Q_morpho, type=\"Standard\")phyMorpho.clamp(morpho)All of the components of the model are now specified.Complete MCMC AnalysisCreate Model ObjectWe can now create our workspace model variable with our fully specified model DAG.We will do this with the model() function and provide a single node in the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph and allows us to pass the model to various functions that are specific to our MCMC analysis.Specify Monitors and Output FilenamesThe next important step for our Rev script file is to specify the monitors and output file names.The first monitor we will create will monitor every named random variable in our model graph.This will include every stochastic and deterministic node using the mnModel monitor.In this case, it will only be our rate variable $\\mu$.It is still useful to specify the model monitor this way for later extensions of the model.We will also name the output file for this monitor and indicate that we wish to sample our MCMC every 10 cycles.monitors.append( mnModel(filename=\"output/mk.log\", printgen=10) )The second monitor we will add to our analysis will print information to the screen.Like with mnFile we must tell mnScreen which parameters we’d like to see updated on the screen.monitors.append( mnScreen(printgen=100) )The third and final monitor might be new to you: the mnJointConditionalAncestralState monitor computes and writes the ancestral states to file.monitors.append( mnJointConditionalAncestralState(tree=phylogeny,                                                   ctmc=phyMorpho,                                                   filename=\"output/mk.states.txt\",                                                   type=\"Standard\",                                                   printgen=1,                                                   withTips=true,                                                   withStartStates=false) )The core arguments this monitor needs are a tree object (tree=phylogeny),the phylogenetic model (ctmc=phyMorpho), an output filename (filename=\"output/mk.states.txt\"),the data type for the characters (type=\"Standard\"), and the sampling frequency (printgen=10}.The final argument, withTips=true, indicates that we do wish to record the tip states because we didn’t know all tip values and might be interested in the most plausible values.The monitor will produce a joint sample of ancestral states, where every ancestral state is conditional on the drawn value of its parent node state (except for the root node),storing the samples every 10 iterations to the file \"output/mk.states.txt\".Viewing the states file, we seeIteration\tend_1\tend_2\tend_3\tend_4\tend_5\t...0\t2\t3\t3\t3\t3\t...1\t2\t3\t3\t3\t3\t...2\t2\t3\t3\t3\t3\t...3\t2\t3\t3\t3\t3\t...4\t2\t3\t3\t3\t3\t...5\t2\t3\t3\t3\t3\t...6\t2\t3\t3\t3\t3\t...7\t2\t3\t3\t3\t3\t...8\t2\t3\t3\t3\t3\t...9\t2\t3\t3\t3\t3\t...10\t2\t3\t3\t3\t3\t......Set-Up the MCMCOnce we have set up our model, moves, and monitors, we can now create the workspace variable that defines our MCMC run.We do this using the mcmc() function that simply takes the three main analysis components as arguments.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")The MCMC object that we named mymcmc has a member method called .run().This will execute our analysis and we will set the chain length to 10000 cycles using the generations option.mymcmc.run(generations=10000, tuningInterval=200)Once our Markov chain has terminated, we will process the ancestral state samples.This function will compute the posterior probabilities of the ancestral states from the samples.Later we can visuallize our ancestral states.anc_states = readAncestralStateTrace(\"output/mk.states.txt\")anc_tree = ancestralStateTree(tree=phylogeny, ancestral_state_trace_vector=anc_states, include_start_states=false, file=\"output/ase_mk.tree\", burnin=0.25, summary_statistic=\"MAP\", site=1)writeNexus( anc_tree, filename=\"output/ase_mk.tree\" )Finally we can close RevBayes.Tell the program to quit using the q() function.q()  You made it! Save your file.Execute the MCMC AnalysisWith all the parameters specified and all analysis components in place, you are now ready to run your analysis.The Rev script you just created will be used by RevBayes and loaded in the appropriate order.  Begin by running the RevBayes executable.Provided that you started RevBayes from the correct directory (RB_DiscreteMorphology_RateASE_Tutorial),you can then use the source() function to feed RevBayes your master script file (mcmc_ase_mk.Rev).source(\"scripts/mcmc_ase_mk.Rev\")This will execute the analysis and you should see the following output (though not the exact same values):   Processing file \"scripts/mcmc_ase_mk.Rev\"   Successfully read one character matrix from file 'data/mammals_thinned_placenta_type.nex'   Attempting to read the contents of file \"mammals_thinned.tree\"   Successfully read file   Running MCMC simulation   This simulation runs 2 independent replicates.   The simulator uses 1 different moves in a random move schedule with 2 moves per iterationIter        |      Posterior   |     Likelihood   |          Prior   |    elapsed   |        ETA   |----------------------------------------------------------------------------------------------------0           |       -359.492   |       -359.194   |      -0.298007   |   00:00:00   |   --:--:--   |100         |       -62.0258   |       -62.0245   |     -0.0013356   |   00:00:03   |   --:--:--   |200         |       -62.3349   |       -62.3336   |    -0.00123787   |   00:00:05   |   00:02:00   |300         |        -62.005   |       -62.0024   |    -0.00256593   |   00:00:07   |   00:01:49   |400         |       -61.3682   |       -61.3664   |    -0.00177658   |   00:00:09   |   00:01:43   |500         |        -61.683   |       -61.6807   |    -0.00235834   |   00:00:12   |   00:01:48   |...When the analysis is complete, RevBayes will quit and you will have a new directory called output that will contain all of the files you specified with the monitors (Section ).Plotting the tree with ancestral statesWe will now switch to R using the package RevGadgets.Make sure that you have the package installed, usinglibrary(devtools)install_github(\"GuangchuangYu/ggtree\")install_github(\"revbayes/RevGadgets\")Now that we have our posterior distribution of ancestral states, we want to visualize those results.This section will aim to produce a pdf containing figures for the ancestral state estimates.We have written a little R package called \\RevGadgets that can be used to visualize the output of \\RevBayes.  Start R from the same working directory as you started RevBayes.This should be the directory where you now have you directory called output with the MCMC output files.First, we need to load the R package RevGadgetslibrary(RevGadgets)Second, we specify the name of the tree file.tree_file = \"output/ase_mk.tree\"Then, you plot the tree with ancestral states nicely mapped onto it.You may want to experiment with some of the settings to make the plot look prettier.For example, if you set show_posterior_legend=TRUE and node_size_range=c(1, 3),then the size of the circles will represent the posterior probability.g &lt;- plot_ancestral_states(tree_file, summary_statistic=\"MAP\",                      tip_label_size=1,                      xlim_visible=NULL,                      node_label_size=0,                      show_posterior_legend=FALSE,                      node_size_range=c(2.5, 2.5),                      alpha=0.75)Finally, we save the output into a PDF.ggsave(\"Mammals_ASE_MK.pdf\", g, width = 11, height = 9)  You can also find all these commands in the file called plot_anc_states.R which you can run as a script in R. shows the result of this analysis.Ancestral state estimation of the placenta type.Example: Unequal Transition Rates  Make a copy of the MCMC and model files you just made.Call them mcmc_ase_mk.Rev and `model_ase_FreeK.Rev.These will contain the new model parameters and models.The Mk model makes a number of assumptions, but one that may strike you as unrealisticis the assumption that characters are equally likely to change from any one state to any other state.That means that a trait is as likely to be gained as lost.While this may hold true for some traits, we expect that it may be untrue for many others.RevBayes has functionality to allow us to relax this assumption.Modifying the MCMC SectionAt each place in which the output files are specified in the MCMC file, change the output path so you don’t overwrite the output from the previous exercise.For example, you might call your output file output/ase_freeK.log.Change source statement to indicate the new model file.Modifying the Model SectionOur goal here is to create a rate matrix with 6 free parameters.We will assume an exponential prior distribution for each of the rates.Thus, we start be specifying the rate of this exponential prior distribution.A good guess might be that 10 events happened along the tree, so the rate should be the tree-length divided by 10.rate_pr := phylogeny.treeLength() / 10Now we can create our six independent rate variables drawn from an identical exponential distributionrate_12 ~ dnExponential(rate_pr)rate_13 ~ dnExponential(rate_pr)rate_21 ~ dnExponential(rate_pr)rate_23 ~ dnExponential(rate_pr)rate_31 ~ dnExponential(rate_pr)rate_32 ~ dnExponential(rate_pr)As usual, we will apply a scaling move to each of the rate variables.moves.append( mvScale( rate_12, weight=2 ) )moves.append( mvScale( rate_13, weight=2 ) )moves.append( mvScale( rate_21, weight=2 ) )moves.append( mvScale( rate_23, weight=2 ) )moves.append( mvScale( rate_31, weight=2 ) )moves.append( mvScale( rate_32, weight=2 ) )Next, we put all the rates together into our rate matrix.Don’t forget to say that we do not rescale the rate matrix (rescale=false).We would only rescale if we use relative rates.Q_morpho := fnFreeK( [ rate_12, rate_13, rate_21, rate_23, rate_31, rate_32 ], rescale=false )In this model, we also decide to specify an additional parameter for the root state frequencies instead of assuming the root state to be drawn from the stationary distribution.We will use a Dirichlet prior distribution for the root state frequencies.rf_prior &lt;- [1,1,1]rf ~ dnDirichlet( rf_prior )moves.append( mvBetaSimplex( rf, weight=2 ) )moves.append( mvDirichletSimplex( rf, weight=2 ) )We need to modify the dnPhyloCTMC to pass in our new root frequencies parameter.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, Q=Q_morpho, rootFrequencies=rf, type=\"Standard\")  Now you are done with your unequal rates model. Give it a run!Reversible-jump MCMC to test for irreversibilityIn the previous section we assumed that there are 6 different rates, which are all $&gt;0$.Now, we will apply a reversible-jump MCMC (missing reference) to test if any of the rates is significantly larger than $0$.  Make a copy of the Rev script file you just made.Call them `mcmc_ase_freeK_RJ.Rev.This will contain the new model parameters and models.Modifying the MCMC SectionAt each place in which the output files are specified in the MCMC section, change the output path so you don’t overwrite the output from the previous exercise.For example, you might call your output file output/freeK_ASE.log.Modifying the Model SectionThe only part in the model section that we are going to modify is the prior distributions and moves on the rate parameters.We will assume the same rate for the exponential prior distribution as before.rate_pr := phylogeny.treeLength() / 10Next, we specify that we have a 0.5 probability, a priori, that a rate is equal to 0.mix_pr &lt;- 0.5Now we can create our reversible-jump distributions, which take in a constant value, 0.0 in this case, and a distribution.Thus, the value is either drawn to be exactly equal to the constant value (0.0 here), or drawn from the base distribution (the exponential distribution in this case).rate_12 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_13 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_21 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_23 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_31 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)rate_32 ~ dnRJMixture(0.0, dnExponential(rate_pr), p=mix_pr)Since we are interested in the probability that a rate is equal to 0.0, we want to compute this posterior probability directly.Therefore, we will use the ifelse function, which will return 1.0 if the rate is equal to 0.0, and 0.0 otherwise (if the rate is unequal to 0.0).Hence, the frequency with which we sample a 1.0 gives us the posterior probability that a given rate is equal to 0.0.prob_rate_12 := ifelse( rate_12 == 0, 1.0, 0.0 )prob_rate_13 := ifelse( rate_13 == 0, 1.0, 0.0 )prob_rate_21 := ifelse( rate_21 == 0, 1.0, 0.0 )prob_rate_23 := ifelse( rate_23 == 0, 1.0, 0.0 )prob_rate_31 := ifelse( rate_31 == 0, 1.0, 0.0 )prob_rate_32 := ifelse( rate_32 == 0, 1.0, 0.0 )We also need to specify specific moves that ``jump’’ in parameter dimension.We will use the mvRJSwitch move that changes the value to be either equal to the constant valueprovided from the dnRJMixture or a value drawn from the base distribution (the exponential distribution).moves.append( mvRJSwitch( rate_12, weight=2 ) )moves.append( mvRJSwitch( rate_13, weight=2 ) )moves.append( mvRJSwitch( rate_21, weight=2 ) )moves.append( mvRJSwitch( rate_23, weight=2 ) )moves.append( mvRJSwitch( rate_31, weight=2 ) )moves.append( mvRJSwitch( rate_32, weight=2 ) )Additionally, we also need to specify moves that change the rates if they are not equal to 0.0.As usual, we use the standard scaling moves.moves.append( mvScale( rate_12, weight=2 ) )moves.append( mvScale( rate_13, weight=2 ) )moves.append( mvScale( rate_21, weight=2 ) )moves.append( mvScale( rate_23, weight=2 ) )moves.append( mvScale( rate_31, weight=2 ) )moves.append( mvScale( rate_32, weight=2 ) )  This is all that you need to do for this ``fancy’’ reversible-jump model. Give it a try!Evaluate and Summarize Your ResultsVisualizing Ancestral State EstimatesWe have previously seen in  how the ancestral states of the simple model look.You should repeat plotting the ancestral states now also for the freeK and freeK_RJ analyses.My output is shown in Ancestral state estimates of placenta type under the freeK model.You should observe that the estimated root states have changed!",
        "url": "/tutorials/morph/morph_more.html",
        "index": "false"
      }
      ,
    
      "tutorials-mcmc-moves-html": {
        "title": "Overview of Moves in MCMC",
        "content": "OverviewThis is the very first tutorial for you in RevBayes. The goal of this set of tutorials isgetting you started and familiar with the basics in RevBayes. If you have some familiaritywith R or similar software, then this should be straight forward. Nevertheless, we recommendyou to work through these tutorials to learn all the specific quirks of RevBayes.Continuous VariablesOver the full real lineMoves available in RevBayes for continuous variables.            Move      Description      Citation                  mvMirror                           mvRandomDive                           mvSlice                           mvSlide                           mvSlideBactrian                   Positive only (or negative only) continuous variablesPositive continuous variables, such as drawn from a Gamma, Exponental or Lognormal distribution, require moves that don’t change the sign.In principle it is possible to apply also moves that work on the full real line, and then let the moves be rejected if they are outside the boundary, but this may be inefficient.Moves available in RevBayes for positive continuous variables.            Move      Description      Citation                  mvGammaScale                           mvLevyJump                           mvLevyJumpSum                           mvMirrorMultiplier                           mvScale                           mvScaleBactrian                           mvScaleBactrianCauchy                   Multiple Continuous VariablesMoves available in RevBayes for positive continuous variables.            Move      Description      Citation                  mvAVMVN                           mvEllipticalSliceSamplingSimple                           mvMultipleElementVectorScale                           mvUpDownScale                           mvShrinkExpand                           mvShrinkExpandScale                           mvUpDownScale                           mvSynchronizedVectorFixedSingleElementSlide                           mvUpDownSlide                           mvUpDownSlideBactrian                           mvVectorBinarySwitch                           mvVectorFixedSingleElementSlide                           mvVectorScale                           mvVectorSingleElementSlide                           mvVectorSlide                           mvVectorSlideRecenter                   GMRF and HSMRFMoves available in RevBayes for positive continuous variables.            Move      Description      Citation                  mvGMRFHyperpriorGibbs                           mvGMRFUnevenGridHyperpriorGibbs                           mvHSRFHyperpriorsGibbs                           mvHSRFIntervalSwap                           mvHSRFUnevenGridHyperpriorsGibbs                   MatricesMoves available in RevBayes for positive continuous variables.            Move      Description      Citation                  mvConjugateInverseWishart                           mvCorrelationMatrixElementSwap                           mvCorrelationMatrixRandomWalk                           mvCorrelationMatrixSingleElementBeta                           mvCorrelationMatrixSpecificElementBeta                           mvCorrelationMatrixUpdate                           mvGraphFlipClique                           mvGraphFlipEdge                           mvGraphShiftEdge                           mvMatrixElementScale                           mvMatrixElementSlide                           mvSymmetricMatrixElementSlide                   Probabilities (numbers bounded between 0 and 1)Moves available in RevBayes for integer and natural number variables.            Move      Description      Citation                  mvBetaProbability                           mvProbabilityElementScale                   SimplicesMoves available in RevBayes for positive continuous variables.            Move      Description      Citation                  mvBetaSimplex                           mvDirichletSimplex                           mvElementSwapSimplex                           mvSimplex                           mvSimplexElementScale                   Natural and Integer numbersMoves available in RevBayes for integer and natural number variables.            Move      Description      Citation                  mvBinarySwitch      Flipping a value from 0 to 1 and vice versa                    mvRandomGeometricWalk                           mvRandomIntegerWalk                           mvRandomNaturalWalk                   TreesMoves available in RevBayes for tree variables.            Move      Description      Citation                  mvAddRemoveTip                           mvBranchLengthScale                           mvCollapseExpandFossilBranch                           mvEmpiricalTree                           mvFNPR                           mvFossilTimeSlideUniform                           mvGPR                           mvIndependentTopology                           mvNNI                           mvNarrow                           mvNodeTimeScale                           mvNodeTimeSlideBeta                           mvNodeTimeSlidePathTruncatedNormal                           mvNodeTimeSlideUniform                           mvNodeTimeSlideUniformAgeConstrained                           mvResampleFBD                           mvRootTimeScaleBactrian                           mvRootTimeSlideUniform                           mvSPR                           mvSubtreeScale                           mvSubtreeSwap                           mvTipTimeSlideUniform                           mvTreeScale                   On trees and other variablesMoves available in RevBayes for tree variables.            Move      Description      Citation                  mvGibbsDrawCharacterHistory                           mvBirthDeathEvent                           mvBirthDeathEventContinuous                           mvBirthDeathEventDiscrete                           mvBirthDeathFromAgeEvent                           mvBurstEvent                           mvCharacterHistory                           mvContinuousEventScale                           mvDiscreteEventCategoryRandomWalk                           mvEventTimeBeta                           mvEventTimeSlide                           mvLayeredScaleProposal                           mvNarrowExchangeRateMatrix                           mvNodeRateTimeSlideBeta                           mvNodeRateTimeSlideUniform                           mvRateAgeBetaShift                           mvRateAgeProposal                           mvRateAgeSubtreeProposal                   Species TreesMoves available in RevBayes for tree variables.            Move      Description      Citation                  mvSpeciesTreeScale                           mvSpeciesNarrow                           mvSpeciesNodeTimeSlideUniform                           mvSpeciesSubtreeScale                           mvSpeciesSubtreeScaleBeta                           mvSpeciesTreeScale                   UnassignedWeird moves …Moves available in RevBayes for positive continuous variables.            Move      Description      Citation                  xxx                           xxx                           xxx                           mvContinuousCharacterDataSlide                           mvDPPAllocateAuxGibbs                           mvDPPGibbsConcentration                           mvDPPValueBetaSimplex                           mvDPPValueScaling                           mvDPPValueSliding                           mvGibbsMixtureAllocation                           mvHomeologPhase                           mvMixtureAllocation                           mvMultiValueEventBirthDeath                           mvMultiValueEventScale                           mvMultiValueEventSlide                           mvMultiValueEventSlide                           mvRJSwitch                           mvUPPAllocation                   ",
        "url": "/tutorials/mcmc/moves.html",
        "index": "true"
      }
      ,
    
      "tutorials-cont-traits-multivariate-bm-html": {
        "title": "Multivariate Brownian Motion",
        "content": "Estimating Correlated EvolutionThis tutorial demonstrates how to specify a multivariate Brownian motion model for multiple continuous characters. Specifically, we’ll use a parameter separation strategy to separate the relative rates of evolution among characters from the correlations among characters (Caetano and Harmon 2019). We provide the probabilistic graphical model representation of each component for this tutorial. After specifying the model, you will estimate the correlations among characters using Markov chain Monte Carlo (MCMC). We will then measure the strength of correlation among characters to determine if there is evidence that the characters are correlated.A Multivariate Model of Brownian-motion EvolutionWhen analyzing multiple continuous characters, we might be interested in measuring the strength of correlation among sets of those characters. Alternatively, we may be investigating something about the evolutionary process where the correlations among characters are not of direct interest, e.g., how rates vary among lineages and/or over time. In the former case, the correlations are a focal parameter, whereas in the latter case, the correlations are a nuisance parameter, in the sense that failing to model correlations may lead us to incorrect conclusions about the process of interest (Adams et al. 2017).The multivariate Brownian motion (mvBM) process is a model that accommodates for both variation in rate among continuous characters, and for correlations among each pair of characters. For a dataset with $c$ continuous characters, the mvBM process is completely described by a $c \\times c$ variance-covariance matrix, $\\Sigma$:\\[\\begin{equation}    \\Sigma = \\begin{bmatrix}                \\sigma^2_1                   &amp; \\sigma_1 \\sigma_2 \\rho_{1,2} &amp; \\ldots &amp; \\sigma_1 \\sigma_c \\rho_{1,c} \\\\                \\sigma_2 \\sigma_1 \\rho_{1,2} &amp; \\sigma^2_2                   &amp; \\ldots &amp; \\sigma_2 \\sigma_c \\rho_{2,c} \\\\                \\vdots                       &amp; \\vdots                       &amp; \\ddots &amp; \\vdots \\\\                \\sigma_c \\sigma_1 \\rho_{1,c} &amp; \\ldots                       &amp; \\ldots &amp; \\sigma^2_c             \\end{bmatrix},\\end{equation}\\]where $\\sigma_i^2$ is the rate of evolution of character $i$, and $\\rho_{i,j}$ is the correlation coefficient between characters $i$ and $j$. Note that the variance-covariance matrix is symmetric across the diagonal and is completely determined by $\\boldsymbol{\\sigma^2} = [ \\sigma_1^2, \\ldots, \\sigma_c^2 ]$ and $\\boldsymbol{\\rho} = [ \\rho_{1,2}, \\rho_{1,3}, \\ldots, \\rho_{c-1, c} ]$. By convention, we place the correlation parameters in a correlation matrix, $R$:\\[\\begin{equation}    R = \\begin{bmatrix}            1 &amp; \\rho_{1,2} &amp; \\ldots &amp; \\rho_{1,c} \\\\            \\rho_{1,2} &amp; 1 &amp; \\ldots &amp; \\rho_{2,c} \\\\            \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\            \\rho_{1,c} &amp; \\ldots &amp; \\ldots &amp; 1        \\end{bmatrix}\\end{equation}\\]Over a branch of length $t$, a set of characters evolving under mvBM changes by a multivariate-normal random variable, $\\boldsymbol{\\Delta}$:\\[\\begin{equation}    \\boldsymbol{\\Delta} \\sim \\text{MVN}( \\boldsymbol{0}, t\\Sigma )\\end{equation}\\]where the vector of zeros, $\\boldsymbol{0}$, indicates that the average amount of change for each character is zero, and the variance in the changes scales with the length of the branch, $t$.In a Bayesian setting, it is possible to specify a prior distribution directly on $\\Sigma$. Unfortunately, the mathematics of multivariate Brownian motion require that the variance-covariance matrix obeys certain constraints (i.e., it must be positive semidefinite) that make it difficult to specify arbitrary prior distributions on $\\Sigma$. In this tutorial, we will adopt a separation strategy that decomposes this matrix into three distinct components: 1) the average rate of change for all characters, 2) the relative rate of change among characters, and 3) the correlations among characters. This separation strategy makes it possible to specify biologically interpretable priors on $\\Sigma$ that obey the mathematical constraints of variance-covariance matrices.The ‘‘average’’ rate of change, $\\sigma^2$, is a single parameter that governs the rate of change of the average character in the dataset. The parameter $\\boldsymbol{\\zeta^2} = [ \\zeta_1^2, \\zeta_2^2, \\ldots, \\zeta_c^2 ]$ is a vector of rates, one per character, that determine the relative rate of change of each character. The rate of change for character $i$ is therefore $\\sigma^2 \\zeta_i^2$. The relative rates are constrained to have a mean of 1, so that $\\sigma^2$ can be correctly interpreted as the rate of change for the average character. The correlation matrix, $R$, is as described above. For given values of $\\sigma^2$, $\\boldsymbol{\\zeta^2}$, and $R$, we can simply construct the variance-covariance matrix $\\Sigma$ as shown in the equation above.The separate strategy requires that we specify a prior distribution on each of the three components of the variance-covariance matrix. The average rate, $\\sigma^2$, does not have any special constraints and can be chosen from any arbitrary (positive) prior. The relative rates are required to have a mean of 1, but this is easy to achieve. However, the correlation matrix, $R$, must be positive-semidefinite. We will draw the correlation matrix from a special prior distribution, called the LKJ distribution after its authors (Lewandowski et al. 2009), that is designed especially for correlation matrices. This distribution is controlled by a single concentration parameter, $\\eta &gt; 0$, that controls the amount of correlation in the matrix; as $\\eta$ increases, the prior correlation between each pair of characters decreases toward 0. Importantly, the prior expected amount of correlation is always 0, and the distribution of correlations is symmetric around 0. When $\\eta = 1$, the LKJ distribution is ‘‘uniform’’ over all positive-semidefinite correlation matrices. The probabilistic graphical model for this separation strategy is represented in represented in figure ().The graphical model representation of the multvariate Brownian-motion (mvBM) process. For more information about graphical model representations see Höhna et al. (2014).In this tutorial, we use the phylogenies and continuous character datasets from (missing reference) to estimate correlations among a set of continuous-characters.⇨ The full multivariate BM-model specification is in the file called mcmc_multivariate_BM.Rev.Read the dataWe begin by reading in the (time-calibrate) tree of the Haemulids.T &lt;- readTrees(\"data/haemulidae.nex\")[1]Next, we read in the continuous-character data.data &lt;- readContinuousCharacterData(\"data/haemulidae_trophic_traits.nex\")We record the number of characters for future reference.nchar &lt;- data.nchar()Additionally, we initialize a variable for our vector ofmoves and monitors:moves    = VectorMoves()monitors = VectorMonitors()Specifying the modelTree modelIn this tutorial, we assume the tree is known without area. We create a constant node for the tree that corresponds to the observed phylogeny.tree &lt;- TAverage rate of evolutionWe draw the average rate of evolution, $\\sigma^2$, from a vague lognormal prior. This prior is uniform on the log scale, which means that it is represents ignorance about the order of magnitude of the average rate of evolution. We use a scaling move to propose updates to this parameter.sigma2 ~ dnLoguniform(1e-3, 1)moves.append( mvScale(sigma2, weight=1.0) )Relative rates of evolution among charactersWe draw proportional rates of evolution among characters from a symmetric Dirichlet distribution with concentration parameter $\\alpha$. We then multiply these proportional rates by the number of characters to get the relative rates. This ensures that the relative rates have a mean of 1. Increasing the value of $\\alpha$ will decrease the amount of rate variation under the prior.alpha &lt;- 1.0proportional_rates ~ dnDirichlet( rep(alpha, nchar) )relative_rates := proportional_rates * ncharWe apply an mvBetaSimplex move to the proportional rates.moves.append( mvBetaSimplex(proportional_rates, weight=2.0) )Correlation matrixWe draw the correlation matrix from an LKJ prior distribution with concentration parameter eta &lt;- 1.0. Larger values of eta correspond to less prior correlation among characters. This distribution draws a correlation matrix with nchar rows and columns.eta &lt;- 1.0R ~ dnLKJ( eta, nchar )We use special moves to update the correlation matrix. The first move, mvCorrelationMatrixRandomWalk, perturbs each element of the matrix by a small amount simultaneously. The second move, mvCorrelationMatrixSingleElementBeta, updates a randomly chosen element of the correlation matrix by drawing Beta random variable centered on the current correlation (stretched from -1 to 1).moves.append( mvCorrelationMatrixRandomWalk(R, weight=3.0) )moves.append( mvCorrelationMatrixSingleElementBeta(R, weight=5.0) )We extract the elements in the upper triangle of the correlation matrix for reference. These parameters are the vector of pairwise correlation parameters, $\\boldsymbol{\\rho}$ in the upper triangular part of the matrix, in natural reading order (left to right, top to bottom).correlations := R.upperTriangle()Parameterizing the partial correlation matrixAn alternative to specifying a prior on the correlation matrix is to specify a prior on the partial correlation matrix, $P$. You can think of the partial correlations between character $i$ and $j$ as the correlation between those characters, controlling for their induced correlations through other characters. Parameterizing the partial correlation matrix can be helpful when there are strong correlations between the estimates of the correlation parameters themselves. This makes it difficult for the MCMC to move across the posterior distribution of correlation matrices.To parameterize the partial correlation matrix, we draw it from an LKJ prior:P ~ dnLKJPartial( eta, nchar )This distribution is designed so that the induced prior on $R$ is identical to the prior you would be specifying if you drew $R$ from the same LKJ distribution.Next, we apply moves to the partial correlation matrix.moves.append( mvCorrelationMatrixRandomWalk(P, weight=3.0) )moves.append( mvCorrelationMatrixSingleElementBeta(P, weight=5.0) )Then, we compute the correlation matrix from the partial correlation matrix.R := fnPartialToCorr(P)Variance-covariance matrixHaving specified the separate components, we assemble the variance-covariance matrix. Here, we provide the square-roots of the relative rates (i.e., the standard deviations) and the correlation matrix. We exclude the average rate, sigma2, because we may want to allow different branches of the phylogeny to have different rates.V := fnDecompVarCovar( relative_rates^0.5, R )Multivariate Brownian motionFinally, we draw the continuous character data from a phylogenetic mvBM model. We provide the variance-covariance matrix, V, as well as the square-root of average rate of evolution.X ~ dnPhyloMultivariateBrownianREML(tree, branchRates=sigma2^0.5, rateMatrix=V)We clamp the observed data to this stochastic node to represent that they are observed.X.clamp(data)Finally, we create a workspace object for the entire model with model(). Remember that workspace objects are initialized with the = operator, and are not themselves part of the Bayesian graphical model. The model() function traverses the entire model graph and finds all the nodes in the model that we specified. This object provides a convenient way to refer to the whole model object, rather than just a single DAG node.mymodel = model(sigma2)Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we need to set up a vector of monitors to record the states of our Markov chain. The monitor functions are all called mn*, where * is the wildcard representing the monitor type. First, we will initialize the model monitor using the mnModel function. This creates a new monitor variable that will output the states for all model parameters when passed into a MCMC function.monitors.append( mnModel(filename=\"output/multivariate_BM.log\", printgen=10) )Additionally, create a screen monitor that will report the states ofspecified variables to the screen with mnScreen:monitors.append( mnScreen(printgen=1000, sigma2) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, wecan now set up the MCMC algorithm that will sample parameter values inproportion to their posterior probability. The mcmc() function willcreate our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=50000)When the analysis is complete, you will have the monitored files in youroutput directory.⇨ The Rev file for performing this analysis: mcmc_multivariate_BM.RevYou can then visualize the correlation parameters in RevGadgets.First, we need to load the R package RevGadgetslibrary(RevGadgets)Next, read the MCMC output:samples &lt;- readTrace(\"output/multivariate_BM.log\")Finally, plot the posterior distribution of the first 8 correlation parameters rate parameters (the top row of the correlation matrix, which represents how characters two through 9 are correlated with body size):plotTrace(samples, vars=paste0(\"correlations[\",1:8,\"]\"))Estimates of the posterior distribution of the correlation between the first trait (body size) against each of the remaining characters, visualized in RevGadgets. Characters 6 and 9 (buccal length and head length, respectively) are highly correlated with body size, while character 8 (head height) seems to be less positively correlated.⇨ The R file for plotting these results: plot_mvBM.RAdvanced: Hypothesis Tests for Correlation ParametersWe can test the hypothesis that characters $i$ and $j$ are correlated using the Savage-Dickey ratio (missing reference). Using the Savage-Dickey ratio, the Bayes Factor for the hypothesis that parameter $\\theta$ has a specific value is $\\text{BF} = P(\\theta \\mid X) \\div P(\\theta)$. Therefore, we can use the Savage-Dickey ratio to compute the Bayes factor that two characters are uncorrelated if we can compute the posterior probability of $\\rho_{i,j} = 0$ (the numerator) and the prior probability of $\\rho_{i,j} = 0$ (the denominator).Under the LKJ distribution, the marginal prior distribution of $(\\rho_{i,j} + 1) / 2$ is a Beta distribution with parameters $\\alpha = \\beta = \\eta + (c - 2) / 2$ (where $c$ is the number of continuous characters). Knowing this, we can use our posterior samples to compute the Bayes factor for the uncorrelated hypothesis (which is the inverse the Bayes factor for the correlated hypothesis) in R:library(RevGadgets)eta   &lt;- 1c     &lt;- 9alpha &lt;- eta + (c - 2) / 2# choose the correlation to testcorr  &lt;- 20 # this is the correlation parameter between characters 3 and 8# read the samples from the posteriorsamples &lt;- readTrace(\"output/multivariate_BM.log\")correlation_samples &lt;- samples[[1]][,paste0(\"correlations[\",corr,\"]\")]# fit a density to the samplesposterior_density &lt;- density(correlation_samples)# compute the approximate posterior probability# of the point hypothesis that rho_ij = 0post &lt;- approxfun(posterior_density)(0)# compute the prior probability of the uncorrelated hypothesis# we use x = (0 + 1) / 2 = 0.5 because rho = 0 corresponds to the middle# of the beta distributionprior &lt;- dbeta(0.5, alpha, alpha)bf_uncorrelated &lt;- post / priorprint(bf_uncorrelated)The resulting Bayes Factor $\\approx 0.75$, meaning there is some weak evidence that correlation parameter 20 (between characters 3 and 8) is non-zero.We urge caution when using the Savage-Dickey ratio for testing correlation hypotheses. First, the density estimator we are using may be unrealiable when the posterior probability of $\\rho_{i,j} = 0$ is very low, because we many not have enough samples to accurately characterize that part of the posterior distribution. More importantly, because the correlation matrix must obey the positive-semidefinite constraint, the correlation parameters are not independent in the posterior (or the prior, for that matter); therefore, we recommend against performing multiple Savage-Dickey comparisons for the same analysis (i.e., testing multiple correlation hypotheses).Exercise 1  Run an MCMC simulation to estimate the posterior distribution of the correlation parameters (correlations).  Run an MCMC under the prior to compute the prior distribution for the correlation parameters. Are the posterior correlations different from their prior?  Change the prior on the concentration parameter, eta, to decrease the prior weight on high correlation parameters.  Compare the posterior distribution of correlations under these two values of eta. Are posterior estimates sensitive to the prior concentration parameter?Variable Rates Among LineagesIn the previous section, we assumed that the average rate of evolution was the same across all of the branches in the phylogeny. However, if you have followed the State-Dependent Brownian Rate Estimation tutorial, you are probably aware that this dataset demonstrates variation in rates of evolution among lineages.We can relax the assumption that the average rate of evolution is constant across branches the same way that we did for univariate Brownian motion models (see Relaxed Brownian Rate Estimation). Importantly, this tutorial assumes that the relative rates among characters and the correlation matrix are shared among all branches. Otherwise, we can simply rely on the ‘‘random local clock’’ model that we used in the relaxed univariate tutorial.⇨ The relaxed mvBM-model is specified in the file called mcmc_relaxed_multivariate_BM.Rev. The majority of this script is the same as mcmc_multivariate_BM.Rev, except as described below.Relaxing the morphological clockTo specify the relaxed morphological clock, we simply modify the component of the model that controls the average rate, $\\sigma^2$. We specify the average rate at the root of the tree, $\\sigma_R^2$.sigma2_root ~ dnLoguniform(1e-3, 1)moves.append( mvScale(sigma2_root, weight=1.0) )Next, we specify the prior on the expected number of rate shifts.expected_number_of_shifts &lt;- 5rate_shift_probability    &lt;- expected_number_of_shifts / nbranchesWe must also specify the prior on the magnitude of rate shifts (when they occur). This prior supposes that rate shifts result in changes of rate within one order of magnitude.sd = 0.578rate_shift_distribution = dnLognormal(-sd^2/2, sd)Then we draw the rate multiplier from each branch from a dnReversibleJumpMixture distribution, and compute the average rate of evolution for each branch.for(i in nbranches:1) {    # draw the rate multiplier from a mixture distribution    branch_rate_multiplier[i] ~ dnReversibleJumpMixture(1, rate_shift_distribution, Probability(1 - rate_shift_probability) )    # compute the rate for the branch    if ( tree.isRoot( tree.parent(i) ) ) {       branch_rates[i] := beta_root * branch_rate_multiplier[i]    } else {       branch_rates[i] := background_rates[tree.parent(i)] * branch_rate_multiplier[i]    }    # keep track of whether the branch has a rate shift    branch_rate_shift[i] := ifelse( branch_rate_multiplier[i] == 1, 0, 1 )    # use reversible-jump to move between models with and without    # shifts on the branch    moves.append( mvRJSwitch(branch_rate_multiplier[i], weight=1) )    # include proposals on the rate mutliplier (when it is not 1)    moves.append( mvScale(branch_rate_multiplier[i], weight=1) )}We may also wish to keep track of the total number of rate shifts.num_rate_changes := sum( branch_rate_shift )We then provide these relaxed branch rates to dnPhyloMulivariateBrownianREML instead of sigma2.X ~ dnPhyloMultivariateBrownianREML(tree, branchRates=branch_rates^0.5, rateMatrix=V)X.clamp(data)When we create our monitors, we include an extended Newick monitor to keep track of the branch-specific rates.monitors.append( mnExtNewick(filename=\"output/relaxed_multivariate_BM.trees\", isNodeParameter=TRUE, printgen=10, separator=TAB, tree=tree, branch_rates) )Finally, when the MCMC completes, we create an tree with branches annotated with the branch-specific rates.treetrace = readTreeTrace(\"output/relaxed_multivariate_BM.trees\")map_tree = mapTree(treetrace,\"output/relaxed_multivariate_BM_MAP.tre\")⇨ The Rev file for performing this analysis: mcmc_relaxed_multivariate_BM.RevYou can then visualize the branch-specific rates by plotting them using our R package RevGadgets. Just start R in the main directory for this analysis and then type the following commands.First, load RevGadgets:library(RevGadgets)Next, read in the tree annotated with the branch rates:tree &lt;- readTrees(\"output/relaxed_multivariate_BM_MAP.tre\")Finally, plot the tree with the branch rates:plotTree(tree, color_branch_by=\"branch_rates\")⇨ The R script for plotting this output: plot_relaxed_multivariate_BM.REstimated rates of multivariate-Brownian-motion evolution.We show the estimated average rate of evolution for each branch under the mvBM model.Exercise 2  Estimate the posterior distribution of branch_rates under the relaxed multivariate BM model.  Compare the estimated correlation parameters under the relaxed mvBM model to those when the average rate is constant across branches. Do the estimated correlation coefficients differ? Do they differ in a consistent direction?  Modify the mcmc_relaxed_multivariate_BM.Rev script to estimate the posterior distribution of branch_rates when assuming that characters are uncorrelated. This can be achieved by setting the off-diagonal elements of correlation matrix to 0:    R &lt;- diagonalMatrix(nchar)        How do branch-specific rates of evolution vary between the correlated and uncorrelated models?  ",
        "url": "/tutorials/cont_traits/multivariate_bm.html",
        "index": "true"
      }
      ,
    
      "workshops-munich-palges-2019-html": {
        "title": "Bayesian phylogenetics and macroevolution in RevBayes",
        "content": "",
        "url": "/workshops/munich_palges_2019.html",
        "index": ""
      }
      ,
    
      "developer-setup-netbeans-html": {
        "title": "",
        "content": "Conventions: File paths are relative to the repository root.PreparationsMake sure you have Java (JRE) on your system: if the commandwhich javaresponds with a file path, like /usr/bin/java, then you’re fine, otherwise browse to the java download page and download the JRE.Now, browse to netbeans download page and download the C++ or “All” bundled Netbeans distribution. Navigate to the downloaded file in a terminal and run “chmod +x &lt;netbeans-script&gt;” then ./&lt;netbeans-script&gt; to install. Note, the “All” bundle will probably require JDK (Java Development Kit) to be installed on your computer, download jdk.By default Netbeans has quite limited memory available and you probably want to increase it to make things run faster:  Open the configuration file, &lt;your home folder&gt;/netbeans-&lt;version&gt;/etc/netbeans.conf in a text editor. On a MacIntosh, the path is likely to be /Applications/NetBeans/NetBeans\\ &lt;version&gt;.app/Contents/Resources/NetBeans/etc/netbeans.conf.  Prepend the settings to the default_options so that line will look like this: “netbeans_default_options=”-J-Xmx2g -J-XX:+UseConcMarkSweepGC &lt;other settings&gt;”.The Netbeans project described here will use the Makefile generated by the CMake script /projects/cmake/regenerate.sh. Before setting up our project it is conventient to have all the files needed in place so let’s start with running these commands in a terminal:cd  &lt;path-to-revbayes&gt;/revbayes/projects/cmake ./regenerate cmake . -DCMAKE_BUILD_TYPE=debugfull make -j 4 (4 = the number of processors on your computer) Netbeans setup      In Netbeans, Press CTRL+shift+n to start the new project wizard. Select category C++ and project C++ project with existing sources and move to next step.        In this step, select the git root directory as source folder, and tick the “custom” mode.        Select the Makefile to use        Configure build settings. Alternatively you can set “build command” = “${MAKE} -j 4” and “build result” pointing to the “rb” executable.        Code assistance can be slow if choosing all files, so narrow it down by ticking “Custom” and append “/src” to the path suggested by Netbeans.        That’s it, finish the wizard and compile your code by pressing the F11 key.  Here are some handy keyboard shortcuts to start with:  Ctrl+click, navigate to definition  Alt+←, go back where you where before Ctrl+click  Ctrl+r, refactor code  Alt+shift+f, format code  Ctrl+shift+c, comment/comment out codeOn Netbeans wiki you can download the full map for keyboard shortcuts. Happy coding!",
        "url": "/developer/setup/netbeans.html",
        "index": ""
      }
      ,
    
      "tutorials-dating-nodedate-html": {
        "title": "Molecular dating",
        "content": "Exercise 3In exercises 1 and 2 we inferred the phylogeny of extant bears and relative speciation times assuming a molecular clock model. However, it would be much more useful to have estimates of speciation times in the context of geological time.In this exercise we will use information from the fossil record to calibrate the molecular substitution rate to absolute time using node dating. This approach involves assigning probability densities that incorporate temporal information from the fossil record to particular nodes in the tree.The dataThe molecular data used in this exercise is the same as the previous exercise (bears_cytb.nex). We will also use the same substitution and clock models (the GTR + $\\Gamma$ model and the uncorrelated exponential clock model).We will also use the same tree model (the constant rate birth-death process), however, we will add calibration information from the fossil record to generate timetrees on a non-arbitrary timescale.The file bears_taxa.tsv contains information about the age ranges for 20 bear species, including 12 extinct species. We’re not going to use all of the information from this file in this exercise, because the node dating approach to calibration limits the amount of data we can take advantage of, but we’ll use some of this information to constrain the age of two nodes. In this file max is the age of the first appearance (i.e. the oldest) of each species and min is the age of the last appearance (i.e. the youngest) (t = 0.0 represents the present).Again, there are just three steps you need to complete before running the analysis in this exercise. First, we need to create a script for the tree model and add our calibration information.Second, we need to switch out the tree model in our master script and update the name of the output files, so we don’t overwrite the output generated in the previous exercise.The tree modelWe are going to add our calibrations to the script that incorporates the tree model.  Create a copy of tree_BD.Rev, call it tree_BD_nodedate.Rev and open it in your text editor.Node calibrations are used to specify the age of monophyletic groups that are defined a priori.Since the age of the node will not be known precisely, we can use evidence from the fossil record to define minimum (or maximum) bounds, and use a probability density (distribution) to reflect prior uncertainty in the age of that node.We’re going to add two node calibrations: one on the root and one on the internal node for the clade Ursinae, which we defined in exercise 1.Root calibrationThe oldest first appearance of a crown group bear in our dataset is Ursus americanus at 1.84 Ma. This means that the last common ancestor of all living bears can not be younger that this. Fossil calibrations exert a large influence on Bayesian posterior estimates of speciation times and should not be selected arbitrarily. In practice it is very challenging to select distributions and parameters objectively. In this instance, we will take advantage of a previous estimate ($\\sim$49 Ma) for the age of caniforms, which is the clade containing bears and other “dog-like” mammals, from (dos Reis et al. 2012). We will assume that the age of crown bears can not be older than this.First, specify the prior on the root. The following commands will replace extant_mrca &lt;- 1.0 in your tree model script, before the the tree_dist variable is specified.extant_mrca_min &lt;- 1.84extant_mrca_max &lt;- 49.0extant_mrca ~ dnUniform(extant_mrca_min, extant_mrca_max)moves.append( mvScale(extant_mrca, lambda=1, tune=true, weight=5.0) )Here, we have specified the minimum and maximum constraints described above and stochastic node for the age of the root extant_mrca. Finally, we define a move to sample the age of this parameter.Internal node calibrationThe second calibration will apply to the clade Ursinae that we defined previously. In addition to being the oldest crown group bear, the first appearance of Ursus americanus is also the first appearance of the group Ursinae. We’ll use a diffuse exponential prior offset by the age of this fossil to constrain the age of this node.In RevBayes, calibrated internal nodes are treated differently than  other Bayesian programs for estimating species divergence times (e.g. BEAST).This is because the graphical model structure used in RevBayes does not allow a stochastic node to be assigned more than one prior distribution. By contrast, the common approach to applying calibration densities as used in other dating software leads to incoherence in the calibration prior.Basically, common calibration approaches assume that the age of a calibrated node is modelled by the tree-wide diversification process (e.g. birth-death model) and a parametric density parameterized by the occurrence time of a fossil (or other external prior information).This can induce a calibration prior density that is not consistent with the birth-death process or the parametric prior distribution. For much more information about this see (Warnock et al. 2012; Heled and Drummond 2012; Heath et al. 2014).Approaches that condition the birth-death process on the calibrated nodes are more statistically coherent (Yang and Rannala 2006).In RevBayes, calibration densities are applied in a different way, treating fossil observation times like data. The age of the calibration node (i.e. the internal node specified as the MRCA of the fossil and a set of living species) is a deterministic node – e.g. denoted $o_1$ for fossil $\\mathcal{F}_1$ – and acts as an offset on the stochastic node representing the age of the fossil specimen.The fossil age, $\\mathcal{F}_i$, is specified as a stochastic node and clamped to its observed age in the fossil record. The node $\\mathcal{F}_i$ is modelled using a distribution that describes the waiting time from the speciation event to the appearance of the observed fossil. Thus, if the MCMC samples any state for which the age of $\\mathcal{F}_i$ has a probability of 0, then that state will always be rejected, effectively calibrating the birth-death process without applying multiple prior densities to any calibrated node.From your script, you’ll recall that we previously defined the Ursinae clade and used it to generate a constrained tree topology. We also created a deterministic node age_ursinae to keep track of the age of this node.To calibrate the age of this node we will specify a diffuse exponential density with an expected value (mean) = 1.0, offset by the age of fossil.obs_age_ursinae ~ age_ursinae - dnExponential(1.0)obs_age_ursinae.clamp(1.84)The master Rev script  Copy the master script from the previous exercise and call it MCMC_dating_ex3.Rev.First, change the file used to specify the tree model from tree_BD.Rev to tree_BD_nodedate.Rev.source(\"scripts/tree_BD_nodedate.Rev\") # BD tree prior + node calibrationsSecond, update the name of the output files.monitors.append( mnModel(filename=\"output/bears_nodedate.log\", printgen=10)\t)monitors.append( mnFile(filename=\"output/bears_nodedate.trees\", printgen=10, timetree) )Don’t forget to update the commands used to generate the summary tree.trace = readTreeTrace(\"output/bears_nodedate.trees\")mccTree(trace, file=\"output/bears_nodedate.mcc.tre\" )That’s all you need to do!  Run your MCMC analysis!Note that the root age is no longer a constant number (= 1) and scale of the diversification parameter may have changed.Running the analysis without the sequence dataIt is always useful to examine the output of your MCMC analysis when using fossil age data but ignoring sequence data (i.e. without calculating the likelihood that comes from the substitution model).Setting this up in RevBayes is very easy.Let’s do this while the above analysis is still running.  Copy the master script you just created and call it MCMC_dating_ex3_only_fossil_data.Rev.We just need to mark the clamped node phySeq that represents DNA sequence information as being ignored.mymodel = model(sf)\tmymodel.ignoreData(phySeq) \t\t# obs_age_ursinae is retainedNote that we retained the clamped node obs_age_ursinae that represents fossil age information.Again, we need to rename the output files.monitors.append( mnModel(filename=\"output/bears_nodedate_only_fossil_data.log\", printgen=10)\t)We’re not going to bother summarizing the trees, so if you want you can simply remove/comment out the second monitor (mnFile) and the tree summary functions (readTreeTrace and mccTree).This analysis will show you the estimates of node ages obtained under the tree model in combination with the constraint applied at the root of the tree. Note that although this step is often called “running the model under the prior”, the distinction between the prior and posterior varies between programs and becomes less clear once we incorporate fossil data into the tree.  When the above analysis is done, run the MCMC analysis under the prior!Examining the outputLet’s examine the output in Tracer.  Open the program Tracer and load the log files bears_nodedate.log and bears_nodedate_only_fossil_data.log.Select both log files and compare the age estimates obtained for the root (extant_mrca) and the MRCA of Ursinae (age_ursinae). To reproduce the Tracer images shown below, click on the node of interest and select Colour by: Trace file and Legend: Top-Right.The Marginal Density panel in Tracer showing the marginal estimates for the age of the root.The Marginal Density panel in Tracer showing the marginal estimates for the age of Ursinae.You’ll notice that including sequence data and the internal node calibration in our analysis reduces the uncertainty in our prior estimates.However, there is still a large degree of uncertainty in our posterior node ages – the 95% HPD interval for root age spans nearly 46 Myr, which is basically the width of the uniform prior that we used to constrain the age of this node in the first place!The tree outputTake a look at the posterior MCC tree in FigTree. In the case of this analysis, the scale bar is actually meaningful and represents time in units of Myr.Try to produce the image shown below including the 95% HPDs.The FigTree window. To open your tree you can use File &gt; Open. Select Node Labels to view the absolute node ages and Node Bars to display the 95% HPDs.If you wanted to visualise the impact of the internal node calibrations, without the influence of the sequence data, you could run the MCMC analysis under the posterior and use an empty sequence alignment (this would be equivalent to running the analysis under the prior in BEAST and MCMCTree).CaveatNote that there are many more fossil species in the file bears_taxa.tsv with associated age information that we didn’t use in this exercise.This is because, in the context of node dating, the calibration information is redundant with information already utilised (e.g. all other Urisinae species are younger than the fossil we used to constrain the age of this clade) or because we don’t have good prior knowledge about the phylogenetic position of the species.Next  Click below to begin the next exercise!  Estimating speciation times using the fossilized birth-death process",
        "url": "/tutorials/dating/nodedate.html",
        "index": "false"
      }
      ,
    
      "workshops-online2020-html": {
        "title": "Stay-at-Home RevBayes Workshop Summer 2020",
        "content": "Tutorials  Introduction to MCMC using RevBayes  Nucleotide substitution models  Partitioned data analysis  Estimating a Time-Calibrated Phylogeny of Fossil and Extant Taxa using Morphological Data  Convergence assessment  Introduction to Posterior Prediction  Additional tutorials (optional)Videos  RevBayes Intro to MCMC  RevBayes Substitution Models  RevBayes Partitioned Data Analysis  RevBayes FBD Tutorial  RevBayes Convergence Assessment  RevBayes Intro to Posterior Prediction  History of Phylogenetic Comparative Methods  RevBayes BiSSE TutorialWorkshop Code of ConductAll attendees and instructors have agreed to the terms outlined in the workshop code of conduct.https://drive.google.com/file/d/194cxLEmlona0EDpRxO73HI15P0gbEOug/view?usp=sharingWorkshop SWAGPurchase t-shirts, stickers, mugs, etc. with the workshop logo on Redbubble!",
        "url": "/workshops/online2020.html",
        "index": ""
      }
      ,
    
      "workshops-online2021-html": {
        "title": "Stay-at-Home RevBayes Workshop Spring 2021",
        "content": "Tutorials  Introduction to MCMC using RevBayes  Nucleotide substitution models  Partitioned data analysis  Estimating a Time-Calibrated Phylogeny of Fossil and Extant Taxa using Morphological Data  Bayesian polymorphism-aware phylogenetic inference  Convergence assessment  Introduction to Posterior Prediction  Introduction to RevGadgets  Additional tutorials (optional)Videos  RevBayes Intro to MCMC  RevBayes Substitution Models  RevBayes Partitioned Data Analysis  RevBayes FBD Tutorial  RevBayes Convergence Assessment  RevBayes Intro to Posterior Prediction  History of Phylogenetic Comparative Methods  Introduction to RevGadgets  RevBayes BiSSE TutorialWorkshop Code of ConductAll attendees and instructors will have to agree to the terms outlined in the workshop code of conduct.The code of conduct for the previous edition of the workshop can be found here.",
        "url": "/workshops/online2021.html",
        "index": ""
      }
      ,
    
      "jobs-phd-position-2018-sebastian-html": {
        "title": "PhD position&amp;#58; Developing new methods and models for robust gene-tree estimation",
        "content": "",
        "url": "/jobs/phd_position_2018_sebastian.html",
        "index": ""
      }
      ,
    
      "jobs-phd-position-2019-sebastian-divrates-html": {
        "title": "PhD position&amp;#58; Modeling Species Diversification",
        "content": "",
        "url": "/jobs/phd_position_2019_sebastian_divrates.html",
        "index": ""
      }
      ,
    
      "jobs-phd-position-2019-sebastian-speciestree-html": {
        "title": "PhD position&amp;#58; Species Tree Estimation",
        "content": "",
        "url": "/jobs/phd_position_2019_sebastian_speciestree.html",
        "index": ""
      }
      ,
    
      "tutorials-coalescent-piecewise-html": {
        "title": "Piecewise Coalescent Model",
        "content": "OverviewThis exercise describes how to run a piecewise coalescent analysis in RevBayes.In this case, we will define an individual demographic function with different basic “pieces”.The pieces can be either constant, linear or exponential.For all of these pieces, the different values of $N_e$ and the change-points in between can be estimated.Definition of the different base demographic modelsThe three implemented base demographic models in RevBayes are a constant, a linear and an exponential model.For the constant model, the population size through time is easily defined:\\[N_e(t) = N_e(t_{i,j}),\\]with $t_{i,j}$ being the time at the beginning of the $j^{th}$ interval.For the linear model, the slope depends on the starting and ending values of the population size at the interval change-points.We define $\\alpha$ as the slope.\\[\\alpha = \\frac{N_e(t_{i,(j+1)}) - N_e(t_{i,j})}{t_{i,(j+1)} - t_{i,j}}.\\]Then, the effective population size through time is calculated as follows:\\[N_e(t) = N_e(t_{i,j}) + (t-t_{i,j}) * \\alpha.\\]Finally, for the exponential model, $\\alpha$ is defined as follows:\\[\\alpha = \\frac{log(\\frac{N_e(t_{i,(j+1)})}{N_e(t_{i,j})})}{t_{i,j} - t_{i,(j+1)}},\\]and the effective population size is:\\[N_e(t) = N_e(t_{i,j}) exp((t_{i,j} - t)\\alpha).\\]Inference Example  For your info  The entire process of the coalescent estimation can be executed by using the mcmc_isochronous_piecewise_6diff.Rev script in the scripts folder.You can type the following command into RevBayes:  source(\"scripts/mcmc_isochronous_piecewise_6diff.Rev\")    We will walk you through every single step in the following section.We will mainly highlight the parts of the script that change compared to the constant coalescent model.Read the dataRead in the data as described in the first exercise.The Piecewise CoalescentFor the piecewise model, you need to define which kinds of pieces should be included.For each piece, one or two population sizes will be estimated.Choose a prior and add a move for each population size.In the case of a constant coalescent process, one population size is needed.For the two other processes, one population size for the start of the piece and one for the end of the piece are needed.Here, we would like to test six different pieces.Two should be constant, two linear and two exponential.Thus, we need five population sizes.for (i in 1:5){    pop_size[i] ~ dnUniform(0,1E8)    pop_size[i].setValue(100000)    moves.append( mvScale(pop_size[i], lambda=0.1, tune=true, weight=2.0) )}We also set prior distributions on the times of the change-points between pieces.change_points[1] ~ dnUniform(1E4,2E4)change_points[2] ~ dnUniform(3E4,4E4)change_points[3] ~ dnUniform(6E4,9E4)change_points[4] ~ dnUniform(1.2E5,1.7E5)change_points[5] ~ dnUniform(2.2E5,3.2E5)for (i in 1:5){    moves.append( mvSlide(change_points[i], delta=0.1, tune=true, weight=2.0) )}Now, we need to define the different pieces.Depending on the type of piece, different parameters need to be added:dem_exp_1 = dfExponential(N0 = pop_size[1], N1=pop_size[2], t0=0, t1=change_points[1])dem_exp_2 = dfExponential(N0 = pop_size[2], N1=pop_size[3], t0=change_points[1], t1=change_points[2])dem_lin_1 = dfLinear(N0 = pop_size[3], N1=pop_size[4], t0=change_points[2], t1=change_points[3])dem_const_1 = dfConstant(pop_size[4])dem_lin_2 = dfLinear(N0 = pop_size[4], N1=pop_size[5], t0=change_points[4], t1=change_points[5])dem_const_2 = dfConstant(pop_size[5])The TreeNow, we will instantiate the stochastic node for the tree with dnCoalescentDemography.In this case, we set the vector of demographic models and the change-points as input.psi ~ dnCoalescentDemography([dem_exp_1,dem_exp_2,dem_lin_1,dem_const_1,dem_lin_2,dem_const_2], changePoints=change_points, taxa=taxa)For this analysis, we constrain the root age as before and add the same moves for the tree.Substitution Model and other parametersThis part is also taken from the constant coalescent exercise.Finalize and run the analysisIn the end, we need to wrap our model as before.Finally, we add the monitors and then run the MCMC.monitors.append( mnModel(filename=\"output/horses_iso_piecewise_6diff.log\",printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_piecewise_6diff.trees\",psi,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_piecewise_6diff_NEs.log\",pop_size,printgen=THINNING) )monitors.append( mnFile(filename=\"output/horses_iso_piecewise_6diff_times.log\",change_points,printgen=THINNING) )monitors.append( mnScreen(pop_size, root_age, printgen=100) )ResultsAfter running your analysis, you can plot the results using the R package RevGadgets and by additionally defining the demographic models from this exercise.R codelibrary(RevGadgets)burnin = 0.1probs = c(0.025, 0.975)summary = \"median\"num_grid_points = 500max_age_iso = 5e5min_age = 0spacing = \"equal\"population_size_log_skyline = \"output/horses_iso_skyline_NEs.log\"interval_change_points_log_skyline = \"output/horses_iso_skyline_times.log\"df_skyline &lt;- processPopSizes(population_size_log_skyline, interval_change_points_log_skyline, burnin = burnin, probs = probs, summary = summary, num_grid_points = num_grid_points, max_age = max_age_iso, min_age = min_age, spacing = spacing)p_skyline &lt;- plotPopSizes(df_skyline) + ggplot2::coord_cartesian(ylim = c(1e3, 1e8))population_size_log = \"output/horses_iso_piecewise_6diff_NEs.log\"interval_change_points_log = \"output/horses_iso_piecewise_6diff_times.log\"pop_sizes &lt;- readTrace(population_size_log, burnin = burnin)[[1]]interval_times &lt;- readTrace(interval_change_points_log, burnin = burnin)[[1]]pop_size_medians = apply(pop_sizes[,grep(\"size\", names(pop_sizes))], 2, median)pop_size_quantiles = apply(pop_sizes[,grep(\"size\", names(pop_sizes))], 2, quantile, probs = probs)time_medians =apply(interval_times[,grep(\"change_points\", names(interval_times))], 2, median)exponential_dem &lt;- function(t, N0, N1, t0, t1){  alpha = log( N1/N0 ) / (t0 - t1)  return (N0 * exp( (t0-t) * alpha))}linear_dem &lt;- function(t, N0, N1, t0, t1){  alpha = ( N1-N0 ) / (t1 - t0)  return (N0 + (t-t0) * alpha)}all_combined &lt;- function(t){  if (t &lt; time_medians[1]){    return(exponential_dem(t, N0 = pop_size_medians[1], N1 = pop_size_medians[2], t0 = 0, t1 = time_medians[1]))  } else if (t &lt; time_medians[2]){    return(exponential_dem(t, N0 = pop_size_medians[2], N1 = pop_size_medians[3], t0 = time_medians[1], t1 = time_medians[2]))  } else if (t &lt; time_medians[3]){    return(linear_dem(t, N0 = pop_size_medians[3], N1 = pop_size_medians[4], t0 = time_medians[2], t1 = time_medians[3]))  } else if (t &lt; time_medians[4]){    return(pop_size_medians[4])  } else if (t &lt; time_medians[5]){    return(linear_dem(t, N0 = pop_size_medians[4], N1 = pop_size_medians[5], t0 = time_medians[4], t1 = time_medians[5]))  } else {    return(pop_size_medians[5])  }}all_lower &lt;- function(t){  if (t &lt; time_medians[1]){    return(exponential_dem(t, N0 = pop_size_quantiles[1,1], N1 = pop_size_quantiles[1,2], t0 = 0, t1 = time_medians[1]))  } else if (t &lt; time_medians[2]){    return(exponential_dem(t, N0 = pop_size_quantiles[1,2], N1 = pop_size_quantiles[1,3], t0 = time_medians[1], t1 = time_medians[2]))  } else if (t &lt; time_medians[3]){    return(linear_dem(t, N0 = pop_size_quantiles[1,3], N1 = pop_size_quantiles[1,4], t0 = time_medians[2], t1 = time_medians[3]))  } else if (t &lt; time_medians[4]){    return(pop_size_quantiles[1,4])  } else if (t &lt; time_medians[5]){    return(linear_dem(t, N0 = pop_size_quantiles[1,4], N1 = pop_size_quantiles[1,5], t0 = time_medians[4], t1 = time_medians[5]))  } else {    return(pop_size_quantiles[1,5])  }}all_upper &lt;- function(t){  if (t &lt; time_medians[1]){    return(exponential_dem(t, N0 = pop_size_quantiles[2,1], N1 = pop_size_quantiles[2,2], t0 = 0, t1 = time_medians[1]))  } else if (t &lt; time_medians[2]){    return(exponential_dem(t, N0 = pop_size_quantiles[2,2], N1 = pop_size_quantiles[2,3], t0 = time_medians[1], t1 = time_medians[2]))  } else if (t &lt; time_medians[3]){    return(linear_dem(t, N0 = pop_size_quantiles[2,3], N1 = pop_size_quantiles[2,4], t0 = time_medians[2], t1 = time_medians[3]))  } else if (t &lt; time_medians[4]){    return(pop_size_quantiles[2,4])  } else if (t &lt; time_medians[5]){    return(linear_dem(t, N0 = pop_size_quantiles[2,4], N1 = pop_size_quantiles[2,5], t0 = time_medians[4], t1 = time_medians[5]))  } else {    return(pop_size_quantiles[2,5])  }}grid = seq(0, 3.5e5, length.out = 500)pop_size_median &lt;- sapply(grid, all_combined)pop_size_lower &lt;- sapply(grid, all_lower)pop_size_upper &lt;- sapply(grid, all_upper)df &lt;-tibble::tibble(.rows = length(grid))df$value &lt;- pop_size_mediandf$lower &lt;- pop_size_lowerdf$upper &lt;- pop_size_upperdf$time &lt;- gridp &lt;- p_skyline +  ggplot2::geom_line(data = df, ggplot2::aes(x = time, y = value), linewidth = 0.9, color = \"blue\") +  ggplot2::geom_ribbon(data = df, ggplot2::aes(x = time, ymin = lower, ymax = upper), fill = \"blue\", alpha = 0.2)ggplot2::ggsave(\"figures/horses_iso_piecewise_6diff.png\", p)Example output from plotting the piecewise analysis with six pieces run in this exercise. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals. The reference skyline result is shown in green and the result of the piecewise analysis is shown in blue.SummaryWhen you are done with all exercises, have a look at the tutorial with heterochronous data or the summary.",
        "url": "/tutorials/coalescent/piecewise.html",
        "index": "false"
      }
      ,
    
      "tutorials-mcmc-poisson-html": {
        "title": "Introduction to MCMC",
        "content": "Exercise: Poisson Regression Model for Airline FatalitiesThis exercise will demonstrate how to approximate the posteriordistribution of some parameters using a simple Metropolis algorithm. Thefocus here lies in the Metropolis algorithm, Bayesian inference, andmodel specification—but not in the model or the data. After completingthis computer exercise, you should be familiar with the basic Metropolisalgorithm, analyzing output generated from a MCMC algorithm, andperforming standard Bayesian inference.Model and DataWe will use the data example from Gelman et al. (2003). A summary is given in.            Year      1976      1977      1978      1979      1980      1981      1982      1983      1984      1985                  Fatalities      24      25      31      31      22      21      26      20      16      22      Airline fatalities from 1976 to 1985. Reproduced from Gelman et al. (2003) [Table 2.2 on p. 69].These data can be loaded into RevBayes by typing:observed_fatalities &lt;- v(24,25,31,31,22,21,26,20,16,22)The model is a Poissonregression model withparameters $\\alpha$ and $\\beta$\\[y \\sim \\text{Poisson}(\\exp(\\alpha+\\beta*x))\\]where $y$ is the number of fatal accidents in year $x$. For simplicity,we choose uniform priors for $\\alpha$ and $\\beta$.\\[\\begin{aligned}\\alpha &amp; \\sim \\text{Uniform}(-10,10)\\\\\\beta &amp;  \\sim \\text{Uniform}(-10,10)\\end{aligned}\\]The probability density can be computed in RevBayes for a single year bydpoisson(y[i],exp(alpha+beta*x[i]))ProblemsMetropolis AlgorithmThe source file for this sub-exercise ‘airline_fatalities_part1.Rev‘.Let us construct a Metropolis algorithm that simulates from theposterior distribution $P(\\alpha,\\beta|y)$. We will construct thisalgorithm explicitly, without using the high-level functions existing inRevBayes to perform MCMC. In the next section, we will repeat the sameanalysis, this time using the high-level functions. (More background onMCMC is provided in the Introduction to Markov Chain Monte CarloAlgorithmstutorial.)For simplicity of the calculations you can “normalize” the years, e.g.x &lt;- 1976:1985 - mean(1976:1985)A common proposal distribution for $\\alpha^{\\prime} \\sim P(\\alpha[i-1])$is the normal distribution with mean $\\mu = \\alpha[i-1]$ and standarddeviation $\\sigma = \\delta_\\alpha$:alpha_prime &lt;- rnorm(1,alpha[i-1],delta_alpha)A similar distribution should be used for $\\beta^{\\prime}$.delta_alpha &lt;- 1.0delta_beta &lt;- 1.0After you look at the output of the MCMC (later), play around to findappropriate values for $\\delta_{\\alpha}$ and $\\delta_{\\beta}$.Now we need to set starting values for the MCMC algorithm. Usually,these are drawn from the prior distribution, but sometimes if the prioris very uninformative, then these parameter values result in alikelihood of 0.0 (or log-likelihood of -Inf).alpha[1] &lt;- -0.01     # you can also use runif(-1.0,1.0)beta[1] &lt;- -0.01      # you can also use runif(-1.0,1.0)Next, create some output for our MCMC algorithm. The output will bewritten into a file that can be read into Ror Tracer(Rambaut and Drummond 2011).# create a file outputwrite(\"iteration\",\"alpha\",\"beta\",file=\"airline_fatalities.log\")write(0,alpha[1],beta[1],file=\"airline_fatalities.log\",append=TRUE)Note that we need a first iteration with value 0 so that Tracer can loadin this file.Finally, we set up a ‘for‘ loop over each iteration of the MCMC.for (i in 2:10000) {Within the ‘for‘ loop we propose new parameter values.    alpha_prime &lt;- rnorm(1,alpha[i-1],delta_alpha)[1]    beta_prime &lt;- rnorm(1,beta[i-1],delta_beta)[1]For the newly proposed parameter values we compute the prior ratio. Inthis case we know that the prior ratio is 0.0 as long as the newparameters are within the limits.    ln_prior_ratio &lt;- dunif(alpha_prime,-10.0,10.0,log=TRUE) + dunif(beta_prime,-10.0,10.0,log=TRUE) - dunif(alpha[i-1],-10.0,10.0,log=TRUE) - dunif(beta[i-1],-10.0,10.0,log=TRUE)Similarly, we compute the likelihood ratio for each observation.    ln_likelihood_ratio &lt;- 0    for (j in 1:x.size() ) {       lambda_prime &lt;- exp( alpha_prime + beta_prime * x[j] )       lambda &lt;- exp( alpha[i-1] + beta[i-1] * x[j] )       ln_likelihood_ratio += dpoisson(observed_fatalities[j],lambda_prime) - dpoisson(observed_fatalities[j],lambda)    }    ratio &lt;- ln_prior_ratio + ln_likelihood_ratioAnd finally we accept or reject the newly proposed parameter values withprobability ‘ratio‘.    if ( ln(runif(1)[1]) &lt; ratio) {       alpha[i] &lt;- alpha_prime       beta[i] &lt;- beta_prime    } else {       alpha[i] &lt;- alpha[i-1]       beta[i] &lt;- beta[i-1]    }Then we log the current parameter values to the file by appending thefile.    # output to a log-file    write(i-1,alpha[i],beta[i],file=\"airline_fatalities.log\",append=TRUE) }As a quick summary you can compute the posterior mean of the parameters.mean(alpha)mean(beta)You can also load the file into Ror Tracer to analyze theoutput.In this section of the first exercise we wrote our own little Metropolisalgorithm in Rev. This becomes very cumbersome, difficult and slow ifwe’ld need to do this for every model. Here we wanted to show you onlythe basic principle of any MCMC algorithm. In the next section we willuse the built-in MCMC algorithm of RevBayes.MCMC analysis using the built-in algorithm in RevBayesBefore starting with this new approach it would be good if you eitherstart a new RevBayes session or clear all previous variables using the‘clear‘ function. Currently we may have some minor memory problems andif you get stuck it may help to restart RevBayes.We start by loading in the data to RevBayes.observed_fatalities &lt;- v(24,25,31,31,22,21,26,20,16,22)x &lt;- 1976:1985 - mean(1976:1985)Then we create the parameters with their prior distributions.alpha ~ dnUnif(-10,10) beta ~ dnUnif(-10,10)It may be good to set some reasonable starting values especially if youchoose a very uninformative prior distribution. If by chance you hadstarting values that gave a likelihood of -Inf, then RevBayes will tryseveral times to propose new starting values drawn from the priordistribution.# let us use reasonable starting valuealpha.setValue(0.0)beta.setValue(0.0)Our next step is to set up the moves. Moves are algorithms that proposenew values and know how to reset the values if the proposals arerejected. We use the same sliding window move as we implemented above byourselves.mi &lt;- 0moves[mi++] = mvSlide(alpha)moves[mi++] = mvSlide(beta)Then we set up the model. This means we create a stochastic variable foreach observation and clamp its value with the observed data.for (i in 1:x.size() ) {    lambda[i] := exp( alpha + beta * x[i] )    y[i] ~ dnPoisson(lambda[i])    y[i].clamp(observed_fatalities[i])}We can now create the model by pulling up the model graph from anyvariable that is connected to our model graph.mymodel = model( alpha )We also need some monitors that report the current values during theMCMC run. We create two monitors, one printing all numeric non-constantvariables to a file and one printing some information to the screen.monitors[1] = mnModel(filename=\"output/airline_fatalities.log\",printgen=10, separator = \"\t\")monitors[2] = mnScreen(printgen=10, alpha, beta)Finally we create an MCMC object. The MCMC object takes in a modelobject, the vector of monitors and the vector of moves.mymcmc = mcmc(mymodel, monitors, moves)On the MCMC object we call its member method ‘run‘ to run the MCMC.mymcmc.run(generations=3000)And now we are donePosterior Distribution of $\\alpha$ and $\\beta$  Report the posterior mean and 95% credible intervals for $\\alpha$ and $\\beta$.  Additionally, plot the posterior distribution of $\\alpha$ and $\\beta$ by plotting a histogram of the samples.  Plot the curve of \\(m(x) = \\text{E}[\\exp(\\alpha+\\beta*x) |y]\\) for $x = [1976,1985]$. You can generate draws from the posterior distribution of the expected value fora specific $x$ by recording the current expected value at a iteration$i$ of the Metropolis algorithm \\(m_{sample}(x)[i] = \\text{E}[\\exp(\\alpha[i]+\\beta[i]*x) |y]\\) and takingthe mean of those samples ($\\bar{m}_{sample}(x)$) afterwards. Since RevBayesprovides you with the samples of\\(m(x) = \\text{E}[\\exp(\\alpha+\\beta*x) |y] = \\lambda_x\\) you can simplyplot these posterior curves.  Produce a histogram of the predictive distribution of the number offatalities in 2014 and estimate the posterior mean. The predictivedistribution can be approximated simultaneously with the Metropolisalgorithm. This means, for any iteration $i$ you simulate draws from theconditional distribution for $x = 2014$ and the current values of$\\alpha[i]$ and $\\beta[i]$.  Estimate the distribution of the mean of the posterior predictivedistribution of the number of fatalities in 2014.  Let us denote the expected value of the posterior distribution by $\\mu$. Sincewe do not know this value $\\mu$ exactly, we can follow the Bayesianapproach and associate a probability for each value $m$ as being thetrue expected value of the posterior distribution, given theobservations $y$ ($P(m = \\mu|y)$). You can approximate this distributionby recording the expected value for the number of fatalities in 2014($\\text{E}[\\exp(\\alpha+\\beta*x)|y]$) in each iteration $i$ of theMetropolis algorithm. Plot a histogram of the expected values, computethe mean of the expected values and compare it to the previouslyobtained estimate of the mean of the posterior predictive distribution.  Follow the same approach as for the posterior predictive distributionfor $x = 2014$, but this time for $x = 2016$ and estimate theprobability of no fatality.Exercise: Poisson Regression Model for Coal-mine AccidentsWe will analyze a dataset coal-mine accidents. The values are the datesof major (more than 10 casualties) coal-mining disasters in the UK from1851 to 1962.A model for disastersA common model for the number of events that occur over a period of timeis a Poisson process, in which the numbers of events in disjointtime-intervals are independent and Poisson-distributed. We willdiscretize and look at the yearly number of accidents.In order to take into account the possible change of rate, we will allowfor different rates before and after year $\\theta$, where $\\theta$ isunknown to us. Thus, the observation distribution of our model is$y_t \\sim Poisson(\\lambda_t)$ with $t = 1851,\\ldots,1962$ and\\(\\begin{aligned}\\lambda_t = &amp; \\begin{cases}\\beta &amp; \\mbox{if } t &lt; \\theta \\\\\\gamma &amp; \\mbox{if } t \\geq \\theta\\end{cases}\\end{aligned}\\) Thus, the rate $\\lambda_t$ is defined bythree unknown parameters: $\\beta$, $\\gamma$ and $\\theta$. A hierarchicalchoice of priors is given by \\(\\begin{aligned} \\eta &amp; \\sim Gamma(10.0;20.0) \\\\  \\beta &amp; \\sim Gamma(2.0;\\eta) \\\\ \\gamma &amp; \\sim Gamma(2.0;\\eta) \\\\ \\theta &amp; \\sim Uniform(1852,\\ldots,1962)\\end{aligned}\\) which bringsan additional parameter $\\eta$ in the model. For $\\theta$ we have used auniform prior over the years, but excluded year 1851 in order to makesure at least one year has rate $\\beta$. The hierarchical prior carriesthe belief that $\\beta$ and $\\gamma$ are somewhat similar in size, sincethey both depend on $\\eta$.The model in RevWe start as usual by loading in the data.observed_fatalities &lt;-  v(4, 5, 4, 1, 0, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1)year &lt;- 1851:1962In Rev we specify this prior choice byeta ~ dnGamma(10.0,20.0)beta ~ dnGamma(2.0,eta)gamma ~ dnGamma(2.0,eta)theta ~ dnUnif(1852.0,1962.0)Then we select moves for each parameter. For the rate parameters — whichare defined only on the positive real line — we choose a scaling move.Only for ‘theta‘ we choose the sliding window proposal.mi &lt;- 0moves[mi++] = mvScale(eta)moves[mi++] = mvScale(beta)moves[mi++] = mvScale(gamma)moves[mi++] = mvSlide(theta)Then, we set up the model by computing the conditional rate of thePoisson distribution, creating random variables for each observation andattaching (clamping) data to the variables.for (i in 1:year.size() ) {    rate[i] := ifelse(theta &gt; year[i], beta, gamma)    y[i] ~ dnPoisson(rate[i])    y[i].clamp(observed_fatalities[i])}Finally, we create the model object from the variables, add somemonitors and run the MCMC algorithm.mymodel = model( theta )monitors[1] = mnModel(filename=\"output/coal_accidents.log\",printgen=10, separator = \"\t\")monitors[2] = mnScreen(printgen=10, eta, lambda, gamma, theta)mymcmc = mcmc(mymodel, monitors, moves)mymcmc.run(generations=3000)Batch ModeIf you wish to run this exercise in batch mode, the files are providedfor you.You can carry out these batch commands by providing the file name whenyou execute the ‘rb‘ binary in your unix terminal (this will overwriteall of your existing run files).rb RevBayes_scripts airline_fatalities_part1.Revrb RevBayes_scripts airline_fatalities_part2.Revrb RevBayes_scripts coalmine_accidents.Rev",
        "url": "/tutorials/mcmc/poisson.html",
        "index": "true"
      }
      ,
    
      "jobs-postdoc-position-2018-sebastian-html": {
        "title": "Postdoc position&amp;#58; Developing methods and models for gene-tree species-tree estimation",
        "content": "",
        "url": "/jobs/postdoc_position_2018_sebastian.html",
        "index": ""
      }
      ,
    
      "tutorials-coalescent-postprocessing-html": {
        "title": "Postprocessing the Output",
        "content": "OverviewThis exercise describes how to plot the results from a coalescent analysis with RevBayes.You need the R package RevGadgets.Processing OutputWith RevGadgets, you can use the function processPopSizes to summarize your output.Type ?processPopSizes in R to see the parameters of the function.Plotting Population Size TrajectoriesFor plotting the population size trajectory from your analysis, you need the plotPopSizes function.Type ?plotPopSizes in R to see the parameters of the function.The input should be a dataframe created with processPopSizes with the option distribution = FALSE, which is the default.Checking for ConvergenceTo check for convergence, you can calculate the Kolmogorov-Smirnov test statistic for the population size distributions of your replicates.By using processPopSizes with distribution = TRUE and using the respective files of the different runs as input, you get these distributions of your samples at your choice of grid points.Afterwards, you can use the ks.test function to calculate the statistic at each grid point.Fabreti and Höhna (2022) show that the threshold for the statistic should be \\({D}_{crit} = 0.0921\\).If the statistic is below ${D}_{crit}$, the two sets of samples can be considered to be drawn from the same distribution.This means that the MCMCs have converged.In the tutorial Convergence assessment, you can find more information on the Kolmogorov-Smirnov test.",
        "url": "/tutorials/coalescent/postprocessing.html",
        "index": "false"
      }
      ,
    
      "tutorials-model-testing-pps-pps-data-html": {
        "title": "Assessing Phylogenetic Reliability Using RevBayes and $P^{3}$",
        "content": "OverviewThis tutorial presents the general principles of assessing thereliability of a phylogenetic inference through the use of posteriorpredictive simulation (PPS). PPS works by assessing the fit of anevolutionary model to a given dataset, and analyzing several teststatistics using a traditional goodness-of-fit framework to help explainwhere a model may be the most inadequate.IntroductionAssessing the fit of an evolutionary model to the data is critical asusing a model with poor fit can lead to spurious conclusions. However, acritical evaluation of absolute model fit is rare in evolutionarystudies. Posterior prediction is a Bayesian approach to assess the fitof a model to a given dataset(Bollback 2002; Brown 2014; Höhna et al. 2018), that relies on theuse of the posterior and the posterior predictive distributions. Theposterior distribution is the standard output from Bayeisan phylogeneticinference. The posterior predictive distribution represents a range ofpossible outcomes given the assumptions of the model. The most commonmethod to generate these possible outcomes, is to sample parameters fromthe posterior distribution, and use them to simulate new replicatedatasets (). If these simulated datasets differ from the empiricaldataset in a meaningful way, the model is failing to capture somesalient feature of the evolutionary process.A schematic presentation of data- versusinference-based approaches to assessing model plausibility withposterior predictivesimulation. Most statistics proposed for testingmodel plausibility compare data-based characteristics of the originaldata set to the posterior predictive data sets (e.g., variation inGC-content across species). ‘RevBayes‘ additionally implements teststatistics that compare the inferences resulting from different datasets (e.g., the distribution of posterior probability acrosstopologies). Multiple sequence alignments (MSAs) are represented asshaded matrices and arrows originating from MSAs point to the MCMCsamples of tree topologies and scalar model parameters ($\\theta$)resulting from Bayesian analysis of that MSA. Subscripts of MCMC samplestaken during analysis of the original data index the samples$(1,    …, n)$. Subscripts for each posterior predictive data setindicate which MCMC sample was used in its simulation. Subscripts forMCMC samples resulting from analysis of a posterior predictive data setfirst indicate the posterior predictive data set that was analyzed andnext index the MCMC samples from analysis of that particular data set$(1, …, m)$.The framework to construct posterior predictive distributions, andcompare them to the posterior distribution is conveniently built in to‘RevBayes‘ (Höhna et al. 2014; Höhna et al. 2016; Höhna et al. 2018). In this tutorial we will walk you through using thisfunctionality to perform a complete posterior predictive simulation onan example dataset.Data- Versus Inference-Based ComparisonsMost statistics proposed for testing model plausibility comparedata-based characteristics of the original data set to the posteriorpredictive data sets (e.g., variation in GC-content across species). Indata-based assessments of model fit one compares the empirical data todata simulated from samples of the posterior distribution. ‘RevBayes‘additionally implements test statistics that compare the inferencesresulting from different data sets (e.g., the distribution of posteriorprobability across topologies). These are called inference-basedassessments of model fit. For these assessments one must run an MCMCanalysis on each simulated data set, and then compare the inferencesmade from the simulated data to the inference made from the empiricaldata.In this tutorial we will only cover the data-based method of assessing model plausibility. The inference-based method can be a powerful tool that you may want to explore at anothertime.Substitution ModelsThe models we use here are equivalent to the models described in theprevious exercise on substitution models (continuous time Markovmodels). To specify the model please consult the previous exercise.Specifically, you will need to specify the following substitutionmodels:  Jukes-Cantor (JC) substitution model (missing reference)  General-Time-Reversible (GTR) substitution model (missing reference)  Gamma (+G) model for among-site rate variation (missing reference)  Invariable-sites (+I) model (missing reference)Assessing Model Fit with Posterior PredictionThe entire process of posterior prediction can be executed by using thepps_analysis_JC.Rev script in the scripts folder. If youwere to type the following command into ‘RevBayes‘:    source(\"scripts/pps_analysis_JC.Rev\")the entire data-based posterior prediction process would run on theexample dataset. However, in this tutorial, we will walk through eachstep of this process on an example dataset to explain what is happening in detail.Empirical MCMC AnalysisTo begin, we first need to generate a posterior distribution from whichto sample for simulation. This is the normal, and often only, stepconducted in phylogenetic studies. Here we will specify our dataset,evolutionary model, and run a traditional MCMC analysis.This code is all in the data_pp_analysis_JC.Rev file.Set up the workspaceFirst, let’s set up some workspace variables we’ll need. We’ll specify ageneral name to apply to your analysis. This will be used for futureoutput files, so make sure it’s something clear and easy to understand.    analysis_name = \"pps_example\"    model_name = \"JC\"    model_file_name = \"scripts/pps_\"+model_name+\"_Model.Rev\"Now specify and read in the input file. This is your sequence alignmentin NEXUS format.    inFile = \"data/primates_and_galeopterus_cytb.nex\"    data &lt;- readDiscreteCharacterData(inFile)Specify the modelNow we’ll call a separate script pps_JC_Model.Rev that specifies the JCmodel:    source( model_file_name ) If we open the pps_JC_Model.Rev script, most of this script should lookfamiliar from the Nucleotide substitution models. Nevertheless, we will look at some parts of script for a very brief repitition.    Q &lt;- fnJC(4)Here we are specifying that we should use the Jukes-Cantor model, andhave it applied uniformly to all sites in the dataset. While thisobviously is not likely to be a good fitting model for most datasets, weare using it for simplicity of illustrating the process.    topology ~ dnUniformTopology(names)This sets a uniform prior on the tree topology.    br_lens[i] ~ dnExponential(10.0)This sets an exponential distribution as the branch length prior.    phylogeny := treeAssembly(topology, br_lens)This builds the tree by combining the topology with branch lengthsupport values.Run the MCMCNow let’s run MCMC on our empirical dataset, just like a normalphylogenetic analysis. Here we use the Jukes-Cantor substition model.    source(\"scripts/pps_MCMC_Simulation.Rev\")After specifying a model, we will conduct the MCMC analysis with thescript pps_MCMC_Simulation.Rev. Much of that script should look familiarfrom the Nucleotide substitution models, but let’s take a look at the script andbreak down and revisit some of the major pieces as a refresher. Much ofthis will be a familiar to you from the Nucleotide substitution models.First we need to specify some monitors.    mni = 0    monitors[++mni] = mnModel(filename=\"output\" + n + \"/\" + analysis_name + \"_posterior.log\",printgen=10, separator = TAB)    monitors[++mni] = mnFile(filename=\"output\" + n + \"/\" + analysis_name + \"_posterior.trees\",printgen=10, separator = TAB, phylogeny)    monitors[++mni] = mnScreen(printgen=1000, TL)    monitors[++mni] = mnStochasticVariable(filename=\"output\" + n + \"/\" + analysis_name + \"_posterior.var\",printgen=10)Here the monitors are just being advanced by the value of mni which isbeing incremented each generation.Next, we will call the MCMC function, passing it the model, monitors andmoves we specified above to set to build our mymcmc object.    mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Specify a burnin for this run.    mymcmc.burnin(generations=2000,tuningInterval=200)Finally, we will execute the run for a specified number of generations.The number of generations and printed generations is important to consider here for avariety of reasons, in particular for posterior predictive simulation.When we simulate datasets in the next step, we can only simulate 1dataset per sample in our posterior. So, while the number of posteriorsamples will almost always be larger than the number of datasets we willwant to simulate, it’s something to keep in mind.Now let’s actually run the MCMC in ‘RevBayes‘.    mymcmc.run(generations=10000)MCMC outputAfter the process completes, the results can be found in theoutput_JC folder. You should see a number of familiar lookingfiles, all with the name we provided under the analysis_namevariable, pps_example in this case. Since we set the number of runs(nruns=2) in our MCMC, there will be two files of each type (.log .trees.var) with an _N where N is the run number. You will also see 3files without any number in their name. These are the combined files ofthe output. These will be the files we use for the rest of the process.If you open up one of the combined .var file, you should see that thereare 1000 samples. This was produced by our number of generations(10,000) divided by our number of printed generations (10), which wespecified earlier. This is important to note, as we will need to thinthese samples appropriately in the next step to get the proper number ofsimulated datasets.Posterior Predictive Data SimulationThe next step of posterior predictive simulation is to simulate newdatasets by drawing samples and parameters from the posteriordistribution generated from the empirical MCMC anlaysis.In the pps_analysis_JC.Rev script, that is conducted usingthe following lines of RevScript:    source(\"scripts/pps_Simulation.Rev\")Let’s take a look at each of the calls in thepps_Simulation.Rev script so that we can understandwhat is occurring.First, we read in the trace file of the Posterior Distribution ofVariables.    trace = readStochasticVariableTrace(\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.var\", delimiter=TAB)Now we call the posteriorPredictiveSimulation() function, whichaccepts any valid model of sequence evolution, and output directory, anda trace. For each line in the trace, it will simulate a new datasetunder the specified model.    pps = posteriorPredictiveSimulation(mymodel, directory=\"output\" + \"/\" + analysis_name + \"_post_sims\", trace)Now we run the posterior predictive simulation, generating a new datasetfor each line in the trace file that was read in. This is the part wherewe need to decide how many simulated datasets we want to generate. If wejust use the pps.run() command, one dataset will be generated for eachsample in our posterior distribution. In this case, since we are readingin the combined posterior trace file with 2000 samples, it would generate2000 simulated datasets. If you want to generate fewer, say 1000 datasets,you need to use the thinning argument as above. In this case, we arethinning the output by 2, that is we are dividing our number of samplesby 2. So that in our example case, we will end up simulating 1000 newdatasets.    pps.run(thinning=2)This process should finish in just a minute or two. If you look in theoutput_JC folder, there should be another folder calledpps_example_post_sims. This folder is where the simulateddatasets are saved. If you open it up, you should see 100 folders namedposterior_predictive_sim_N. Where N is the number of thesimulated dataset. In each of these folders, you should find a seq.nexfile. If you open one of those files, you’ll see it’s just a basic NEXUSfile. These will be the datasets we analyze in the next step.Calculating the Test StatisticsNow we will calculate the test statistics from the empirical data andthe simulated data sets. The part in the pps_analysis_JC.Revscript that generates the test statistics is the following line:    num_post_sims = listFiles(path=\"output_\"+model_name+\"/\" + analysis_name + \"_post_sims\").size()    source(\"scripts/pps_DataSummary.Rev\")We will look at the major concepts of thepps_DataSummary.Rev to better understand how itworks. For a more complete discussion of the statistics involved, pleasereview (Höhna et al. 2018). In general, this script and thesestatistics work by calculating the statistics of interest across eachposterior distribution from the simulated datasets, and comparing thosevalues to the values from the empirical posterior distribution.The current version of this script generates over 30 summary statisticsincluding:  Number Invariant Sites  Max Pairwise Difference  Max GC Content  Min GC Content  Mean GC ContentThere are functions built-in to ‘RevBayes‘ to calculate these values foryou. Here are some examples from thepps_DataSummary.Rev script:    max_pd          = sim_data.maxPairwiseDifference( excludeAmbiguous=FALSE )    mean_gc_1       = sim_data.meanGcContentByCodonPosition(1, excludeAmbiguous=FALSE )    var_gc_1        = sim_data.varGcContentByCodonPosition(1, excludeAmbiguous=FALSE )These same statistics are calculated for both the posteriordistributions from the simulated datasets and the posterior distributionfrom the empirical dataset.Calculating P-values and effect sizesOnce we have the test statistics calculated for the simulated andempirical posterior distributions, we can compare the simulated to theempirical to get a goodness-of-fit. One simple way to do this is tocalculate a posterior predictive Pvalue for each of the teststatistics of interest. This is done in thedata_pp_analysis_JC.Rev with the following lines :    emp_pps_file = \"results_\" + model_name + \"/empirical_data_\" + analysis_name + \".csv\"    sim_pps_file = \"results_\" + model_name + \"/simulated_data_\" + analysis_name + \".csv\"    outfileName = \"results_\" + model_name + \"/data_pvalues_effectsizes_\" + analysis_name + \".csv\"    source(\"scripts/pps_PValues.Rev\")The 4 posterior predictive P-values currently calculated by ‘RevBayes‘are:  Lower 1-tailed  Upper 1-tailed  2-tailed  MidpointThe posterior predictive P-value for a lower one-tailed test is theproportion of samples in the distribution where the value is less thanor equal to the observed value, calculated as:\\[p_l=p(T(X_{rep})\\leqslant T(X_{emp}))\\]The posterior predictive P-value for a lower one-tailed test is theproportion of samples in the distribution where the value is greaterthan or equal to the observed value, calculated as:\\[p_u=p(T(X_{rep})\\geqslant T(X_{emp}))\\]and the two-tailed posterior predictive P-value is simply twice theminimum of the corresponding one-tailed tests calculated as:\\[p_2=2min(p_l,p_u)\\]Finally, the midpoint posterior predictive P-value iscalculated as:\\[p_m=p(T(X_{rep})\\leqslant T(X_{emp})) + \\frac{1}{2} p(T(X_{rep})= T(X_{emp}))\\]For more a more detailed description please look at (Höhna et al. 2018).The pps_PValues.Rev scriptLet’s take a look at the pps_PValues.Rev script toget a better idea of what is happening.Using the equations outlined above, this script reads in the simulatedand empirical data we just calculated, and calls a few simple functionsfor each of the test statistics:    ## Calculate and return a vector of lower, equal, and upper pvalues for a given test statistic    p_values &lt;- posteriorPredictiveProbability(numbers, empValue)    ## 1-tailed    lower_p_value &lt;- p_values[1]     equal_p_value &lt;- p_values[2]     upper_p_value &lt;- p_values[3]     ## mid-point    midpoint_p_value = lower_p_value + 0.5*equal_p_value    ## 2-tailed    two_tail_p_value = 2 * (min(v(lower_p_value, upper_p_value)))these snippets calculate the various p-values of interest to serve asour goodness-of-fit test.Posterior predictive effect sizeAnother way that you can calculate the magnitude of the discrepancybetween the empirical and PP datasets is by calculating the effect sizeof each test statistic. Effect sizes are useful in quantifying themagnitude of the difference between the empirical value and thedistribution of posterior predictive values. The test statistic effectsize can be calculated by taking the absolute value of the differencebetween the median posterior predictive value and the empirical valuedivided by the standard deviation of the posterior predictivedistribution (missing reference). Effect sizes are calculated automaticallyfor the inference based test statistics in the P analysis. The effectsizes for each test statistics are stored in the same output file as theP-values.    effect_size = abs((m - empValue) / stdev(numbers))calculates the effect size of a given test statistic.Viewing the ResultsOnce execution of the script is complete, you should see a newdirectory, named results_JC. In this folder there should be 3files. Each of these is a simple comma-delimited (csv) file containing thetest statistic calculation output.  empirical_pps_example.csv  pps_example.csv  pvalues_pps_example.csvIf you have these 3 files, and there are results in them, you can goahead and quit ‘RevBayes‘.    q()The distribution of mean GC values calculatedfrom the simulated data set is shown. The dotted line represents themean GC calculated from the empirical data set.The Jukes-Cantor modeldoes not adequately describe the empirical data. This plot was generatedin Rusing the scripts/plot_results.R script.You can get an estimate of how the model performed by examining theP-values in the  data_pvalues_effectsizes_pps_example.csv file. In thisexample, a quick view shows us that most of the statistics show a valueless than 0.05. This leads to a rejection of the model, suggesting thatthe model employed is not a good fit for the data. This is the result weexpected given we chose a very simplistic model (JC) that would likelybe a poor fit for our data. However, it’s a good example of thesensitivity of this method, showing how relatively short runtimes and alow number of generations will still detect poor fit.You can also use Rto plot the results. Run theRscript scripts/plot_results.R. This will generate apdf plot for every test statistic. See  and its legend to learn how to interpret these plots.ExercisesIncluded in the scripts folder is a second model script calledpps_GTR_Model.Rev. As a personal exercise and a good test case, takesome time now, and run the same analysis, substituting thepps_GTR_Model.Rev model script for the pps_JC_Model.Rev script we usedin the earlier example. You should get different results, this is anexcellent chance to explore the results and think about what theysuggest about the fit of the specified model to the dataset.Additional ExercisesSome Questions to Keep in Mind:      Do you find the goodness-of-fit results to suggest that the GTR orJC model is a better fit for our data?        Which test statistics seem to show the strongest effect from the useof a poorly fitting model?  Batch Processing of Large DatasetsIn this tutorial you have learned how to use ‘RevBayes‘ to assess thefit of a substitution model to a given sequence alignment. As you havediscovered, the observed data should be plausible under the posteriorpredictive simulation if the model is reasonable. In phylogeneticanalyses we choose a model, which explicitly assumes that it provides anreasonable explanation of the evolutionary process that generated ourdata. However, just because a model may be the ’best’ model available,does not mean it is an appropriate model for the data. This distinctionbecomes both more critical and less obvious in modern analyses, wherethe number of genes often number in the thousands. Posterior predictivesimulation in ‘RevBayes‘, allows you to easily check model fit for alarge number of genes by using global summaries to check the posteriorpredictive distributions with a comfortable goodness-of-fit styleframework.The process described above is for a single gene or alignment. However,batch processing a large number of genes with this method is arelatively straight forward process.‘RevBayes‘ has built in support for MPI so running ‘RevBayes‘ on morethan a single processor, or on a cluster is as easy as calling it withopenmpi.For example:    mpirun -np 16 rb-mpi scripts/full_analysis.Revwould run the entire posterior predictive simulation analysis on asingle dataset using 16 processors instead of a single processor. Use ofthe MPI version of ‘RevBayes‘ will speed up the process dramatically.Setting up the full_analysis.Rev script to cycle through a largenumber of alignments is relatively simple as well. One easy way is toprovide a list of the data file names, and to loop through them. As anexample:    data_file_list = \"data_file_list.txt\"     data_file_list &lt;- readDiscreteCharacterData(data_file_list)    file_count = 0    for (n in 1:data_file_list.size()) {    FULL_ANALYSIS SCRIPT GOES HERE    file_count = file_count + 1    }Then, anywhere in the full analysis portion of the script that thelines    inFile = \"data/8taxa_500chars_GTR.nex\"    analysis_name = \"pps_example\"appear, you would replace them with something along the lines of:    inFile = n    analysis_name = \"pps_example_\" + file_countThis should loop through all of the data files in the list provided, andrun the full posterior predictive simulation analysis on each file.Using a method like this, and combining it with the MPI call above, youcan scale this process up to multiple genes and spread the computationaltime across several cores to speed it up.",
        "url": "/tutorials/model_testing_pps/pps_data.html",
        "index": "true"
      }
      ,
    
      "tutorials-model-testing-pps-pps-inference-html": {
        "title": "Assessing Phylogenetic Reliability Using RevBayes and $P^{3}$",
        "content": "OverviewThis tutorial presents the general principles of assessing thereliability of a phylogenetic inference through the use of posteriorpredictive simulation (PPS). PPS works by assessing the fit of anevolutionary model to a given dataset, and analyzing several teststatistics using a traditional goodness-of-fit framework to help explainwhere a model may be the most inadequate.PreparationThis tutorial expects you have compelted the prerequisite tutorials listed above. It will also expect you to be reasonably familiar with phylogenetic analyses, command line usage and if you want to explore your results, having at least a basic understanding of R. If you run this tutorial all the way through with the single full_analysis_JC.Rev script it takes approximately 20 - 25 minutes depending on your computer. If you work through every line-by-line to get a better understanding it takes approximately 60 minutes.IntroductionAssessing the fit of an evolutionary model to the data is critical asusing a model with poor fit can lead to spurious conclusions. However, acritical evaluation of absolute model fit is rare in evolutionarystudies. Posterior prediction is a Bayesian approach to assess the fitof a model to a given dataset(missing reference), that relies on theuse of the posterior and the posterior predictive distributions. Theposterior distribution is the standard output from Bayeisan phylogeneticinference. The posterior predictive distribution represents a range ofpossible outcomes given the assumptions of the model. The most commonmethod to generate these possible outcomes, is to sample parameters fromthe posterior distribution, and use them to simulate new replicatedatasets (Fig. 1). If these simulated datasets differ from the empiricaldataset in a meaningful way, the model is failing to capture somesalient feature of the evolutionary process.The framework to construct posterior predictive distributions, andcompare them to the posterior distribution is conveniently built in toRevBayes through the $P^{3}$ pipeline (missing reference). In this tutorial we will walk you through using thisfunctionality to perform a complete posterior predictive simulation onan example dataset.If you would like more information than we provide here, we would highly encourage you to read the $P^{3}$ manuscript for a deeper discussion of the full feature set.  Höhna S., Coghill L.M., Mount G.G., Thomson R.C., Brown J.M. 2017. P3: Phylogenetic Posterior Prediction in RevBayes. Molecular Biology and Evolution. 35:1028–1034. 10.1093/molbev/msx286 Data-Based Versus Inference-Based ComparisonsMost statistics proposed for testing model plausibility comparedata-based characteristics of the original data set to the posteriorpredictive data sets (e.g., variation in GC-content across species). Indata-based assessments of model fit one compares the empirical data todata simulated from samples of the posterior distribution. RevBayesadditionally implements test statistics that compare the inferencesresulting from different data sets (e.g., the distribution of posteriorprobability across topologies). These are called inference-basedassessments of model fit and will be the focus of this tutorial. For these assessments one must run an MCMCanalysis on each simulated data set, and then compare the inferencesmade from the simulated data to the inference made from the empiricaldata.Overview of the $P^{3}$ workflow as implemented in RevBayes together with the specific commands necessary in each step (missing reference). Step 1 involves sampling parameters, for example, tree topology and branch lengths, from the posterior distribution using MCMC simulation. Step 2 simulates new data sets given the parameter samples from step 1. Step 3 estimates posterior distributions of the same parameters but from the simulated data sets. This third step is optional and is only needed for inference-based test statistics. Next, step 4 involves computing data- and/or inference-based test statistics and comparing the distribution of the test statistic from the simulated data with the test statistic value from the observed data. Finally, data sets and models can be rejected or ranked based on the posterior predictive p-values or the posterior predictive effect size (PPES), which is the difference between the median of the posterior predictive distribution and the empirical value, normalized by the distribution’s SD.Due to time constraints, in today’s tutorial we will only cover theinference-based method of assessing model plausibility. The data-basedmethod can be a powerful tool that you may want to explore at anothertime by visiting that tutorial.Substitution ModelsThe models we use here are equivalent to the models described in theprevious exercise on substitution models (continuous time Markovmodels). To specify the model please consult the previous exercise.Specifically, you will need to specify the following substitutionmodels:      Jukes-Cantor (JC) substitution model (Jukes and Cantor 1969)        General-Time-Reversible (GTR) substitution model (Tavaré 1986)  Assessing Model Fit with Posterior Prediction  For your info  The entire process of posterior prediction can be executed by using thefull_analysis_JC.Rev script in the scripts folder. If youwere to type the following command into RevBayes:  source(\"scripts/data_pp_analysis_JC.Rev\")    the entire data-based posterior prediction process would run on theexample dataset. However, in this tutorial, we will walk through eachstep of this process on an example dataset. As we follow along,you will want to execute the commands in the light blue boxes with black text only. The light blueboxes with grey text will show output from the commands, the grey boxes with white background will highlightpieces of scripts that are helpful, but don’t need to be executed.Empirical MCMC AnalysisTo begin, we first need to generate a posterior distribution from whichto sample for simulation. This is the normal, and often only, stepconducted in phylogenetic studies. Here we will specify our dataset,evolutionary model, and run a traditional MCMC analysis.This code is all in the full_analysis_JC.Rev file.Set up the workspaceFirst, let’s read in our dataset.## EMPIRICAL MCMCinFile = \"data/primates_cytb.nex\"data &lt;- readDiscreteCharacterData(inFile)   Successfully read one character matrix from file data/primates_and_galeopterus_cytb.nexGreat, we have our data in RevBayes now. Next, we’ll set up some workspace variables we will need. We can begin by specifying ageneral name to apply to your analysis. This will be used for futureoutput files, so make sure it’s something clear and easy to understand.analysis_name = \"pps_example\"model_name = \"JC\"model_file_name = \"scripts/\"+model_name+\"_Model.Rev\"Specify the modelNow we’ll need to specify our model Jukes-Cantor in this case. We can do this bysourcing the name we gave our model script a few lines ago.source( model_file_name )   Processing file \"scripts/JC_Model.Rev\"   Processing of file \"scripts/JC_Model.Rev\" completed Now we have our data and our model for our empirical analyses read into RevBayes.This command will process the JC_Model.Rev script in full. If you want to tweak any of the model parameters, or just want a reminder about how model scripts in RevBayes work,you can dig a little deeper in the aside box below or revist the Nucleotide substitution models.JC Model DetailsTaking a brief look at some of the details of the JC_Model.Rev script.A few specific lines we can look at that might be interest, as we areusing a unrooted tree for this analysis are:#### specify the Jukes-Cantor substitution model applied uniformly to all sites ###Q := fnJC(4) Here we are specifying that we should use the Jukes-Cantor model, andhave it applied uniformly to all sites in the dataset. While thisobviously is not likely to be a good fitting model for most datasets, weare using it for simplicity of illustrating the process.#### Specify a uniform prior on the tree topology #### This sets a uniform prior on the tree topology.  # We use here the exponential distribution with rate 1.0 as the branch length prior  br_lens[i] ~ dnExponential(10.0)This sets an exponential distribution as the branch length prior.# Build the tree by combining the topology with the branch length.phylogeny := treeAssembly(topology, br_lens)This builds the tree by combining the topology with branch lengthsupport values.Run the MCMCNow let’s run MCMC on our empirical dataset, just like a normalphylogenetic analysis. We need to set some variables:First, we need to setup a counter for our monitors:## set a monitor countermni = 0Next, we need to setup the monitors themselves:## setup our monitorsmonitors[++mni] = mnModel(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.log\",printgen=100, separator = TAB)monitors[++mni] = mnFile(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.trees\",printgen=100, separator = TAB, phylogeny)monitors[++mni] = mnScreen(printgen=100, TL)monitors[++mni] = mnStochasticVariable(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.var\",printgen=100)Next, we need to setup the MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2)mymcmc.burnin(generations=2000,tuningInterval=10)   Running burn-in phase of Monte Carlo sampler for 2000 iterations.   This simulation runs 2 independent replicates.   The simulator uses 23 different moves in a random move schedule with 23 moves per iterationProgress:0---------------25---------------50---------------75--------------100********************************************************************Finally, we can start the MCMC run:## start the runmymcmc.run(generations=10000)   Running MCMC simulation   This simulation runs 2 independent replicates.   The simulator uses 45 different moves in a random move schedule with 45 moves per iterationIter        |      Posterior   |     Likelihood   |          Prior   |             TL   |    elapsed   |        ETA   |-----------------------------------------------------------------------------------------------------------------------0           |       -16086.3   |       -16099.8   |        13.4989   |       3.139019   |   00:00:00   |   --:--:--   |100         |       -16088.8   |       -16101.1   |        12.3286   |       3.256052   |   00:00:01   |   --:--:--   |...-----------------------------------------------------------------------------------------------------------------------10000       |       -16085.4   |       -16099.3   |        13.8874   |        3.10017   |   00:01:12   |   00:00:00   |Here we are only using a small number of generations for the tutorial,however with empirical data you will most likely need a much largernumber of generations to get a well mixed sample. The number ofgenerations and printed generations is important to consider here for avariety of reasons, in particular for posterior predictive simulation.When we simulate datasets in the next step, we can only simulate 1dataset per sample in our posterior. So, while the number of posteriorsamples will almost always be larger than the number of datasets we willwant to simulate, it’s something to keep in mind.MCMC outputAfter the process completes, the results can be found in theoutput_JC folder. You should see a number of familiar lookingfiles, all with the name we provided under the analysis_namevariable, pps_example in this case. Since we set the number of runs(nruns=2) in our MCMC, there will be two files of each type (.log .trees.var) with an _N where N is the run number. You will also see 3files without any number in their name. These are the combined files ofthe output. These will be the files we use for the rest of the process.If you open up one of the combined .var file, you should see that thereare 200 samples. This was produced by our number of generations(10000) divided by our number of printed generations, (100) what wespecified earlier, combined from the two independent runs. This is important to note, as we will need to thinthese samples appropriately in the next step to get the proper number ofsimulated datasets.Posterior Predictive Data SimulationThe next step of posterior predictive simulation is to simulate newdatasets by drawing samples and parameters from the posteriordistribution generated from the empirical MCMC anlaysis. This functionality isbuilt into the backend of RevBayes in order to simplify the process.  In the full_analysis_JC.Rev script, that is conducted usingthe following line of RevScript:&gt; source(\"scripts/PosteriorPredictive_Simulation.Rev\")However, we will process each line of this script so that you can better understand what functions are being called.First, we read in the trace file of the Posterior Distribution ofVariables.## Reading Tracetrace = readStochasticVariableTrace(\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.var\", delimiter=TAB)Now we call the posteriorPredictiveSimulation() function, whichaccepts any valid model of sequence evolution, and output directory, anda trace. For each line in the trace, it will simulate a new datasetunder the specified model.## Creating Posterior Predictive Simulationpps = posteriorPredictiveSimulation(mymodel, directory=\"output_\" + model_name + \"/\" + analysis_name + \"_post_sims\", trace)Now we run the posterior predictive simulation, generating a new datasetfor each line in the trace file that was read in. This is the part wherewe need to decide how many simulated datasets we want to generate. If wejust use the pps.run() command, one dataset will be generated for eachsample in our posterior distribution. In this case, since we are readingin the combined posterior trace file with 200 samples, it would generate200 simulated datasets. If you want to generate fewer, say 100 datasets,you need to use the thinning argument as above. In this case, we arethinning the output by 2, that is we are dividing our number of samplesby 2. So that in our example case, we will end up simulating 100 newdatasets.## Running the posterior predictive simulation, here we are thinning the datasets by halfpps.run(thinning=2)This process should finish in just a minute or two. If you look in theoutput_JC folder, there should be another folder calledpps_example_post_sims. This folder is where the simulateddatasets are saved. If you open it up, you should see 100 folders namedposterior_predictive_sim_N. Where N is the number of thesimulated dataset. In each of these folders, you should find a seq.nexfile. If you open one of those files, you’ll see it’s just a basic NEXUSfile. These will be the datasets we analyze in the next step.Posterior Predictive MCMC AnalysisThe next step of posterior predictive simulation is to conduct a full MCMC analysis on each of the new simulated datasets. This is by far, the most computationally intensive partof this process, as you are conducting 100 individual MCMC analyses. In a later section, we outlinehow this can be parallelized and sped up dramatically on HPC clusters or servers. However, for now, we will run this locally in serial mode with a low number of generations as an example.In the full_analysis_JC.Rev script, that is conducted usingthe following line of RevScript:source(\"scripts/PosteriorPredictive_MCMC.Rev\")However, we will process each line of this script so that you can better understand what functions are being called.First, we need to specify a counter for our monitors:## first we set a counter for our monitorsmni = 0Next, we need to specify a couple of monitors for our MCMC moves:## next we setup our monitors, like in our previous MCMC analysesmonitors[++mni] = mnModel(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.log\",printgen=100, separator = TAB)monitors[++mni] = mnFile(filename=\"output_\" + model_name + \"/\" + analysis_name + \"_posterior.trees\",printgen=100, separator = TAB, topology)Now we can setup our individual MCMC objects:## now we setup our MCMC object for each posterior predictive datasetmymcmc = mcmc(mymodel, monitors, moves, nruns=1)Next, specify a directory for our output files:## specify and output directory for each analysisdirectory = \"output_\" + model_name + \"/\" + analysis_name + \"_post_sims\"This step is a little different from previous MCMC setups. Here we are create a new pps_mcmc object for all of our individual analyses:## specify a new pps_mcmc object for all of the independent analysesmy_pps_mcmc = posteriorPredictiveAnalysis(mymcmc, directory)Finally, we run the 100 individual MCMC analyses:## run the actual analysesmy_pps_mcmc.run(generations=10000)Running posterior predictive analysis ...Sim   1 / 100Iter        |      Posterior   |     Likelihood   |          Prior   |             TL   |    elapsed   |        ETA   |-----------------------------------------------------------------------------------------------------------------------0           |       -9345.75   |       -9353.53   |        7.77435   |       2.322464   |   00:00:00   |   --:--:--   |100         |   9.04115e+271   |              0   |   9.04115e+271   |       2.322464   |   00:00:00   |   --:--:--   |Now that we have a full set of MCMC analyses for our simulated data, we can move on to calculating test statistics.Calculating the Test StatisticsNow we will calculate the test statistics from the empirical data andthe simulated data sets. Let’s go ahead and calculate our test statistics by entering the following lines into RevBayes.num_post_sims = listFiles(path=\"output_\"+model_name+\"/\" + analysis_name + \"_post_sims\").size()source(\"scripts/PosteriorPredictive_TreeSummary.Rev\")Progress:0---------------25---------------50---------------75--------------100********************************************************************   Processing file \"output_JC/posterior_predictive_sim_1/pps_example_posterior.trees\"...   Processing file \"output_JC/pps_example_posterior.trees\"Progress:0---------------25---------------50---------------75--------------100********************************************************************These are the same lines used in the full_analysis_JC.Rev script.We will take a closer look at these lines but for a more complete discussion of the statistics involved, pleasereview (Brown 2014) (Doyle et al. 2015). In general, this script and thesestatistics work by calculating the statistics of interest across eachposterior distribution from the simulated datasets, and comparing thosevalues to the values from the empirical posterior distribution.The current version of this script generates several summary statisticsincluding:      Various quantiles        Mean RF        Mean tree length        Tree length variance        Entropy  Example topological test statistic calculations from a posterior distribution (Brown 2014). In this hypothetical scenario, four unique topologies were found in 100 MCMC samples from a Bayesian analysis. The prior (uniform) and estimated posterior probabilities are given next to each unique topology. Bidirectional arrows are labeled with the symmetric (Robinson–Foulds) distance between topologies, along with the number of times this distance will be included in the vector of all pairwise distances between posterior samples.Since these values are calculated by iterating over the entire series of posterior predictive analysesthey are a bit unweildy to run line by line. However, we will talk about some of the individual functionsand how they work so if you wish you can develop your own test statistics at a later time.Here are some examples of these functions from thePosteriorPredictive_TreeSummary.Rev script:  This functions calculates pairwise RF distances between all trees in a tree trace.rf_dists &lt;- sim_tree_trace.computePairwiseRFDistances(credibleTreeSetSize=1.0,verbose=FALSE)  This function collects the tree lengths of all trees in a tree trace, these values are used for the tree length and variance statistics.tree_length &lt;- sim_tree_trace.computeTreeLengths()  This function calculates the entropy statistic for all trees in a given tree trace. Entropy is analogous to the uncertainty associated with sampling from the prior or posterior. As the data provide more information and the posterior probabilities of various topologies become increasingly uneven, entropy decreases causing the difference between the entropy of the prior and the posterior to increase. Equation $\\ref{equation1}$.entropy &lt;- sim_tree_trace.computeEntropy(credibleTreeSetSize=1.0,numTaxa=data.ntaxa(),verbose=FALSE)Assuming a uniform prior on topologies, the change in entropy can be calculated as:\\[T_e(X,M_c)=ln[B(N)]+\\sum_{i=1}^{B(N)} p(\\tau_i|X)ln[p(\\tau_i|X)] \\label{equation1}\\tag{1}\\]These same statistics are calculated for both the posterior distributions from the simulated datasets and the posterior distribution from the empirical dataset. This framework is designed to be flexible, so if in the future you can imagine a test statistic you think would be informative for your model,it is easy to expand these calculations in RevScript.Calculating P-values and effect sizesOnce we have the test statistics calculated for the simulated andempirical posterior distributions, we can compare the simulated to theempirical to get a goodness-of-fit. One simple way to do this is tocalculate a posterior predictive P-value for each of the teststatistics of interest. This is done in thefull_analysis_JC.Rev with the following lines :First, we set our input file names.emp_pps_file = \"results_\" + model_name + \"/empirical_inference_\" + analysis_name + \".csv\"sim_pps_file = \"results_\" + model_name + \"/simulated_inference_\" + analysis_name + \".csv\"Next, we set our output file name.outfileName = \"results_\" + model_name + \"/inference_pvalues_effectsizes_\" + analysis_name + \".csv\"Now, let’s create a vector to hold the results for faster calculations:statID = v(\"\", \"mean_rf\", \"quantile25\", \"quantile50\", \"quantile75\", \"quantile99\", \"quantile999\", \"mean_tl\", \"var_tl\", \"entropy\")We will talk about the individual calculations shortly, but for now, let’s go ahead and calculate allof our P-values for our test statistics.Finally, we source the PosteriorPredictive_PValues.Rev script to run the calculations.source(\"scripts/PosteriorPredictive_PValues.Rev\")   Processing file \"scripts/PosteriorPredictive_PValues.Rev\"   Processing of file \"scripts/PosteriorPredictive_PValues.Rev\" completedThis script will calculate 3 posterior predictive P-values:      Lower 1-tailed (Equation $\\ref{equation2}$)        Upper 1-tailed (Equation $\\ref{equation3}$)        2-tailed (Equation $\\ref{equation4}$)  The posterior predictive P-value for a lower one-tailed test is theproportion of samples in the distribution where the value is less thanor equal to the observed value, calculated as:\\[p_l=p(T(X_{rep})\\leqslant T(X)|(X) \\label{equation2}\\tag{2}\\]The posterior predictive P-value for an upper one-tailed test is theproportion of samples in the distribution where the value is greaterthan or equal to the observed value, calculated as:\\[p_u=p(T(X_{rep})\\geqslant T(X)|(X) \\label{equation3}\\tag{3}\\]and the two-tailed posterior predictive P-value is simply twice theminimum of the corresponding one-tailed tests calculated as:\\[p_2=2min(p_l,p_u) \\label{equation4}\\tag{4}\\]Let’s take a look at the PosteriorPredictive_PValues.Rev script toget a better idea of what is happening.  Calculates and returns a vector of lower, equal, and upper P-values for a given test statistic as outlined above in equation X.p_values &lt;- posteriorPredictiveProbability(numbers, empValue)  Calculates the midpoint P-value as outlined above in equation X.midpoint_p_value = lower_p_value + 0.5*equal_p_value  Calculates the two-tailed P-value as outlined above in equation X.two_tail_p_value = 2 * (min(v(lower_p_value, upper_p_value)))Another way that you can calculate the magnitude of the discrepancybetween the empirical and PP datasets is by calculating the effect sizeof each test statistic. Effect sizes are useful in quantifying themagnitude of the difference between the empirical value and thedistribution of posterior predictive values. The test statistic effectsize can be calculated by taking the absolute value of the differencebetween the median posterior predictive value and the empirical value, anddividing it by the standard deviation of the posterior predictivedistribution (Doyle et al. 2015). Effect sizes are calculated automaticallyfor the inference based test statistics in the $P^{3}$ analysis. The effectsizes for each test statistics are stored in the same output file as theP-values.  The line:effect_size = abs((m - empValue) / stdev(numbers))  calculates the effect size of a given test statistic.Now that we have all of these values calculated, we can visualize them in any number of ways.Visualizing the ResultsVisualizing the Results with RAll of the output from these analyses are saved to simple CSV text files. You can visualize your distributions and the fit of theempirical values to the distribution of simulated values in anyway that is intuitive for you.Example distribution of simulated pps test statistic values with the median value plotted as the green line and the empirical value plotted as the red line. In this case, this test statistic suggests that the model is a poor fit for the data.Let’s walk through visualizing one of the test statistic datasets in a very basic R script so you can get a feel for one way it could be done.Our data from this analyses is all stored in the results_JC/ folder. So it would be helpful to set your R workspace to that folder.setwd(\"/YOUR/PATH/HERE/results_JC\")Once that’s done, the first thing we’ll want to do is to read in our empirical values.empirical_inference_pps_example &lt;- read.csv(\"/results_JC/empirical_inference_pps_example.csv\", header=TRUE)Next, we’ll read in the simulated values.simulated_inference_pps_example &lt;- read.csv(\"/results_JC/simulated_inference_pps_example.csv\", header=TRUE)Once we have our two datasets imported into R, we can easily create a plot like the example plot above. For this example, we’ll plot themean tree length (mean_tl) value from these analyses.First, let’s create our histogram plot.hist(simulated_inference_pps_example$mean_tl, breaks=20)Next, let’s add a line showing the median just for a reference.abline(v=median(simulated_inference_pps_example$mean_tl), col=\"green\", lty=2)Finally, let’s plot our empirical value to get a feel for how it compares to our simulated values.abline(v=empirical_inference_pps_example$mean_tl, col=\"red\", lty=2)Mean Tree Length test statistic distribution from posterior predictive simulation analyses of the primates_cytb.nexus dataset. Green line is the median value of the simulated dataset, red line is the empirical value.Additional Individual ExercisesIncluded in the scripts folder is a second model script calledGTR_Model.Rev. As a personal exercise and a good test case, takesome time now, and run the same analysis, substituting theGTR_Model.Rev model script for the JC_Model.Rev script we usedin the earlier example. In order to speed this process up, you can duplicate thefull_analysis_JC.Rev and name the new copy full_analysis_GTR.Rev. You can then justedit the lines in that one script to point to the GTR_Model.Rev script, and re-run your with a single command:source(\"scripts/full_analysis_GTR.Rev\")You should get different results, this is anexcellent chance to explore the results and think about what theysuggest about the fit of the specified model to the dataset.Test of GTR Model AdequacySome Questions to Keep in Mind:      Do you find the goodness-of-fit results to suggest that the GTR orJC model is a better fit for our data?        Which test statistics seem to show the strongest effect from the useof a poorly fitting model?        Other than P-values, what other ways might you explore the teststatistic distributions to identify poor fit?    For your consideration  In this tutorial you have learned how to use RevBayes to assess thefit of a substitution model to a given sequence alignment. As you havediscovered, the observed data should be plausible under the posteriorpredictive simulation if the model is reasonable. In phylogeneticanalyses we choose a model, which explicitly assumes that it provides anreasonable explanation of the evolutionary process that generated ourdata. However, just because a model may be the ’best’ model available,does not mean it is an appropriate model for the data. This distinctionbecomes both more critical and less obvious in modern analyses, wherethe number of genes often number in the thousands. Posterior predictivesimulation in RevBayes, allows you to easily check model fit for alarge number of genes by using global summaries to check the posteriorpredictive distributions with a comfortable goodness-of-fit styleframework.Batch Processing of Large DatasetsThe process described above is for a single gene or alignment. However,batch processing a large number of genes with this method is arelatively straight forward process.RevBayes has built in support for MPI so running RevBayes on morethan a single processor, or on a cluster is as easy as calling it withopenmpi.For example:mpirun -np 16 rb-mpi scripts/full_analysis.Revwould run the entire posterior predictive simulation analysis on asingle dataset using 16 processors instead of a single processor. Use ofthe MPI version of RevBayes will speed up the process dramatically.Setting up the full_analysis.Rev script to cycle through a largenumber of alignments is relatively simple as well. One easy way is toprovide a list of the data file names, and to loop through them. As anexample:data_file_list = \"data_file_list.txt\" data_file_list &lt;- readDiscreteCharacterData(data_file_list)file_count = 0for (n in 1:data_file_list.size()) {FULL_ANALYSIS SCRIPT GOES HEREfile_count = file_count + 1}Then, anywhere in the full_analysis portion of the script that thelinesinFile = \"data/8taxa_500chars_GTR.nex\"analysis_name = \"pps_example\"appear, you would replace them with something along the lines of:inFile = nanalysis_name = \"pps_example_\" + file_countThis should loop through all of the data files in the list provided, andrun the full posterior predictive simulation analysis on each file.Using a method like this, and combining it with the MPI call above, youcan scale this process up to multiple genes and spread the computationaltime across several cores to speed it up.",
        "url": "/tutorials/model_testing_pps/pps_inference.html",
        "index": "true"
      }
      ,
    
      "jobs-programmer-position-2018-sebastian-html": {
        "title": "Programmer position&amp;#58; Supporting development of RevBayes",
        "content": "",
        "url": "/jobs/programmer_position_2018_sebastian.html",
        "index": ""
      }
      ,
    
      "tutorials-recommended": {
        "title": "Recommended Software",
        "content": "For our tutorials we recommend that you download and install the latest releaseof RevBayes (Höhna et al. 2016), which is available for Mac OS X, Windows,and Linux operating systems. Directions for downloading and installingthe software are available on the program webpage:http://revbayes.com. The exercises provided often alsorequire additional programs for editing text files and visualizingoutput. The following are very useful tools for working with RevBayes:      A good text editor – if you do not already have one that you like,  we recommend one that has features for syntax coloring, easy  navigation between different files, line numbers, etc. Good options  include Sublime Text,   Atom or NotePad++, which are available for Mac OSX, Windows,  and Linux.        Tracer – for  visualizing and assessing numerical parameter samples from  RevBayes        FigTree – a tree visualization program        Some of the output can be automatically visualized using pre-made   R functions provided by   our R-package RevGadgets.   You should make sure that you have a recent release of   R and   RevGadgets installed.        IcyTree – a web-hosted  phylogenetic tree visualization tool that is supported for  Firefox or  Google Chrome browsers  ",
        "url": "/tutorials/recommended",
        "index": ""
      }
      ,
    
      "tutorials-dating-relaxed-html": {
        "title": "Molecular dating",
        "content": "Exercise 2In exercise 1 we inferred the phylogeny of living bears and estimated relative node ages assuming a global molecular clock model.In this exercise we will relax the assumption of constant rates across the tree and use a relaxed molecular clock model that allows each branch in the tree to have an independent rate.A graphical model of the uncorrelated exponential relaxed clock model. In this model, the clock rate on each branch is independent and identically distributed according to an exponential density with mean drawn from an exponential hyperprior distribution.The dataThe data used in this exercise is the same as in the previous exercise (bears_cytb.nex). We will also use the same tree model (the birth-death process model) and the same substitution model (the GTR + $\\Gamma$ model).There are just three steps you need to complete before running the analysis in this exercise. First, we need to create a script for the relaxed clock model.Second, we need to switch out the global clock model for the relaxed clock model in our master script and we need to update the name of the output files, so we don’t overwrite the output generated in the previous exercise.The clock modelRemember the clock (or branch-rate) model describes how rates of substitution vary (or not) across the tree.  Create a script called clock_relaxed_exponential.Rev and open it in your text editor.We are going to use the uncorrelated exponential relaxed clock model. In this model rates for each branch will be drawn independently from an exponential distribution.It’s a bit more tricky to set up this clock model. First, we’ll define the mean branch rate as an exponential random variable (branch_rates_mean). Then, specify a scale proposal move on this parameter.branch_rates_mean ~ dnExponential(10.0)moves.append( mvScale(branch_rates_mean, lambda=0.5, tune=true, weight=3.0) )Before creating a rate parameter for each branch, we need to define the number of branches in the tree. For rooted trees with $n$ taxa, the number of branches is $2n−2$.n_branches &lt;- 2 * n_taxa - 2Then, use a for loop to define a rate for each branch. The branch rates are independent and identically exponentially distributed with mean equal to the mean branch rate parameter we specified above. For each rate parameter we will also create scale proposal moves.for(i in 1:n_branches){    branch_rates[i] ~ dnExp(1/branch_rates_mean)    moves.append( mvScale(branch_rates[i], lambda=0.5, tune=true, weight=1.0) )}Note that now we have a vector of rates branch_rates, where each entry corresponds to a different branch in the tree, instead of a single rate that applies to all branches.\tLastly, we will use two more specific moves to help improve MCMC convergence.First, we will use a vector scale move to propose changes to all branch rates simultaneously. This way we can sample the total branch rate independently of each individual rate, which can improve mixing.Second, we will use a move (mvRateAgeBetaShift) that changes the node ages and branch rates jointly,so that the effective branch length (the product of branch time and branch rate) remains the same.Thus, the move is proposing values with the same likelihood but a different prior probability.moves.append( mvVectorScale(branch_rates, lambda=0.5, tune=true, weight=4.0) )moves.append( mvRateAgeBetaShift(tree=timetree, rates=branch_rates, tune=true, weight=n_taxa) )The master Rev script  Copy the master script from the previous exercise and call it MCMC_dating_ex2.Rev.First, change the file used to specify the clock model from clock_global.Rev to clock_relaxed_exponential.Rev.source(\"scripts/clock_relaxed_exponential.Rev\") # Relaxed exponential clock modelSecond, update the name of all the output files.monitors.append( mnModel(filename=\"output/bears_relaxed_exponential.log\", printgen=10) )monitors.append( mnFile(filename=\"output/bears_relaxed_exponential.trees\", printgen=10, timetree) )Don’t forget to update the commands used to generate the summary tree.trace = readTreeTrace(\"output/bears_relaxed_exponential.trees\")mccTree(trace, file=\"output/bears_relaxed_exponential.mcc.tre\" )That’s all you need to do!  Run your MCMC analysis!Examining the outputLet’s compare the output from the two different clock models in Tracer.  Open the program Tracer and load the log files bears_global.log and bears_relaxed_exponential.log.You may notice that convergence isn’t as good for this analysis, which is probably caused by having a larger number of parameters.Have a look at the estimate for the mean branch-rate parameter (branch_rates_mean) in comparison to the estimate recovered in the previous analysis assuming a global molecular clock (branch_mean). You’ll notice that median estimates for these parameters differ quite a bit.In Tracer you can also highlight multiple parameters simultaneously, by using the shift key. Have a look at the rates obtained across different branches. You can see that there appears to be signal in the data for variation in rates along different branches.The estimates panel in Tracer showing the rates estimated for different branches.You can also compare the same parameters estimated using different models by selecting multiple trace files.The estimates panel in Tracer showing the likelihood estimates obtained using two different clock models.Note that the likelihood obtained using the relaxed clock model is higher than for the global clock model, which hints that it might be a better fit to our data. However, this analysis has quite a lot of additional parameters (i.e. one addition rate parameter for each branch). To work out whether this model really is more appropriate for our data, we would need to use a more robust model testing approach. For more on this topic see Tracy Heath’s tutorial Relaxed Clocks &amp; Time Trees.Scroll through the other parameter estimates and see if you can spot any differences.The tree outputLet’s also have a quick look at the trees.The FigTree window. To open your tree you can use File &gt; Open. Select Node Labels to view the relative node ages.If you open the trees generated using the global versus relaxed clock models in FigTree, you can compare them to see whether these models made an important difference to the inferred topology and/or relative node ages. Another useful thing to look at are the posterior probabilities obtained for different nodes. Go to the options under Node Labels and select Display &gt; posterior.ExerciseWe have seen above how to specify the UCE (uncorrelated exponential) clock model.For this exercise, we want you the change the UCE clock model into a UCLN clock model (uncorrelated relaxed clock).That means, we will need to replace the prior on the branch_rates so that they are drawn from a lognormal distribution.  Copy the script called clock_relaxed_exponential.Rev, name it clock_relaxed_lognormal.Rev and open it in your text editor.Since the lognormal distribution is parameterized by the log of the mean, we transform first the mean into the log mean.ln_branch_rates_mean := ln( branch_rates_mean )And then we’ll set the standard deviation of the lognormal distributionclock_relaxed_lognormal.RevNow we can replace the for-loop and specify that we use a lognormal distributionfor(i in 1:n_branches){    branch_rates[i] ~ dnLognormal(ln_branch_rates_mean,sd=branch_rates_sd)    moves.append( mvScale(branch_rates[i], lambda=0.5, tune=true, weight=1.0) )}Next, we are ready to set up the master script to run the analysis.  Copy the master script from the previous exercise and call it MCMC_dating_ex2b.Rev.Change the file used to specify the clock model from clock_relaxed_exponential.Rev to clock_relaxed_lognormal.Rev.source(\"scripts/clock_relaxed_lognormal.Rev\") # Relaxed exponential clock modelDon’t forget to update the filenames of the output (e.g., from bears_relaxed_exponential to bears_relaxed_lognormal).  Run your MCMC analysis!Next  Click below to begin the next exercise!  Estimating speciation times using node datingFor further options and information about clock models see Tracy Heath’s tutorial Relaxed Clocks &amp; Time Trees.",
        "url": "/tutorials/dating/relaxed.html",
        "index": "false"
      }
      ,
    
      "tutorials-cont-traits-relaxed-bm-html": {
        "title": "Relaxed Brownian Rate Estimation",
        "content": "Estimating Branch-Specific Rates of EvolutionThis tutorial demonstrates how to specify a relaxed morphological clock model for continuous characters. Specifically, we will specify a ‘‘random local clock’’ model, which allows for a small number of shifts in rates across branches. We provide the probabilistic graphical model representation of each component for this tutorial. After specifying the model, you will estimate the branch-specific rates of Brownian-motion evolution using reversible-jump Markov chain Monte Carlo (rjMCMC).Relaxing the Morphological ClockUnder a simple Brownian-motion (BM) model, the evolution of a continuous character is entirely determined by a single rate parameter, $\\sigma^2$. We can allow rates of evolution to vary among branches of the phylogeny by letting each branch have its own rate parameter, $\\sigma^2_i$. However, there will usually be insufficient information in a single continuous character to estimate each branch-specific rate as a free parameter. Therefore, in a Bayesian setting, we specify a ‘‘relaxed morphological clock’’ prior model that strikes a balance between biological realism (rates vary) and statistical reliability (there is only so much information to go around).Here, we will use the ‘‘relaxed local clock’’ model described by Eastman et al. (2011), known as AUTEUR. In this model, we assume that each branch in the phylogeny either does or does not have a rate shift. When there is no rate shift on a branch, the rate on the branch is inherited directly from its ancestral branch; when there is a rate shift, the ancestral rate is multiplied by a rate shift parameter that is drawn from a prior distribution. We specify a prior probability, $p$, that a given branch experiences a rate shift. For a tree with $n$ branches, the expected number of rate shifts is $E(k) = n \\times p$. To control the number of rate shifts, we specify a prior on the expected number of rate shifts, $E(k)$, and then calculate the prior probability for a rate shift on a particular branch, $p = E(k) / n$. The graphical model shows the relationship between the branch-specific rates and priors (fig_bm_relaxed_gm).The graphical model representation of the relaxed Brownian-motion (BM) process under the random local clock. For more information about graphical model representations see Höhna et al. (2014).In this tutorial, we use the primates dataset and log-transformed female body mass to estimate branch-specific rates of body-size evolution.⇨ The full relaxed BM-model specification is in the file called mcmc_relaxed_BM.Rev.Read the dataWe begin by deciding which of the traits to use. Here, we assume we are analyzing the first trait (female body mass), but you should feel free to choose any of the trait.trait &lt;- 1Now, we read in the (time-calibrated) tree corresponding to our chosen dataset.T &lt;- readTrees(\"data/primates_tree.nex\")[1]We also want to keep track of the number of branches for our relaxed clock model.ntips     &lt;- T.ntips()nbranches &lt;- 2 * ntips - 2Next, we read in the character data for the same dataset.data &lt;- readContinuousCharacterData(\"data/primates_cont_traits.nex\")We have to exclude all other traits that we are not interested in and only include our focal trait.This can be done in RevBayes using the member methods .excludeAll() and .includeCharacter().data.excludeAll()data.includeCharacter( trait )Additionally, we initialize a variable for our vector ofmoves and monitors:moves    = VectorMoves()monitors = VectorMonitors()Specifying the modelTree modelIn this tutorial, we assume the tree is known without area. We create a constant node for the tree that corresponds to the observed phylogeny.tree &lt;- TRate parameter at the rootThe relaxed BM model places a prior on the rate at the root of the tree, $\\sigma^2$. We draw this rate parameter from a loguniform prior. This prior is uniform on the log scale, which means that it is represents ignorance about the order of magnitude of the rate at the root of the tree. However, you should be careful in specifying the boundaries for this parameter, as it strongly depends on your specific trait. If you notice that your parameters are stuck at one boundary of the prior, then come back here and modify your prior range.sigma2_root ~ dnLoguniform(1e-5, 1e-1)Because $\\sigma^2_R$ is a rate parameter, and must therefore be positive, we use a scaling move called mvScale.moves.append( mvScale(sigma2_root, weight=1.0) )Relaxed clock modelWe begin the specification of the relaxed clock model by specifying a prior on the probability of rate shifts for each branch. We parameterize our prior expected number of rate shifts, and then compute the probability of a shift on a given branch.expected_number_of_shifts &lt;- 5rate_shift_probability    &lt;- expected_number_of_shifts / nbranchesNext, we specify the prior distribution on the size of rate shifts (when they occur). We draw each rate shift from a lognormal distribution with a median of 1, and a standard deviation such that rate shifts range over about one order of magnitude.sd = 0.578rate_shift_distribution = dnLognormal(0, sd)Now, we loop over each branch, drawing a rate-shift multiplier from a mixture distribution. This mixture distribution places prior probability $p$ on the rate multiplier being drawn from the lognormal distribution we just specified, and prior probability $1 - p$ on the rate shift being exactly equal to 1 (i.e., no rate shift). We then compute the rate on the branch by multiplying the ancestral rate by the rate shift multiplier. Note that we loop over the branches in reverse order; this ensures that the ancestral rate exists when we specify the rate for a given branch.for(i in nbranches:1) {    # draw the rate multiplier from a mixture distribution    branch_rate_multiplier[i] ~ dnReversibleJumpMixture(constantValue=1, rate_shift_distribution, p=Probability(1 - rate_shift_probability) )    # compute the rate for the branch    if ( tree.isRoot( tree.parent(i) ) ) {       branch_rates[i] := sigma2_root * branch_rate_multiplier[i]    } else {       branch_rates[i] := branch_rates[tree.parent(i)] * branch_rate_multiplier[i]    }    # keep track of whether the branch has a rate shift    branch_rate_shift[i] := ifelse( branch_rate_multiplier[i] == 1, 0, 1 )    # use reversible-jump to move between models with and without    # shifts on the branch    moves.append( mvRJSwitch(branch_rate_multiplier[i], weight=1) )    # include proposals on the rate mutliplier (when it is not 1)    moves.append( mvScale(branch_rate_multiplier[i], weight=1) )}We also keep track of the total number of rate shifts.num_rate_changes := sum( branch_rate_shift )Brownian-motion modelNow that we have specified the branch-specific rate parameters, we can draw the character data from the corresponding phylogenetic Brownian-motion model, just as we did for the simple BM models. In this case, we provide the square root of the branch-specific rates to the branchRates argument.X ~ dnPhyloBrownianREML(tree, branchRates=branch_rates^0.5)Noting that $X$ is the observed data (), we clamp the data to this stochastic node.X.clamp(data)Finally, we create a workspace object for the entire model with model(). Remember that workspace objects are initialized with the = operator, and are not themselves part of the Bayesian graphical model. The model() function traverses the entire model graph and finds all the nodes in the model that we specified. This object provides a convenient way to refer to the whole model object, rather than just a single DAG node.mymodel = model(sigma2_root)Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we need to set up a vector of monitors to record the states of our Markov chain. The monitor functions are all called mn*, where * is the wildcard representing the monitor type. First, we will initialize the model monitor using the mnModel function. This creates a new monitor variable that will output the states for all model parameters when passed into a MCMC function.monitors.append( mnModel(filename=\"output/relaxed_BM.log\", printgen=10) )Additionally, create a screen monitor that will report the states ofspecified variables to the screen with mnScreen:monitors.append( mnScreen(printgen=1000, sigma2_root, num_rate_changes) )Finally, create a monitor that associates the branch-specific rates to each branch in the tree.monitors.append( mnExtNewick(filename=\"output/relaxed_BM.trees\", isNodeParameter=TRUE, printgen=10, separator=TAB, tree=tree, branch_rates) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, we can now set up the MCMC algorithm that will sample parameter values in proportion to their posterior probability. The mcmc() function willcreate our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=50000)Now, summarize the branch-specific rates in the annotated tree file.treetrace = readTreeTrace(\"output/relaxed_BM.trees\")map_tree = mapTree(treetrace,\"output/relaxed_BM_MAP.tre\")⇨ The Rev file for performing this analysis: mcmc_relaxed_BM.RevYou can then visualize the branch-specific rates by plotting them using our R package RevGadgets. Just start R in the main directory for this analysis and then type the following commands.First, load RevGadgets:library(RevGadgets)Next, read in the tree annotated with the branch rates:tree &lt;- readTrees(\"output/relaxed_BM_MAP.tre\")Finally, plot the tree with the branch rates:plotTree(tree, color_branch_by=\"branch_rates\")⇨ The R script for plotting this output: plot_relaxed_BM.REstimated branch-specific rates of Brownian-motion evolution.Here we show the results of our example analysis.Exercise 1  Run an MCMC simulation to estimate the posterior distribution of the branch-specific rates (branch_rates) assuming a prior expected number of rates shifts of 5.  Plot the rates among branches under this prior.  Change the prior expected number of rate shifts to 1, and then plot the branch-specific rates on the tree.  Compare the posterior distributions for the total number of rate shifts under both priors using Tracer. Does the posterior distribution for the  total number of rate shifts seem to be sensitive to the prior?  Compare the posterior mean estimates for branch-specific rate estimates under each prior. Does the posterior mean estimate branch-rate estimates appear to be sensitive to this prior?  Click below to begin the next exercise!  State-Dependent Brownian Rate Estimation",
        "url": "/tutorials/cont_traits/relaxed_bm.html",
        "index": "true"
      }
      ,
    
      "tutorials-cont-traits-relaxed-ou-html": {
        "title": "Relaxed Ornstein-Uhlenbeck Models",
        "content": "Estimating Branch-Specific Evolutionary OptimaThis tutorial demonstrates how to specify an Ornstein-Uhlenbeck model where the optimal phenotype is allowed to vary over branches of a time-calibrated phylogeny (Uyeda and Harmon 2014). We provide the probabilistic graphical model representation of each component for this tutorial. After specifying the model, you will estimate the parameters of branch-specific Ornstein-Uhlenbeck evolution using reversible-jump Markov chain Monte Carlo (rjMCMC).Relaxing the OU ModelUnder a simple Ornstein-Uhlenbeck (OU) model, the optimal of a continuous character is determined by a single rate parameter, $\\theta$. Many evolutionary questions are related to how the optimal phenotype changes among lineages. In a Bayesian setting, we can specify a ‘‘relaxed’’ OU prior model that allows the optimal phenotype to vary over the phylogeny.Here, we will use the ‘‘random local clock’’ model, similar to the one described by Uyeda and Harmon (2014) in the software bayOU. In this model, we assume that each branch in the phylogeny either does or does not have a optimum shift. When there is no shift on a branch, the optimum on the branch is inherited directly from its ancestral branch; when there is a shift, the ancestral optimum is shifted by an amount that is drawn from a specified prior distribution. We specify a prior probability, $p$, that a given branch experiences a shift. For a tree with $n$ branches, the expected number of shifts is $E(k) = n \\times p$. To control the number of shifts, we specify a prior on the expected number of shifts, $E(k)$, and then calculate the prior probability for a shift on a particular branch, $p = E(k) / n$. The graphical model shows the relationship between the branch-specific optima and priors (fig_ou_relaxed_gm).The graphical model representation of the relaxed Ornstein-Uhlenbeck (OU) process under the random local clock. For more information about graphical model representations see Höhna et al. (2014).In this tutorial, we use the 66 vertebrate phylogenies and (log) body-size datasets from (Landis and Schraiber 2017) to estimate branch-specific optima of body-size evolution.⇨ The full relaxed BM-model specification is in the file called mcmc_relaxed_OU.Rev.Read the dataWe begin by deciding which of the traits to use. Here, we assume we are analyzing the first trait (female body mass), but you should feel free to choose any of the trait.trait &lt;- 1Now, we read in the (time-calibrated) tree corresponding.T &lt;- readTrees(\"data/primates_tree.nex\")[1]We also want to keep track of the number of branches for our relaxed clock model.ntips     &lt;- T.ntips()nbranches &lt;- 2 * ntips - 2Next, we read in the character data for the same dataset.data &lt;- readContinuousCharacterData(\"data/primates_cont_traits.nex\")We have to exclude all other traits that we are not interested in and only include our focal trait.This can be done in RevBayes using the member methods .excludeAll() and .includeCharacter().data.excludeAll()data.includeCharacter( trait )Additionally, we initialize a variable for our vector ofmoves and monitors:moves    = VectorMoves()monitors = VectorMonitors()Specifying the modelTree modelIn this tutorial, we assume the tree is known without area. We create a constant node for the tree that corresponds to the observed phylogeny.tree &lt;- TRate parameterThe stochastic rate of evolution is controlled by the rate parameter, $\\sigma^2$. We draw the rate parameter from a loguniform prior. This prior is uniform on the log scale, which means that it is represents ignorance about the order of magnitude of the rate. As before, we provide a scale move that proposes changes to the parameter during MCMC.sigma2 ~ dnLoguniform(1e-3, 1)moves.append( mvScale(sigma2, weight=1.0) )Adaptation parameterThe rate of adaptation toward the optimum is determined by the parameter $\\alpha$. We draw $\\alpha$ from an exponential prior distribution, and place a scale proposal on it. This parameter is assumed to be constant across the tree (even though the optimum will vary). We specify the mean of the exponential prior distribution on $\\alpha$ to be half the root age divided by $\\ln(2)$, which means that we expect a phylogenetic half life of half the tree age.root_age := tree.rootAge()alpha ~ dnExponential( abs(root_age / 2.0 / ln(2.0)) )moves.append( mvScale(alpha, weight=1.0) )OptimaWe begin by drawing the optimum value at the root of the tree, and including a slide move.theta_root ~ dnUniform(-10, 10)moves.append( mvSlide(theta_root, weight=1.0) )We specify the relaxed-optimum model by specifying a prior on the probability of shifts for each branch. We parameterize our prior expected number of shifts, and then compute the probability of a shift on a given branch.expected_number_of_shifts &lt;- 5shift_probability &lt;- expected_number_of_shifts / nbranchesNext, we specify the prior distribution on the size of shifts (when they occur). We draw each rate shift from a uniform distribution.shift_distribution = dnNormal(0, 0.587)Now, we loop over each branch, drawing a the change in the optima value from a mixture distribution. This mixture distribution places prior probability $p$ on the rate shift being drawn from the uniform distribution we just specified, and prior probability $1 - p$ on the shift being exactly equal to 0 (i.e., no shift). We then compute the optimum on the branch by adding the change in the optimum to the ancestral optimum. Note that we loop over the branches in reverse order; this ensures that the ancestral optimum exists when we specify the optimum for a given branch.for (i in nbranches:1) {    # draw the theta shift from a mixture distribution    branch_deltas[i] ~ dnReversibleJumpMixture(0, shift_distribution, Probability(1 - shift_probability) )    # compute the theta for the branch    if ( tree.isRoot( tree.parent(i) ) ) {       branch_thetas[i] := theta_root + branch_deltas[i]    } else {       branch_thetas[i] := branch_thetas[tree.parent(i)] + branch_deltas[i]    }    # keep track of whether the branch has a shift    branch_theta_shift[i] := ifelse( branch_deltas[i] == 0, 0, 1 )    # use reversible-jump to move between models with and without    # shifts on the branch    moves.append( mvRJSwitch(branch_deltas[i], weight=1) )    # include proposals on the shift (when it is not 1)    moves.append( mvScale(branch_deltas[i], weight=1) )}We also keep track of the total number of rate shifts.num_theta_changes := sum( branch_theta_shift )OU modelNow that we have specified the branch-specific theta parameters, we can draw the character data from the corresponding phylogenetic OU model, just as we did for the simple OU models. In this case, we provide the vector of branch_thetas to the theta argument.X ~ dnPhyloOrnsteinUhlenbeckREML(tree, alpha, branch_thetas, sigma2^0.5, rootStates=theta_root)Noting that $X$ is the observed data (), we clamp the data to this stochastic node.X.clamp(data)Finally, we create a workspace object for the entire model with model(). Remember that workspace objects are initialized with the = operator, and are not themselves part of the Bayesian graphical model. The model() function traverses the entire model graph and finds all the nodes in the model that we specified. This object provides a convenient way to refer to the whole model object, rather than just a single DAG node.mymodel = model(X)Running an MCMC analysisSpecifying MonitorsFor our MCMC analysis, we need to set up a vector of monitors to record the states of our Markov chain. The monitor functions are all called mn*, where * is the wildcard representing the monitor type. First, we will initialize the model monitor using the mnModel function. This creates a new monitor variable that will output the states for all model parameters when passed into a MCMC function.monitors.append( mnModel(filename=\"output/relaxed_OU.log\", printgen=10) )Additionally, create a screen monitor that will report the states ofspecified variables to the screen with mnScreen:monitors.append( mnScreen(printgen=1000, sigma2, num_theta_changes) )Finally, we include an extended Newick monitor to keep track of the branch-specific theta values on each branch of the tree.monitors.append( mnExtNewick(filename=\"output/relaxed_OU.trees\", isNodeParameter=TRUE, printgen=10, separator=TAB, tree=tree, branch_thetas) )Initializing and Running the MCMC SimulationWith a fully specified model, a set of monitors, and a set of moves, wecan now set up the MCMC algorithm that will sample parameter values inproportion to their posterior probability. The mcmc() function willcreate our MCMC object:mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")Now, run the MCMC:mymcmc.run(generations=50000)Finally, we summarize the branch-specific rate after we run the MCMC analysis.treetrace = readTreeTrace(\"output/relaxed_OU.trees\")map_tree = mapTree(treetrace,\"output/relaxed_OU_MAP.tre\")When the analysis is complete, you will have the monitored files in youroutput directory.⇨ The Rev file for performing this analysis: mcmc_relaxed_OU.RevYou can then visualize the branch-specific rates by plotting them using our R package RevGadgets. Importantly, these plots allow you to tease apart the relative contributions of background- and state-dependent-rate variation to overall patterns of rate variation across the tree. Just start R in the main directory for this analysis and then type the following commands:library(RevGadgets)# read the annotated treetree &lt;- readTrees(\"output/relaxed_OU_MAP.tre\")# plot the objectsplotTree(tree, color_branch_by=\"branch_thetas\")Estimated branch-specific optima across branches of the tree.Here we show the results of our example analysis.Exercise 1  Run an MCMC simulation to estimate the posterior distribution of the branch-specific thetas (branch_thetas) assuming a prior expected number of shifts of 5.  Plot the thetas among branches under this prior.  Change the prior expected number of shifts to 1, and then plot the branch-specific thetas on the tree.  Compare the posterior distributions for the total number of shifts under both priors. Does the posterior distribution for the total number of shifts seem to be sensitive to the prior?  Compare the posterior mean estimates under each prior. Do the posterior mean estimates estimates appear to be sensitive to this prior?",
        "url": "/tutorials/cont_traits/relaxed_ou.html",
        "index": "true"
      }
      ,
    
      "tutorials-intro-rev-html": {
        "title": "Rev Language Syntax",
        "content": "Basic Rev CommandsThis tutorial demonstrates the basic syntactical features of RevBayesand the Rev scripting language. A good reference for probabilistic graphical models forBayesian phylogenetic inference is given in (Höhna et al. 2014). Let’s start with the basic conceptsfor the interactive use of RevBayes with Rev (the language ofRevBayes). You should try to execute the statements step by step, lookat the output and try to understand what and why things are happening.First, open up your terminal and type RevBayes. This should launch RevBayes and give youa command prompt (the &gt; character). This means RevBayes is waiting for input.Operators and FunctionsRev is an interpreted language for statistical computing and phylogeneticanalysis. Therefore, the basics are simple mathematical operations.Entering each of the following lines will automatically execute theseoperations.# Simple mathematical operators:1 + 1                            # Addition10 - 5                           # Subtraction5 * 5                            # Multiplication10 / 2                           # Division2^3                              # Exponentiation5%2                              # ModuloFrom now on, we will omit images of the terminal.Each set of operations constitutes a statement. As you work throughthese tutorials, it is helpful to write the statements you enter into ablank text file, then copy-and-paste the statements into to executethem. This way, you have a complete history of everything you’ve done,and can easily start over without having to rewrite everything. We referto the text file containing the list of commands as a script, becauseit describes line-by-line instructions for the program to follow.You can write multiple statements in the same line if you separate themby a semicolon (;). The statements will execute as if you wrote eachon a single line.1 + 1; 2 + 2                    # Multiple statements in one lineHere you can see that comments always start with the hash symbol (#).Everything after the #-symbol will be ignored. In addition to thesesimple mathematical operations, provides some standard math functionswhich can be called by:# Math functionsexp(1)                           # exponential functionln(1)                            # logarithmic function with natural basesqrt(16)                         # square root function power(2,2)                       # power function: power(a,b) = a^bNotice that Rev is case-sensitive. That means, Rev distinguishes upper and lowercase letter for both variable names and function names. For example,only the first of these two calls will work:exp(1)                           # correct lower case nameExp(1)                           # wrong upper case nameVariable Declaration and AssignmentOne of the most important features of (or any programming language,really) is the ability to declare and assign variables. Variables storeinformation to be referenced later, and can change throughout theexecution of the program. There are three kinds of variables in RevBayes, calledconstant, deterministic, and stochastic variables. Constantvariables contain values that are not random in your model.Deterministic variables are functions of other variables. Stochasticvariables are random variables in your model, and will change duringyour analysis; importantly, stochastic variables (being randomvariables) are always associated with a particular statisticaldistribution.Different types of variables differ in how you create them and assignvalues to them. We will begin by creating a constant variable with namea that starts with the value 1. The left arrow assignment (&lt;-)always creates a constant variable, and automatically assigns thefollowing value to it.# Variable assignment: constanta &lt;- 1                           # assignment of constant node `a'You see the value of ‘a’ by just typing in the variable name andpressing enter.a                                # printing the value of `a'Next, we create a deterministic variable b using the := assignmentcomputed by exp(a) and another deterministic variable c computed byln(b). Deterministic variables are always created using thecolon-equal assignment (:=).# Variable assignment: deterministic# assignment of deterministic node `b' with# the exponential function with parameter `a'b := exp(a)  b# assignment of deterministic node `c' with# logarithmic function with parameter `b'c := ln(b)              c Finally, we will create the third type of variables in RevBayes: stochasticvariables. We will create a random variable x from an exponentialdistribution with parameter lambda. Stochastic assignments use the$\\sim$ operation.# Variable assignment: stochastic# assign constant node `lambda' with value `1'lambda &lt;- 1.0# create stochastic node with exponential # distribution and parameter `lambda'x ~ dnExponential(lambda)The value of x is a random draw from the distribution. You can see thevalue and the probability (or log-probability) of the current valueunder the current parameter values byx                                # print value of stochastic node `x'x.probability()                  # print the probability if `x'x.lnProbability()                # print the log-probability if `x'Distributions and Random NumbersRevBayes provides functions for common statistical distributions. We’lldemonstrate by generating random exponential numbers as we did inlecture. Recall that we can transform a random variable $u$ sampled froma Uniform(0,1) distribution into an exponential distribution with rateparameter $\\lambda$:\\[\\begin{aligned}    u &amp;\\sim \\text{Uniform(0,1)}\\\\    x &amp;= -\\frac{\\ln u}{\\lambda} \\end{aligned}\\]In RevBayes, we might describe$u$ as a stochastic variable, and $x$ as a deterministic variable (sinceit is a function of $u$):# create the random variable uu ~ dnUniform(0,1)u# determine the rate parameterlambda &lt;- 1.0# create x as a deterministic function of ux := -ln(u) / lambdaxAlternatively, we can create $x$ directly as an exponential randomvariable:# create the random variable xx ~ dnExponential(lambda)xVectorsIndividual variables can have more than one value. Variables that havemore than one value are called vectors. The simplest way to create avector is like this:z &lt;- v(1.0,2.0,3.0)              # create a vectorYou can refer to a specific value in the vector using brackets, [i],where i is the index of the variable of interest.z[1]                             # print the first entryz[1] &lt;- 10                       # change the value of the first entryzfor loopsfor loops are important programming structures that allow you torepeat the same statement a number of times on different variables. Thebasic structure of a for loop is:# a for loopfor ([variable] in [set of values]) {   [statements using variable]}The for statement is followed by a set of parenthesis containing[variable], which contains the name of the variable being iterated,and [set of values], which are the values that the variable iteratesover. The for loop variable is a special variable that is created bythe for loop: you do not have to create it before executing the loop.This simple for loop creates the variable i, and for each value ofi from 1 to 100, prints the value of i to the screen.for (i in 1:100) {   i}for loops are very powerful programming tools. We can use a for loopto create an entire vector of uniform random numbers, and transformthem into a second vector of exponential random numbers.for (i in 1:100) {   y[i] ~ dnUniform(0,1)   z[i] := -ln(y[i]) / lambda }Close using the statement q().q()",
        "url": "/tutorials/intro/rev.html",
        "index": "true"
      }
      ,
    
      "tutorials-intro-revgadgets-html": {
        "title": "Introduction to RevGadgets",
        "content": "Overview Through user-friendly data pipelines, RevGadgets guides users through importing RevBayes output into R, processing the output, and producing figures or other summaries of the results. RevGadgets provide paired processing and plotting functions built around commonly implemented analyses, such as tree building and divergence-time estimation, diversification-rate estimation, ancestral-state reconstruction and biogeographic range reconstruction, and posterior predictive simulations.Using the general framework of ggplot2, the tidyverse, and associated packages (Wickham 2011; Wickham et al. 2019), plotting functions return plot objects with default aesthetics that users may customize. Below, we walk you through installation and several case studies to illustrate primary functionalities.Installation RevGadgets is available to download from GitHub using devtools:install.packages(\"devtools\")devtools::install_github(\"cmt2/RevGadgets\")Note about magick dependencyRevGadgets depends on the R package magick, which in turn depends on external software ImageMagick. If RevGadgets installation fails, you may need to install ImageMagick. On macOS or Linux, this can be done using homebrew at the command line:brew install imagemagickAlternatively, visit the ImageMagick website for more download options.⚙ RevGadgets is under active developmentRevGadgets is still under active development. If you encounter any bugs or issues while using RevGadgets, please submit a bug report with Issues on GitHub.We are currently transitioning our tutorials to the new RevGadgets, so you may notice inconsistencies between instructions here and in those tutorials. If you are using an existing tutorial, make sure you are using the RevGadgets version referenced in that tutorial. Older tutorials will use this older version of RevGadgets. Tutorials that use the new version of RevGadgets indicate this tutorial as a dependency.Getting StartedTo run this tutorial, download the associated files from the Data files and scripts menu, preserving the structure of the zipped folder. All scripts should be in a subdirectory called scripts and all data files in a subdirectory called data. Open R and make sure your working directory is set to the directory with the downloaded files. For more information on how to customize these plots, see the associated documentation for each function (e.g., ?readTrace)).Before running the code chunks below, load all the libraries used in the tutorial. You may need to install these packages separately.library(RevGadgets)library(coda)library(ggplot2)library(ggtree)library(grid)library(gridExtra)Visualizing Parameter Estimates RevGadgets provides several tools that facilitate the visualization of posterior distributions of parameters. The output of most RevBayes analyses is a tab-delimited file where rows correspond to samples of an MCMC analysis and columns correspond to parameters in the model. Most information of interest to researchers must be extracted from these output files.Visualizing MCMC output is also critical for evaluating and troubleshooting analyses, especially for diagnosing MCMC pathologies.The following code demonstrates how to process and visualize the MCMC trace file of a general time-reversible (GTR) substitution model analysis (Tavaré 1986), in which we have estimated the substitution rate and stationary frequency parameters for a single gene in a sample of 23 primates (Springer et al. 2012). This analysis is covered in detail the Nucleotide substitution models tutorial.⇨ The code in this section is contained in the script: scripts/parameter_estimates.RFirst, read in and process the trace file. Burnin (the samples taken before the Markov chain reached stationarity) may be removed at this stage or after examining the trace file further.# specify the input filefile &lt;- \"data/primates_cytb_GTR.log\"# read the trace and discard burnintrace_quant &lt;- readTrace(path = file, burnin = 0.1)# or read the trace _then_ discard burnintrace_quant &lt;- readTrace(path = file, burnin = 0)trace_quant &lt;- removeBurnin(trace = trace_quant, burnin = 0.1)The output of readTrace() may be passed to R packages specializing in MCMC diagnoses such as coda (Plummer et al. 2006). (Note that RevGadgets does not require coda, so you will have to install it separately.)For example:# assess convergence with coda library(coda)trace_quant_MCMC &lt;- as.mcmc(trace_quant[[1]])effectiveSize(trace_quant_MCMC)traceplot(trace_quant_MCMC)Alternatively, use the R package convenience (described here: Convergence assessment) to assess convergence before processing the data with RevGadgets.RevGadgets provides its own core functions for summarizing and visualizing traces of specific parameters. SummarizeTrace() calculates the mean and 95% credible interval for quantitative variables and the 95% credible set for qualitative variables.To examine the stationary frequency (pi) parameter values in our trace file, summarize their distributions:summarizeTrace(trace = trace_quant, vars =  c(\"pi[1]\",\"pi[2]\",\"pi[3]\",\"pi[4]\"))The following summaries will be printed to the screen:$`pi[1]`$`pi[1]`$trace_1        mean        median        MAP          0.3280593   0.3282250     0.3265728              quantile_2.5  quantile_97.5                 0.3080363     0.3481651        ...$`pi[4]`$`pi[4]`$trace_1        mean        median        MAP          0.2152190   0.2145595     0.2144797                quantile_2.5  quantile_97.5        0.2020019     0.2278689Then plot these distributions. plotTrace() produces a list of ggplot2 objects, with multiple plots if there are multiple runs in the trace object or if you provide a mix of quantitative and qualitative parameters. Here, only one plot is produced, so we print the plot using [[1]] after the function call.plotTrace(trace = trace_quant, vars = c(\"pi[1]\",\"pi[2]\",\"pi[3]\",\"pi[4]\"))[[1]]The posterior densities of the nucleotide stationary frequencies under a GTR substitution model. Colored areas under the curve correspond to the 95% credible interval.These functions may also process and plot posterior estimates of discrete variables, such as the the binary character indicating if certain transition rates among character states exist (i.e. if the corresponding transitions are possible), from a reversible jump MCMC (rjMCMC) ancestral-state reconstruction analysis. See the Discrete morphology - Ancestral State Estimation tutorial for information on performing this RevBayes analysis.First, read and summarize the data:# read in tracefile &lt;- \"data/freeK_RJ.log\"trace_qual &lt;- readTrace(path = file)# summarize parameters summarizeTrace(trace_qual,                vars = c(\"prob_rate_12\", \"prob_rate_13\", \"prob_rate_21\",                        \"prob_rate_23\", \"prob_rate_31\", \"prob_rate_32\"))The following summaries will be printed to the screen:$prob_rate_12$prob_rate_12$trace_1credible_set        1         0 0.6440396 0.3559604 ...$prob_rate_32$prob_rate_32$trace_1        0 0.9724475Then plot the distributions as histograms:plotTrace(trace = trace_qual,           vars = c(\"prob_rate_12\", \"prob_rate_13\",                   \"prob_rate_31\", \"prob_rate_32\"))[[1]]The posterior distributions of whether particular rates are included in a model of character evolution. Colored bars are included in the 95% credible set.If you provide both quantitative and qualitative variables to the vars argument, plotTrace() will produce multiple plots. Say you want to visualize some rates in addition to the qualitative probability parameters:# produce plots of probabplots &lt;- plotTrace(trace = trace_qual,                    vars = c(\"prob_rate_12\", \"prob_rate_13\",                            \"prob_rate_31\", \"prob_rate_32\",                             \"rate_31\", \"rate_32\"))plots is now a list of length two. Visualize them together with the grid package:grid.newpage()grid.draw( # draw the following matrix of plots    rbind( # bind together the columns      ggplotGrob(plots[[1]]),      ggplotGrob(plots[[2]])))A combined figure of the plots for both quantitative and qualitative variables. Top: the posterior densities of transition rate parameters.Bottom: the posterior distributions of whether particular rates are included in a model of character evolution (same as the sandalone figure above).Visualizing Phylogenies Phylogenies are central to all analyses in RevBayes, and accurate and information-rich visualizations of evolutionary trees are thus critical.RevGadgets contains methods to visualize phylogenies and their associated posterior probabilities, divergence time estimates, geological time scales, and branch rates.Additionally, text annotation may be added to specify associated data, such as posterior probabilities of nodes or node ages. Users may modify aesthetics such as colors, sizes, branch thickness, and tip label formatting through specific function arguments or by adding layers to the resulting ggplot object.⇨ To reproduce this section, see: scripts/visualize_trees.RBasic tree plotsRevGadgets reads and processes single trees, such as those produced by the Nucleotide substitution models tutorial, and tree traces with readTrees():file &lt;- \"data/primates_cytb_GTR_MAP.tre\"tree &lt;- readTrees(paths = file)rerootPhylo() roots the tree and plotTree() produces a basic tree plot, which may be modified by changing the formatting of tip labels, adjusting tree line width, and adding posterior probabilities of nodes as internal node labels. This plot object is modifiable in the same way as ggplot. Here, we add a scale bar:# reroot the tree with Galeopterus variegatus # you can also specify multiple tips to reroot using a cladetree_rooted &lt;- rerootPhylo(tree = tree, outgroup = \"Galeopterus_variegatus\")# create the plot of the rooted treeplot &lt;- plotTree(tree = tree_rooted,                 # label nodes the with posterior probabilities                 node_labels = \"posterior\",                  # offset the node labels from the nodes                 node_labels_offset = 0.005,                 # make tree lines more narrow                 line_width = 0.5,                 # italicize tip labels                  tip_labels_italics = TRUE)# add scale bar to the tree and plot with ggtreelibrary(ggtree)plot + geom_treescale(x = -0.35, y = -1)The maximum a posterior phylogeny of primates inferred under a GTR substitution model.Fossilized birth-death treesRevGadgets elaborates on the plotTree to plot fossilized birth death analyses, such as those described in the Estimating a Time-Calibrated Phylogeny of Fossil and Extant Taxa using Morphological Data tutorial. This plot includes a geological timescale, labeled sampled ancestors along branches and their species names as annotated text in the top left corner, and node and tip age bars colored by their corresponding posterior probabilities.file &lt;- \"data/bears.mcc.tre\"# read in the tree tree &lt;- readTrees(paths = file)# plot the FBD treeplotFBDTree(tree = tree,             timeline = T,             geo_units = \"epochs\",            tip_labels_italics = T,            tip_labels_remove_underscore = T,            tip_labels_size = 3,             tip_age_bars = T,            node_age_bars = T,             age_bars_colored_by = \"posterior\",            label_sampled_ancs = TRUE) +     # use ggplot2 to move the legend and make     # the legend background transparent        theme(legend.position=c(.05, .6),              legend.background = element_rect(fill=\"transparent\"))The sampled-ancestor maximum-clade credibility phylogeny of extant and extinct bears inferred under the fossilized birth-death model.Bars correspond to the 95% credible interval of node (or tip) ages, and are colored by the posterior probability of the clade (for internal nodes), the posterior probability that the tip is not a sampled ancestor (for tip nodes) or the posterior probability that the node is a sampled ancestor (for sampled ancestor nodes).Coloring branches by variablesThe plotTree() function can color the branches of the tree, which is useful for indicating branch rates or other continuous parameters. For example, here plotTree() colors the branches by branch-specific optima (thetas) from a relaxed Ornstein-Uhlenbeck model of body size evolution in whales. The Relaxed Ornstein-Uhlenbeck Models tutorial covers this type of analysis.file &lt;- \"data/relaxed_OU_MAP.tre\"# read in the treetree &lt;- readTrees(paths = file)# plot the tree with ratesplotTree(tree = tree,          # italicize tip labels         tip_labels_italics = FALSE,         # specify variable to color branches         color_branch_by = \"branch_thetas\",          # thicken the tree lines         line_width = 1.7) +     # move the legend with ggplot2    theme(legend.position=c(.1, .9))Branch-specific optima, $\\theta$, under a relaxed Ornstein-Uhlenbeck model. Branches are colored according to the posterior-mean estimate of $\\theta$ for the branch.Ancestral-State Reconstruction Ancestral state reconstruction methods allow users to model how heritable characters, such as phenotypes or geographical distributions, have evolved across a phylogeny, producing probability distributions of states for each node of the phylogeny.This aspect of RevGadgets functionality allows users to plot the maximum a posteriori (MAP) estimate of ancestral states via plotAncStatesMAP() or a pie chart showing the most probable states via plotAncStatesPie().For anagenetic evolutionary models (including standard CTMC models such as the GTR and Mk models), ancestral-state estimates are plotted at the nodes.However, more complex models may include cladogenetic evolution, so that state changes can occur at speciation events in addition to anagenetic evolution along branches (Ree and Smith 2008; Goldberg and Igić 2012). RevGadgets can plot the results of inferences for modes with cladogenetic events by plotting ancestral states on the “shoulders” as well as the nodes.Ancestral-state plotting functions in RevGadgets allow users to specify character states and their posterior probabilities by modifying the colors, shapes, and sizes of node and shoulder symbols. Text annotations may be added to specify states, state posterior probabilities, and the posterior probabilities of nodes.⇨ To reproduce this section, see: scripts/anc_states.RStandard (anagenetic) modelsTo plot the output of an ancestral state estimation of placenta type across mammals, RevGadgets first summarizes the RevBayes output file using processAncStates(). This function reads in and processes an annotated tree file and labels the states. If no state labels are provided to processAncStates(), the function will provide default labels. Next, RevGadgets creates the plot object using a plotting function. In this example, we plot the MAP states using plotAncStatesMAP(). The analysis that produced this output file is described in the Discrete morphology - Ancestral State Estimation tutorial.file &lt;- \"data/ase_freeK.tree\"# process the ancestral statesfreeK &lt;- processAncStates(file,                          # Specify state labels.                           # These numbers correspond to                           # your input data file.                          state_labels = c(\"1\" = \"Epitheliochorial\",                                            \"2\" = \"Endotheliochorial\",                                            \"3\" = \"Hemochorial\"))# produce the plot object, showing MAP states at nodes.# color corresponds to state, size to the state's posterior probabilityplotAncStatesMAP(t = freeK,                  tree_layout = \"circular\") +     # modify legend location using ggplot2    theme(legend.position = c(0.57,0.41))Ancestral-state estimates of mammalian placental under an asymmetric model of character evolution. Symbol colors correspond to the state with the highest posterior probability at that node; the size of the symbol is proportional to the posterior probability of that state.Cladogenetic modelsMany biogeographic models, including the popular Dispersal-Extirpation-Cladogenesis model described in Introduction to phylogenetic biogeography with the DEC model, include cladogenetic change.plotAncStatesPie() represents the distribution of ancestral states at nodes as pie charts of the three most probable states for that node plus an “other” category of any remaining probability.We demonstrate this functionality with a visualization of the ancestral ranges of Hawaiian silverswords estimated using a DEC biogeographic analysis and include shoulder states to indicate cladogenetic as well as anagenetic changes. Because of the large number of states in this analysis (15 possible ranges and one “other” category), more pre-plotting processing is necessary.We pass the appropriate ancestral area names to processAncStates() and specify custom colors in a named vector.To plot the ancestral states, we provide the processed data, specify that the corresponding model is “cladogenetic”, add text labels to the tips specifying the character state, and modify sizes and horizontal positions for aesthetics.We also modify the order at which states appear in the legend and the legend position.file &lt;- \"data/simple.ase.tre\"# Create the labels vector.# This is a named vector where names correspond # to the computer-readable numbers generated # in the biogeographic analysis and the values # are character strings of whatever you'd like # as labels on the figure. The state.labels.txt# file produced in the analysis links the # computer-readable numbers with presence/ absence# data for individual ranges.labs &lt;- c(\"1\"  = \"K\",   \"2\"  = \"O\",           \"3\"  = \"M\",   \"4\"  = \"H\",           \"5\"  = \"KO\",  \"6\"  = \"KM\",           \"7\"  = \"OM\",  \"8\"  = \"KH\",           \"9\"  = \"OH\",  \"10\" = \"MH\",           \"11\" = \"KOM\", \"12\" = \"KOH\",           \"13\" = \"KMH\", \"14\" = \"OMH\",           \"15\" = \"KOMH\")# pass the labels vector and file name to the processing scriptdec_example &lt;- processAncStates(file, state_labels = labs)We could plot this as is with little processing using the following code (output not shown). However, we are going to walk through creating a custom color palette and then compare this plot to the same data plotted with plotAncStatesMAP().# plotAncStatesPie(dec_example, cladogenetic = T, tip_labels_offset = 0.2)Here’s how you specify a custom color palette:# You can see the states sampled in the analysis in the# dec_example@state_labels vector. This may be different # from the `labs` vector you provided above if not all # possible states are included in the annotated tree.dec_example@state_labels# We are going to generate colors for these states using# a color palette, but you could also specify a color per# state manually. # Get the length of the dec_example$state_labels vector# to know how many colors you need. ncol &lt;- length(dec_example@state_labels)# We use colorRampPalette() to generate a function that will# expand the RevGadgets color palette (colFun) to the necessary# number of colors, but you can use any colors you like as long # as each state_label has a color. colors &lt;- colorRampPalette(colFun(12))(ncol)# Name the color vector with your state labels and then order # it in the order you'd like the ranges to appear in your legend.# Otherwise, they will appear alphabetically. names(colors) &lt;- dec_example@state_labelscolors &lt;- colors[c(1,2,9,11,                   3,4,6,10,12,13,                   5,7,14,                   8)]# Plot the results with pies at nodespie &lt;- plotAncStatesPie(t = dec_example,                        # Include cladogenetic events                        cladogenetic = TRUE,                         # Add text labels to the tip pie symbols                        tip_labels_states = TRUE,                        # Offset those text labels slightly                        tip_labels_states_offset = .05,                        # Pass in your named and ordered color vector                        pie_colors = colors,                         # Offset the tip labels to make room for tip pies                        tip_labels_offset = .2,                         # Move tip pies right slightly                         tip_pie_nudge_x = .07,                        # Change the size of node and tip pies                          tip_pie_size = 0.8,                        node_pie_size = 1.5) +  # Move the legend   theme(legend.position = c(0.1, 0.75))plotAncStatesMAP() can also plot cladogenetic models. The plotAncStatesMAP() function can plot the same processed output dec_example using the same color vector.map &lt;- plotAncStatesMAP(t = dec_example,                         # Include cladogenetic events                        cladogenetic = T,                        # Pass in the same color vector                        node_color = colors,                        # adjust tip labels                         tip_labels_offset = 0.1,                        # increase tip states symbol size                        tip_states_size = 3) +  # adjust legend position and remove color guide  theme(legend.position = c(0.2, 0.87)) +   guides(color = FALSE)Compare the two plots side by side using the gridExtra package:grid.arrange(pie,map, ncol = 2)Two visualizations of ancestral-state estimates of biogeographic area of the Hawaiian silverswords. On the left, pies represent the posterior probability of each state at the node; pies at the branching event correspond to the state of the ancestor immediately before the branching event, and pies on the “shoulders” of the two descendant branches are the states of the two descendants immediately after the branching event. On the right, symbols at the nodes and shoulders are colored by state and sized by posterior probability.While these examples demonstrate cladogenetic change for plotAncStatesPie() only, plotAncStatesMAP() can also plot cladogenetic change, and plotAncStatesPie() can also plot the results of anagenetic models. These functions provide plotting tools for any discrete ancestral-state estimation including the results of chromosome count reconstructions (as in Chromosome Evolution) and discrete state-dependent speciation and extinction (SSE) models (as in State-dependent diversification with BiSSE and MuSSE, among others).Diversification Analysis Diversification-rate estimation is a major goal of many comparative and epidemiological analyses.The diversification models implemented in RevBayes allow the speciation, extinction, and sampling/ fossilization rates to vary through time, across branches of the phylogeny, in conjunction with the evolution of a focal trait, etc.The examples below demonstrate how to use RevGadgets to visualize the (often complex) output of analyses performed under these models.⇨ To reproduce this section, see: scripts/divrates.RState-Dependendent Diversification AnalysisDiversification rates might depend on the state of an evolving character. State-dependent diversification models can be used to estimate diversification rates associated with each character state, and can also reconstruct ancestral states on the phylogeny. The analysis plotted here is described in the State-dependent diversification with BiSSE and MuSSE tutorial.RevGadgets first reads in and processes the rate file and then plots the state-specific posterior rate distributions.# read in and process the log filebisse_file &lt;- \"data/primates_BiSSE_activity_period.log\"pdata &lt;- processSSE(bisse_file)# plot the ratesplotMuSSE(pdata)Posterior distributions of the rates for a BiSSE analysis.While the above example includes two states (a BiSSE analysis), the same workflow is appropriate for models of multi-state traits (MuSSE). For plotting the rates from an analysis with hidden states (HiSSE), check out plotHiSSE().The ancestral state estimates may be plotted similarly to in the ancestral states section above.# read in and process the ancestral states bisse_anc_states_file &lt;- \"data/anc_states_BiSSE.tree\"p_anc &lt;- processAncStates(path = bisse_anc_states_file)# plot the ancestral statesplotAncStatesMAP(p_anc, tree_layout = \"circular\")Ancestral states estimated with a BiSSE analysis.Lineage-Specific Diversification AnalysisTo examine diversification rate variation across the branches of the tree (described in Branch-Specific Diversification Rate Estimation), RevBayes estimates branch-specific speciation and extinction rates. Those rates can be plotted by reading in the tree and rate log files, associating the rates with the phylogeny (using processBranchData()), and plotting the rate of interest by coloring the branches of the phylogeny.branch_specific_file &lt;- \"data/primates_BDS_rates.log\"branch_specific_tree_file &lt;- \"data/primates_tree.nex\"rates &lt;- readTrace(branch_specific_file)tree  &lt;- readTrees(branch_specific_tree_file)combined &lt;- processBranchData(tree    = tree,                               dat     = rates,                              net_div = TRUE)plotTree(combined, color_branch_by = \"net_div\",          tip_labels_size = 2, tree_layout = \"circular\")Net diversifification plotted as the color of branches of a circular phylogeny.Episodic Diversification AnalysisInstead of varying rates across branches of the phylogeny, the episodic birth death process varies rates through time (see the Episodic Diversification Rate Estimation tutorial). RevGadgets visualizes these rates through time with skyline/ episodic plots.# specify the output filesspeciation_time_file &lt;- \"data/primates_EBD_speciation_times.log\" speciation_rate_file &lt;- \"data/primates_EBD_speciation_rates.log\" extinction_time_file &lt;- \"data/primates_EBD_extinction_times.log\"  extinction_rate_file &lt;- \"data/primates_EBD_extinction_rates.log\"# read in and process ratesrates &lt;- processDivRates(speciation_time_log = speciation_time_file,                         speciation_rate_log = speciation_rate_file,                          extinction_time_log = extinction_time_file,                          extinction_rate_log = extinction_rate_file,                          burnin = 0.25)# plot rates through time plotDivRates(rates = rates) +         # change labels with ggplot2        xlab(\"Millions of years ago\") +        ylab(\"Rate per million years\")Diversification-rates over time estimated from the primate phylogeny. Lines correspond to the posterior mean estimate over time, and shaded regions correspond to the 95% credible interval.Posterior-Predictive Analysis Posterior predictive simulation is a powerful tool for assessing the adequacy of the model and assessing the reliability of phylogenetic inference. ⇨ To reproduce this section, see: scripts/post_pred.RThe analysis that produced this output file is describe in the Assessing Phylogenetic Reliability Using RevBayes and $P^{3}$ tutorial.# specify the simulated statistics filesim &lt;- \"data/simulated_data_pps_example.csv\"# specify the empirical statistics fileemp &lt;- \"data/empirical_data_pps_example.csv\"# read the statistics filesstats &lt;- processPostPredStats(path_sim = sim,                               path_emp = emp)# create the posterior-predictive plotsplots &lt;- plotPostPredStats(data = stats)To plot a subset of the parameters in a single figure, use the grid package.# arrange a subset of them with grid and ggplot2grid.newpage()grid.draw( # draw the following matrix of plots    cbind( # bind together the columns into a matrix        rbind( # bind together the first column            ggplotGrob(plots[[1]]),            ggplotGrob(plots[[5]])),        rbind( # bind together the last column (exclude the y-axis label in the last column)            ggplotGrob(plots[[3]] + theme(axis.title.y = element_blank())),            ggplotGrob(plots[[7]] + theme(axis.title.y = element_blank()))))))Posterior-predictive distribution of statistics (curves) and observed statistics (dashed vertical lines), and posterior-predictive p-values (upper right corner). Blue and red regions correspond to the 10% and 5% rejection regions, respectively.Bonus VideosCoding for a new dataset (FBD)Coding for a new dataset (DEC)",
        "url": "/tutorials/intro/revgadgets.html",
        "index": "true"
      }
      ,
    
      "tutorials-divrate-sampling-html": {
        "title": "Diversification Rate Estimation with Missing Taxa",
        "content": "Estimating Speciation &amp; Extinction Rates Through TimeOutlineThis tutorial describes how to specify different models of incompletetaxon sampling (missing reference) for estimatingdiversification rates in RevBayes (Höhna et al. 2016). Incomplete taxonsampling, if not modeled correctly, severely biases diversification-rateparameter estimates (missing reference). Specifically, we willdiscuss uniform, diversified, and empirical taxon sampling. Allanalyses in this tutorial will focus on diversification rate estimationthrough-time and use a birth-death process where diversification ratesvary episodically which we model by piecewise constant rates RevBayes(missing reference). The probabilistic graphical model is givenonly once for this tutorial as an overview. The model itself does notchange between the different analyses; only the assumptions ofincomplete taxon sampling. For each analysis you will estimatespeciation and extinction rates through-time using Markov chain MonteCarlo (MCMC) and assess the impact of incomplete taxon sampling as wellas the sampling scheme.RequirementsWe assume that you have read and hopefully completed the followingtutorials:      Gettingstarted        Very Basic Introduction toRev        General Introduction to the Revsyntax        General Introduction to MCMC using an archeryexample        General Introduction to MCMC using a coin-flippingexample        Basic Diversification RateEstimation        Diversification Rates ThroughTime  Note that the Rev basicstutorialintroduces the basic syntax of Rev but does not cover any phylogeneticmodels. We tried to keep this tutorial very basic and introduce all thelanguage concepts and theory on the way. You may only need the Revsyntaxtutorialfor a more in-depth discussion of concepts in Rev.For this tutorial it is especially important that you have read the twotutorials on diversification rate estimation: Basic DiversificationRate Estimationtutorialand Diversification Rates Through Timetutorial.Specifically the Diversification Rates Through Timetutorialpresent the underlying diversification model and thus foundation forthis tutorial. Here we will build on the tutorial by modifying theassumptions of incomplete taxon sampling in different ways.Data and filesWe provide the data file(s) which we will use in this tutorial. You maywant to use your own data instead. In the ‘data‘ folder, you will findthe following file  ‘primates.tre‘: Dated primates phylogeny including 23 out of377 species.Note that we use here the small primate phylogeny including only 23 ofthe 377 taxa instead of the much more complete primate phylogeny from(Springer et al. 2012). This choice was solely made to emphasize the point andimpact of incomplete taxon sampling, which is a very prominent featurein many large scale phylogenies.Open the tree ‘data/primates.tre‘ inFigTree.  Two scenarios ofbirth-death models. On the left we show constant diversification. On theright we show an example of an episodic birth-death process where ratesare constant in each time interval (epoch). The top panel of this figureshows example realization under the given rates.Episodic Birth-Death ModelHere we study the impact of incomplete taxon sampling by estimatingdiversification rates through time. The goal is to compare the impact ofdifferent taxon sampling strategies rather than the description of thediversification-rate model itself. The episodic birth-death model usedhere is equivalent to the model described in our previous tutorial.Please read the Diversification Rates ThroughTimetutorial for more detailed information about the model.We have included in Figure [fig:EBD] again the cartoon of episodicbirth-death process with piecewise constant diversification rates. Asmentioned above, diversification rate estimates are biased when only afraction of the species is included and the sampling scheme is notaccommodated appropriately(missing reference). Hence, thediversification rate through time model will be an excellent example tostudy the impact of the assumed incomplete sampling strategy ondiversification rates.  A graphical modelwith the outline of the Rev code. On the left we see the graphicalmodel describing the correlated (Brownian motion) model forrate-variation through time. On the right we show the correspond Revcommands to instantiate this model in computer memory. This figure givesa complete overview of the model that we use here in this analysis.We additionally include the graphical model representing the episodicbirth-death process with autocorrelated diversification rates. Thisgraphical model shows you which variables are included in the model, andthe dependency between the variables. Thus, it makes the structure andassumption of the model clear and visible instead of a black-box(Höhna et al. 2014). Here we will focus only on the variable ‘rho‘, thesampling probability, to model incomplete taxon sampling.Specifying the model in RevWe will give a very brief and compressed version of the model with fewercomments and explanation. The more detailed explanation can be found inthe Diversification Rates Through Timetutorial.Any attempt from us to present the full description here would only be aduplication/copy of the original tutorial with the additional to be lesscomplete and less up to date.Here are the summarized steps for running the episodic birth-death modelin Rev.######################## Reading in the Data ########################### Read in the \"observed\" treeT &lt;- readTrees(\"data/primates.tre\")[1]# Get some useful variables from the data. We need these later on.taxa &lt;- T.taxa()# set my move indexmvi = 0mni = 0NUM_INTERVALS = 10##################### Create the rates ###################### first we create the standard deviation of the rates between intervals# draw the sd from an exponential distributionspeciation_sd ~ dnExponential(1.0)moves[++mvi] = mvScale(speciation_sd,weight=5.0)extinction_sd ~ dnExponential(1.0)moves[++mvi] = mvScale(extinction_sd,weight=5.0)# create a random variable at the present timelog_speciation[1] ~ dnUniform(-10.0,10.0)log_extinction[1] ~ dnUniform(-10.0,10.0)# apply moves on the ratesmoves[++mvi] = mvSlide(log_speciation[1], weight=2)moves[++mvi] = mvSlide(log_extinction[1], weight=2)speciation[1] := exp( log_speciation[1] )extinction[1] := exp( log_extinction[1] )for (i in 1:NUM_INTERVALS) {    index = i+1    # specify normal priors (= Brownian motion) on the log of the rates    log_speciation[index] ~ dnNormal( mean=log_speciation[i], sd=speciation_sd )    log_extinction[index] ~ dnNormal( mean=log_extinction[i], sd=extinction_sd )    # apply moves on the rates    moves[++mvi] = mvSlide(log_speciation[index], weight=2)    moves[++mvi] = mvSlide(log_extinction[index], weight=2)    # transform the log-rate into actual rates    speciation[index] := exp( log_speciation[index] )    extinction[index] := exp( log_extinction[index] )}moves[++mvi] = mvVectorSlide(log_speciation, weight=10)moves[++mvi] = mvVectorSlide(log_extinction, weight=10)moves[++mvi] = mvShrinkExpand( log_speciation, sd=speciation_sd, weight=10 )moves[++mvi] = mvShrinkExpand( log_extinction, sd=extinction_sd, weight=10 )interval_times &lt;- T.rootAge() * (1:NUM_INTERVALS) / (NUM_INTERVALS) * 0.8### rho is the probability of sampling species at the present### fix this to 23/377, since there are ~377 described species of primates### and we have sampled 23rho &lt;- T.ntips()/377timetree ~ dnEpisodicBirthDeath(rootAge=T.rootAge(), lambdaRates=speciation, lambdaTimes=interval_times, muRates=extinction, muTimes=interval_times, rho=rho, samplingStrategy=\"uniform\", condition=\"survival\", taxa=taxa)### clamp the model with the \"observed\" treetimetree.clamp(T)############## The Model ################# workspace model wrapper ###mymodel = model(timetree)### set up the monitors that will output parameter values to file and screenmonitors[++mni] = mnModel(filename=\"output/primates_uniform.log\",printgen=10, separator = TAB)monitors[++mni] = mnFile(filename=\"output/primates_uniform_speciation_rates.log\",printgen=10, separator = TAB, speciation)monitors[++mni] = mnFile(filename=\"output/primates_uniform_speciation_times.log\",printgen=10, separator = TAB, interval_times)#monitors[++mni] = mnFile(filename=\"output/primates_uniform_extinction_rates.log\",printgen=10, separator = TAB, extinction)monitors[++mni] = mnFile(filename=\"output/primates_uniform_extinction_times.log\",printgen=10, separator = TAB, interval_times)monitors[++mni] = mnScreen(printgen=1000, extinction_sd, speciation_sd)################# The Analysis #################### workspace mcmc ###mymcmc = mcmc(mymodel, monitors, moves)### pre-burnin to tune the proposals ###mymcmc.burnin(generations=10000,tuningInterval=200)### run the MCMC ###mymcmc.run(generations=50000)This Rev code shows the template for estimating episodicdiversification rates. In the next sections we will tweak the script forthe different sampling schemes.Uniform Taxon SamplingIn our first analysis we will assume uniform taxon sampling [see@Hoehna2011} (missing reference). Uniform taxon sampling corresponds to theassumption that every species has the same probability $\\rho$ to beincluded (i.e.,sampled) in our study.Imagine flipping a coin that has the probability $\\rho$ to show upheads. For every species you flip the coin and are going to include thespecies, for example by sequencing it, in your study. This is what theassumption of uniform taxon sampling means.For our study, we know that we have sampled 23 out of 377 living primatespecies. To account for this we can set the sampling parameter as aconstant variable with a value of 23/377.rho &lt;- T.ntips()/377Note that in principle you could specify a hyperprior distribution onthe sampling probability ‘rho‘. However, all three parameters(speciation rate, extinction rate, and sampling probability) are notidentifiable (Stadler 2009). Nevertheless, we could specify informativepriors on the sampling fraction if, for example, we know that the truediversity is in same range.Moreover, we specify the uniform sampling scheme by setting‘samplingStrategy=“uniform”‘ in the birth-death process.timetree ~ dnEpisodicBirthDeath(rootAge=T.rootAge(), lambdaRates=speciation, lambdaTimes=interval_times, muRates=extinction, muTimes=interval_times, rho=rho, samplingStrategy=\"uniform\", condition=\"survival\", taxa=taxa)This is exactly what we did in the Rev script above.The Rev file for performing this analysis:‘mcmc_uniform.Rev‘.Exercise 1      Run an MCMC simulation to estimate the posterior distribution of thespeciation rate and extinction rate through time assuming uniformtaxon sampling. You can use the script ‘mcmc_uniform.Rev‘ to runthe analysis.        Visualize the rate through time usingRand RevGadgets.    Resulting diversification rateestimations when using 20 intervals and assuming uniform taxon sampling.You should create similar plots for the other sampling schemes andcompare the rates through time.Summarizing and plotting diversification rates through timeWhen the analysis is complete, you will have the monitored files in youroutput directory. You can then visualize the rates through time usingRusing our package RevGadgets. If you don’t have theR-package RevGadgets installed, or if you have trouble with thepackage, then please read the separate tutorial about the package.Just start Rin the main directory for this analysis andthen type the following commands:library(RevGadgets)tree &lt;- read.nexus(\"data/primates.tre\")files &lt;- c(\"output/primates_uniform_speciation_times.log\", \"output/primates_uniform_speciation_rates.log\", \"output/primates_uniform_extinction_times.log\", \"output/primates_uniform_extinction_rates.log\")rev_out &lt;- rev.process.output(files,tree,burnin=0.25,numIntervals=100)pdf(\"uniform.pdf\")par(mfrow=c(2,2))rev.plot.output(rev_out)dev.off()You can see the resulting plot in Figure [fig:EBD_Results].Diversified Taxon SamplingIn the previous analysis we assumed that species were sampled uniformly.However, this assumption is very often violated(missing reference). For example, the primate phylogeny that weuse in this tutorial includes one species for almost all genera. Thus,we had selected the species for the study neither uniformly nor randomlybut instead by including one species per genera and hence maximizingdiversity. This sampling scheme is called diversified taxon sampling(Höhna et al. 2011).  Example ofdiversified taxon sampling. a) An example phylogeny showing that allspecies after a certain time are not sampled. b) The cumulativeprobability of a speciation event occurring as a function of time. Herewe see that the highest probability for a speciation event is morerecently.Figure [fig:DiversifiedSampling] shows an example of diversifiedsampling. The example shows the same tree as in Figure [fig:BDP] where5 species are sampled. In fact, here we sampled 5 species so that everygroup is included and the most recent speciation events are excluded(not sampled).In RevBayes we can specify diversified taxon sampling in the sameway as we did uniform taxon sampling. First, we specify a constantvariable for the sampling fraction ‘rho‘ which we set to the number ofincluded (sampled) taxa divided by the number of total taxa in thegroup.rho &lt;- T.ntips()/377Then, we specify that our sampling strategy was diversified by settingthe argument of the birth-death processto‘samplingStrategy=“diversified”‘.timetree ~ dnEpisodicBirthDeath(rootAge=T.rootAge(), lambdaRates=speciation, lambdaTimes=interval_times, muRates=extinction, muTimes=interval_times, rho=rho, samplingStrategy=\"diversified\", condition=\"time\", taxa=taxa)This is all we needed to do to change our previous script to modeldiversified taxon sampling.Exercise 2      Copy the Rev script ‘mcmc_uniform.Rev‘ and nameit ‘mcmc_diversified.Rev‘.        Make the changes so that you assume now diversifiedtaxon sampling.        Change the file names in the monitors from ‘uniform‘to ‘diversified‘.        Run the analysis and plot the diversification rates.        How does the new sampling assumption influence your estimated rates?  Empirical Taxon SamplingUnfortunately, diversified taxon sampling was derived under a strictmathematical concept that assumes all species that speciated before agiven time were included and all other species were discarded (notsampled); see Figure [fig:DiversifiedSampling]. The diversifiedsampling strategy is clearly to restrictive to be realistic and can biasparameter estimates too (Höhna 2014). As another alternative we applyan empirical taxon sampling strategy that uses empirical informationon the clade relationships and speciation times of the missing species.For example, in the primate phylogeny we know the crown age ofHominoidea and know that 19 additional speciation events must havehappened between the crown age of the Hominoidea and the present time toaccommodate the 19 missing species (seeFigure [fig:EmpiricalSampling]). In fact, we can obtain for all largergroups the crown ages and the number of missing species and thus narrowdown with empirical evidence the times when these missing speciationevents have happened.  Cartoon of empirical taxonsampling. The triangle in the phylogeny depict clades with missingspecies. To illustrate the point we have written the names of highertaxa on the right with the number of species belonging to them. Fromthis number of taxa in the clade we can compute how many species aremissing per clade and which crown age the clade.In your phylogeny you can count the number of species belonging to agiven clade and thus compute how many species are missing for thisclade. Then, you can pick two or more species to define the clade. Thesespecies will be used to compute the crown age. For example, we usePan_paniscus Hylobates_lar to define the Hominoidae clade. If wewould have Homo_sapiens sampled as well then we could additionallyinclude it in the clade but we could not leave out Hylobates_lar.In Rev we specify several clades and give the number of missingspecies.Galagidae             = clade(\"Galago_senegalensis\",  \"Otolemur_crassicaudatus\", missing= 17)Lorisidae             = clade(\"Perodicticus_potto\", \"Loris_tardigradus\", \"Nycticebus_coucang\", missing=6)Cheirogaleoidea       = clade(\"Cheirogaleus_major\", \"Microcebus_murinus\", missing= 19)Lemuridae             = clade(\"Lemur_catta\", \"Varecia_variegata_variegata\", missing=17)Lemuriformes          = clade(Lemuridae, Cheirogaleoidea, missing=29)Atelidae_Aotidae      = clade(\"Alouatta_palliata\", \"Aotus_trivirgatus\", missing=30)NWM                   = clade(Atelidae_Aotidae,\"Callicebus_donacophilus\", \"Saimiri_sciureus\", \"Cebus_albifrons\", missing=93)Hominoidea            = clade(\"Pan_paniscus\", \"Hylobates_lar\", missing=19)Cercopithecoidea      = clade(\"Colobus_guereza\", \"Macaca_mulatta\", \"Chlorocebus_aethiops\", missing=60)Next, we combine all clades into a single vector for later use.missing_species_per_clade = v(Galagidae, Lorisidae, Cheirogaleoidea, Lemuridae, Lemuriformes, Atelidae_Aotidae, NWM, Hominoidea, Cercopithecoidea)In the birth-death model we include these missing speciation events byintegrating over the known interval when these speciation events havehappened (between the crown age and the present). This integral of theprobability density of a speciation event is exactly the same as oneminus the cumulative distribution function of a speciation event,\\(F(t|N(t_1)=1,t_1\\leq t \\leq T) = 1 - \\frac{1-P(N(T)&gt;0|N(t)=1)\\exp{(r(t,T))}}{1-P(N(T)&gt;0|N(t_1)=1)\\exp{(r(t_1,T))}} \\label{spec_dist}\\)which was previously derived by @Hoehna2014a [Equation (6)] (see also@Yang1997 [Equation (3)] for constant rates and @Hoehna2013[Equation (8)]).Let us define $\\mathbb{K}$ as the set of missing species and$|\\mathbb{K}|$ the number of clades with missing species. In our examplewe have $|\\mathbb{K}| = 9$ clades. Additionally, we define $c_i$ as thetime of most recent common ancestor of the $i^{th}$ clade.Then, the joint probability density of the sampled reconstructed treeand the empirically informed missing speciation times is\\(\\begin{aligned}f(\\Psi,\\mathbb{K}|N(t_1\\!=\\!0)\\!=\\!2,S(2,t_1\\!=\\!0,T))  &amp; = &amp; f(\\Psi|N(t_1\\!=\\!0)\\!=\\!2,S(2,t_1\\!=\\!0,T)) \\nonumber \\\\&amp; &amp;  \\times\\prod_{i=1}^{|\\mathbb{K}|}\\left(1-F(t|N(c_{i})=1,c_{i}\\leq t \\leq T)\\right)^{k_i}  \\nonumber\\\\ \\label{eq:timesAndMissing}\\end{aligned}\\)Equation ([eq:timesAndMissing]) is actually proportional to theoriginal equation under the birth-death process times the probabilitiesof the missing species. There are two things to consider when specifyingempirical taxon sampling in RevBayes. First, we set the “traditional”sampling fraction to one (‘rho=1.0‘). Second, we provide the clades withmissing species as an argument of the birth-death model(‘incompleteClades=missing_species_per_clade‘).timetree ~ dnEpisodicBirthDeath(rootAge=T.rootAge(), lambdaRates=speciation, lambdaTimes=interval_times, muRates=extinction, muTimes=interval_times, rho=1.0, taxa=taxa, incompleteClades=missing_species_per_clade, condition=\"time\")These are the only necessary changes to perform a diversification rateanalysis under empirical taxon sampling.Exercise 3      Copy the Rev script ‘mcmc_uniform.Rev‘ and nameit ‘mcmc_empirical.Rev‘.        Make the changes so that you assume now empirical taxon sampling.        Change the file names in the monitors from ‘uniform‘ to ‘empirical‘.        Run the analysis and plot the diversification rates.        How does the new sampling assumption influence your estimated rates?  ",
        "url": "/tutorials/divrate/sampling.html",
        "index": "true"
      }
      ,
    
      "tutorials-morph-ase-scm-hrm-html": {
        "title": "Discrete morphology - Stochastic Character Mapping and Hidden Rates",
        "content": "OverviewIn this tutorial we focus on two new elements.First, we will focus on how to model rate variation among lineages using hidden rate models.Second, we will apply stochastic character mapping to estimate the location of character transitions.Previously, we primarily estimated the ancestral states, which indirectly also show the transitions.Ancestral state estimates are more powerful in their summary in that they show uncertainties of the estimated states.Stochastic character maps are more powerful in showing a specific sequence of changes, especially for somewhat ordered models, such as the hidden rate models.Stochastic character maps could be used to build ancestral state estimates as well, but this is not the purpose here.Hidden rates to test for rate variationRates of morphological evolution, e.g., rates of gains and losses, might not have been constant over time and/or among different lineages.For example, phylogenetically local clusters of plant lineages appear to transition between herbaceous and woody habits at relatively high rates, so one might want to quantify where these bursts occur (Beaulieu et al. 2013).There are two approaches how to include this rate variation.First, we could employ relaxed clock models (see Relaxed Clocks &amp; Time Trees Tutorial) which are very powerful and flexible.Second, we can use hidden rates models (HRMs).HRMs do not observe the hidden state that induce the mode-shifts.Instead, HRMs expand the character’s state space by a factor of $K$, and observe the character once for each of the $K$ categories.For example, take a binary character modeled with $K=2$ hidden state classes.The model would treat a character that is observed as being in state 0 as possibly being in either of the $K=2$ classes (0,1) and (0,2).In practice, this is done by setting the likelihood of observing those $0k$ states to equal 1, thus, similar as ambiguous characters.Remember the rate matrix for our independent rates model,\\(Q = \\begin{smallmatrix} &amp; \\begin{smallmatrix}0 &amp; 1\\end{smallmatrix} \\\\\\begin{smallmatrix}0\\\\1\\end{smallmatrix} &amp;  \\left(\\begin{smallmatrix}- &amp; \\mu_1\\\\\\mu_2 &amp; -\\end{smallmatrix}\\right)\\\\\\end{smallmatrix}\\)where we can say that $\\mu_1$ is the rate of gain and $\\mu_2$ is the rate of loss.Now if we say that there are two rate classes, one where characters evolve fast and the other where characters evolve slow, then we have a 4x4 rate matrix which is\\(Q = \\begin{smallmatrix} &amp; \\begin{smallmatrix}0S &amp; 1S &amp; 0F &amp; 1F\\end{smallmatrix} \\\\\\begin{smallmatrix}0S\\\\1S\\\\0F\\\\1F\\end{smallmatrix} &amp;  \\left(\\begin{smallmatrix}- &amp; \\mu_{1,s} &amp; \\alpha &amp; 0 \\\\ \\mu_{2,s} &amp; - &amp; 0 &amp; \\alpha\\\\ \\beta &amp; 0 &amp; - &amp; \\mu_{1,f} \\\\ 0 &amp; \\beta &amp; \\mu_{2,f} &amp; -  \\end{smallmatrix}\\right)\\\\\\end{smallmatrix}\\)As you can see, we have now 6 parameters.These parameters are:  $\\mu_{1,s}$ the slow rate of gain  $\\mu_{1,f}$ the fast rate of gain  $\\mu_{2,s}$ the slow rate of loss  $\\mu_{2,f}$ the fast rate of loss  $\\alpha$ the rate of switching from the slow to the fast category  $\\beta$ the rate of switching from the fast to the slow categoryIn principle, one can add arbitrarily many more rate categories, for example, using three rate categories (slow, intermediate and fast) or four rate categories (very slow, slow, fast, very fast).In this tutorial, we show you the basic steps how to set up this hidden rates model with a slow and a fast class.  Let us start with a fresh Rev script.Create an empty text file and call it `mcmc_scm_hrm.Rev.Load Data MatricesAs before, use the function readDiscreteCharacterData() to load a data matrix to the workspace from a formatted file.Import the morphological character matrix and assign it to the variable morpho.morpho &lt;- readDiscreteCharacterData(\"data/primates_solitariness.nex\")However, now we also need to expand the state space to include the 2 categories.In RevBayes, the character data matrix has a member function .expandCharacters() which will create these additional categories.morpho_exp = morpho.expandCharacters( 2 )To understand better what has happened, let us look at the character data matrices in RevBayes.First, we look at the original character data matrix.morpho   Standard character matrix with 233 taxa and 1 characters   ========================================================   Origination:                   primates_solitariness.nex   Number of taxa:                233   Number of included taxa:       233   Number of characters:          1   Number of included characters: 1   Datatype:                      StandardNotice that the Datatype for this character data matrix is Standard.Now look at the expanded character data matrix.&gt; morpho_exp   NaturalNumbers character matrix with 233 taxa and 1 characters   ==============================================================   Origination:                      Number of taxa:                233   Number of included taxa:       233   Number of characters:          1   Number of included characters: 1   Datatype:                      NaturalNumbersNotice that the Datatype here is NaturalNumbers.Let us now also look at how the states are expanded.Again, we look first at the original character data matrix.&gt; morpho.show()   Allenopithecus_nigroviridis   0   Allocebus_trichotis   1   Alouatta_belzebul   0   Alouatta_caraya   0   Alouatta_coibensis   0   Alouatta_fusca   0   Alouatta_palliata   0   Alouatta_pigra   0   Alouatta_sara   ?...And next for the expanded character data matrix.&gt; morpho_exp.show()   Allenopithecus_nigroviridis   (0 2)   Allocebus_trichotis   (1 3)   Alouatta_belzebul   (0 2)   Alouatta_caraya   (0 2)   Alouatta_coibensis   (0 2)   Alouatta_fusca   (0 2)   Alouatta_palliata   (0 2)   Alouatta_pigra   (0 2)   Alouatta_sara   ?...We see that state 0 (e.g., for Allenopithecus nigroviridis) was expanded to state (0 2) and that state 1 (e.g., Allocebus trichotis) was expanded to (1 3).The state (0 2) means that the species is either in state 0 or state 2, we don’t know which.This is exactly how ambiguous data is coded.It also means that our previous state 0 now corresponds to 0 = No-Slow and 2 = No-Fast.It is important to remember how the state space was expanded to set the rates up correctly.Create Helper VariablesAs before, we need to instantiate a couple “helper variables” that will be used by downstream parts of our model specification files.Create vectors of moves and monitorsmoves = VectorMoves()monitors = VectorMonitors()The PhylogenyAs usual for morphological analysis, we assume the phylogeny to be know.Thus, we read in the tree as a constant variable:phylogeny &lt;- readTrees(\"data/primates_tree.nex\")[1]The Hidden Rates Model (HRM)Now we need to specify the hidden rates model.Have a look again above at the rate matrix that we need to specify.In the current example, we assume a binary morphological character and two rate categories.This gives 4 states in total and therefore a 4x4 rate matrix.Start with creating a matrix called rates where all elements are 0.0.# we will fill the non-zero elements belowfor (i in 1:4) {  for (j in 1:4) {    rates[i][j] &lt;- 0.0  }}Next, we need to specify some priors for our rates.It is probably quite challenging to have a good idea of a reasonable rate for the hidden category changes, i.e., the rate of changing between the slow and the fast rate categories.We simply assume the same prior as before, that is, we assume on average 10 changes along the given phylogeny.rate_pr := phylogeny.treeLength() / 10Next, we need to assume some model how the fast and the slow rates are specified.If we simply use free parameters for $\\mu_{1,s}$ and $\\mu_{1,f}$, we could easily estimate that $\\mu_{1,s} &gt; \\mu_{1,f}$ or the other way around.So this model is clearly non-identifiable and we need to restrict that $\\mu_{1,s} &lt; \\mu_{1,f}$.One option is to say that $\\mu_{1,f} = \\gamma * \\mu_{1,s}$ where $\\gamma \\leq 1.0$.In words, $\\gamma$ is the factor how much the fast rate of gain is faster than the slow rate of gain.An alternative solution is to specify that $\\mu_{1}$ comes from a distribution, for example, a lognormal or gamma distribution, and $\\mu_{1,i}$ corresponds to the median of the i-th quantile.This approach is analogous in idea to the well known $+\\Gamma$ model of among site rate variation for nulceotide evolution (Yang 1994).We will use the second approach but give some thoughts below on how to specify the first approach.First, we need to specify the prior distribution on the standard deviation of the lognormal distribution.Let us assume an exponential that on standard deviation with mean of $0.587405$, which means that the 95% probability interval of the lognormal distribution spans 1 order of magnitude.H &lt;- 0.587405SD_PRIOR &lt;- 1/HNow that we have the hyper-prior parameters, we can start with the prior distribution.Let us start with the rate of gain.rate_gain_median ~ dnExponential( rate_pr )rate_gain_sd ~ dnExponential( SD_PRIOR )Since these are positive real variable, we apply scaling moves on them.moves.append( mvScale( rate_gain_median, weight=2 ) )moves.append( mvScale( rate_gain_sd, weight=2 ) )Next, we use the prior median and prior standard deviation to construct the median quantiles of the lognormal distribution.We will use the function fnDiscretizeDistribution, which takes as arguments the distribution and the number of quantiles, which is 2 in our case for 2 rate categories (slow vs fast).rate_gain := fnDiscretizeDistribution( dnLognormal( ln(rate_gain_median), rate_gain_sd ), 2 )Now repeat exactly the same for the loss rate.rate_loss_median ~ dnExponential( rate_pr )rate_loss_sd ~ dnExponential( SD_PRIOR )moves.append( mvScale( rate_loss_median, weight=2 ) )moves.append( mvScale( rate_loss_sd, weight=2 ) )rate_loss := fnDiscretizeDistribution( dnLognormal( ln(rate_loss_median), rate_loss_sd ), NUM_HIDDEN_STATES )Finally, we create the two rate variables for the switching rates between the fast and slow rate categories.As mentioned before, we will simply assume an exponential prior distribution with a mean of 10 events along the phylogeny.switch_slow_fast ~ dnExponential( rate_pr )switch_fast_slow ~ dnExponential( rate_pr )We also should not forget the moves on the switching rates.We will use as usual the scaling move since these are rate variables (positive real numbers).moves.append( mvScale( switch_slow_fast, weight=2 ) )moves.append( mvScale( switch_fast_slow, weight=2 ) )Now we have created all the rate variables.We need to connect them to our rate matrix.As a help, look again at the rate matrix described in the introduction.rates[1][2] := rate_gain[1]     # 0S-&gt;1Srates[1][3] := switch_slow_fast # 0S-&gt;0Frates[2][1] := rate_loss[1]     # 1S-&gt;0Srates[2][4] := switch_slow_fast # 1S-&gt;1Frates[3][1] := switch_fast_slow # 0F-&gt;0Srates[3][4] := rate_gain[2]     # 0F-&gt;1Frates[4][2] := switch_fast_slow # 1F-&gt;1Srates[4][3] := rate_loss[2]     # 1F-&gt;2FFinally, we can create our transition rate matrix Q using the rate matrix function fnFreeK.Q_morpho := fnFreeK(rates, rescaled=FALSE)For this model, we also want to specify parameters for the root frequencies $\\pi$, and thus also their prior distributions.We assume a flat Dirichlet distribution, which assigns each combination of root frequencies the exact same prior probability.Remember that we 2 states for the observed characters and 2 states for the rate categories.Thus we need a vector of 2*2 filled with ones.rf_prior &lt;- rep(1,2*2)We use this for our Dirichlet distribution.rf ~ dnDirichlet( rf_prior )We apply two different moves to the root frequencies, a mvBetaSimplex that changes a single frequencies and rescales the other frequencies, and a mvDirichletSimplex that redraws all root frequencies together.moves.append( mvBetaSimplex( rf, weight=2 ) )moves.append( mvDirichletSimplex( rf, weight=2 ) )Lastly, we set up the CTMC.Not that this time we need to specify the type=NaturalNumbers, as we saw this is used in the expanded data matrix.phyMorpho ~ dnPhyloCTMC(tree=phylogeny, Q=Q_morpho, rootFrequencies=rf, type=\"NaturalNumbers\")We conclude the model specification by attaching the expanded data matrix to the CTMC object.phyMorpho.clamp(morpho_exp)Complete MCMC AnalysisCreate Model ObjectWe can now create our workspace model variable with our fully specified model DAG.We will do this with the model() function and provide a single node in the graph (phylogeny).mymodel = model(phylogeny)The object mymodel is a wrapper around the entire model graph and allows us to pass the model to various functions that are specific to our MCMC analysis.Specify Monitors and Output FilenamesIn this exercise we wanted to explore stochastic character mapping.Stochastic character mapping, similar to ancestral state estimation, is achieved in RevBayes using the help of monitors, specifically the mnStochasticCharacterMap monitor.We will specify the same model monitor (mnModel), screen monitor (mnScreen) and ancestral state monitor (mnJointConditionalAncestralState) as before (Discrete morphology - Ancestral State Estimation).# 1. for the full model #monitors.append( mnModel(filename=\"output/solitariness_hrm.log\", printgen=1) )# 2. and a few select parameters to be printed to the screen #monitors.append( mnScreen(printgen=10) )# 3. add an ancestral state monitormonitors.append( mnJointConditionalAncestralState(tree=phylogeny,                                                  ctmc=phyMorpho,                                                  filename=\"output/solitariness_hrm.states.txt\",                                                  type=\"NaturalNumbers\",                                                  printgen=1,                                                  withTips=true,                                                  withStartStates=false) )Now add the new stochastic character mapping monitor, mnStochasticCharacterMap.This monitor also requires that you specify the CTMC object, which is phyMorpho in our example, and output filename, the frequency how often you want to generate stochastic character maps, e.g., once every iteration or every 10 generations, and whether we want to include the simmap states (yes, very important).# 4. add an stochastic character map monitormonitors.append( mnStochasticCharacterMap(ctmc=phyMorpho,                                          filename=\"output/solitariness_hrm_stoch_char_map.log\",                                          printgen=1,                                          include_simmap=true) )This monitor will create the output/solitariness_hrm_stoch_char_map.log file.Just like the other log files, each row in this file represents a different sample from the MCMC.Each column in the file, though, is the character history for a different node in the phylogeny.Set-Up the MCMCSetup the MCMC analysis as before (Discrete morphology - Ancestral State Estimation).This will run 2 replicated MCMC runs with 5,000 iterations and auto-tuning the moves every 200 iterations.mymcmc = mcmc(mymodel, monitors, moves, nruns=2, combine=\"mixed\")mymcmc.run(generations=5000, tuningInterval=200)Summarizing the MCMC outputAfter the MCMC simulation, we can calculate the maximum a posteriorimarginal, joint, or conditional character history.As before (Discrete morphology - Ancestral State Estimation), we will compute the ancestral state estimates.# Read in the tree trace and construct the ancestral states (ASE) #anc_states = readAncestralStateTrace(\"output/solitariness_hrm.states.txt\")anc_tree = ancestralStateTree(tree=phylogeny, ancestral_state_trace_vector=anc_states, include_start_states=false, file=\"output/solitariness_ase_hrm.tree\", burnin=0.25, summary_statistic=\"MAP\", site=1, nStates=2*2)In a very similar way, we summarize the output of the stochastic character mapping.First, we load in the ancestral state trace (sampled character histories)anc_states_stoch_map = readAncestralStateTrace(\"output/solitariness_hrm_stoch_char_map.log\")Then we use the characterMapTree function.This generates two SIMMAP (Bollback 2006)formatted files:1) the maximum a posteriori character history,and 2) the posterior probabilities of the entire character history.These can be plotted using the phytools R package (Revell 2012).char_map_tree = characterMapTree(tree=phylogeny,                 ancestral_state_trace_vector=anc_states_stoch_map,                 character_file=\"output/solitariness_hrm_marginal_character.tree\",                 posterior_file=\"output/solitariness_hrm_marginal_posterior.tree\",                 burnin=0.25,                 num_time_slices=500)This is all you need for this analysis.Don’t forget to quit RevBayes at the end of the script.# Quit RevBayes #q()  This is all that you need to do for the rate variation analysis with hidden rate categories and stochastic character mapping. Save your script and give it a try!Evaluate and Summarize Your ResultsVisualizing Ancestral State EstimatesWe have previously plotted the ancestral states, both the maximum a posterior (MAP) states as well as the posterior probabilities of all states shown as pie chart Discrete morphology - Ancestral State Estimation.You should repeat plotting the ancestral states now also for the hidden rates model (hrm) analyses.My output is shown in .Finally, we also want to plot the stochastic character mapping.Unfortunately, this cannot be done yet in RevGadgets and instead we are going to use phytools (Revell 2012).Here is a slightly simplified version of the script whithout changing the color scheme.library(plotrix)library(phytools)character_file = \"output/solitariness_hrm_marginal_character.tree\"sim2 = read.simmap(file=character_file, format=\"phylip\")colors = vector()for (i in 1:length( sim2$maps ) ) {    colors = c(colors, names(sim2$maps[[i]]) )}colors = sort(as.numeric(unique(colors)))col_idx = colors + 1cols = setNames( rainbow(length(colors), start=0.0, end=0.9), colors)cols = cols[col_idx]pdf( \"Primates_solitariness_HRM_simmap.pdf\" )plotSimmap(sim2, cols, fsize=0.5, lwd=1, split.vertical=TRUE, ftype=\"i\")# add legendx = 0y = 225leg = c(\"no - slow\", \"yes - slow\", \"no - fast\", \"yes - fast\")leg = leg[col_idx]colors = colsy = y - (0:(length(leg) - 1))*10x = rep(x, length(y))text(x + 0.005, y, leg, pos=4, cex=1.15)mapply(draw.circle, x=x, y=y, col=colors, MoreArgs = list(nv=200, radius=1, border=\"white\"))dev.off()Stochastic character map (left) and ancestral state estimates (middle and right) of solitariness under the hidden rates model. You might notice that there is a switch from the fast to the slow rate category for the no state, but we never visited the slow rate category for the yes state.  How can you explain the observed allocation of clades to the slow and fast rate categories?Do ancestral state estimates match with the stochastic character maps?How certain are we in the ancestral state estimates?  Click below to begin the next exercise!  Testing for Correlation between Characters",
        "url": "/tutorials/morph_ase/scm_hrm.html",
        "index": "true"
      }
      ,
    
      "search-html": {
        "title": "Search Results",
        "content": "    Search     {% include search-box-page.html %}                {% include navbar.html %}",
        "url": "/search.html",
        "index": ""
      }
      ,
    
      "developer-docker-setup-html": {
        "title": "Setting up Docker",
        "content": "## Setting up DockerThe first step to this is installing [Docker](https://docs.docker.com/get-docker/). For Windows or MacOS this means installing Docker Desktop. For Linux this means installing just regular old Docker. For this tutorial I will also be using [vscode](https://code.visualstudio.com/) which is an open-source editor made by Microsoft (strange as that sentence is for me to type). The tutorial could easily be done in the terminal also (e.g. PowerShell, bash). If you want to use vscode you will also need the [Remote Explorer](https://github.com/Microsoft/vscode-remote-release) extension. If you search remote in the extensions tab its the one called Remote - Containers. This lets us connect our vscode window to the docker container. OK now we at least have Docker. We can run the Docker pull (currently this is not hosted on our organization page). Run this command in your terminal:    docker pull docker.pkg.github.com/wadedismukes/revbayes-dockerfiles/revbayesdockerfiles:ubuntu-latestThis will pull the image down which will take a bit. After this completes you will be able to run a Rev terminal using:    docker run --rm -it docker.pkg.github.com/wadedismukes/revbayes-dockerfiles/revbayesdockerfiles:ubuntu-latestIf you want a terminal inside of the container you can use:    docker run -it --entrypoint /bin/bash revbayesdockerfiles:ubuntu-latestWithout any errors you opened either revbayes or a terminal inside the container.",
        "url": "/developer/docker/setup.html",
        "index": ""
      }
      ,
    
      "tutorials-clocks-simple-html": {
        "title": "Simple Diversification Models &amp; Estimating Time Trees",
        "content": "{% section Introduction | introduction %}This tutorial describes how to perform a simple analysis that compares three different birth-death models using Bayes factors. The selected model will be used as a tree prior to estimate a new dated phylogeny of all living bears. For details about the format of this tutorial, please see the Tutorial Format Guide.{% section Getting Set Up | thedata %}### SoftwareThis tutorial requires that you download and install the current version of RevBayes. Additionally, you will need to download and install the program [Tracer](http://tree.bio.ed.ac.uk/software/tracer/) {% cite Rambaut2011 %}.### The DataThe various exercises in this tutorial take you through the stepsrequired to perform phylogenetic analyses of the example dataset.For this, you will need to download the data files.  To keep things organized, it is recommended that you create a new directory for this tutorial, and then place all of the data files in a subdirectory.> Create a new directory on your computer called `RB_div_clock_tutorial`.> > Within the `RB_div_clock_tutorial` directory, create a subdirectory called `data`. > Then, dowload the [`bears_dosReis.tre`](https://revbayes.github.io/revbayes-site/tutorials/clocks/data/bears_dosReis.tre) and [`bears_irbp.nex`](https://revbayes.github.io/revbayes-site/tutorials/clocks/data/bears_irbp.nex) and place them in the `data` folder.{:.instruction}Now you can execute RevBayes. > Navigate to the `RB_div_clock_tutorial` directory and execute the `rb` binary. > One option for doing this is to move the `rb` executable to the `RB_div_clock_tutorial`> directory.> > Alternatively, if you are on a Unix system, and have added RevBayes to your path, > you simply have to type `rb` in your Terminal to run the program. {:.instruction}Once you execute RevBayes, you will be in the console. The rest of this tutorial will proceed using the interactive console.{% section Comparing Two Diversification Models using Bayes Factors | secdiv %}In this first section, we will evaluate two different diversification models: (1) the Yule process and (2) the birth-death process.To compare the relative fit of the two models, we can use [_Bayes factors_](https://en.wikipedia.org/wiki/Bayes_factor) {% cite Suchard2001 Lartillot2006 Xie2011 Baele2012 Baele2013 %}.In Bayesian inference, the Bayes factor is used as a method for model comparison,allowing us to evaluate the relative support for different models given a single dataset.Given two models, $M_0$ and $M_1$, the Bayes-factor comparison assessingthe relative fit of each model to the data, $BF(M_0,M_1)$, is:$$BF(M_0,M_1) = \\frac{\\mbox{posterior odds}}{\\mbox{prior odds}}.$$ Theposterior odds is the posterior probability of $M_0$ given the data,$\\mathbf X$, divided by the posterior odds of $M_1$ given the data:$$\\mbox{posterior odds} = \\frac{\\mathbb{P}(M_0 \\mid \\mathbf X)}{\\mathbb{P}(M_1 \\mid \\mathbf X)},$$and the prior odds is the prior probability of $M_0$ divided by theprior probability of $M_1$:$$\\mbox{prior odds} = \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)}.$$ Thus,the Bayes factor measures the degree to which the data alter our beliefregarding the support for $M_0$ relative to $M_1$ {% cite Lavine1999 %}:$$\\begin{equation}BF(M_0,M_1) = \\frac{\\mathbb{P}(M_0 \\mid \\mathbf X, \\theta_0)}{\\mathbb{P}(M_1 \\mid \\mathbf X, \\theta_1)} \\div \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)} \\tag{1}\\label{eq:one}\\end{equation}$$Note that interpreting Bayes factors involves some subjectivity. Thatis, it is up to *you* to decide the degree of your belief in $M_0$relative to $M_1$. Despite the absence of an absolutely objectivemodel-selection threshold, we can refer to the scale, outlined by{% citet Jeffreys1961 %},  that provides a \"[rule-of-thumb](https://en.wikipedia.org/wiki/Bayes_factor#Interpretation)\" for interpreting thesemeasures.Unfortunately, it is generally not possible to directly calculate theposterior odds to prior odds ratios. However, we can further define theposterior odds ratio as: $$\\begin{aligned}\\frac{\\mathbb{P}(M_0 \\mid \\mathbf X)}{\\mathbb{P}(M_1 \\mid \\mathbf X)} = \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)} \\frac{\\mathbb{P}(\\mathbf X \\mid M_0)}{\\mathbb{P}(\\mathbf X \\mid M_1)},\\end{aligned}$$where $\\mathbb{P}(\\mathbf X \\mid M_i)$ is the *marginal likelihood* ofthe data (this may be familiar to you as the denominator of BayesTheorem, which is variously referred to as the *model evidence* or*integrated likelihood*). Formally, the marginal likelihood is theprobability of the observed data ($\\mathbf X$) under a given model($M_i$) that is averaged over all possible values of the parameters ofthe model ($\\theta_i$) with respect to the prior density on $\\theta_i$$$\\begin{aligned}\\mathbb{P}(\\mathbf X \\mid M_i) = \\int \\mathbb{P}(\\mathbf X \\mid \\theta_i) \\mathbb{P}(\\theta_i)dt.\\end{aligned}$$This makes it clear that more complex (parameter-rich) models arepenalized by virtue of the associated prior: each additional parameterentails integration of the likelihood over the corresponding priordensity. If you refer back to equation \\eqref{eq:one}, you can see that, withvery little algebra, the ratio of marginal likelihoods is equal to theBayes factor: $$\\begin{aligned}BF(M_0,M_1) = \\frac{\\mathbb{P}(\\mathbf X \\mid M_0)}{\\mathbb{P}(\\mathbf X \\mid M_1)} = \\frac{\\mathbb{P}(M_0 \\mid \\mathbf X, \\theta_0)}{\\mathbb{P}(M_1 \\mid \\mathbf X, \\theta_1)} \\div \\frac{\\mathbb{P}(M_0)}{\\mathbb{P}(M_1)}. \\end{aligned}$$Therefore, we can perform a Bayes factor comparison of two models bycalculating the marginal likelihood for each one. Alas, exact solutionsfor calculating marginal likelihoods are not known for phylogeneticmodels, thus we must resort to numericalintegration methods to estimate or approximate these values. In thisexercise, we will estimate the marginal likelihood for each partitionscheme using both the stepping-stone {% cite Xie2011 Fan2011 %} estimator.These algorithms aresimilar to the familiar MCMC algorithms, which are intended to samplefrom (and estimate) the joint posterior probability of the modelparameters. Stepping-stone algorithms are like a series of MCMCsimulations that iteratively sample from a specified number ofdistributions that are discrete steps between the posterior and theprior probability distributions. The basic idea is to estimate theprobability of the data for all points between the posterior and theprior—effectively summing the probability of the data over the priorprobability of the parameters to estimate the marginal likelihood.Technically, the steps correspond to a series of power posteriors,where the likelihood is iteratively raised to a series of numbersbetween 1 and 0. When the likelihood is raised tothe power of 1 (typically the first stepping stone), samples are drawnfrom the (untransformed) posterior. By contrast, when the likelihood israised to the power of 0 (typically the last stepping stone), samplesare drawn from the prior. To perform a stepping-stone simulation, weneed to specify (1) the number of stepping stones (power posteriors)that we will use to traverse the path between the posterior and theprior (*e.g.,* we specify 50 or 100 stones),(2) the spacing of the stones between the posterior and prior(*e.g.,* we may specify that the stones aredistributed according to a beta distribution), (3) the number of samples(and their thinning) to be drawn from each stepping stone, and (4) thedirection we will take (*i.e.,* from theposterior to the prior or vice versa).This analysis will assume that the phylogeny of the 8 extant bear species and their speciation times estimated in the study by {% citet DosReis2012 %} are observed data. Of course, it is not possible to observe a diversification process spanning over 30 My, but it is not unreasonable to use a well-supported phylogeny to compare models before analysis of a new dataset.{% subsection The Yule Model | yulesec %}{% citet Yule1925 %} described a stochastic process as a generating model for living taxa. This model makes the very simple assumption that over the course of diversification, there is a constant rate of speciation and the rate of extinction is 0.0. #### Read in the tree and get some helper variablesTo start our evaluation of the Yule model, we must first read in the published phylogeny andthe branch lengths (in units of millions of years). For this, we will create the variable `T` to represent the published tree and use the function `readTrees()` to get the tree from the file in the `data` folder.    T dot graph description language. This file can be opened using the program [Graphviz](https://www.graphviz.org/), or you can try pasting the contents of the file in a [web version](http://www.webgraphviz.com/) that can display the graph.We will call the graph `yule_graph.dot`. You can see the image generated by Graphviz in {% ref yulegraphv %}.    mymodel.graph(\"yule_graph.dot\"){% figure yulegraphv %}{% figcaption %}The probabilistic graphical model of the Yule diversification model for the bear phylogeny.This image was generated by running the `.graph()` method fo the model object and opening theresulting file in the program [Graphviz](https://www.graphviz.org/).{% endfigcaption %}{% endfigure %}#### Setting up the monitorsFor analysis methods in RevBayes, we typically want to save states to a file. To do this we create a list of monitors that specify how our samples are saved.To estimate the marginal likelihood using the stepping-stone approach, we will save thestates sampled for each step. All of the files written will be placed in a new directorycalled `output` that RevBayes will create in the directory where we are running it (if it isn't there already.)We also make a screen monitor so that we can see the progress of our analysis.    monitors[1] = mnModel(filename=\"output/bears_yule.log\",printgen=10, separator = TAB)    monitors[2] = mnScreen(printgen=1000, speciation)#### The analysis: Computing the marginal likelihood of the modelAbove in the [introduction section](#secdiv) of this part of the tutorial, we explainedBayes factors and the need to compute the marginal likelihoods. This approach uses the RevBayes function `powerPosterior()` to perform the series of MCMC simulations to estimate the marginal likelihood. The function takes the model, list of moves, and list of monitors. We must also specify an output file where the power posteriors are saved for each step and the number of steps taken. Here we will set the number of steps to `100`, using `cats=100`, this will result in 101 power posteriors (or \"stones\").    pow_p = powerPosterior(mymodel, moves, monitors, \"output/yule_powp.out\", cats=100, sampleFreq=10)We can start the power-posterior analysis by first burning in the chainand and discarding the first 10000 states. This will help ensure thatanalysis starts from a region of high posterior probability, rather thanfrom some random point.    pow_p.burnin(generations=10000,tuningInterval=200)Now we can execute the run.    pow_p.run(generations=10000)Once the power posteriors have been saved to file, create a steppingstone sampler. This function can read any file of power posteriors andcompute the marginal likelihood using stepping-stone sampling.    ss = steppingStoneSampler(file=\"output/yule_powp.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")The workspace variable `ss` now holds the estimated marginal likelihood. We can view the valueand print it to our screen.    ss.marginal()**Write down the value displayed. This is the marginal likelihood of the Yule model ($M_0$).**{% subsection The Birth-Death Model | bdsec %}The Yule model is a simple version of a more general stochastic branching process called the _birth-death process_ {% cite Kendall1948 Nee1994b Gernhard2008 %}.Under the birth-death process, extinction is assumed to be greater than 0. We will specify this model and also change the prior assumed for the speciation rate. Once we have completed our run of the stepping stone estimator, we can then compare the marginal likelihoods calculated for each.Before we begin a new model, we should first clear our workspace. This ensures that we won't have any variables left over from the previous section.    clear()#### Read in the tree and get some helper variablesWe will initialize the analysis just like we did above.    T {% figcaption %}The probabilistic graphical model of the birth-death diversification model for the bear phylogeny.This image was generated by running the `.graph()` method fo the model object and opening theresulting file in the program [Graphviz](https://www.graphviz.org/).{% endfigcaption %}{% endfigure %}#### Setting up the monitorsWe will create file and screen monitors like we did for the Yule model. It is important toremember to give the file monitor a different name than you did for the Yule analysis.    monitors[1] = mnModel(filename=\"output/bears_BDP.log\",printgen=10, separator = TAB)    monitors[2] = mnScreen(printgen=1000, speciation, extinction)#### The analysis: Computing the marginal likelihood of the modelNow it is time to setup and run our power posteriors:    pow_p = powerPosterior(mymodel, moves, monitors, \"output/BDP_powp.out\", cats=100, sampleFreq=10)    pow_p.burnin(generations=10000,tuningInterval=200)    pow_p.run(generations=10000)Use stepping-stone sampling to calculate marginal likelihood and print the value to the screen.    ss = steppingStoneSampler(file=\"output/BDP_powp.out\", powerColumnName=\"power\", likelihoodColumnName=\"likelihood\")    ss.marginal()**Write down the value displayed. This is the marginal likelihood of the birth-death model ($M_1$).**{% subsection Bayes Factors for Comparing Two Models | bayfacsec %}Now that we have estimates of the marginal likelihood for each of ourthe candidate substitution models, we can evaluate their relative fit tothe datasets using Bayes factors. Phylogenetic programs log-transformthe likelihood values to avoid[underflow](http://en.wikipedia.org/wiki/Arithmetic_underflow):multiplying likelihoods (numbers $ if $\\mathcal{K} > 1$, model $M_0$ is preferred>> if $\\mathcal{K} Substitution Model Tutorial. {% subsection Putting it All Together | phyloctmcsec %}Now we can link all of the elements in our graphical model. We will define a stochastic noderepresenting the sequence alignment. The distribution that generated our sequence data is a _phylogenetic continuous time Markov chain_. In RevBayes, this distribution is called `dnPhyloCTMC()`. For this model, the `dnPhyloCTMC()` expects the following arguments:* `tree`: a tree with branch lengths* `Q`: the instantaneous rate matrix* `branchRates`: the rate of substitution for each branch in the tree* `nSites`: the number of characters in our data matrix* `type`: the type of data, *i.e.*, `DNA`, `RNA`, `binary`, etc.We will call the stochastic node for our sequence data `phySeq`.    phySeq ~ dnPhyloCTMC(tree=timetree, Q=Q, branchRates=branch_rates, nSites=data.nchar(), type=\"DNA\")This node represents _observed data_. Thus, we will not perform any moves on this stochasticnode. Instead, we will \"clamp\" the node to the observed value. The `.clamp()` method will fix`phySeq` to the observed `data` matrix we imported in the section [above](#setting-up-the-workspace-and-importing-the-data).    phySeq.clamp(data)All of the parameters are now defined and linked via the phylogenetic CTMC. We can create amodel object to use the model in our workspace. It is also possible to visualize this modelusing `mymodel.graph(\"timetree_gm.dot\")` ({% ref timetreegv %}). However, this modelinvolves many more parameters than in the previous sections, so this tool becomes less useful.    mymodel = model(speciation){% figure timetreegv %}{% figcaption %}The probabilistic graphical model of all parameters in the phylogenetic analysis.This image was generated by running the `.graph()` method fo the model object and opening theresulting file in the program [Graphviz](https://www.graphviz.org/).{% endfigcaption %}{% endfigure %}{% subsection Run the MCMC | mcmcsec %}Now that we have our full model, we can set up and run our MCMC. First, we will define a list monitors for our parameters. Since we are now dealing with a tree object, we have to be sure to save the sampled trees to file. The `mnModel()` function will log all of the numerical parameters, but not the tree. We can use the `mnFile()` monitor to write our MCMC samples of the tree.     monitors.append(mnModel(filename=\"output/TimetTree_bears_mcmc.log\",    printgen=10))    monitors.append(mnFile(filename=\"output/TimeTree_bears_mcmc.trees\",    printgen=10, timetree))    monitors.append(mnScreen(printgen=10, root_time, tmrca_Ursidae))Create the MCMC object:    mymcmc = mcmc(mymodel, monitors, moves)Run the burn-in:    mymcmc.burnin(generations=5000,tuningInterval=100)This will give you the following output. ~~~   Running burn-in phase of Monte Carlo sampler for 5000 iterations.   This simulation runs 1 independent replicate.   The simulator uses 27 different moves in a random move schedule with 123 moves per iterationProgress:0---------------25---------------50---------------75--------------100****************************************~~~{:.Rev-output}Once the burn-in phase is completed, run the MCMC    mymcmc.run(generations=40000)~~~   Running MCMC simulation   This simulation runs 1 independent replicate.   The simulator uses 27 different moves in a random move schedule with 123 moves per iterationIter        |      Posterior   |     Likelihood   |          Prior   |      root_time   |    elapsed   |        ETA   |-----------------------------------------------------------------------------------------------------------------------0           |       -1890.92   |       -1980.98   |        90.0529   |       27.01989   |   00:00:00   |   --:--:--   |100         |       -1895.17   |       -1979.32   |        84.1559   |       31.20937   |   00:00:00   |   --:--:--   |200         |       -1894.08   |       -1977.15   |        83.0738   |       30.89059   |   00:00:00   |   00:00:00   |300         |       -1893.08   |       -1980.07   |        86.9973   |       29.60776   |   00:00:01   |   00:02:12   |...~~~{:.Rev-output}{% subsection Summarize the MCMC | summarysec %}#### Inspect the MCMCNow that our MCMC has completed, we need to evaluate the output. The first thing we can do is to visually inspect the sampled numerical parameters in our `TimeTree_bears_mcmc.log` file. To do this, we can use the handy program [Tracer](http://tree.bio.ed.ac.uk/software/tracer/) {% cite Rambaut2011 %}. > Open the program Tracer and load the `TimeTree_bears_mcmc.log` from the `output` folder{:.instruction}{% figure tracer1 %}{% figcaption %}The histogram of the **_Posterior_** statistic in [Tracer](http://tree.bio.ed.ac.uk/software/tracer/).{% endfigcaption %}{% endfigure %}Looking at the list of parameters in {% ref tracer1 %}, you can see that all have high **_ESS_** values. This column is the _effective sample size_ of your MCMC for that parameterand indicates the level of correlation among samples. In Tracer, if this value is coloredred for any parameter, it means that there may have been issues with mixing.> Browse through the various views and parameters in Tracer like {% ref tracer2 %}. Does it appear> as though your MCMC had good or bad mixing?{:.instruction}{% figure tracer2 %}{% figcaption %}The trace plot of the `root_time` parameter in Tracer.{% endfigcaption %}{% endfigure %}It is important to perform multiple independent MCMC runs so that you can determine if your chains converged on the same stationary distribution. Additionally, you should also runyou chain under the prior to diagnose any issues with your model or MCMC and identify parametersfor which your data are not informative. However, given limited time for this tutorial, we willonly conduct one run.#### Summarize the TreeAssuming we did our due diligence to ensure that our MCMC sampled joint posterior distributioneffectively, we can now summarize the sampled trees. If we return to RevBayes, we can read in the sampled trees in `output/TimeTree_bears_mcmc.trees`. The `readTreeTrace()` function returnsa tree-trace object that we will call `tt`.     tt = readTreeTrace(\"output/TimeTree_bears_mcmc.trees\", \"clock\")We will find the maximum _a posteriori_ tree in our MCMC sample. This is the tree with the highest posterior probability. To do this we can use the `mapTree()` function on the tree-traceobject.     mapTree(tt, \"output/TimeTree_bears_mcmc_MAP.tre\")To view this tree, we can use the online tree-viewer [IcyTree](https://icytree.org/) developed by [Tim Vaughan](https://github.com/tgvaughan). > Open your web browser and go to [https://icytree.org](https://icytree.org).> > You can load the `TimeTree_bears_mcmc_MAP.tre` file by going to > the **_File -> Load from file..._** in the menu, or by simpling dragging and dropping your> file into the browser window. >> Look through the options in IcyTree and try to replicate the tree in {% ref mapicy %}.{:.instruction}{% figure mapicy %}{% figcaption %}The maximum _a posteriori_ tree plotted in [IcyTree](https://icytree.org/). The node bars indicatethe 95% credible interval of the node age and the internal node labels denote the bipartitionposterior probability.{% endfigcaption %}{% endfigure %}",
        "url": "/tutorials/clocks/simple.html",
        "index": "false"
      }
      ,
    
      "tutorials-divrate-simple-html": {
        "title": "Simple Diversification Rate Estimation",
        "content": "{% section Estimating Constant Speciation & Extinction Rates | bdp_rate_estimation %}This tutorial describes how to specify basic birth-death models in RevBayes.Specifically, we will use the pure-birth (Yule; {% citet Yule1925 %}) process and the constant-rate birth-death process{% cite Kendall1948 Thompson1975 Nee1994b %}.The probabilistic graphical model is given for each component of this tutorial.After each model is specified, you will estimate speciation and extinction rates using Markov chain Monte Carlo (MCMC).Finally, you will estimate the marginal likelihood of the model and evaluate therelative support using Bayes factors.You should read first the {% page_ref divrate/div_rate_intro %} tutorial, which explains the theory andgives some general overview of diversification rate estimation.{% include_relative modules/simple_Yule_parameter_estimation.md %}{% include_relative modules/simple_Yule_marginal_likelihood_estimation.md %}{% include_relative modules/simple_birth_death.md %}>Click below to begin the next exercise!{:.instruction}* [Diversification Rates Through Time Estimation]({{ base.url }}/tutorials/divrate/ebd)",
        "url": "/tutorials/divrate/simple.html",
        "index": "true"
      }
      ,
    
      "tutorials-cont-traits-simple-bm-html": {
        "title": "Simple Brownian Rate Estimation",
        "content": "{% section Estimating Constant Rates of Evolution %}This tutorial demonstrates how to specify a Brownian-motion model where the rate of evolution is assumed to be constant among branches of a time-calibrated phylogeny {% cite Felsenstein1985a %}. We provide the probabilistic graphical model representation of each component for this tutorial. After specifying the model, you will estimate the rate of Brownian-motion evolution using Markov chain Monte Carlo (MCMC).### The data>Create a directory on your computer for this tutorial.>In this directory, create a subdirectory called **data**, and download the data files that you can find on the left of this page.{:.instruction}We have taken the phylogeny from {% citet MagnusonFord2012 %}, who took it from {% citet Vos2006 %} and then randomly resolved the polytomies using the method of {% citet Kuhn2011 %} and the trait data from {% citet Redding2010 %}. In the **data** folder, you should now have the following files:-   [primates_tree.nex](data/primates_tree.nex):    Dated primate phylogeny including 233 out of 367 species.-   [primates_cont_traits.nex](data/primates_cont_traits.nex):    A file with the primates continuous data.The dataset includes:-   **Female Mass**: Body mass in grams of adult female, log-transformed, mean measurement.-   **Tail Length – Body Length Residuals**: The residuals of linear model of tail length (cm) and body length (cm), sqrt-transformed, mean measurement-   **Body Length – Mass Residuals**: The residuals of linear model of body length (cm) and adult female mass (g), sqrt-transformed, mean measurement-   **Maximum Age**: Maximum age reported in years, log-transformed, mean measurement-   **Sexual Dimorphism**: Adult male average weight divided by adult female average weight, log-transformed, mean measurement-   **Geographic Range Size**: Geographical extent (km2) of species' occurrence, log-transformed-   **Latitudinal Midpoint**: Latitude of species’ geographic range mid-point, sqrt-transformed-   **Distance to Continental Centroid**: Distance of geographic range mid-point to mid-point of the combined area of all species found in that continent-   **Population Density**: Average number of individuals per km2, log-transformed-   **Home Range Size**: Size in m2 of group or individual's land use, log-transformed-   **Group Size**: Number of individuals in social group, log-transformed-   **Gestation Duration**: Duration in months of gestation periods, mean measurement-   **Litter size**: Size of litter, mean measurement{% include_relative modules/simple_BM_parameter_estimation.md %}{% include_relative modules/simple_BM_exercise_1.md %}>Click below to begin the next exercise!{:.instruction}* [Relaxed Brownian Rate Estimation]({{ base.url }}/tutorials/cont_traits/relaxed_bm)",
        "url": "/tutorials/cont_traits/simple_bm.html",
        "index": "true"
      }
      ,
    
      "tutorials-cont-traits-simple-ou-html": {
        "title": "Simple Ornstein-Uhlenbeck Models",
        "content": "{% section Estimating Evolutionary Optima %}This tutorial demonstrates how to specify an Ornstein-Uhlenbeck model where the optimal phenotype is assumed to be constant among branches of a time-calibrated phylogeny {% cite Hansen1997 Butler2004 %}. We provide the probabilistic graphical model representation of each component for this tutorial. After specifying the model, you will estimate the parameters of Ornstein-Uhlenbeck evolution using Markov chain Monte Carlo (MCMC).{% include_relative modules/simple_OU_parameter_estimation.md %}{% include_relative modules/simple_OU_exercise_1.md %}{% include_relative modules/simple_OU_model_selection.md %}",
        "url": "/tutorials/cont_traits/simple_ou.html",
        "index": "true"
      }
      ,
    
      "singularity": {
        "title": "Singularity",
        "content": "## What is Singularity?Singularity is a container runtime designed for use in HPC. Containers can run without root access on any Linux system with Singularity installed. [Singularity FAQ](https://sylabs.io/singularity/faq/)Containers are immutable and directories are mounted to read or write from files. Some directories are mounted automatically when an image is run. [More information is available here](https://sylabs.io/guides/3.4/user-guide/quick_start.html#working-with-files).## How do I install the Singularity runtime?Installing the Singularity runtime requires root access to a Linux machine.Your HPC system may already have it installed.If not, your HPC staff can get it from a EPEL (CentOS/RHEL) or a Debian/Ubuntu repository. [More information is available here.](https://sylabs.io/guides/3.4/user-guide/installation.html#distribution-packages-of-singularity)## How to use the RevBayes imageFirst, download the RevBayes Singularity image. [RevBayes_Singularity_{{ site.version }}.simg](https://github.com/revbayes/revbayes/releases/download/{{ site.version }}/RevBayes_Singularity_{{ site.version }}.simg).#### To get to an interative shell```singularity run --app rb RevBayes_Singularity_{{ site.version }}.simg ```#### To run a script file```singularity run --app rb RevBayes_Singularity_{{ site.version }}.simg myscript.rev```Any arguments after RevBayes_Singularity_{{ site.version }}.simg are passed to the rb command.### MPIThe container also contains the MPI version of RevBayes. This version was built with the ubuntu-provided OpenMPI implementation (3.1.3) and may not work with other MPI versions.To run a script file:```mpirun singularity run --app rbmpi RevBayes_Singularity_{{ site.version }}.simg myscript.rev```If it does not work with the version of MPI you have on your cluster, you could ask your administrator to rebuild the image for the appropriate MPI version for your environment (see recipe below). [More information is available here](https://sylabs.io/guides/3.4/user-guide/mpi.html).",
        "url": "/singularity/",
        "index": ""
      }
      ,
    
      "tutorials-cont-traits-state-dependent-bm-html": {
        "title": "State-Dependent Brownian Rate Estimation",
        "content": "{% section Estimating State-Dependent Rates of Evolution %}This tutorial demonstrates how to estimate state-dependent rates of continuous-characterevolution. Specifically, we will specify a state-dependent rate model that assigns aBrownian-motion rate parameter for each state of a discrete character. We provide theprobabilistic graphical model representation of each component for this tutorial. Afterspecifying the model, you will estimate the state-dependent rates of Brownian-motionevolution using Markov chain Monte Carlo (MCMC).You should read the {% page_ref cont_traits/simple_bm %} tutorial, which provides ageneral introduction to simple Brownian-motion models, and {% page_ref morph_ase %},which describes models of discrete-character evolution, before using this tutorial.{% include_relative modules/state_dependent_BM_parameter_estimation.md %}{% include_relative modules/state_dependent_BM_exercise_1.md %}{% include_relative modules/state_dependent_BM_parameter_estimation_RJ.md %}{% include_relative modules/state_dependent_BM_exercise_2.md %}{% include_relative modules/state_dependent_BM_relaxed.md %}{% include_relative modules/state_dependent_BM_exercise_3.md %}>Click below to begin the next exercise!{:.instruction}* [Multivariate Brownian Motion]({{ base.url }}/tutorials/cont_traits/multivariate_bm)",
        "url": "/tutorials/cont_traits/state_dependent_bm.html",
        "index": "true"
      }
      ,
    
      "tutorials-sequential-bayes-stepwise-dating-html": {
        "title": "Stepwise Bayesian inference of phylogeny",
        "content": "# Estimating a time-calibrated phylogeny from a posterior sample of unrooted phylogenies{:.section}In the previous [exercise 1]({{ base.url }}/tutorials/sequential_bayes/unrooted_gene_trees) we inferred the phylogeny of the North American firefly genus *Photinus*.In this exercise, we will use the previous output to infer a time-calibrated phylogeny assuming a relaxed-clock model.## Reading the data (posterior tree samples){:.subsection}The first step is to read in the posterior tree samples from the previous analysis.Here, we assume that we had 50,000 sampled trees (2x an MCMC of 25,000 iterations) in the previous step.If we want to remove 20% as burnin, that leaves 40,000 samples.Then, if we want 100 trees, we should take every 400th sample, thus specifying the option `thin=400`.Obviously we would like to use as many trees as possible, but this comes at a computational cost {% cite Hoehna2024 %}.```treetrace = readTreeTrace(\"output/photinus_COI.trees\", treetype=\"non-clock\", thinning=400, burnin=0.2)```Let us double check how many trees we have for this analysis.It should be 100 (or 101, if the tree for iteration 0 was included).```treetrace.size(true)```## Specifying helper variables{:.subsection}We get the taxon information from the first tree```taxa = treetrace.getTree(1).taxa()n_species {% figcaption %}MAP time-calibrated phylogeny plotted with `RevGadgets` {% cite Tribble2022 %}. For more information see [RevGadget tutorial]({{ base.url }}/tutorials/intro/RevGadgets).{% endfigcaption %}{% endfigure %}## Exercise",
        "url": "/tutorials/sequential_bayes/stepwise_dating.html",
        "index": "false"
      }
      ,
    
      "tutorials-coalescent-summary-html": {
        "title": "Summary of the Coalescent Analyses",
        "content": "{% section Overview %}After running all the exercises for coalescent analyses, we want to compare all population size plots.{% section Results %}{% figure all-outputs-iso %}            {% figcaption %}Example output from plotting the output from a choice of coalescent analyses with isochronous data.The resulting population size trajectories are from (left to right) the constant analysis, the skyline analysis, the GMRF analysis, the HSMRF analysis, the Skyfish analysis and the piecewise analysis with six pieces. The bold line represents the median of the posterior distribution of the population size and the shaded are shows the $95\\%$ credible intervals. For the piecewise analysis, the reference skyline result is shown in green and the result of the piecewise analysis is shown in blue.{% endfigcaption %}{% endfigure %}",
        "url": "/tutorials/coalescent/summary.html",
        "index": "false"
      }
      ,
    
      "tutorials-dating-tefbd-html": {
        "title": "Molecular dating",
        "content": "Exercise 5==========={:.section}In [exercise 4]({{ base.url }}/tutorials/dating/fbdr) we inferred the phylogeny of extant bears and a timeline for their evolution using the fossized birth-death range process. This tree model explictly incorporates the fossil recovery process, meaning we can directly include extinct samples as part of the tree. Using this model we can take advantage of fossils with and without character data. This is because the fossil sampling times are informative about the FBD model parameters and speciation times, even if their phylogenetic position remains unknown or unresolved. In the previous exercise, we didn't include any character data for our extinct samples, so we could not infer their phylogenetic relationship and pruned the fossils from the output. In this exercise we will infer the phylogeny of extant and fossil bear species by including morphological character data in our analysis, which is available for both extant and extinct species, in addition to the molecular data, which is available for living species only. This approach to dating can be referred to as *tip-dating*, *combined-evidence* or *total-evidence* dating {% cite Ronquist2012a Zhang2016 Gavryushkina2016 %} (although it does not technically use *all* the evidence we could potentially incorporate into our dating analyses).{% figure fig_module_gm %} {% figcaption %} Modular components of the graphical model used in the \"combined-evidence\" analysis described in this tutorial.{% endfigcaption %}{% endfigure %}### The dataThe data used in this exercise will still include the molecular data used in the previous exercises (**bears_cytb.nex**). We will also use the same substitution and clock models (the GTR + $\\Gamma$ model and the uncorrelated exponential relaxed clock model). For this exercise we'll also use a matrix of 62 discrete, binary (coded \"0\" or \"1\") morphological characters for 18 species of fossil and extant bears. **bears_cytb.nex** contains the matrix in NEXUS format. We'll analyse this data using the Mk + v model. Again, we'll take advantage of all the stratigraphic age information in **bears_taxa.tsv** and use the fossized birth-death range process as our tree model. Note that there are two fossil species not included in our character matrix -- these are still valuable to include in our analysis but we will prune these taxa from the output trees.To complete this exercise, we need to create a script for the substitution and clock model that will be applied to the morphological character data. We'll also need to make some small changes to the master and the tree model scripts. ### The master Rev scriptAgain, we'll start by making some changes to the master Rev script.>Copy the master script from the previous exercise and call it **MCMC_dating_ex5.Rev**. {:.instruction}Write the commands to read the morphological character data and add missing character data (i.e. for the two species for which character data is unvailable).```morpho Creating a copy of your **tree_FBD.Rev** script, call it **tree_TEFBD.Rev** and open it in your text editor. {:.instruction}#### Monitoring parameters of interest using deterministic nodesThe only change we need to make in this file is to `fnPruneTree` There are two fossil species for which we do not have any morphological characters *Parictis montanus* and *Ursus abstrusus*, so we will not be able to resolve their tolopological position and we should keep them pruned from the tree. We do not want to remove the other species this time because we are interested in there topological placement.```pruned_tree := fnPruneTree(timetree, prune=v(\"Parictis_montanus\",\"Ursus_abstrusus\")) ```### The morphological clock modelWe will apply a simple global clock model to our morphological partition.>Copy the script for the global clock model **clock_global.Rev** and call it **clock_morpho.Rev**. Open it in your text editor.{:.instruction}For the branch-rates parameter we will also use an exponential prior, with parameters `rate` = 10. The expected value (or mean) of this distribution is 0.1. The same branch-rate will apply to every branch in the tree.Simply change the name of `branch_rates` to `branch_rates_morpho`.```branch_rates_morpho ~ dnExponential(10.0)moves.append( mvScale(branch_rates_morpho,lambda=0.5,tune=true,weight=5.0) )```### The Mk modelThe next step is to specify the model that describes how morphological characters evolve along the tree and across sites.You can find a full description about the Mk model in the {% page_ref morph_tree %} Tutorial.>Create a script called **sub_Mk.Rev** and open it in your text editor. {:.instruction}The Mk model is a generalization of the Jukes-Cantor model, so we will initialize our Q matrix from a Jukes-Cantor matrix.```Q_morpho Run your MCMC analysis!{:.instruction}### Examining the outputLet's examine the output in Tracer.>Open the program Tracer and load the log files **bears_FBD.log** and **bears_TEFBD.log**.{:.instruction}Let's compare the results we obtained using the FBD and TEFBD analysis. Start by examining the estimates for the MRCA of extant bears.{% figure fig_trace %} {% figcaption %} The Marginal Density panel in Tracer showing the posterior estimates for the MRCA of extant bears under the FBD process with (total-evidence dating) and without morphological character data for extant and fossil species. To reproduce this figure select Colour by: Trace file and Legend: Top-Right.{% endfigcaption %}{% endfigure %}Do you observe any important differences for any other parameters?#### Sampled ancestor treesWhen there are sampled ancestors present in the tree, visualizing the tree can be fairly difficult in traditional tree viewers. We will make use of a browser-based tree viewer called [IcyTree](http://tgvaughan.github.io/icytree/), created by Tim Vaughan. IcyTree has many unique options for visualizing phylogenetic trees and can produce publication-quality vector image files (i.e. SVG). Additionally, it correctly represents sampled ancestors on the tree as nodes, each with only one descendant.>Navigate to https://icytree.org/ and open the file **bears.mcc.tre** in IcyTree.{:.instruction}Try to replicate the tree shown in the figure below. (Hint: Style > Mark Singletons).{% figure icy_tree %} {% figcaption %} The Icy window. To open your tree you can use File > Open. Select Node Labels to view the relative node ages.{% endfigcaption %}{% endfigure %}Examine the posterior probabilities and ages for the sampled ancestors and node ages.### NextHooray! You've reached the end of the tutorial!For further options and information about the tree model used in this exercise see Tracy Heath, April Wright and Walker Pett's tutorial [Combined-Evidence Analysis and the Fossilized Birth-Death Process for Stratigraphic Range Data]({{ base.url }}/tutorials/fbd/).For further information about morphological models of character evolution see April Wright, Michael Landis and Sebastian Höhna's tutorial [Discrete morphology - Tree Inference]({{ base.url }}/tutorials/morph/morph_dec2018).",
        "url": "/tutorials/dating/tefbd.html",
        "index": "false"
      }
      ,
    
      "tutorials-tutorial-structure-scripts-test-rmd": {
        "title": "Using RevBayes in R",
        "content": "# InstallationRevticulate can be installed in two ways.The first is via CRAN, using the default `install.packages` function in R:```{r eval=FALSE}install.packages(\"Revticulate\")```The second is via the remotes package, a lightweight package enabling installation from GitHub repositories.```{r}remotes::install_github(\"revbayes/Revticulate\")``` The GitHub repository for Revticulate contains cutting-edge features and may contain bugfixes, but the CRAN is known to be stable for everyday use.Upon first installation, Revticulate will run a package check.This check searches for and .Renviron file that contains a RevBayes path. If the package doesn’t find this file, or finds it without the path, the package prompts the user to use `usethis::edit_r_environ()`. This opens the .Renviron file, and the user will enter `rb={absolute path to revbayes}`. This can be edited at any time if there are multiple installs on the system, or if you recompile RevBayes and want to use a new version.Before using Revticulate in knitr, make sure the following is in your setup chunk:```{r}library(Revticulate)knitRev()```First, we will test that RevBayes is accessible to us. This is a Rev chunk, which we know because the header says 'rb'. This will fail if RevBayes cannot be found. If this is the case, check that the path in your Renviron goes to RevBayes, and restart.```{rb}variable <- \"Hi there! Welcome to RevBayes! I am now going to read in some test data.\"variable```Next, we will attempt to read in some data. Note that this file is stored in `tutorial_structure/scripts`, and so we'll going up one directory to the data directory.```{rb}molecular_data <- readDiscreteCharacterData(\"../data/example_file.nex\")molecular_data```",
        "url": "/tutorials/tutorial_structure/scripts/test.Rmd",
        "index": ""
      }
      ,
    
      "tutorials-cont-traits-time-varying-bm-html": {
        "title": "Simple Brownian Rate Estimation",
        "content": "This tutorial demonstrates how to specify a Brownian-motion model where the rate of evolution is assumed to be constant among branches of a time-calibrated phylogeny {% cite Felsenstein1985a %} using the datasets of (log) body-size across vertebrate clades from {% cite Landis2017%}. We provide the probabilistic graphical model representation of each component for this tutorial. After specifying the model, you will estimate the rate of Brownian-motion evolution using Markov chain Monte Carlo (MCMC).{% include_relative modules/simple_BM_parameter_estimation.md %}{% include_relative modules/simple_BM_exercise_1.md %}",
        "url": "/tutorials/cont_traits/time_varying_bm.html",
        "index": "false"
      }
      ,
    
      "tutorials": {
        "title": "Tutorials",
        "content": "This list shows all of the RevBayes tutorials for learning various aspects of RevBayes and Bayesian phylogenetic analysis.Each one explicitly walks you through model specification and analysis set-up for different phylogenetic methods.These tutorials have been written for new users to learn RevBayes at home, at workshops, and in courses taught at the undergraduate and graduate levels.You may find that the styles are somewhat different between tutorials and that some  have overlapping content.Please see the {% page_ref format %} guide for details about how to read the tutorials.Please see {% page_ref recommended %} for links to various software programs you may need to download in order to follow the tutorials.Contribute!{% comment %}{% include keywords.html input=true %}{% assign keywords = site.empty_array %}{% endcomment %}{% assign levels = site.tutorials | sort:\"level\",\"last\" | group_by:\"level\" %}{% for level in levels %}{% assign i = forloop.index | minus: 1 %}{% if page.levels[i] %}{{ page.levels[i] }}{:id=\"{{ page.levels_id[i] }}\"}{% else %}Miscellaneous Tutorials{% endif %}{% assign tutorials = level.items | sort:\"order\",\"last\" %}{% for tutorial in tutorials %}{% if tutorial.index %}{% comment %}{% assign keywords = tutorial.keywords | concat: keywords %}{% endcomment %}{% if tutorial.title-old and tutorial.redirect %}{{ tutorial.title | markdownify }}{% else %}{{ tutorial.title | markdownify }}{% endif %}{{ tutorial.subtitle | markdownify }}{% endif %}{% endfor %}{% endfor %}{% comment %}{% include keywords.html script=true %}{% endcomment %}",
        "url": "/tutorials/",
        "index": ""
      }
      ,
    
      "tutorials-sequential-bayes-unrooted-gene-trees-html": {
        "title": "Stepwise Bayesian inference of phylogeny",
        "content": "Estimating posterior distribution of unrooted topologies with branch lengths==========={:.section}## The data{:.subsection}The data used in this exercise is a single gene, COI, for the North American firefly genus *photinus* (**photinus_COI.fas**). We will use the standard substitution model (the GTR + $\\Gamma$ model) for phylogeny inference. If you want, you can also use a different locus. More details about the analysis can be found in the [CTMC tutorial]({{ base.url }}/tutorials/ctmc).## The Rev script{:.subsection}First, we need to create a script for the unrooted phylogeny analysis.>Create a new Rev script and call it **MCMC_unrooted_photinus_COI.Rev**.{:.instruction}Let's start with writing the script for this analysis. We are going to be brief in the explanation as more details are already given in the [CTMC tutorial]({{ base.url }}/tutorials/ctmc).First, read in the sequence data.```### Read in sequence data for the genedata = readDiscreteCharacterData( \"data/photinus_COI.fas\" )```Then, obtain some useful variable, such as the number of taxa and number of branches.```# Get some useful variables from the data. We need these later on.num_taxa Run your MCMC analysis!{:.instruction}&#8680; The `Rev` file for performing this analysis: `mcmc_unrooted_gene_tree.Rev`## Examining the output{:.subsection}Let us first look at convergence of this analysis.Just start `R` in the main directory for this analysis and then type the following commands:```Rlibrary(convenience)# Analysis settingsN_REPS       = 2LOCUS        = \"COI\"treefile_names = paste0(\"output/photinus_\",LOCUS,\"_run_\",1:N_REPS,\".trees\")logfile_names  = paste0(\"output/photinus_\",LOCUS,\"_run_\",1:N_REPS,\".log\")format         = \"revbayes\"conv.control = makeControl( tracer = NULL,                            burnin = 0.2,                            precision = NULL,                            namesToExclude = NULL                           )check_conv {% figcaption %}Convergence assessment using `convenience` {% cite Fabreti2022 %}. For more information see [RevGadget tutorial]({{ base.url }}/tutorials/convergence){% endfigcaption %}{% endfigure %}### Plotting the MAP tree in RevGadgets{:.subsubsection}Next, we plot the MAP tree and inspect it.```Rlibrary(ggtree)library(RevGadgets)LOCUS        = \"COI\"tree  {% figcaption %}MAP phylogeny plotted with `RevGadgets` {% cite Tribble2022 %}. For more information see [RevGadget tutorial]({{ base.url }}/tutorials/intro/RevGadgets).{% endfigcaption %}{% endfigure %}## Exercise{:.subsection}Above we have seen how to infer the posterior distribution of gene trees.Now, after your have run this analysis, try to run the same analysis for different genes as well as a concatenated analysis of all genes.>Run your MCMC analysis!{:.instruction}### Next>Click below to begin the next exercise!{:.instruction}* [Estimating divergence times from posterior sampled unrooted trees]({{ base.url }}/tutorials/sequential_bayes/stepwise_dating)",
        "url": "/tutorials/sequential_bayes/unrooted_gene_trees.html",
        "index": "false"
      }
      ,
    
      "developer-setup-vim-html": {
        "title": "Setting up vim for RevBayes development",
        "content": "[Vim](http://www.vim.org) is a text editor that some people love.It's not an IDE, but it can provide useful IDE-like behaviors.If you are reading this, you are probably a vim-lover and already have a set of customizations that you like.Here are some more to consider.YouCompleteMe----------------------[YouCompleteMe](https://github.com/Valloric/YouCompleteMe) is an extremely useful plugin that provides suggestions as you type for function names, prompts about their arguments, etc.There are several steps to get it working.### Install dependenciesThis seems to be sufficient on Ubuntu 16.04:```sudo apt-get install build-essential cmake python-dev python3-dev clang```{:.bash}### Get the vim code itselfGrab these two plugins:* [YouCompleteMe](https://github.com/Valloric/YouCompleteMe)* [YCM-Generator](https://github.com/rdnetto/YCM-Generator)Put them wherever you put your plugins, e.g., `.vim/bundle/` if you're using [Pathogen](https://github.com/tpope/vim-pathogen).For YCM, you also need to get its submodules:```cd YouCompleteMe/git submodule update --init --recursive```{:.bash}(If you manage your plugins as git subtrees, note that you probably can't for YouCompleteMe because it contains submodules itself.)### Compile the YCM pluginYCM has a compiled component as well as vim code.  This may take a few minutes to run.```cd YouCompleteMe/./install.py --clang-completer```{:.bash}### Provide the compilation flags to YCMThe above was to install YCM in general.To use it specifically with RevBayes (or any other project), you need to give it information about the codebase.YCM-Generator is one way to do this.```cd revbayes/  # or wherever you keep revbayescd projects/cmake/build/~/.vim/bundle/YCM-Generator/config_gen.py . # adjust the vim path if necessary```{:.bash}That should take a few seconds to run.Then move the result to the top-level directory:```mv .ycm_extra_conf.py ../../../```{:.bash}### Try it outThat should be it.If YCM is working, when you open a revbayes `.cpp` or `.h` file, vim will ask `Found revbayes/.ycm_extra_conf.py. Load?`If you find that you don't want YCM operating on all your other filetypes, you can put something like this in your `.vimrc`.```let g:ycm_filetype_whitelist = { 'cpp': 1, 'c': 1, 'python': 1 }```{:.vim}### Debugging with GDBIf you use vim, we recommend debugging in GDB. You'll need to compile RevBayes with the `-debug true` flag:```./build.sh -debug true```{:.bash}The you can debug RevBayes with GDB:```gdb rb```{:.bash}See [here](https://www.cs.cmu.edu/~gilpin/tutorial/) for more on using GDB.",
        "url": "/developer/setup/vim.html",
        "index": ""
      }
      ,
    
      "workshops-code-of-conduct-virtual-coc-html": {
        "title": "Virtual Workshop Code of Conduct",
        "content": "# Policy on harassment and discriminationHarassment of others by any participant (attendee or instructor) will not be tolerated. Unacceptable treatment of others includes (but is not limited to) unwanted verbal attention, intimidation, stalking, shaming, or bullying. Inappropriate behavior via any medium (e.g., direct message, chats, email, video conferencing) is a violation of this code of conduct.  Discrimination or exclusion on the basis of gender or gender identity, sexual orientation, age, disability, physical appearance, race, religion, national origin, ethnicity, or similar will not be tolerated. Critiques of scientific work are appropriate and important, but all forms of communication must be free of offensive, discriminatory or disrespectful elements, including (but not limited to) words and images that are derogatory or demeaning to individuals or groups [1]. Inappropriate comments presented in a joking manner constitute unacceptable behavior. Retaliation for reporting inappropriate behavior is also unacceptable, as is reporting an incident in bad faith.Workshop participants or instructors wishing to report a violation of this code of conduct by any member of this group can contact any of the instructors via direct message on Slack or email. Incidents of inappropriate and uncivil behavior are taken extremely seriously. Confidentiality will be maintained unless disclosure is legally required.Workshop instructors will act as moderators in the Slack space and during interactive sessions. Instructors reserve the right to enforce this code of conduct in any manner deemed appropriate. Anyone violating the code of conduct may be: (a) asked to stop, (b) subjected to limits on or revocation of any means of active participation in the workshop (i.e., muted microphone, blocked video access, removed Slack access), (c) expelled from the workshop. Establishing and enforcing this code of conduct is intended to prevent incidents of harassment, discrimination, and incivility, and to maintain a high quality of scientific discourse.[1] Disagreements about science are normal and healthy parts of academic discourse. Civil and constructive criticism of someone’s work for a perceived methodological flaw or a misinterpretation of results is appropriate. Demeaning a scientist for being sloppy, misleading or stupid and other ad hominem attacks are inappropriate. ​[[https://www.sciencemag.org/careers/2000/07/joy-criticism](https://www.sciencemag.org/careers/2000/07/joy-criticism)]  # Policy on liabilityInstructors shall not be responsible for any defamatory, offensive, or illegal conduct of all participants, and shall not be held liable for or damage of any kind suffered by the participants at or in connection with the event. By registering for and attending the workshop, each participant acknowledges that they have read this Disclaimer, and expressly releases the instructors from any and all liability in connection with the workshop.# ProceduresViolations of the Code of Conduct are reportable to any of the instructors via direct message on Slack or email (see contact list on workshop website). In the case that a workshop attendee or instructor wishes to report inappropriate behavior of one of the workshop instructors, please consider contacting an instructor at a different institution. # SourceThis Code of Conduct was adapted from policies established by [Safe Evolution](https://www.evolutionmeetings.org/safe-evolution.html) for the online and in-person Evolution Meeting activities sponsored by the Society of Systematic Biologists, the Society for the Study of Evolution, and the American Society of Naturalists. ",
        "url": "/workshops/code_of_conduct/virtual_coc.html",
        "index": ""
      }
      ,
    
      "developer-setup-vscode-html": {
        "title": "Setting up Visual Studio Code for RevBayes development",
        "content": "Visual Studio Code (or VSCode) is an open-source text editor by Microsoft. You can download and install it here. Prerequisites----------------------You will first need to install RevBayes from source. Once RevBayes is installed (i.e. once you have successfully ran `./build.sh`), open VSCode. Once VSCode is open you will need to select the RevBayes folder from wherever you have stored it on your computer. Now you will need to install three extensions to get RevBayes to work nicely. To do this click on the button shown below in VSCode. Getting RevBayes working ------------------------Once here you need to search for the  \"C/C++ extension\" , the  \"CMake extension\" , and the  \"CMake tools extension\" .Now that these are installed, CMake tools will prompt in the lower right asking you to locate the file named `CMakeLists.txt`. If this prompt does not appear, configure CMake manually by typing Ctrl+Shift+P (or CMD+Shift+P) to bring up the command palette, typing \"configure\", and running `CMake: Configure` or `CMake: Delete cache and Reconfigure`. This should prompt the choice to locate `CMakeLists.txt`. Choose the file located in `revbayes/src/CMakeLists.txt`. This will prompt a choice of compiler kit. You can choose `clang` or `gcc`, either will work. This should begin building RevBayes using CMake and a new folder should appear in `revbayes` called `build`. This will contain the compiled files and will eventually be added to `.gitignore`. If it is not currently in it, you will need to add this to the `.gitignore` file. Specifically, add `build/**` and `vscode/**` to `.gitignore`. If the build does not start automatically click the build button (the gear icon) on the blue bar on the bottom of the screen. Note that VScode will build RevBayes to a different location than running `./build.sh` on the command line. The rb executable built by VSCode will be in `revbayes/build`.Once the CMake build finishes, you can setup debugging by clicking the triangle with the small beatle in the lower left. Now click on the gear shown below. This will open a new file called `launch.json`. You need to change the values for the `\"program:\"` object to the path where your `rb` executeable is. For example, `\"${workspaceFolder}/build/rb\"`. In VSCode `${workspaceFolder}` refers to the root folder of the project you opened (unless you have changed the folder's name, this is most likely `revbayes`). To test a specific rev script, change the value of the `\"args\"` object to a location of a Rev script. With that you can use debugging in VSCode by pressing the play button on the debug panel, or using the command palette (Ctrl+Shift+P or CMD+Shift+P) and using the \"CMake: Debug\" command. VSCode will then run the file you added to `\"args\"`, but stop the process whenever it reaches a line you have set with a breakpoint (i.e. where you have clicked to the left of the line number, adding a small red dot). When stopped at a breakpoint, you can use the debugging panels to investigate variables and functions to debug your program.",
        "url": "/developer/setup/vscode.html",
        "index": ""
      }
      ,
    
      "workshops-woodshole2018-html": {
        "title": "Bayesian Phylogenetics in RevBayes",
        "content": "## Using RevBayes on the MBL Cluster ### Executing RevBayesFor the tutorials used in this workshop, you may want to use the version of RevBayes on the MBL cluster. Once you log in, you can use RevBayes from any directory on the cluster by just typing `rb`.```rb```{:.bash}```Build from master (94149b) on Mon Jul 23 15:55:28 EDT 2018Visit the website www.RevBayes.com for more information about RevBayes.RevBayes is free software released under the GPL license, version 3. Type 'license()' for details.To quit RevBayes type 'quit()' or 'q()'.>```{:.Rev-output}### Downloading Data FilesSome of the tutorials you will us have data files associated with them. You will find hyperlinks to all of the required files on the tutorial page. These should be listed in the \"Data files and scripts\" box at the top of the tutorial page below the table of contents.For this workshop, we recommend that you create a new folder for each tutorial that you work through. Follow the directions in the tutorial for creating your directory structure. Then download the individual files using `wget`.For example, in the tutorial on divergence times, you will need to download the sequence data file:```wget https://revbayes.github.io/tutorials/fbd/data/bears_cytb.nex```{:.bash}### Writing Rev ScriptsSome tutorials require that you write several modular Rev scripts to define your model and MCMC analysis. We reccomend that you compose these files on your own computer or using the text editor on the MBL cluster.If you create these files on your personal machine, you will have to upload them to the cluster using `scp` or cyberduck.",
        "url": "/workshops/woodshole2018.html",
        "index": ""
      }
      ,
    
      "workshops-woodshole2019-html": {
        "title": "Bayesian Phylogenetics in RevBayes",
        "content": "## Using RevBayes on the MBL Cluster ### Executing RevBayesFor the tutorials used in this workshop, you may want to use the version of RevBayes on the MBL cluster. Once you log in, you can use RevBayes from any directory on the cluster by just typing `rb`.```rb```{:.bash}```Build from master (94149b) on Mon Jul 23 15:55:28 EDT 2019Visit the website www.RevBayes.com for more information about RevBayes.RevBayes is free software released under the GPL license, version 3. Type 'license()' for details.To quit RevBayes type 'quit()' or 'q()'.>```{:.Rev-output}### Downloading Data FilesSome of the tutorials you will us have data files associated with them. You will find hyperlinks to all of the required files on the tutorial page. These should be listed in the \"Data files and scripts\" box at the top of the tutorial page below the table of contents.For this workshop, we recommend that you create a new folder for each tutorial that you work through. Follow the directions in the tutorial for creating your directory structure. Then download the individual files using `wget`.For example, in the tutorial on divergence times, you will need to download the sequence data file:```wget https://revbayes.github.io/tutorials/fbd/data/bears_cytb.nex```{:.bash}### Writing Rev ScriptsSome tutorials require that you write several modular Rev scripts to define your model and MCMC analysis. We reccomend that you compose these files on your own computer or using the text editor on the MBL cluster.If you create these files on your personal machine, you will have to upload them to the cluster using `scp` or cyberduck.",
        "url": "/workshops/woodshole2019.html",
        "index": ""
      }
      ,
    
      "workshops-woodshole2022-html": {
        "title": "Bayesian Phylogenetics in RevBayes",
        "content": "### Course Website[https://molevolworkshop.github.io](https://molevolworkshop.github.io)",
        "url": "/workshops/woodshole2022.html",
        "index": ""
      }
      ,
    
      "workshops": {
        "title": "Workshops",
        "content": "Throughout the year, the members of the RevBayes development team and our collaborators teach workshops on molecular evolution, phylogenetics, and Bayesian inference using RevBayes. Additionally, we have occasional hackathons which bring together developers to work on the software and methods for phylogenetic analysis. {% assign workshops = site.pages | where:\"type\", \"workshop\" | sort: \"startdate\" | reverse %}{% for workshop in workshops %}\t{% capture startdate %}{{ workshop.startdate | date: '%s' | divided_by: 3600 | divided_by: 24 }}{% endcapture %}    {% if workshop.enddate %}        {% capture enddate %}{{ workshop.enddate | date: '%s' | divided_by: 3600 | divided_by: 24 }}{% endcapture %}    {% else %}        {% assign enddate = startdate %}    {% endif %}\t{% capture event %}\t\t\t\t{{ workshop.startdate | date: \"%B %-d, %Y\" }}\t\t{{ workshop.title }}\t\t{{ workshop.location }}\t\t{{ workshop.instructors | join: \", \" }}\t\t\t{% endcapture %}\t{% assign events = events | append: event %}{% endfor %}DateCourse TitleLocationInstructors{{ events }}    May 31, 2018    PPS with Revbayes and P3    Columbus, OH    Lyndon Coghill    October 23-27, 2017    Bayesian Phylogenetics Workshop    Gothenberg, Sweden    June Walker    October 21, 2017    Bayesian fossil tip dating using RevBayes    Geological Society of America Meeting, Seattle, Washington    Rachel Warnock and Michael Landis    September 25-28, 2017    Bayesian inference of phylogenies with RevBayes    International Biogeography Society Meeting, Bangalore, India    June Walker, Tracy Heath    August 13-14, 2017    Bayesian Phylogenetics in RevBayes    Iowa State University, Ames, Iowa    June Walker, Tracy Heath    August 7-11, 2017    Bayesian phylogenetics using RevBayes    NIMBioS, Knoxville, Tennessee    Bastien Boussau, Will Freyman, Emma Goldberg, Tracy A. Heath, Sebastian Höhna, John Huelsenbeck    July 20-30, 2017    Bayesian Phylogenetics in RevBayes    Workshop on Molecular Evolution at Woods Hole, Massachusetts    Tracy Heath and Michael Landis    March 11-17, 2017    Workshop in Applied Phylogenetics    Bodega Bay Applied Phylogenetics, California    Peter Wainright, April Wright, Bob Thomson, Rachel Warnock, Sebastian Höhna,    Jeremy Brown, Joanna Chiu, Michael Landis, Sam Price, Bruce Rannala    January 27, 2017    Bayesian phylogenetic inference and divergence time estimation using RevBayes    Workshop on Phylogenomics, Český Krumlov, Czechia    Tracy Heath    January 7-8, 2017            RevBayes Introduction,         Biogeography,         Fossilized-Birth-Death and         Model Adequacy        SSB Standalone Meeting, Baton Rouge, Louisiana    Sebastian Höhna, Lyndon Coghill, Will Freyman, Michael Landis, Tracy Heath, June Walker, and April Wright    December 9+16, 2017    An introduction to Bayesian phylogenetics and biogeographic dating    Yale University, New Haven, Connecticut    Michael Landis    July 18-29, 2016    Bayesian Phylogenetics in RevBayes    Workshop on Molecular Evolution at Woods Hole, Massachusetts    Tracy Heath and Michael Landis    March 21-23, 2016    Phylogenetic inference using RevBayes    UC Irvine, California    Sebastian Höhna    September 14, 2015    Bayesian phylogenetic using RevBayes    LSU, Baton Rouge, Louisiana        July 20-29, 2015    Bayesian Phylogenetics in RevBayes    Workshop on Molecular Evolution at Woods Hole, Massachusetts    Tracy Heath and Michael Landis    June 26, 2015    Model-based Molecular Systematics Workshop    Guaruja, Brazil    Tracy Heath    March 2015    Introduction to RevBayes    University of Minnesota, St. Paul MN, USA    Tracy Heath    March 23-27, 2015    Introduction to Bayesian phylogenetics using RevBayes    UC Berkeley, California    Sebastian Höhna, Michael Landis, John Huelsenbeck    March 8-14, 2015    Workshop in Applied Phylogenetics    Bodega Bay Applied Phylogenetics, California    Jonathan Eisen, Rich Glor, Tracy Heath, Sebastian Höhna, John Huelsenbeck, Michael Landis, Sarah Longo, Mike May, Brian Moore, Peter Wainwright, Sam Price, Bruce Rannala, Bob Thomson    Jan 25 - Feb 7, 2015    Bayesian phylogenetic inference using RevBayes    Molecular Evolution Workshop, Český Krumlov, Czechia    Sebastian Höhna    August 25-31, 2014    NESCent Academy Course: Phylogenetic Analysis Using RevBayes    NESCent, Durham, North Carolina    Tracy Heath, Brian Moore, Fredrik Ronquist, John Huelsenbeck, Michael Landis, Sebastian Höhna, Tanja Stadler, Bastien Boussau    July 28-August 6, 2014    Bayesian phylogenetic inference using RevBayes    Workshop on Molecular Evolution at Woods Hole, Massachusetts    Tracy Heath and Michael Landis    March 11-15, 2013    First RevBayes workshop    Groningen University, The Netherlands    ",
        "url": "/workshops/",
        "index": ""
      }
      ,
    
      "developer-setup-xcode-html": {
        "title": "Setting up XCode for RevBayes development",
        "content": " XCode is an IDE for Mac OSX. XCode does not keep track of files, so each time you open your RevBayes project in Xcode you must pull the RevBayes master branch from git & remove reference to all of the source. Eclipse Oxygen does a cleaner job of managing the files; you do not need to pull from git each time you work in it.Set up the XCode Project with an internal build system------------------------------------------------------1. Open Xcode and in the *Welcome to Xcode* window, choose **Create a new Xcode project**.2. Select **Command Line Tool** and name it `rb` and click **Next**. ![](figures/xcode-making_xcode_project.png)3. Click **New Folder**  to create an empty directory and name it whatever you'd like.4. Click **Create**.5. Delete all of the files and folders in each of the directories including `main.cpp` so that RevBayes is empty. You can do this by selecting the folders and files, right clicking and selecting delete. When asked, choose **Move to Trash**.6. Add the source files by selecting the appropriate directory and going to the **File** pull-down menu and selecting **Add Files to rb**.7. Click on **Options** at the button of the window, and under the ***Added Folders*** heading, select the **Create Groups** radio button.    * Select the        * `revbayes/src/revlanguage`        * `revbayes/src/core`        * `revbayes/src/libs`    * directories and click **Add**.![](figures/xcode-adding_files.png)    * _Note:_ On some versions of XCode, you may need to click on the \"Options\" Tab, and choose \"Create Groups\" for the import to work properly. This is the default behavior on most XCode installs.8. Add the boost library to your Xcode project:    * You have to install boost from another source and point your Xcode project to that version. If you use Homebrew as a package management system on your Mac, you can install boost using `brew install boost`. This will install the boost libraries in your `/usr/local` directory. To enable this version in your Xcode project, scroll to the **Search Paths** section in the **Build Settings** and add the following paths:        * **Header Search Paths**: `/usr/local/include`        * **Library Search Paths**: `/usr/local/lib`![](figures/xcode-search-paths.png)9. Add boost linker flags    * Scroll to the **Linking** section in the **Build Settings**    * Go to **Other Linker Flags** and double click on the space. This will bring up a box where you can add the following flags: `-lboost_program_options`, `-lboost_filesystem`, `-lboost_system`.![](figures/xcode-linking.png)### Add the `RB_XCODE` Preprocessor Macro to your Xcode project1. Select the RevBayes project and go to the **Build Settings**.2. Search for or Scroll down to the **Apple Clang - Preprocessing** heading and find the sub-heading named **Preprocessor Macros**.3. Double click on right hand column, click on the **+** and enter `RB_XCODE`. Do not replace the debug flag that is already present.![](figures/xcode-macro.png)### Check C++ language options1. Select the RevBayes project and go to the **Build Settings**.2. Search or scroll to: **Apple Clang - Language - C++**.3. Make sure C++ Language Dialect is set to **GNU++17 [-std=gnu++17]**.![](figures/xcode-cpp_lang_options.png)At this point, if everything has been setup correctly, you should be able to build the project. You can try by clicking on **Product - Build** or by using **&#8984;+B**.Set up the XCode Project with an external build system------------------------------------------------------### Prerequisites* To compile RevBayes using Xcode, first install [https://cmake.org/](CMake) and its [https://stackoverflow.com/questions/30668601/installing-cmake-command-line-tools-on-a-mac](command line tools). This is easily done using [https://brew.sh/](homebrew) by running `brew install cmake` in terminal.### Create an Xcode project for RevBayes1. Open Xcode and in the *Welcome to Xcode* window, choose **Create a new Xcode project**.2. Click on the Cross-Platform tab at the top3. Select **External Build System** and name it rb (or whatever you'd like)4. Under **Build Tool** type the following directory `//revbayes/projects/cmake/build.sh`5. Click **Next**.### Configure the build1. After clicking **Next** as directed above, you should see a screen that looks like this:![](figures/xcode-info.png)2. On this screen, under **Arguments** type `$(ACTION) -boost true -debug true`, or if you haven't built the boost libraries in RevBayes already. If you have then type `$(ACTION) -boost false -debug true`.3. Under **Directory** put `//revbayes/projects/cmake/`.4. Add the source files by selecting the appropriate directory and going to the **File** pull-down menu and selecting **Add Files to **.5. This should open a screen that looks like this:![](figures/xcode-adding.png)6. Click on **Options** at the button of the window, and under the ***Added Folders*** heading, select **Create as Folder References**, and add to the target that was created in Step 2.    * Select the following directories from the `revbayes` directory:        * `revbayes/src/revlanguage`        * `revbayes/src/core`        * `revbayes/src/libs`    * Click **Add**.7. At this point, if everything has been setup correctly, you should be able to build the project. You can try by clicking on **Product - Build** or by using **&#8984;+B**.8. Once revbayes has built go to **Product->Scheme->Edit Scheme**, it should bring a window like this:![](figures/xcode-exe.png)9. Click on info, then go to executable and locate the newly built revbayes executable.10. Now you should be able to click the play button or **&#8984;+R**, and you should see the revbayes command line prompt in the loading screen.",
        "url": "/developer/setup/xcode.html",
        "index": ""
      }
      ,
    
      "assets-css-main-css-map": {
        "title": "",
        "content": "{\"version\":3,\"sourceRoot\":\"\",\"sources\":[\"../_scss/home.scss\",\"../_scss/code.scss\",\"../_scss/colors.scss\",\"../_scss/blockquote.scss\",\"../_scss/tutorial.scss\",\"../_scss/tutorial-index.scss\",\"../_scss/figure.scss\",\"../_scss/keywords.scss\",\"../_scss/bibliography.scss\",\"../_scss/help.scss\",\"main.scss\"],\"names\":[],\"mappings\":\"AAAA;EACE;EACA;;;AAGF;EACE;EACA;EACA;EACA;EACA;;;ACUF;EACE;EACA;EACA;;;AAhBA;EACE;EACA,kBAiB6B;EAhB7B,OAgBsC;;AAftC;EACE;;AAEF;EACE;;;AARJ;EACE;EACA,kBCHkB;EDIlB,OAiB0C;;AAhB1C;EACE;;AAEF;EACE;;;AARJ;EACE;EACA,kBCHkB;EDIlB,OAkB0C;;AAjB1C;EACE;;AAEF;EACE;;;AARJ;EACE;EACA,kBCFkB;EDGlB,OCFkB;;ADGlB;EACE;;AAEF;EACE;;;AARJ;EACE;EACA,kBCFkB;EDGlB,OCFkB;;ADGlB;EACE;;AAEF;EACE;;;AARJ;EACE;EACA,kBCJkB;EDKlB,OAqB2C;;AApB3C;EACE;;AAEF;EACE;;;AAkBN;EACE;EACA;EACA;;;AEIF;EA/BE,cAPkB;EAQlB;EACA;EACA;EACA;EACA,cDFoB;ECGpB,gBAbkB;EAclB;;AAEA;EACE,aAjBgB;EAkBhB,gBAlBgB;EAmBhB;EACA;EACA,cDXkB;ECYlB;EACA;;AAEF;EACE;EACA,SAW6C;EAV7C;EACA,cA7BgB;EA8BhB,eA9BgB;EA+BhB;EACA;EACA;;;AAMJ;EAhCE,cAPkB;EAQlB;EACA;EACA;EACA;EACA,cDHoB;ECIpB,gBAbkB;EAclB;;AAEA;EACE,aAjBgB;EAkBhB,gBAlBgB;EAmBhB;EACA;EACA,cDZkB;ECalB;EACA;;AAEF;EACE;EACA,SAY0C;EAX1C;EACA,cA7BgB;EA8BhB,eA9BgB;EA+BhB;EACA;EACA;;;AAOJ;EAjCE,cAPkB;EAQlB;EACA;EACA;EACA;EACA,cDDoB;ECEpB,gBAbkB;EAclB;;AAEA;EACE,aAjBgB;EAkBhB,gBAlBgB;EAmBhB;EACA;EACA,cDVkB;ECWlB;EACA;;AAEF;EACE;EACA,SAamC;EAZnC;EACA,cA7BgB;EA8BhB,eA9BgB;EA+BhB;EACA;EACA;;;AAQJ;EAlCE,cAPkB;EAQlB;EACA;EACA;EACA;EACA;EACA,gBAbkB;EAclB;;AAEA;EACE,aAjBgB;EAkBhB,gBAlBgB;EAmBhB;EACA;EACA,cDTkB;ECUlB;EACA;;AAEF;EACE;EACA,SAcqC;EAbrC;EACA,cA7BgB;EA8BhB,eA9BgB;EA+BhB;EACA;EACA;;AAQ0C;EAAI;;;AAElD;EACE;EACA;EACA,cA9CkB;EA+ClB,eA/CkB;EAgDlB;EACA;;;AAGF;EACE;EACA;EACA,cAvDkB;EAwDlB,eAxDkB;EAyDlB;EACA;;;AC5DF;EACE;EACA;EACA;;AAEA;EACE;EACA;;AAGF;EACE;;;AAIJ;EACE;EACA;EACA;;;AAGF;EACE;;;AAGF;EACE;;;AAGF;EACE;;;AAGF;EACE;EACA;;;AAGF;EACE;;;AAGF;EACI;EACA;EACA;EACA;EACA;EACA;;;AAGJ;EACI;;;ACpDJ;EACE;EACA;;;AAGF;EACE;EACA;EACA;EACA;EAEA;EACA;;AAEA;EACE;;AAEF;EACE;;;AClBJ;EACE;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;AAEA;EACE;;;AAIJ;EACE;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;AAEA;EACE;EACA;EACA;EACA;EACA;EACA;;AAGF;EACE;EACA;EACA;EACA;EACA;EACA;;;AAIJ;EACE;EACA;;;AAGF;EACE;EACA;;;AAGF;EACE;IACE;IAEA;;EAEF;IACE;IAEA;;;ACjEJ;EAAI;;;AACJ;EACE;;;AAKF;AACE;EACA;EACA;EACA;;;AAEF;EACE;;;AAEF;EACE;EACA;EACA;EACA;;;AAEF;EACE;EACA;;;AAEF;EACE;EACA;;;AAEF;EACE;EACA;EACA;EACA;EACA;AACA;EACA;EACA;EACA;;;AAEF;EACE;EACA;EACA;EACA;;;AAEF;AACE;EACA,kBLjDoB;EKkDpB;;;AAEF;AACE;EACA;EACA;;;AAEF;EACE;EACA,kBL3DoB;EK4DpB;;;AC9DF;EACC;;AAEA;EACC;EACA;;;ACLF;EACI;EACA;;;AAEJ;EACI;EACA;;;AAGJ;EACC;;;AAGD;EACC;EACA;EACA;;;AAGD;EACC;;;AAED;EACC;;;AAGD;AAAA;EAEE;;;AAID;EACC;EACA;EACA;;AAED;EACC;EACA;EACA;;AAED;EACC;;;AAIF;EACC;;;ACrCD;EACE;EACA;EACA;EACA;EACA;EACA;;;AAGF;EACE;;;AAGF;EAEI;IAEI;;;AAQR;EACE;;;AAGF;EACE;EACA;EACA;EACA;;;AAGF;AAAA;EAEE;;;AAOF;EACE;;;AAGF;EACE;EACA;EACA;;;AAGF;EACE;;;AAGF;EACE;EACA;;;AAIF;EACE;;;AAGF;EACE;EACA;;;AAGF;EACE;;;AAGF;EACE;;;AAGF;EACE;EACA;;;AAGF;EACE\",\"sourcesContent\":[\".maintitle {\\n  font-family: 'Raleway';\\n  font-size: 2.5em;\\n}\\n\\n.not_found {\\n  margin-top: 4em;\\n  margin-bottom: 8em;\\n  text-align: center;\\n  font-family: 'Raleway';\\n  font-size: 2em;\\n}\",\"//----------------------------------------\\n// Specialized code blocks.\\n//----------------------------------------\\n\\n@use 'colors';\\n\\n@mixin cdSetup($box-color, $text-color: #000000, $prompt: \\\"\\\", $secondary: \\\"\\\") {\\n  pre {\\n    font-size: 14px;\\n    background-color: $box-color;\\n    color: $text-color;\\n    .line::before {\\n      content: $prompt ' ';\\n    }\\n    .secondary::before {\\n      content: $secondary ' ';\\n    }\\n  }\\n}\\n\\n:not(pre) > code {\\n  background-color: #eff0f1;\\n  border-radius: 0;\\n  padding: 1px 5px;\\n}\\n\\n.default      { @include cdSetup(#eff0f1, #000000); }\\n.Rev          { @include cdSetup(colors.$rev, #000000, \\\">\\\", \\\"+\\\"); }\\n.rev          { @include cdSetup(colors.$rev, #000000, \\\">\\\", \\\"+\\\"); }\\n.rev-output   { @include cdSetup(colors.$rev-output-bg, colors.$rev-output); }\\n.Rev-output   { @include cdSetup(colors.$rev-output-bg, colors.$rev-output); }\\n.bash         { @include cdSetup(colors.$bash, #000000, \\\"$\\\", \\\">\\\"); }\\n\\n.instruction {\\n  border: 1px solid;\\n  border-color: #ddd;\\n  display: table;\\n}\\n\",\"$link:                #337ab7;\\n\\n$link2:               #438ac7;\\n\\n// code boxes\\n$bash:                #f0f0f0 !default;\\n$rev:                 #b7cfed !default;\\n$rev-output-bg:       #b7cfed !default;\\n$rev-output:          #808080 !default;\\n\\n// blockquotes\\n$overview:            #b7cfed !default;\\n$files:               #f4fd9c !default;\\n$info:                #eec275 !default;\\n$aside:               #eee0e5 !default;\\n\\n// figures\\n$figure-border:       #B0B0B0 !default;\\n\\n$lavender-blush2:     #eee0e5 !default;\\n\",\"@use 'colors';\\n\\n$codeblock-padding: 5px !default;\\n\\n@mixin bkSetup($color, $glyph) {\\n\\n  $gradientcolor1: $color;\\n  $gradientcolor2: scale-color($color, $lightness: 10%);\\n\\n  padding-left: $codeblock-padding;\\n  padding-top: 0;\\n  padding-bottom: 0;\\n  padding-right: 0;\\n  border: 1px solid;\\n  border-color: $color;\\n  padding-bottom: $codeblock-padding;\\n  margin-top: 0;\\n\\n  >h2:first-child {\\n    padding-top: $codeblock-padding;\\n    padding-bottom: $codeblock-padding;\\n    font-size: 20px;\\n    background: linear-gradient(to bottom, $gradientcolor1, $gradientcolor2);\\n    border-color: $color;\\n    margin-top: 0px;\\n    margin-left: -$codeblock-padding; // to move back to the left margin of the enclosing blockquote\\n  }\\n  >h2:first-child:before {\\n    font-family: 'Glyphicons Halflings';\\n    content: $glyph;\\n    float: left;\\n    padding-left: $codeblock-padding;\\n    padding-right: $codeblock-padding;\\n    display: inline-block;\\n    margin-top: 0px;\\n    -webkit-font-smoothing: antialiased;\\n  }\\n\\n}\\n\\n.tutorial_files{ @include bkSetup(colors.$files, \\\"\\\"); }\\n.overview{ @include bkSetup(colors.$overview, \\\"\\\\e105\\\"); }\\n.info { @include bkSetup(colors.$info, \\\"\\\\e086\\\"); }\\n.aside { @include bkSetup(colors.$aside, \\\"\\\"); & { margin-top: 20px; } }\\n\\n.aside span.fold-unfold:first-child {\\n  cursor: pointer;\\n  float: left;\\n  padding-left: $codeblock-padding;\\n  padding-right: $codeblock-padding;\\n  display: inline-block;\\n  -webkit-font-smoothing: antialiased;\\n}\\n\\n.download_files {\\n  cursor: pointer;\\n  float: left;\\n  padding-left: $codeblock-padding;\\n  padding-right: $codeblock-padding;\\n  display: inline-block;\\n  -webkit-font-smoothing: antialiased;\\n}\\n\",\".titlebar {\\n  margin-bottom: 2em;\\n  text-align: left;\\n  font-family: 'Raleway';\\n\\n  .subtitle {\\n    font-size: 1.8em;\\n    font-style: italic;\\n  }\\n\\n  .authors {\\n    font-size: 1em;\\n  }\\n}\\n\\nh2.section, h2.references {\\n  margin-top: 30pt;\\n  margin-bottom: 0pt;\\n  font-size: 1.85em;\\n}\\n\\nhr.section, hr.references {\\n  margin: 5pt 0pt 5pt 0pt;\\n}\\n\\nh3.subsection {\\n  font-size: 1.6em;\\n}\\n\\nhr.subsection {\\n  display: none;\\n}\\n\\nh4.subsubsection {\\n  margin-top: 15pt;\\n  font-size: 1.2em;\\n}\\n\\nhr.subsubsection {\\n  display: none;\\n}\\n\\n.sidebar {\\n    width: 22em;\\n    float: left;\\n    font-size: 0.9em;\\n    margin-right: 1em;\\n    margin-top: 0em;\\n    padding: 8px;\\n}\\n\\n.MathJax_Display {\\n    pointer-events:none;\\n}\",\".tutorialbox {\\n  display: flex;\\n  flex-wrap: wrap;\\n}\\n\\n.tutorial {\\n  width: \\\"30%\\\";\\n  background-color: #fafafa;\\n  margin: 0.2em;\\n  padding: 0.6em;\\n  \\n  max-width: 13.5em;\\n  text-align: left;\\n\\n  .title {\\n    font-size: 1.2em;\\n  }\\n  .subtitle {\\n    font-style: italic;\\n  }\\n}\",\"figure {\\n  padding: 10px 20px;\\n  counter-increment: figures;\\n  text-align: center;\\n  margin-left: auto;\\n  margin-right: auto;\\n  margin-top: 0.9em;\\n  margin-bottom: 0.5em;\\n  border-left: 5px solid #eee;\\n  border-right: 5px solid #eee;\\n  display: table;\\n  \\n  img {\\n    margin-bottom: 0.5em\\n  }\\n}\\n\\nfigure.table {\\n  padding: 2px 4px;\\n  counter-increment: tables;\\n  align-items: center;\\n  margin-left: auto;\\n  margin-right: auto;\\n  margin-top: 0.9em;\\n  margin-bottom: 0.5em;\\n  border-left: 5px solid #eee;\\n  border-right: 5px solid #eee;\\n  display: table;\\n  \\n  figcaption.table {\\n    padding: 10px 20px;\\n    text-align: center;\\n    margin-left: auto;\\n    margin-right: auto;\\n    margin-top: 0.9em;\\n    margin-bottom: 0.5em;\\n  }\\n\\n  table.table {\\n    padding: 10px 20px;\\n    text-align: left;\\n    margin-left: auto;\\n    margin-right: auto;\\n    margin-top: 0.9em;\\n    margin-bottom: 0.5em;\\n  }\\n}\\n\\nfigure:not(.table) figcaption:before {\\n  content: 'Figure ' counter(figures) '. ';\\n  font-weight: bold;\\n}\\n\\nfigure.table figcaption:before {\\n  content: 'Table ' counter(tables) '. ';\\n  font-weight: bold;\\n}\\n\\n@media print {\\n  figure {\\n    border: 1px solid #999;\\n\\n    page-break-inside: avoid;\\n  }\\n  figure .table {\\n    border: 1px solid #999;\\n\\n    page-break-inside: avoid;\\n  }\\n}\",\"@use 'colors';\\n\\n* { box-sizing: border-box; }\\nbody {\\n  font: 16px Arial; \\n}\\n\\n$keyword-color: colors.$link2;\\n\\n.autocomplete {\\n  /*the container must be positioned relative:*/\\n  position: relative;\\n  display: inline-block;\\n  float: left;\\n}\\n.btn-clear {\\n  margin-left: 10px;\\n}\\ninput {\\n  border: 1px solid transparent;\\n  background-color: #f1f1f1;\\n  padding: 10px;\\n  font-size: 16px;\\n}\\ninput[type=text] {\\n  background-color: #f1f1f1;\\n  width: 80%;\\n}\\ninput[type=submit] {\\n  color: #fff;\\n  width: 15%\\n}\\n.autocomplete-items {\\n  position: absolute;\\n  border: 1px solid #d4d4d4;\\n  border-bottom: none;\\n  border-top: none;\\n  z-index: 99;\\n  /*position the autocomplete items to be the same width as the container:*/\\n  top: 100%;\\n  left: 0;\\n  right: 0;\\n}\\n.autocomplete-items div {\\n  padding: 10px;\\n  cursor: pointer;\\n  background-color: #fff; \\n  border-bottom: 1px solid #d4d4d4; \\n}\\n.autocomplete-items div:hover {\\n  /*when hovering an item:*/\\n  background-color: $keyword-color; \\n  color: #ffffff; \\n}\\n.autocomplete-active {\\n  /*when navigating through the items using the arrow keys:*/\\n  background-color: $keyword-color !important; \\n  color: #ffffff; \\n}\\n.btn-keyword {\\n  margin: 3px;\\n  background-color: $keyword-color;\\n  color: #ffffff; \\n}\\n\",\"ol.bibliography {\\n\\tpadding-left: 0px;\\n\\t\\n\\tli {\\n\\t\\tlist-style-type: none;\\n\\t\\tpadding-bottom: 10px;\\n\\t}\\n}\",\".sidebar_nav {\\n    overflow: auto;\\n    height:75vh;\\n}\\n.help {\\n    overflow: auto;\\n    height:75vh;\\n}\\n\\n.help-title {\\n\\tpadding-bottom: 2pt;\\n}\\n\\n.help-description {\\n\\tpadding-top: 5pt;\\n\\tpadding-bottom: 2pt;\\n\\tfont-size: 13pt;\\n}\\n\\n.help-section-header.arguments {\\n\\tmargin-bottom: 0px;\\n}\\n.help-section-body {\\n\\tpadding-left: 30px;\\n}\\n\\n.table.arguments>tbody>tr>td,\\n.table.arguments>tbody>tr>th {\\n  border-top: none;\\n}\\n\\ntable.arguments {\\n\\t.lhs {\\n\\t\\ttext-align: right;\\n\\t\\tpadding-top: 0px;\\n\\t\\tpadding-bottom: 0px;\\n\\t}\\n\\t.rhs {\\n\\t\\ttext-align: left;\\n\\t\\tpadding-top: 0px;\\n\\t\\tpadding-bottom: 0px;\\n\\t}\\n\\t.lhs.argument,.rhs.argument {\\n\\t\\tpadding-top: 10px;\\n\\t}\\n}\\n\\n.methods > li {\\n\\tpadding-bottom: 4px;\\n}\",\"@use 'colors';\\n@use 'home';\\n@use 'code';\\n@use 'blockquote';\\n@use 'tutorial';\\n@use 'tutorial-index';\\n@use 'figure';\\n@use 'keywords';\\n@use 'bibliography';\\n@use 'help';\\n\\nbody {\\n  display:none;\\n  font-family: HelveticaNeue,\\\"Helvetica Neue\\\",Helvetica,Arial,\\\"Lucida Grande\\\",sans-serif;\\n  font-size: 16px;\\n  counter-reset: figures;\\n  counter-reset: tables;\\n  padding-top: 70px;\\n}\\n\\nhtml {\\n  overflow:   scroll;\\n}\\n\\n@media print\\n{    \\n    .no-print, .no-print *\\n    {\\n        display: none !important;\\n    }\\n}\\n\\n//----------------------------------------\\n// Globals\\n//----------------------------------------\\n\\n.footertext {\\n  text-align: center;\\n}\\n\\nimg.navbar-logo {\\n  height: 40px; // synchronize with height of navbar\\n  padding-top: 5px;\\n  padding-right: 10px;\\n  padding-left: 40px;\\n}\\n\\nul,\\nol {\\n  padding-left: 1em;\\n}\\n\\n//----------------------------------------\\n// Override Bootstrap settings.\\n//----------------------------------------\\n\\nblockquote div:not(:last-child) {\\n  margin-bottom: 10px;\\n}\\n\\ncode {\\n  padding: 0 0;\\n  color: inherit;\\n  background-color: inherit;\\n}\\n\\nimg {\\n  max-width: 100%;\\n}\\n\\n.navbar .navbar-collapse {\\n  height: 50px;\\n  text-align: right;\\n}\\n\\n\\n.navbar-nav {\\n  display: flex;\\n}\\n\\n.navbar-nav > li:last-child {\\n  margin-left: auto;\\n  flex: 1;\\n}\\n\\n.navbar-nav > li:last-child > * {\\n  width: 100%;\\n}\\n\\n.header-search-form {\\n  margin: 8px 0px;\\n}\\n\\n.pull-left {\\n  float: left;\\n  margin-left: 10px;\\n}\\n\\n.navbar-toggle {\\n  float: right;\\n}\\n\"],\"file\":\"main.css\"}",
        "url": "/assets/css/main.css.map",
        "index": ""
      }
      
    
  };
</script>

<!-- Import lunr.js from unpkg.com -->
<script src="https://unpkg.com/lunr/lunr.js"></script>
<!-- Custom search script which we will create below -->
<script src="/assets/js/search.js"></script>

      </div>
      <br>
<footer>
  <div class="container">
  <div class="row">
    <div class="col-sm-12" align="center">
      <a href="https://github.com/revbayes">GitHub</a> | <a href="/license">License</a> | <a href="/citation">Citation</a> | <a href="https://groups.google.com/forum/#!forum/revbayes-users">Users Forum</a>
    </div>
  </div>
  <br>
  </div>
</footer>

    </div>
    <script src="/assets/js/vendor/jquery.min.js"></script>
<script src="/assets/js/vendor/FileSaver.min.js"></script>
<script src="/assets/js/vendor/jszip.min.js"></script>
<script src="/assets/js/vendor/bootstrap.min.js"></script>

<script type="text/javascript">
// Add default language
$(":not(code).highlighter-rouge").each(function() {
  
  if( this.classList == "highlighter-rouge") {
    this.classList = "default highlighter-rouge";
  }
  
});
// $("code.highlighter-rouge").each(function() {
//   
//   if( this.classList == "highlighter-rouge") {
//       this.classList = "default highlighter-rouge";
//   }
//   
// });
</script>
<script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
    });
    MathJax.Hub.Queue(function () {
      $(".aside").each(function() {
          $("div .MathJax", this).hide();
      });
    });
</script>
<script src="/assets/js/base.js"></script>

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0Y03X9Q2TJ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0Y03X9Q2TJ');
</script>

  </body>
</html>
