<!doctype html>
<html lang="en">
<link rel="icon" type="image/png" href="/revbayes-site/assets/img/favicon.png" >
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="search-domain" value="https://revbayes.github.io/revbayes-site/">
    <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet">
    <link rel="stylesheet" href="/revbayes-site/assets/css/syntax.css">
    <link rel="stylesheet" type="text/css" href="/revbayes-site/assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes-site/assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes-site/assets/css/main.css" />
    <title>RevBayes: Assessing Phylogenetic Reliability Using RevBayes and $P^{3}$</title>
  </head>
  <body>
    <div class="container">
      <nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <a href="/revbayes-site/" class="pull-left">
        
        <img class="navbar-logo" src="/revbayes-site/assets/img/aquabayes-desaturated.png" alt="RevBayes Home" />
        
      </a>
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar" align="right"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="/revbayes-site/software">Software</a></li>
        <li><a href="/revbayes-site/tutorials/">Tutorials</a></li>
        <li><a href="/revbayes-site/workshops/">Workshops</a></li>
        <li><a href="/revbayes-site/developer/">Developer</a></li>
      </ul>
      <!-- <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form> -->
    </div>
  </div>
</nav>

      <div class="titlebar">
	<h1 class="maintitle">Assessing Phylogenetic Reliability Using RevBayes and $P^{3}$</h1>
	<h3 class="subtitle">Model adequacy testing using posterior prediction (Inference Version).</h3>
	<h4 class="authors">Lyndon M. Coghill, Will Freyman, Sebastian H&#246;hna and Jeremy M. Brown</h4>
</div>


<div class="sidebar no-print">
<blockquote class="overview" id="overview">
  <h2>Overview</h2>
  
  <div class="row">
    <div class="col-md-9">
        <strong>Prerequisites</strong>
        
          <ul id="prerequisites">
          
            <li><a href="/revbayes-site/tutorials/intro/">Rev Language Syntax</a></li>
          
            <li><a href="/revbayes-site/tutorials/intro_rev/">Basic introduction to Rev & MCMC</a></li>
          
            <li><a href="/revbayes-site/tutorials/mcmc_archery/">Introduction to MCMC using RevBayes</a></li>
          
            <li><a href="/revbayes-site/tutorials/ctmc/">Substitution Models</a></li>
          
          </ul>
        
    </div>
  </div>
  
</blockquote>





<blockquote class="tutorial_files" id="tutorial_files">
    <h2>Data files and scripts</h2>
    
        
        <strong>Data Files</strong>
        <ul id="data_files">
        
        
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/data/primates_cytb.nex">primates_cytb.nex</a></li>
        
        </ul>
    
        
        <strong>Scripts</strong>
        <ul id="scripts">
        
        
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/GTR_Model.Rev">GTR_Model.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/JC_Model.Rev">JC_Model.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/MCMC_Simulation.Rev">MCMC_Simulation.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/PosteriorPredictive_MCMC.Rev">PosteriorPredictive_MCMC.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/PosteriorPredictive_PValues.Rev">PosteriorPredictive_PValues.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/PosteriorPredictive_Simulation.Rev">PosteriorPredictive_Simulation.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/PosteriorPredictive_TreeSummary.Rev">PosteriorPredictive_TreeSummary.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_testing_ppred_inf/scripts/full_analysis_JC.Rev">full_analysis_JC.Rev</a></li>
        
        </ul>
    
</blockquote>


</div>
<h1 id="overview">Overview</h1>

<p>This tutorial presents the general principles of assessing the
reliability of a phylogenetic inference through the use of posterior
predictive simulation (PPS). PPS works by assessing the fit of an
evolutionary model to a given dataset, and analyzing several test
statistics using a traditional goodness-of-fit framework to help explain
where a model may be the most inadequate.</p>

<h1 id="preparation">Preparation</h1>

<p>This tutorial expects you have compelted the prerequisite tutorials listed above. It will also expect you to be reasonably familiar with phylogenetic analyses, command line usage and if you want to explore your results, having at least a basic understanding of <code class="highlighter-rouge">R</code>. If you run this tutorial all the way through with the single <strong>full_analysis_JC.Rev</strong> script it takes approximately 20 - 25 minutes depending on your computer. If you work through every line-by-line to get a better understanding it takes approximately 60 minutes.</p>

<h1 id="introduction">Introduction</h1>

<p>Assessing the fit of an evolutionary model to the data is critical as
using a model with poor fit can lead to spurious conclusions. However, a
critical evaluation of absolute model fit is rare in evolutionary
studies. Posterior prediction is a Bayesian approach to assess the fit
of a model to a given dataset
<a href="#Bollback2002">(Bollback 2002)</a> <a href="#Brown2014">(Brown 2014)</a> <a href="#Gelman2014">(Gelman et al. 2014)</a>, that relies on the
use of the posterior and the posterior predictive distributions. The
posterior distribution is the standard output from Bayeisan phylogenetic
inference. The posterior predictive distribution represents a range of
possible outcomes given the assumptions of the model. The most common
method to generate these possible outcomes, is to sample parameters from
the posterior distribution, and use them to simulate new replicate
datasets (Fig. 1). If these simulated datasets differ from the empirical
dataset in a meaningful way, the model is failing to capture some
salient feature of the evolutionary process.</p>

<p>The framework to construct posterior predictive distributions, and
compare them to the posterior distribution is conveniently built in to
<code class="highlighter-rouge">RevBayes</code> through the $P^{3}$ pipeline <a href="#Hohna2017b">(Höhna et al. 2017)</a>. In this tutorial we will walk you through using this
functionality to perform a complete posterior predictive simulation on
an example dataset.</p>

<p>If you would like more information than we provide here, we would highly encourage you to read the $P^{3}$ manuscript for a deeper discussion of the full feature set.</p>

<blockquote>
  <p><a href="https://academic.oup.com/mbe/article/35/4/1028/4616601">Höhna S., Coghill L.M., Mount G.G., Thomson R.C., Brown J.M. 2017. P3: Phylogenetic Posterior Prediction in RevBayes. Molecular Biology and Evolution. 35:1028–1034. 10.1093/molbev/msx286 </a></p>
</blockquote>

<h2 id="data-based-versus-inference-based-comparisons">Data-Based Versus Inference-Based Comparisons</h2>

<p>Most statistics proposed for testing model plausibility compare
data-based characteristics of the original data set to the posterior
predictive data sets (e.g., variation in GC-content across species). In
data-based assessments of model fit one compares the empirical data to
data simulated from samples of the posterior distribution. <code class="highlighter-rouge">RevBayes</code>
additionally implements test statistics that compare the inferences
resulting from different data sets (e.g., the distribution of posterior
probability across topologies). These are called inference-based
assessments of model fit and will be the focus of this tutorial. For these assessments one must run an MCMC
analysis on each simulated data set, and then compare the inferences
made from the simulated data to the inference made from the empirical
data.</p>

<figure id="example"><p><img src="figures/pps.png" /></p>
<figcaption>Overview of the $P^{3}$ workflow as implemented in RevBayes together with the specific commands necessary in each step <a href="#Hohna2017b">(Höhna et al. 2017)</a>. Step 1 involves sampling parameters, for example, tree topology and branch lengths, from the posterior distribution using MCMC simulation. Step 2 simulates new data sets given the parameter samples from step 1. Step 3 estimates posterior distributions of the same parameters but from the simulated data sets. This third step is optional and is only needed for inference-based test statistics. Next, step 4 involves computing data- and/or inference-based test statistics and comparing the distribution of the test statistic from the simulated data with the test statistic value from the observed data. Finally, data sets and models can be rejected or ranked based on the posterior predictive p-values or the posterior predictive effect size (PPES), which is the difference between the median of the posterior predictive distribution and the empirical value, normalized by the distribution’s SD.</figcaption>
</figure>

<p>Due to time constraints, in today’s tutorial we will only cover the
inference-based method of assessing model plausibility. The data-based
method can be a powerful tool that you may want to explore at another
time by visiting that tutorial.</p>

<h2 id="substitution-models">Substitution Models</h2>

<p>The models we use here are equivalent to the models described in the
previous exercise on substitution models (continuous time Markov
models). To specify the model please consult the previous exercise.
Specifically, you will need to specify the following substitution
models:</p>

<ul>
  <li>
    <p>Jukes-Cantor (JC) substitution model <a href="#Jukes1969">(Jukes and Cantor 1969)</a></p>
  </li>
  <li>
    <p>General-Time-Reversible (GTR) substitution model <a href="#Tavare1986">(Tavaré 1986)</a></p>
  </li>
</ul>

<h1 id="assessing-model-fit-with-posterior-prediction">Assessing Model Fit with Posterior Prediction</h1>

<blockquote class="info">
  <h2 id="for-your-info">For your info</h2>
  <p>The entire process of posterior prediction can be executed by using the
<strong>full_analysis_JC.Rev</strong> script in the <strong>scripts</strong> folder. If you
were to type the following command into <code class="highlighter-rouge">RevBayes</code>:</p>
  <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source("scripts/data_pp_analysis_JC.Rev")
</code></pre></div>  </div>
  <p>the entire data-based posterior prediction process would run on the
example dataset. However, in this tutorial, we will walk through each
step of this process on an example dataset. As we follow along,
you will want to execute the commands in the light blue boxes with black text only. The light blue
boxes with grey text will show output from the commands, the grey boxes with white background will highlight
pieces of scripts that are helpful, but don’t need to be executed.</p>
</blockquote>

<h2 id="empirical-mcmc-analysis">Empirical MCMC Analysis</h2>

<p>To begin, we first need to generate a posterior distribution from which
to sample for simulation. This is the normal, and often only, step
conducted in phylogenetic studies. Here we will specify our dataset,
evolutionary model, and run a traditional MCMC analysis.</p>

<p>This code is all in the <strong>full_analysis_JC.Rev</strong> file.</p>

<h3 id="set-up-the-workspace">Set up the workspace</h3>

<p>First, let’s read in our dataset.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## EMPIRICAL MCMC
inFile = "data/primates_cytb.nex"
data &lt;- readDiscreteCharacterData(inFile)
</code></pre></div></div>

<div class="Rev-output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Successfully read one character matrix from file data/primates_and_galeopterus_cytb.nex
</code></pre></div></div>

<p>Great, we have our data in <code class="highlighter-rouge">RevBayes</code> now. Next, we’ll set up some workspace variables we will need. We can begin by specifying a
general name to apply to your analysis. This will be used for future
output files, so make sure it’s something clear and easy to understand.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>analysis_name = "pps_example"
model_name = "JC"
model_file_name = "scripts/"+model_name+"_Model.Rev"
</code></pre></div></div>

<h3 id="specify-the-model">Specify the model</h3>

<p>Now we’ll need to specify our model Jukes-Cantor in this case. We can do this by
sourcing the name we gave our model script a few lines ago.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source( model_file_name )
</code></pre></div></div>

<div class="Rev-output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Processing file "scripts/JC_Model.Rev"
   Processing of file "scripts/JC_Model.Rev" completed 
</code></pre></div></div>

<p>Now we have our data and our model for our empirical analyses read into <code class="highlighter-rouge">RevBayes</code>.
This command will process the JC_Model.Rev script in full. 
If you want to tweak any of the model parameters, or just want a reminder about how model scripts in <code class="highlighter-rouge">RevBayes</code> work,
you can dig a little deeper in the aside box below or revist the <a href="/revbayes-site/tutorials/ctmc/">Substitution Models</a>.</p>

<blockquote class="aside"><h2>JC Model Details</h2><p>Taking a brief look at some of the details of the <strong>JC_Model.Rev</strong> script.</p>

<p>A few specific lines we can look at that might be interest, as we are
using a unrooted tree for this analysis are:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#### specify the Jukes-Cantor substitution model applied uniformly to all sites ###
Q := fnJC(4) 
</code></pre></div></div>

<p>Here we are specifying that we should use the Jukes-Cantor model, and
have it applied uniformly to all sites in the dataset. While this
obviously is not likely to be a good fitting model for most datasets, we
are using it for simplicity of illustrating the process.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#### Specify a uniform prior on the tree topology #### 
</code></pre></div></div>

<p>This sets a uniform prior on the tree topology.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  # We use here the exponential distribution with rate 1.0 as the branch length prior
  br_lens[i] ~ dnExponential(10.0)
</code></pre></div></div>

<p>This sets an exponential distribution as the branch length prior.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Build the tree by combining the topology with the branch length.
phylogeny := treeAssembly(topology, br_lens)
</code></pre></div></div>

<p>This builds the tree by combining the topology with branch length
support values.</p>
</blockquote>

<h3 id="run-the-mcmc">Run the MCMC</h3>

<p>Now let’s run MCMC on our empirical dataset, just like a normal
phylogenetic analysis. We need to set some variables:</p>

<p>First, we need to setup a counter for our monitors:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## set a monitor counter
mni = 0
</code></pre></div></div>

<p>Next, we need to setup the monitors themselves:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## setup our monitors
monitors[++mni] = mnModel(filename="output_" + model_name + "/" + analysis_name + "_posterior.log",printgen=100, separator = TAB)
monitors[++mni] = mnFile(filename="output_" + model_name + "/" + analysis_name + "_posterior.trees",printgen=100, separator = TAB, phylogeny)
monitors[++mni] = mnScreen(printgen=100, TL)
monitors[++mni] = mnStochasticVariable(filename="output_" + model_name + "/" + analysis_name + "_posterior.var",printgen=100)
</code></pre></div></div>

<p>Next, we need to setup the MCMC object:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymcmc = mcmc(mymodel, monitors, moves, nruns=2)
mymcmc.burnin(generations=2000,tuningInterval=10)
</code></pre></div></div>

<div class="Rev-output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Running burn-in phase of Monte Carlo sampler for 2000 iterations.
   This simulation runs 2 independent replicates.
   The simulator uses 23 different moves in a random move schedule with 23 moves per iteration

Progress:
0---------------25---------------50---------------75--------------100
********************************************************************
</code></pre></div></div>

<p>Finally, we can start the MCMC run:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## start the run
mymcmc.run(generations=10000)
</code></pre></div></div>

<div class="Rev-output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Running MCMC simulation
   This simulation runs 2 independent replicates.
   The simulator uses 45 different moves in a random move schedule with 45 moves per iteration

Iter        |      Posterior   |     Likelihood   |          Prior   |             TL   |    elapsed   |        ETA   |
-----------------------------------------------------------------------------------------------------------------------
0           |       -16086.3   |       -16099.8   |        13.4989   |       3.139019   |   00:00:00   |   --:--:--   |
100         |       -16088.8   |       -16101.1   |        12.3286   |       3.256052   |   00:00:01   |   --:--:--   |
.
.
.
-----------------------------------------------------------------------------------------------------------------------
10000       |       -16085.4   |       -16099.3   |        13.8874   |        3.10017   |   00:01:12   |   00:00:00   |
</code></pre></div></div>

<p>Here we are only using a small number of generations for the tutorial,
however with empirical data you will most likely need a much larger
number of generations to get a well mixed sample. The number of
generations and printed generations is important to consider here for a
variety of reasons, in particular for posterior predictive simulation.
When we simulate datasets in the next step, we can only simulate 1
dataset per sample in our posterior. So, while the number of posterior
samples will almost always be larger than the number of datasets we will
want to simulate, it’s something to keep in mind.</p>

<h3 id="mcmc-output">MCMC output</h3>

<p>After the process completes, the results can be found in the
<strong>output_JC</strong> folder. You should see a number of familiar looking
files, all with the name we provided under the <strong>analysis_name</strong>
variable, <strong>pps_example</strong> in this case. Since we set the number of runs
(nruns=2) in our MCMC, there will be two files of each type (.log .trees
.var) with an _<em>N</em> where <em>N</em> is the run number. You will also see 3
files without any number in their name. These are the combined files of
the output. These will be the files we use for the rest of the process.
If you open up one of the combined .var file, you should see that there
are 200 samples. This was produced by our number of generations
(10000) divided by our number of printed generations, (100) what we
specified earlier, combined from the two independent runs. This is important to note, as we will need to thin
these samples appropriately in the next step to get the proper number of
simulated datasets.</p>

<h2 id="posterior-predictive-data-simulation">Posterior Predictive Data Simulation</h2>
<p>The next step of posterior predictive simulation is to simulate new
datasets by drawing samples and parameters from the posterior
distribution generated from the empirical MCMC anlaysis. This functionality is
built into the backend of <code class="highlighter-rouge">RevBayes</code> in order to simplify the process.</p>

<blockquote class="instruction">
  <p>In the <strong>full_analysis_JC.Rev</strong> script, that is conducted using
the following line of RevScript:
<code class="highlighter-rouge">&gt; source("scripts/PosteriorPredictive_Simulation.Rev")</code></p>
</blockquote>

<p>However, we will process each line of this script so that you can better understand 
what functions are being called.</p>

<p>First, we read in the trace file of the Posterior Distribution of
Variables.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Reading Trace
trace = readStochasticVariableTrace("output_" + model_name + "/" + analysis_name + "_posterior.var", delimiter=TAB)
</code></pre></div></div>

<p>Now we call the <strong>posteriorPredictiveSimulation()</strong> function, which
accepts any valid model of sequence evolution, and output directory, and
a trace. For each line in the trace, it will simulate a new dataset
under the specified model.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Creating Posterior Predictive Simulation
pps = posteriorPredictiveSimulation(mymodel, directory="output_" + model_name + "/" + analysis_name + "_post_sims", trace)
</code></pre></div></div>

<p>Now we run the posterior predictive simulation, generating a new dataset
for each line in the trace file that was read in. This is the part where
we need to decide how many simulated datasets we want to generate. If we
just use the pps.run() command, one dataset will be generated for each
sample in our posterior distribution. In this case, since we are reading
in the combined posterior trace file with 200 samples, it would generate
200 simulated datasets. If you want to generate fewer, say 100 datasets,
you need to use the thinning argument as above. In this case, we are
thinning the output by 2, that is we are dividing our number of samples
by 2. So that in our example case, we will end up simulating 100 new
datasets.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Running the posterior predictive simulation, here we are thinning the datasets by half
pps.run(thinning=2)
</code></pre></div></div>

<p>This process should finish in just a minute or two. If you look in the
<strong>output_JC</strong> folder, there should be another folder called
<strong>pps_example_post_sims</strong>. This folder is where the simulated
datasets are saved. If you open it up, you should see 100 folders named
posterior_predictive_sim_<em>N</em>. Where <em>N</em> is the number of the
simulated dataset. In each of these folders, you should find a seq.nex
file. If you open one of those files, you’ll see it’s just a basic NEXUS
file. These will be the datasets we analyze in the next step.</p>

<h2 id="posterior-predictive-mcmc-analysis">Posterior Predictive MCMC Analysis</h2>

<p>The next step of posterior predictive simulation is to conduct a full MCMC analysis 
on each of the new simulated datasets. This is by far, the most computationally intensive part
of this process, as you are conducting 100 individual MCMC analyses. In a later section, we outline
how this can be parallelized and sped up dramatically on HPC clusters or servers. However, for now, 
we will run this locally in serial mode with a low number of generations as an example.</p>

<p>In the <strong>full_analysis_JC.Rev</strong> script, that is conducted using
the following line of RevScript:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source("scripts/PosteriorPredictive_MCMC.Rev")
</code></pre></div></div>

<p>However, we will process each line of this script so that you can better understand 
what functions are being called.</p>

<p>First, we need to specify a counter for our monitors:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## first we set a counter for our monitors
mni = 0
</code></pre></div></div>

<p>Next, we need to specify a couple of monitors for our MCMC moves:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## next we setup our monitors, like in our previous MCMC analyses
monitors[++mni] = mnModel(filename="output_" + model_name + "/" + analysis_name + "_posterior.log",printgen=100, separator = TAB)
monitors[++mni] = mnFile(filename="output_" + model_name + "/" + analysis_name + "_posterior.trees",printgen=100, separator = TAB, phylogeny)
</code></pre></div></div>

<p>Now we can setup our individual MCMC objects:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## now we setup our MCMC object for each posterior predictive dataset
mymcmc = mcmc(mymodel, monitors, moves, nruns=1)
</code></pre></div></div>

<p>Next, specify a directory for our output files:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## specify and output directory for each analysis
directory = "output_" + model_name + "/" + analysis_name + "_post_sims"
</code></pre></div></div>

<p>This step is a little different from previous MCMC setups. Here we are create a new pps_mcmc object for all of our individual analyses:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## specify a new pps_mcmc object for all of the independent analyses
my_pps_mcmc = posteriorPredictiveAnalysis(mymcmc, directory)
</code></pre></div></div>

<p>Finally, we run the 100 individual MCMC analyses:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## run the actual analyses
my_pps_mcmc.run(generations=10000)
</code></pre></div></div>
<div class="Rev-output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Running posterior predictive analysis ...
Sim   1 / 100

Iter        |      Posterior   |     Likelihood   |          Prior   |             TL   |    elapsed   |        ETA   |
-----------------------------------------------------------------------------------------------------------------------
0           |       -9345.75   |       -9353.53   |        7.77435   |       2.322464   |   00:00:00   |   --:--:--   |
100         |   9.04115e+271   |              0   |   9.04115e+271   |       2.322464   |   00:00:00   |   --:--:--   |
</code></pre></div></div>

<p>Now that we have a full set of MCMC analyses for our simulated data, we can move on to calculating test statistics.</p>

<h2 id="calculating-the-test-statistics">Calculating the Test Statistics</h2>

<p>Now we will calculate the test statistics from the empirical data and
the simulated data sets. Let’s go ahead and calculate our test statistics by entering the following lines into <code class="highlighter-rouge">RevBayes</code>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>num_post_sims = listFiles(path="output_"+model_name+"/" + analysis_name + "_post_sims").size()
source("scripts/PosteriorPredictive_TreeSummary.Rev")
</code></pre></div></div>
<div class="Rev-output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Progress:
0---------------25---------------50---------------75--------------100
********************************************************************

   Processing file "output_JC/posterior_predictive_sim_1/pps_example_posterior.trees"
.
.
.
   Processing file "output_JC/pps_example_posterior.trees"

Progress:
0---------------25---------------50---------------75--------------100
********************************************************************
</code></pre></div></div>

<p>These are the same lines used in the <strong>full_analysis_JC.Rev</strong> script.</p>

<p>We will take a closer look at these lines but for a more complete discussion of the statistics involved, please
review <a href="#Brown2014">(Brown 2014)</a> <a href="#Doyle2015">(Doyle et al. 2015)</a>. In general, this script and these
statistics work by calculating the statistics of interest across each
posterior distribution from the simulated datasets, and comparing those
values to the values from the empirical posterior distribution.</p>

<p>The current version of this script generates several summary statistics
including:</p>

<ul>
  <li>
    <p>Various quantiles</p>
  </li>
  <li>
    <p>Mean RF</p>
  </li>
  <li>
    <p>Mean tree length</p>
  </li>
  <li>
    <p>Tree length variance</p>
  </li>
  <li>
    <p>Entropy</p>
  </li>
</ul>

<figure id="example"><p><img src="figures/teststats.png" /></p>
<figcaption>Example topological test statistic calculations from a posterior distribution <a href="#Brown2014">(Brown 2014)</a>. In this hypothetical scenario, four unique topologies were found in 100 MCMC samples from a Bayesian analysis. The prior (uniform) and estimated posterior probabilities are given next to each unique topology. Bidirectional arrows are labeled with the symmetric (Robinson–Foulds) distance between topologies, along with the number of times this distance will be included in the vector of all pairwise distances between posterior samples.</figcaption>
</figure>

<p>Since these values are calculated by iterating over the entire series of posterior predictive analyses
they are a bit unweildy to run line by line. However, we will talk about some of the individual functions
and how they work so if you wish you can develop your own test statistics at a later time.
Here are some examples of these functions from the
<strong>PosteriorPredictive_TreeSummary.Rev</strong> script:</p>

<blockquote>
  <p>This functions calculates pairwise RF distances between all trees in a tree trace.</p>
</blockquote>

<p><code class="highlighter-rouge">rf_dists &lt;- sim_tree_trace.computePairwiseRFDistances(credibleTreeSetSize=1.0,verbose=FALSE)</code></p>

<blockquote>
  <p>This function collects the tree lengths of all trees in a tree trace, these values are used for the tree length and variance 
statistics.</p>
</blockquote>

<p><code class="highlighter-rouge">tree_length &lt;- sim_tree_trace.computeTreeLengths()</code></p>

<blockquote>
  <p>This function calculates the entropy statistic for all trees in a given tree trace. Entropy is analogous to the uncertainty 
associated with sampling from the prior or posterior. As the data provide more information and the posterior probabilities of 
various topologies become increasingly uneven, entropy decreases causing the difference between the entropy of the prior and the 
posterior to increase. Equation $\ref{equation1}$.</p>
</blockquote>

<p><code class="highlighter-rouge">entropy &lt;- sim_tree_trace.computeEntropy(credibleTreeSetSize=1.0,numTaxa=data.ntaxa(),verbose=FALSE)</code></p>

<p>Assuming a uniform prior on topologies, the change in entropy can be calculated as:</p>

<script type="math/tex; mode=display">T_e(X,M_c)=ln[B(N)]+\sum_{i=1}^{B(N)} p(\tau_i|X)ln[p(\tau_i|X)] \label{equation1}\tag{1}</script>

<p>These same statistics are calculated for both the posterior distributions from the simulated datasets and the posterior distribution from the empirical dataset. This framework is designed to be flexible, so if in the future you can imagine a test statistic you think would be informative for your model,
it is easy to expand these calculations in <code class="highlighter-rouge">RevScript</code>.</p>

<h2 id="calculating-p-values-and-effect-sizes">Calculating <em>P</em>-values and effect sizes</h2>

<p>Once we have the test statistics calculated for the simulated and
empirical posterior distributions, we can compare the simulated to the
empirical to get a goodness-of-fit. One simple way to do this is to
calculate a posterior predictive <em>P</em>-value for each of the test
statistics of interest. This is done in the
<strong>full_analysis_JC.Rev</strong> with the following lines :</p>

<p>First, we set our input file names.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>emp_pps_file = "results_" + model_name + "/empirical_inference_" + analysis_name + ".csv"
sim_pps_file = "results_" + model_name + "/simulated_inference_" + analysis_name + ".csv"
</code></pre></div></div>

<p>Next, we set our output file name.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>outfileName = "results_" + model_name + "/inference_pvalues_effectsizes_" + analysis_name + ".csv"
</code></pre></div></div>

<p>Now, let’s create a vector to hold the results for faster calculations:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>statID = v("", "mean_rf", "quantile25", "quantile50", "quantile75", "quantile99", "quantile999", "mean_tl", "var_tl", "entropy")
</code></pre></div></div>

<p>We will talk about the individual calculations shortly, but for now, let’s go ahead and calculate all
of our <em>P</em>-values for our test statistics.</p>

<p>Finally, we source the <strong>PosteriorPredictive_PValues.Rev</strong> script to run the calculations.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source("scripts/PosteriorPredictive_PValues.Rev")
</code></pre></div></div>

<div class="Rev-output highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Processing file "scripts/PosteriorPredictive_PValues.Rev"
   Processing of file "scripts/PosteriorPredictive_PValues.Rev" completed
</code></pre></div></div>

<p>This script will calculate 3 posterior predictive <em>P</em>-values:</p>

<ul>
  <li>
    <p>Lower 1-tailed (Equation $\ref{equation2}$)</p>
  </li>
  <li>
    <p>Upper 1-tailed (Equation $\ref{equation3}$)</p>
  </li>
  <li>
    <p>2-tailed (Equation $\ref{equation4}$)</p>
  </li>
</ul>

<p>The posterior predictive <em>P</em>-value for a lower one-tailed test is the
proportion of samples in the distribution where the value is less than
or equal to the observed value, calculated as:</p>

<script type="math/tex; mode=display">p_l=p(T(X_{rep})\leqslant T(X)|(X) \label{equation2}\tag{2}</script>

<p>The posterior predictive <em>P</em>-value for an upper one-tailed test is the
proportion of samples in the distribution where the value is greater
than or equal to the observed value, calculated as:</p>

<script type="math/tex; mode=display">p_u=p(T(X_{rep})\geqslant T(X)|(X) \label{equation3}\tag{3}</script>

<p>and the two-tailed posterior predictive <em>P</em>-value is simply twice the
minimum of the corresponding one-tailed tests calculated as:</p>

<script type="math/tex; mode=display">p_2=2min(p_l,p_u) \label{equation4}\tag{4}</script>

<p>Let’s take a look at the <strong>PosteriorPredictive_PValues.Rev</strong> script to
get a better idea of what is happening.</p>

<blockquote>
  <p>Calculates and returns a vector of lower, equal, and upper <em>P</em>-values for a given test statistic as outlined above in equation X.</p>
</blockquote>

<p><code class="highlighter-rouge">p_values &lt;- posteriorPredictiveProbability(numbers, empValue)</code></p>

<blockquote>
  <p>Calculates the midpoint <em>P</em>-value as outlined above in equation X.</p>
</blockquote>

<p><code class="highlighter-rouge">midpoint_p_value = lower_p_value + 0.5*equal_p_value</code></p>

<blockquote>
  <p>Calculates the two-tailed <em>P</em>-value as outlined above in equation X.</p>
</blockquote>

<p><code class="highlighter-rouge">two_tail_p_value = 2 * (min(v(lower_p_value, upper_p_value)))</code></p>

<p>Another way that you can calculate the magnitude of the discrepancy
between the empirical and PP datasets is by calculating the effect size
of each test statistic. Effect sizes are useful in quantifying the
magnitude of the difference between the empirical value and the
distribution of posterior predictive values. The test statistic effect
size can be calculated by taking the absolute value of the difference
between the median posterior predictive value and the empirical value, and
dividing it by the standard deviation of the posterior predictive
distribution <a href="#Doyle2015">(Doyle et al. 2015)</a>. Effect sizes are calculated automatically
for the inference based test statistics in the $P^{3}$ analysis. The effect
sizes for each test statistics are stored in the same output file as the
<em>P</em>-values.</p>

<blockquote>
  <p>The line:</p>
</blockquote>

<p><code class="highlighter-rouge">effect_size = abs((m - empValue) / stdev(numbers))</code></p>

<blockquote>
  <p>calculates the effect size of a given test statistic.</p>
</blockquote>

<p>Now that we have all of these values calculated, we can visualize them in any number of ways.</p>

<h2 id="visualizing-the-results">Visualizing the Results</h2>

<h3 id="visualizing-the-results-with-r">Visualizing the Results with <code class="highlighter-rouge">R</code></h3>

<p>All of the output from these analyses are saved to simple CSV text files. You can visualize your distributions and the fit of the
empirical values to the distribution of simulated values in anyway that is intuitive for you.</p>

<figure id="example"><p><img src="figures/dist.png" /></p>
<figcaption>Example distribution of simulated pps test statistic values with the median value plotted as the green line and the empirical value plotted as the red line. In this case, this test statistic suggests that the model is a poor fit for the data.</figcaption>
</figure>

<p>Let’s walk through visualizing one of the test statistic datasets in a very basic <code class="highlighter-rouge">R</code> script so you can get a feel for one way it could be done.</p>

<p>Our data from this analyses is all stored in the <strong>results_JC/</strong> folder. So it would be helpful to set your R workspace to that folder.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>setwd("/YOUR/PATH/HERE/results_JC")
</code></pre></div></div>

<p>Once that’s done, the first thing we’ll want to do is to read in our empirical values.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>empirical_inference_pps_example &lt;- read.csv("/results_JC/empirical_inference_pps_example.csv", header=TRUE)
</code></pre></div></div>

<p>Next, we’ll read in the simulated values.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>simulated_inference_pps_example &lt;- read.csv("/results_JC/simulated_inference_pps_example.csv", header=TRUE)
</code></pre></div></div>

<p>Once we have our two datasets imported into <code class="highlighter-rouge">R</code>, we can easily create a plot like the example plot above. For this example, we’ll plot the
<strong>mean tree length (mean_tl)</strong> value from these analyses.</p>

<p>First, let’s create our histogram plot.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hist(simulated_inference_pps_example$mean_tl, breaks=20)
</code></pre></div></div>

<p>Next, let’s add a line showing the median just for a reference.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>abline(v=median(simulated_inference_pps_example$mean_tl), col="green", lty=2)
</code></pre></div></div>

<p>Finally, let’s plot our empirical value to get a feel for how it compares to our simulated values.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>abline(v=empirical_inference_pps_example$mean_tl, col="red", lty=2)
</code></pre></div></div>

<figure id="example"><p><img src="figures/rexample.png" /></p>
<figcaption>Mean Tree Length test statistic distribution from posterior predictive simulation analyses of the <strong>primates_cytb.nexus</strong> dataset. Green line is the median value of the simulated dataset, red line is the empirical value.</figcaption>
</figure>

<h2 id="additional-individual-exercises">Additional Individual Exercises</h2>

<p>Included in the <strong>scripts</strong> folder is a second model script called
<strong>GTR_Model.Rev</strong>. As a personal exercise and a good test case, take
some time now, and run the same analysis, substituting the
<strong>GTR_Model.Rev</strong> model script for the <strong>JC_Model.Rev</strong> script we used
in the earlier example. In order to speed this process up, you can duplicate the
<strong>full_analysis_JC.Rev</strong> and name the new copy <strong>full_analysis_GTR.Rev</strong>. You can then just
edit the lines in that one script to point to the <strong>GTR_Model.Rev script</strong>, and re-run your with a single command:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source("scripts/full_analysis_GTR.Rev")
</code></pre></div></div>

<p>You should get different results, this is an
excellent chance to explore the results and think about what they
suggest about the fit of the specified model to the dataset.</p>

<h3 id="test-of-gtr-model-adequacy">Test of GTR Model Adequacy</h3>

<p><strong>Some Questions to Keep in Mind:</strong></p>

<ul>
  <li>
    <p>Do you find the goodness-of-fit results to suggest that the GTR or
JC model is a better fit for our data?</p>
  </li>
  <li>
    <p>Which test statistics seem to show the strongest effect from the use
of a poorly fitting model?</p>
  </li>
  <li>
    <p>Other than <em>P</em>-values, what other ways might you explore the test
statistic distributions to identify poor fit?</p>
  </li>
</ul>

<blockquote class="info">
  <h2 id="for-your-consideration">For your consideration</h2>
  <p>In this tutorial you have learned how to use <code class="highlighter-rouge">RevBayes</code> to assess the
fit of a substitution model to a given sequence alignment. As you have
discovered, the observed data should be plausible under the posterior
predictive simulation if the model is reasonable. In phylogenetic
analyses we choose a model, which explicitly assumes that it provides an
reasonable explanation of the evolutionary process that generated our
data. However, just because a model may be the ’best’ model available,
does not mean it is an appropriate model for the data. This distinction
becomes both more critical and less obvious in modern analyses, where
the number of genes often number in the thousands. Posterior predictive
simulation in <code class="highlighter-rouge">RevBayes</code>, allows you to easily check model fit for a
large number of genes by using global summaries to check the posterior
predictive distributions with a comfortable goodness-of-fit style
framework.</p>
</blockquote>

<blockquote class="aside"><h2>Batch Processing of Large Datasets</h2><hr />

<p>The process described above is for a single gene or alignment. However,
batch processing a large number of genes with this method is a
relatively straight forward process.</p>

<p><code class="highlighter-rouge">RevBayes</code> has built in support for MPI so running <code class="highlighter-rouge">RevBayes</code> on more
than a single processor, or on a cluster is as easy as calling it with
openmpi.</p>

<p>For example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mpirun -np 16 rb-mpi scripts/full_analysis.Rev
</code></pre></div></div>

<p>would run the entire posterior predictive simulation analysis on a
single dataset using 16 processors instead of a single processor. Use of
the MPI version of <code class="highlighter-rouge">RevBayes</code> will speed up the process dramatically.</p>

<p>Setting up the <strong>full_analysis.Rev</strong> script to cycle through a large
number of alignments is relatively simple as well. One easy way is to
provide a list of the data file names, and to loop through them. As an
example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data_file_list = "data_file_list.txt" 
data_file_list &lt;- readDiscreteCharacterData(data_file_list)
file_count = 0

for (n in 1:data_file_list.size()) {

FULL_ANALYSIS SCRIPT GOES HERE

file_count = file_count + 1
}
</code></pre></div></div>

<p>Then, anywhere in the full_analysis portion of the script that the
lines</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inFile = "data/8taxa_500chars_GTR.nex"
analysis_name = "pps_example"
</code></pre></div></div>

<p>appear, you would replace them with something along the lines of:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inFile = n
analysis_name = "pps_example_" + file_count
</code></pre></div></div>

<p>This should loop through all of the data files in the list provided, and
run the full posterior predictive simulation analysis on each file.
Using a method like this, and combining it with the MPI call above, you
can scale this process up to multiple genes and spread the computational
time across several cores to speed it up.</p>
</blockquote>

<ol class="bibliography"><li><span id="Bollback2002">Bollback J.P. 2002. Bayesian model adequacy and choice in phylogenetics. Molecular Biology and Evolution. 19:1171–1180.</span>

</li>
<li><span id="Brown2014">Brown J.M. 2014. Detection of implausible phylogenetic inferences using posterior predictive assessment of model fit. Systematic biology. 63:334–348.</span>

</li>
<li><span id="Doyle2015">Doyle V.P., Young R.E., Naylor G.J.P., Brown J.M. 2015. Can we identify genes with increased phylogenetic reliability? Systematic biology. 64:824–837.</span>

</li>
<li><span id="Gelman2014">Gelman A., Carlin J.B., Stern H.S., Dunson D.B., Vehtari A., Rubin D.B. 2014. Bayesian data analysis. CRC press Boca Raton, FL.</span>

</li>
<li><span id="Hohna2017b">Höhna S., Coghill L.M., Mount G.G., Thomson R.C., Brown J.M. 2017. P3: Phylogenetic Posterior Prediction in RevBayes. Molecular Biology and Evolution. 35:1028–1034.</span>

<a href="https://doi.org/10.1093/molbev/msx286">10.1093/molbev/msx286</a>

</li>
<li><span id="Jukes1969">Jukes T.H., Cantor C.R. 1969. Evolution of Protein Molecules. Mammalian Protein Metabolism. 3:21–132.</span>

<a href="https://doi.org/10.1016/B978-1-4832-3211-9.50009-7">10.1016/B978-1-4832-3211-9.50009-7</a>

</li>
<li><span id="Tavare1986">Tavaré S. 1986. Some Probabilistic and Statistical Problems in the Analysis of DNA Sequences. Some Mathematical Questions in Biology: DNA Sequence Analysis. 17:57–86.</span>

</li></ol>

<script type="text/javascript">
var _ol = document.querySelectorAll('ol');
for (var i = 0, elem_ol; elem_ol = _ol[i]; i++) {
	if ( elem_ol.classList == "bibliography" ) {
		var _li = elem_ol.getElementsByTagName("li");
		//for (var j = 0, elem_li; elem_li = _li[j]; j++)
		//{
		//	elem_li.innerHTML = elem_li.innerHTML.replace(/(https?:\/\/)([^\s<]+)/,"<a href=\"$1$2\">$2");
		//}
		if(_li.length > 0)
			elem_ol.outerHTML = "<h2 class='references'>References</h2><hr class='references'>"+elem_ol.outerHTML
	}
}
</script>

      <br>
<footer>
  <div class="container">
  <div class="row">
    <div class="col-sm-12" align="center">
      <a href="https://github.com/revbayes">GitHub</a> | <a href="/revbayes-site/license">License</a> | <a href="/revbayes-site/citation">Citation</a> | <a href="https://groups.google.com/forum/#!forum/revbayes-users">Users Forum</a>
    </div>
  </div>
  <br>
  </div>
</footer>

    </div>
    <script src="/revbayes-site/assets/js/vendor/jquery.min.js"></script>
<script src="/revbayes-site/assets/js/vendor/FileSaver.min.js"></script>
<script src="/revbayes-site/assets/js/vendor/jszip.min.js"></script>
<script src="/revbayes-site/assets/js/vendor/bootstrap.min.js"></script>

<script type="text/javascript">
// Add default language
$(":not(code).highlighter-rouge").each(function() {
  
  if( this.classList == "highlighter-rouge") {
    this.classList = "Rev highlighter-rouge";
  }
  
});
// $("code.highlighter-rouge").each(function() {
//   
//   if( this.classList == "highlighter-rouge") {
//       this.classList = "Rev highlighter-rouge";
//   }
//   
// });
</script>

<script src="/revbayes-site/assets/js/base.js"></script>

<script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>

  </body>
</html>
