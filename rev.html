<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="search-domain" value="https://willpett.github.io/revbayes_tutorials/">
    <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet">
    <link rel="stylesheet" href="/revbayes_tutorials/assets/css/syntax.css">
    <link rel="stylesheet" type="text/css" href="/revbayes_tutorials/assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes_tutorials/assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes_tutorials/assets/css/main.css" />
    <title>RevBayes: Statistical Inference Using RevBayes</title>
  </head>
  <body>
    <div class="container">
      <nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <a href="https://revbayes.github.io" class="pull-left">
        <img class="navbar-logo" src="/revbayes_tutorials/assets/img/aquabayes-desaturated.png" alt="RevBayes Logo" />
      </a>
      
      
      <a class="navbar-brand" href="/revbayes_tutorials/">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="/revbayes_tutorials/software/">Software</a></li>
        <li><a href="/revbayes_tutorials/developer/">Developer</a></li>
        <li><a href="/revbayes_tutorials/tutorials/">Tutorials</a></li>
        <li><a href="/revbayes_tutorials/workshops/">Workshops</a></li>
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>

      <div class="row">
	<h1 class="maintitle">Statistical Inference Using RevBayes</h1>
	<h3 class="subtitle">Basic introduction to Rev & MCMC</h3>
	<h4 class="maintitle">Sebastian H&#246;hna</h4>
</div>

      
<blockquote class="overview no-print" id="overview">
  <h2>Overview</h2>
  <div class="row">
    <div class="col-md-9">
        <strong>Prerequisites</strong>
        
          <ul>
          <li>None</li>
          </ul>
        
    </div>
  </div>
</blockquote>

<blockquote class="tutorial_files no-print" id="tutorial_files">
  <h2>Data files and scripts</h2>
  
    <div class="row" id="script_row">
      <div class="col-md-9">
          <strong>Scripts</strong>
          <ul id="scripts"></ul>
        </div>
    </div>
</blockquote>
      <h1 id="basic-commands">Basic Commands</h1>

<h2 id="introduction">Introduction</h2>

<p>This tutorial demonstrates the basic syntactical features of and and
shows how to set up and perform an analysis on “toy” statistical models
for linear regression. This tutorial focuses on explaining probabilistic
graphical models and the language <a href="#Hoehna2016b">(Höhna et al. 2016)</a>. A good reference for
probabilistic graphical models for Bayesian phylogenetic inference is
given in <a href="#Hoehna2014b">(Höhna et al. 2014)</a>. The statistical examples are borrowed from a
fourth year statistics course taught in the fall term 2011 at Stockholm
University.</p>

<p>The first section of this tutorial involves</p>

<ol>
  <li>
    <p>Creating different types of variables.</p>
  </li>
  <li>
    <p>Learning about functions.</p>
  </li>
</ol>

<p>Then we will see how to perform statistical inference using and by
implementing a Monte Carlo algorithm. Finally, we will see how ’s
built-in functions vastly simplify this inference task.</p>

<p>All of the files for this analysis are provided for you, and you can run
these without significant effort using the function in the console:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source("RevBayes_scripts/basics.Rev")
</code></pre></div></div>

<p>Nevertheless, you will learn more if you type in the commands directly.</p>

<p>Let’s start with the basic concepts for the interactive use of with (the
language of ). You should try to execute the statements step by step,
look at the output and try to understand what and why things are
happening. We start with some simple concepts to get familiar and used
to . By now you should have executed and you should see the command
prompt waiting for input. The best exercise is to write these statements
exactly in .</p>

<p>is an interpreted language for statistical computing and analyses in
evolutionary biology. Therefore, the basics are simple mathematical
operations, such as</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Simple mathematical operators:
1 + 1                            # Addition
10 - 5                           # Subtraction
5 * 5                            # Multiplication
10 / 2                           # Division
2^3                              # Exponentiation
5%2                              # Modulo
</code></pre></div></div>

<p>Just as a side note, you can also write multiple statements in the same
line if you separate these by a semicolon (). The statements will be
executed as if you wrote each on a single line.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1 + 1; 2 + 2                    # Multiple statements in one line
</code></pre></div></div>

<p>Here you can see that comments always start with the hash symbol ().
Everything after the ‘’-symbol will be ignored. In addition to these
simple mathematical operations, we provide some standard math functions
which can be called by:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Math-Functions
exp(1)                           # exponential function
ln(1)                            # logarithmic function with natural base
sqrt(16)                         # square root function 
power(2,2)                       # power function: power(a,b) = a^b
</code></pre></div></div>

<p>Notice that is case-sensitive. That means, distinguishes upper and lower
case letter for both variable names and function names. For example,
only the first of these two calls will work</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>exp(1)                           # correct lower case name
Exp(1)                           # wrong upper case name
</code></pre></div></div>

<p>Moreover, we provide functions for the common statistical distributions.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># distribution functions
dexp(x=1,lambda=1)       # exponential distribution density function
qexp(0.5,1)              # exponential distribution quantile function
rexp(n=10,1)             # random draws from an exponential distribution
dnorm(-2.0,0.0,1.0)      # normal distribution density function
rnorm(n=10,0,1)          # random draws from a normal distribution
</code></pre></div></div>

<p>You may have noticed that we sometimes provided labels of the arguments
and sometimes not. You can always provide the argument labels and then
will match the arguments based on the labels.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dnorm(x=0.5,mean=0.0,sd=1)       # normal distribution density function
</code></pre></div></div>

<p>If you do not provide the argument labels, then will match the arguments
by the best fitting types and the order in which you provided the
arguments.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dnorm(0.5,0.5,1)         # correct order
dnorm(0.5,1,0.5)         # mismatched order
</code></pre></div></div>

<p>You may provide also just some arguments with labels and leave the other
arguments without labels.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dnorm(0.0,x=0.5,sd=1)    # partially labeled
</code></pre></div></div>

<p>If you do not remember what the parameter name or parameter names of a
function are, then you can simply type in the function name and will
tell you the possible parameters with their names.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dnorm
</code></pre></div></div>

<h2 id="variable-declaration">Variable Declaration</h2>

<p>The next, and very important feature of , is variable declaration. We
have three types of (model) variables, namely constant, deterministic
and stochastic variables, which represent the same three types of DAG
nodes. Here we show how to construct the different variables and how
they behave differently. First, we focus on the difference between
constant and deterministic variables.</p>

<p>Let us begin by creating a constant variable with name and assigned the
value 1 to it. The left arrow assignment () always creates a constant
variable.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Variable assignment: constant and deterministic
a &lt;- 1                           # assignment of constant node 'a'
</code></pre></div></div>

<p>You see the value of ’a’ by just typing in the variable name and
pressing enter.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a                                # printing the value of 'a'
</code></pre></div></div>

<p>If you want to see which type of variable (constant, deterministic or
stochastic) ’a’ has, then call the structure function for it.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>str(a)                           # printing the structure information of 'a'
|*   _variable     = a
|*   _RevType      = Natural
|*   _RevTypeSpec  = [ Natural, Integer, RevObject ]
|*   _value        = 1
|*   _dagType      = Constant DAG node
|*   _children     = [  ]
|*   .methods = void function ()
</code></pre></div></div>

<p>An additional quite useful built-in function in is the function which
gives you only the type information of the variable and thus is a subset
of the function.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>type(a)                          # printing the type information of 'a'
|*    Natural
</code></pre></div></div>

<p>Next, we create a deterministic variable using the assignment computed
by and another deterministic variable computed by . Deterministic
variables are always created using the colon-equal assignment ().</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>b := exp(a)                      # assignment of deterministic node 'b' with the exponential function with parameter 'a'
b                                # printing the value of 'b'
c := ln(b)                       # assignment of deterministic node 'c' with logarithmic function with parameter 'b'
c                                # printing the value of 'c'
</code></pre></div></div>

<p>Again, you see the type of the variable and additional information such
as which the parents and children are by calling the structure function
on it.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>str(b)                           # printing the structure information of 'b'
</code></pre></div></div>

<p>For example, see the difference to the creation of variable ’d’, which
is a constant variable.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>d &lt;- ln(b)                       # assignment of constant node 'd' with the value if the logarithmic function with parameter 'b'
d                                # printing the value of 'd'
str(d)                           # printing the structure information of 'd'
</code></pre></div></div>

<p>Currently, the variables and have the same value. We can check this
using the equal comparison ().</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>e := (c == d)           
e
</code></pre></div></div>

<p>Now, if we assign a new value to variable , then naturally the value of
changes. This has the consequence that all deterministic variables that
use ’a’ as a parameter, i.e., the variable , change their value
automatically too.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a &lt;- 2                           # reassignment of variable a; every deterministic node which has 'a' as a parameter changes its value
a                                # printing the value of 'a'
b                                # printing the value of 'b'
c                                # printing the value of 'c'
d                                # printing the value of 'd'
e
</code></pre></div></div>

<p>Since variable was a constant variable it did not change its value. This
also means that is now false.</p>

<p>Finally, we show you how to create the third type of variables in : the
stochastic variables. We will create a random variable from an
exponential distribution with parameter . Stochastic assignments use the
operation.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Variable assignment: stochastic
lambda &lt;- 1                      # assign constant node 'lambda' with value '1'
x ~ dnExponential(lambda)        # create stochastic node with exponential distribution and parameter 'lambda'
</code></pre></div></div>

<p>The value of is a random draw from the distribution. You can see the
value and the probability (or log-probability) of the current value
under the current parameter values by</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x                                # print value of stochastic node 'x'
x.probability()                  # print the probability if 'x'
x.lnProbability()                # print the log-probability if 'x'
str(x)                           # printing all the information of 'x'
</code></pre></div></div>

<p>Similarly, we create a random variable from a normal distribution by</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mu &lt;- 0
sigma &lt;- 1
y ~ dnNorm(mu,sigma)    
y.probability()                  # print the probability of 'y'
y.lnProbability()                # print the log-probability if 'y'
str(y)                           # printing all the information of 'y'
</code></pre></div></div>

<p>Variables that are not part of a model are assigned with , for example,
.</p>

<p>Now you know everything there is about creating the different types of
variables and the different ways in which these variables behave.</p>

<h3 id="simple-variable-manipulation-and-other-types-of-assignments">Simple variable manipulation and other types of assignments</h3>

<p>provides some convenience variable manipulation operations that are
equivalent to variable manipulations in other programming languages such
as C/C++, Java and Python. You can increment () and decrement () a
variable. The increment operation increases the current value of a
variable by 1 and the decrement operation decreases the value by 1. A
post increment () increases the value after returning the value, that
is, the old value is returned. A pre increment () increases the value
before returning the value, that is, the new value is returned.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>index &lt;- 1
index++                          # post increment
++index                          # pre increment
index--                          # post decrement
--index                          # pre decrement
</code></pre></div></div>

<p>Additionally, you can use addition (), subtraction (), multiplication ()
and division () to an existing variable.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>index += 10                      # add 10 to the current value
index *= 2                       # double the current value
</code></pre></div></div>

<p>These variable manipulations will come in very handy for indices of
vectors/arrays.</p>

<h3 id="vectors">Vectors</h3>

<p>Common values in are of scalar types. That means that not everything is
a vector by default. Instead, you can create a vector using three
different ways. First, you can call the vector function.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v &lt;- v(1,2,3)                    # create a vector
</code></pre></div></div>

<p>Interestingly, we can use the same name for a variable as for a
function: the variable and the function . Both will still be fully
functional and our interpreter checks if you asked for a function or a
variable.</p>

<p>Second, you can use the square bracket notation.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w &lt;- [1,2,3]                     # create a vector
</code></pre></div></div>

<p>And third, you can implicitly create the vector by assigning elements.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>z[1] &lt;-1                         # implicit creation of a vector
z[2] &lt;-2                   
z[3] &lt;-3                  
</code></pre></div></div>

<p>The implicit creation does not need to instantiate the variable
beforehand. There are other useful built-in functions that produce
vectors.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1:10                             # range function
rep(10,1)                        # replicate an element n times
seq(1,20,2)                      # built a sequence from a to b by c
</code></pre></div></div>

<p>Vectors in belong to the class of objects that have methods. You can
call a member method by</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x.&lt;method name&gt;(&lt;arguments&gt;)                 
</code></pre></div></div>

<p>You have seen two methods previously, and . If you don’t remember what
the methods were called, or if this object has any member methods, then
you can get these by</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v.methods()                 
</code></pre></div></div>

<p>In general, this is very, very useful. So for a vector we can get the
size — the number of elements — by calling its member function:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v.size()                 
</code></pre></div></div>

<h3 id="control-structures">Control Structures</h3>

<p>In this next part we will learn about control structures in . The first
control structure that we will look at is the loop. A loop executes a
single statement or a block of statements.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># loops
for (&lt;variable&gt; in &lt;set of value&gt;) &lt;single statement&gt;
 
for (&lt;variable&gt; in &lt;set of value&gt;) 
   &lt;single statement&gt;

for (&lt;variable&gt; in &lt;set of value&gt;) {
   &lt;multiple statements&gt;
   &lt;multiple statements&gt;
   &lt;multiple statements&gt;
}
</code></pre></div></div>

<p>The statement(s) will be executed for each value of variable of the
loop. A simple example is a loop that computes the sum of a sequence.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sum &lt;- 0
for (i in 1:100) {
   sum &lt;- sum + i
}
sum
</code></pre></div></div>

<p>Another example using a loop is the computation of the <a href="http://en.wikipedia.org/wiki/Fibonacci_number">Fibonacci
number</a> for a given
integer.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Fibonacci series using a for loop
fib[1] &lt;- 1
fib[2] &lt;- 1
for (j in 3:10) {
   fib[j] &lt;- fib[j - 1] + fib[j - 2]
}
fib
</code></pre></div></div>

<p>We could also compute the Fibonacci numbers using a loop. The loop
continues to execute the statement(s) until the condition is wrong.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Fibonacci series using a while loop
fib[1] &lt;- 1
fib[2] &lt;- 1
j &lt;- 3
while (j &lt;= 10) {
   fib[j] &lt;- fib[j - 1] + fib[j - 2]
   j++
}
fib
</code></pre></div></div>

<h3 id="user-defined-functions">User Defined Functions</h3>

<p>In you can write your own functions as well. The syntax for writing a
function is:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>function &lt;return value type&gt; &lt;function name&gt; (&lt;list of arguments&gt;) { &lt;statements&gt; }
</code></pre></div></div>

<p>As a simple example, let’s write a function that computes the square of
a number. We expect that the function takes in any real number. The type
of real number is . Since the square is always a positive real number,
we choose the return to be Now we can call our own function the same way
as we call other already built-in functions in .</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a &lt;- square(5.0)
a
</code></pre></div></div>

<p>As an exercise, let’s write a function that computes the factorial of a
natural number.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># function for computing the factorial
function Natural fac(i) {
   if (i &gt; 1) {
      return i * fac(i-1)
   } else {
      return 1
   }
}
b &lt;- fac(6)
b
</code></pre></div></div>

<p>Here you see that within your own function you can call your function as
well, which is commonly called recursive function calls.</p>

<p>Now let us write a recursive function for the sum of numbers which we
computed before using a loop.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># function for computing the sum
function Integer sum(Integer j) {
   if (j &gt; 1) {
      return j + sum(j-1)
   } else {
      return 1
   }
}
c &lt;- sum(100)
c
</code></pre></div></div>

<p>We can do the same for our favorite example, the Fibonacci series.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># function for computing the fibonacci series
function Integer fib(Integer k) {
   if (k &gt; 1) {
      return fib(k-1) + fib(k-2)
   } else {
      return k
   }
}
d &lt;- fib(6)
d
</code></pre></div></div>

<p>Now that should be enough to get you going with our first example
analyses.</p>

<h1 id="exercise-poisson-regression-model-for-airline-fatalities">Exercise: Poisson Regression Model for Airline Fatalities</h1>

<p>This exercise will demonstrate how to approximate the posterior
distribution of some parameters using a simple Metropolis algorithm. The
focus here lies in the Metropolis algorithm, Bayesian inference, and
model specification—but not in the model or the data. After completing
this computer exercise, you should be familiar with the basic Metropolis
algorithm, analyzing output generated from a MCMC algorithm, and
performing standard Bayesian inference.</p>

<h2 id="model-and-data">Model and Data</h2>

<p>We will use the data example from <a href="#Gelman2003">(Gelman et al. 2003)</a>. A summary is given in
Table [tab:airlineFatalities].</p>

<hr />
<dl>
  <dt>Year           1976   1977   1978   1979   1980   1981   1982   1983   1984   1985</dt>
  <dt>  Fatalities       24     25     31     31     22     21     26     20     16     22</dt>
  <dt>  ———— —— —— —— —— —— —— —— —— —— ——</dt>
  <dd>
    <p>Airline fatalities from 1976 to 1985. Reproduced from <a href="#Gelman2003">(Gelman et al. 2003)</a>
  Table 2.2 on p. 69 %}.[]{data-label=”tab:airlineFatalities”}</p>
  </dd>
</dl>

<p>These data can be loaded into by typing:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>observed_fatalities &lt;- v(24,25,31,31,22,21,26,20,16,22)
</code></pre></div></div>

<p>The model is a <a href="http://en.wikipedia.org/wiki/Poisson_regression">Poisson
regression</a> model with
parameters $\alpha$ and $\beta$
<script type="math/tex">y \sim \text{Poisson}(\exp(\alpha+\beta*x))</script> where $y$ is the number
of fatal accidents in year $x$. For simplicity, we choose uniform priors
for $\alpha$ and $\beta$. <script type="math/tex">% <![CDATA[
\begin{aligned}
\alpha & \sim & \text{Uniform}(-10,10)\\
\beta &  \sim & \text{Uniform}(-10,10)\end{aligned} %]]></script> The probability
density can be computed in for a single year by</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dpoisson(y[i],exp(alpha+beta*x[i]))
</code></pre></div></div>

<h2 id="problems">Problems</h2>

<h3 id="metropolis-algorithm">Metropolis Algorithm</h3>

<p>The source file for this sub-exercise .</p>

<p>Let us construct a Metropolis algorithm that simulates from the
posterior distribution $P(\alpha,\beta|y)$. We will construct this
algorithm explicitly, without using the high-level functions existing in
to perform MCMC. In the next section, we will repeat the same analysis,
this time using the high-level functions. (More background on MCMC is
provided in the <a href="https://github.com/revbayes/revbayes_tutorial/raw/master/tutorial_TeX/RB_MCMC_Intro_Tutorial/RB_MCMC_Intro_Tutorial.pdf">Introduction to Markov Chain Monte Carlo Algorithms
tutorial</a>.)</p>

<p>For simplicity of the calculations you can “normalize” the years, e.g.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x &lt;- 1976:1985 - mean(1976:1985)
</code></pre></div></div>

<p>A common proposal distribution for $\alpha^{\prime} \sim P(\alpha[i-1])$
is the normal distribution with mean $\mu = \alpha[i-1]$ and standard
deviation $\sigma = \delta_\alpha$:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>alpha_prime &lt;- rnorm(1,alpha[i-1],delta_alpha)
</code></pre></div></div>

<p>A similar distribution should be used for $\beta^{\prime}$.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>delta_alpha &lt;- 1.0
delta_beta &lt;- 1.0
</code></pre></div></div>

<p>After you look at the output of the MCMC (later), play around to find
appropriate values for $\delta_{\alpha}$ and $\delta_{\beta}$.</p>

<p>Now we need to set starting values for the MCMC algorithm. Usually,
these are drawn from the prior distribution, but sometimes if the prior
is very uninformative, then these parameter values result in a
likelihood of 0.0 (or log-likelihood of -Inf).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>alpha[1] &lt;- -0.01     # you can also use runif(-1.0,1.0)
beta[1] &lt;- -0.01      # you can also use runif(-1.0,1.0)
</code></pre></div></div>

<p>Next, create some output for our MCMC algorithm. The output will be
written into a file that can be read into or Tracer <a href="#Rambaut2011">(Rambaut and Drummond 2011)</a>.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># create a file output
write("iteration","alpha","beta",file="airline_fatalities.log")
write(0,alpha[1],beta[1],file="airline_fatalities.log",append=TRUE)
</code></pre></div></div>

<p>Note that we need a first iteration with value 0 so that Tracer can load
in this file.</p>

<p>Finally, we set up a loop over each iteration of the MCMC.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for (i in 2:10000) {
</code></pre></div></div>

<p>Within the loop we propose new parameter values.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    alpha_prime &lt;- rnorm(1,alpha[i-1],delta_alpha)[1]
    beta_prime &lt;- rnorm(1,beta[i-1],delta_beta)[1]
</code></pre></div></div>

<p>For the newly proposed parameter values we compute the prior ratio. In
this case we know that the prior ratio is 0.0 as long as the new
parameters are within the limits.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ln_prior_ratio &lt;- dunif(alpha_prime,-10.0,10.0,log=TRUE) + dunif(beta_prime,-10.0,10.0,log=TRUE) - dunif(alpha[i-1],-10.0,10.0,log=TRUE) - dunif(beta[i-1],-10.0,10.0,log=TRUE)
</code></pre></div></div>

<p>Similarly, we compute the likelihood ratio for each observation.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ln_likelihood_ratio &lt;- 0
    for (j in 1:x.size() ) {
       lambda_prime &lt;- exp( alpha_prime + beta_prime * x[j] )
       lambda &lt;- exp( alpha[i-1] + beta[i-1] * x[j] )
       ln_likelihood_ratio += dpoisson(observed_fatalities[j],lambda_prime) - dpoisson(observed_fatalities[j],lambda)
    }
    ratio &lt;- ln_prior_ratio + ln_likelihood_ratio
</code></pre></div></div>

<p>And finally we accept or reject the newly proposed parameter values with
probability .</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    if ( ln(runif(1)[1]) &lt; ratio) {
       alpha[i] &lt;- alpha_prime
       beta[i] &lt;- beta_prime
    } else {
       alpha[i] &lt;- alpha[i-1]
       beta[i] &lt;- beta[i-1]
    }
</code></pre></div></div>

<p>Then we log the current parameter values to the file by appending the
file.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    # output to a log-file
    write(i-1,alpha[i],beta[i],file="airline_fatalities.log",append=TRUE)
 }
</code></pre></div></div>

<p>As a quick summary you can compute the posterior mean of the parameters.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mean(alpha)
mean(beta)
</code></pre></div></div>

<p>You can also load the file into or Tracer to analyze the output.</p>

<p>In this section of the first exercise we wrote our own little Metropolis
algorithm in . This becomes very cumbersome, difficult and slow if we’ld
need to do this for every model. Here we wanted to show you only the
basic principle of any MCMC algorithm. In the next section we will use
the built-in MCMC algorithm of .</p>

<h3 id="mcmc-analysis-using-the-built-in-algorithm-in">MCMC analysis using the built-in algorithm in</h3>

<p>Before starting with this new approach it would be good if you either
start a new session or clear all previous variables using the function.
Currently we may have some minor memory problems and if you get stuck it
may help to restart .</p>

<p>We start by loading in the data to .</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>observed_fatalities &lt;- v(24,25,31,31,22,21,26,20,16,22)
x &lt;- 1976:1985 - mean(1976:1985)
</code></pre></div></div>

<p>Then we create the parameters with their prior distributions.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>alpha ~ dnUnif(-10,10) 
beta ~ dnUnif(-10,10)
</code></pre></div></div>

<p>It may be good to set some reasonable starting values especially if you
choose a very uninformative prior distribution. If by chance you had
starting values that gave a likelihood of -Inf, then will try several
times to propose new starting values drawn from the prior distribution.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># let us use reasonable starting value
alpha.setValue(0.0)
beta.setValue(0.0)
</code></pre></div></div>

<p>Our next step is to set up the moves. Moves are algorithms that propose
new values and know how to reset the values if the proposals are
rejected. We use the same sliding window move as we implemented above by
ourselves.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mi &lt;- 0
moves[mi++] = mvSlide(alpha)
moves[mi++] = mvSlide(beta)
</code></pre></div></div>

<p>Then we set up the model. This means we create a stochastic variable for
each observation and clamp its value with the observed data.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for (i in 1:x.size() ) {
    lambda[i] := exp( alpha + beta * x[i] )
    y[i] ~ dnPoisson(lambda[i])
    y[i].clamp(observed_fatalities[i])
}
</code></pre></div></div>

<p>We can now create the model by pulling up the model graph from any
variable that is connected to our model graph.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymodel = model( alpha )
</code></pre></div></div>

<p>We also need some monitors that report the current values during the
MCMC run. We create two monitors, one printing all numeric non-constant
variables to a file and one printing some information to the screen.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>monitors[1] = mnModel(filename="output/airline_fatalities.log",printgen=10, separator = "   ")
monitors[2] = mnScreen(printgen=10, alpha, beta)
</code></pre></div></div>

<p>Finally we create an MCMC object. The MCMC object takes in a model
object, the vector of monitors and the vector of moves.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymcmc = mcmc(mymodel, monitors, moves)
</code></pre></div></div>

<p>On the MCMC object we call its member method to run the MCMC.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymcmc.run(generations=3000)
</code></pre></div></div>

<p>And now we are done</p>

<h3 id="posterior-distribution-of-alpha-and-beta">Posterior Distribution of $\alpha$ and $\beta$</h3>

<p>Report the posterior mean and 95% credible intervals for $\alpha$ and
$\beta$. Additionally, plot the posterior distribution of $\alpha$ and
$\beta$ by plotting a histogram of the samples. Plot the curve of
$m(x) = \text{E}[\exp(\alpha+\beta<em>x)|y]$ for $x = [1976,1985]$. You can
generate draws from the posterior distribution of the expected value for
a specific $x$ by recording the current expected value at a iteration
$i$ of the Metropolis algorithm
$m_sample(x)[i] = \text{E}[\exp(\alpha[i]+\beta[i]</em>x)|y]$ and taking
the mean of those samples () afterwards. Since provides you with the
samples of $m(x) = \text{E}[\exp(\alpha+\beta*x)|y] = \lambda_x$ you can
simply plot these posterior curves.</p>

<p>Produce a histogram of the predictive distribution of the number of
fatalities in 2014 and estimate the posterior mean. The predictive
distribution can be approximated simultaneously with the Metropolis
algorithm. This means, for any iteration $i$ you simulate draws from the
conditional distribution for $x = 2014$ and the current values of
$\alpha[i]$ and $\beta[i]$.</p>

<p>Estimate the distribution of the mean of the posterior predictive
distribution of the the number of fatalities in 2014. Therefore, let us
denote the expected value of the posterior distribution by $\mu$. Since
we do not know this value $\mu$ exactly, we can follow the Bayesian
approach and associate a probability for each value $m$ as being the
true expected value of the posterior distribution, given the
observations $y$ ($P(m = \mu|y)$). You can approximate this distribution
by recording the expected value for the number of fatalities in 2014
($\text{E}[\exp(\alpha+\beta*x)|y]$) in each iteration $i$ of the
Metropolis algorithm. Plot a histogram of the expected values, compute
the mean of the expected values and compare it to the previously
obtained estimate of the mean of the posterior predictive distribution.</p>

<p>Follow the same approach as for the posterior predictive distribution
for $x = 2014$, but this time for $x = 2016$ and estimate the
probability of no fatality.</p>

<h1 id="exercise-poisson-regression-model-for-coal-mine-accidents">Exercise: Poisson Regression Model for Coal-mine Accidents</h1>

<p>We will analyze a dataset coal-mine accidents. The values are the dates
of major (more than 10 casualties) coal-mining disasters in the UK from
1851 to 1962.</p>

<h2 id="a-model-for-disasters">A model for disasters</h2>

<p>A common model for the number of events that occur over a period of time
is a Poisson process, in which the numbers of events in disjoint
time-intervals are independent and Poisson-distributed. We will
discretize and look at the yearly number of accidents.</p>

<p>In order to take into account the possible change of rate, we will allow
for different rates before and after year $\theta$, where $\theta$ is
unknown to us. Thus, the observation distribution of our model is
$y_t \sim Poisson(\lambda_t)$ with $t = 1851,\ldots,1962$ and
<script type="math/tex">% <![CDATA[
\begin{aligned}
\lambda_t & = & \begin{cases}
\beta & \mbox{if } t < \theta \\
\gamma & \mbox{if } t \geq \theta
\end{cases}\end{aligned} %]]></script> Thus, the rate $\lambda_t$ is defined by
three unknown parameters: $\beta$, $\gamma$ and $\theta$. A hierarchical
choice of priors is given by <script type="math/tex">% <![CDATA[
\begin{aligned}
 \eta & \sim & Gamma(10.0;20.0) \\ 
 \beta & \sim & Gamma(2.0;\eta) \\
 \gamma & \sim &Gamma(2.0;\eta) \\
 \theta & \sim & Uniform(1852,\ldots,1962)\end{aligned} %]]></script> which brings
an additional parameter $\eta$ in the model. For $\theta$ we have used a
uniform prior over the years, but excluded year 1851 in order to make
sure at least one year has rate $\beta$. The hierarchical prior carries
the belief that $\beta$ and $\gamma$ are somewhat similar in size, since
they both depend on $\eta$.</p>

<h2 id="the-model-in--the-model-in-unnumbered">The model in  {#the-model-in .unnumbered}</h2>

<p>We start as usual by loading in the data.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>observed_fatalities &lt;-  v(4, 5, 4, 1, 0, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 3, 3, 0, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1)
year &lt;- 1851:1962
</code></pre></div></div>

<p>In we specify this prior choice by</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eta ~ dnGamma(10.0,20.0)
beta ~ dnGamma(2.0,eta)
gamma ~ dnGamma(2.0,eta)
theta ~ dnUnif(1852.0,1962.0)
</code></pre></div></div>

<p>Then we select moves for each parameter. For the rate parameters — which
are defined only on the positive real line — we choose a scaling move.
Only for we choose the sliding window proposal.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mi &lt;- 0
moves[mi++] = mvScale(eta)
moves[mi++] = mvScale(beta)
moves[mi++] = mvScale(gamma)
moves[mi++] = mvSlide(theta)
</code></pre></div></div>

<p>Then, we set up the model by computing the conditional rate of the
Poisson distribution, creating random variables for each observation and
attaching (clamping) data to the variables.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for (i in 1:year.size() ) {
    rate[i] := ifelse(theta &gt; year[i], beta, gamma)
    y[i] ~ dnPoisson(rate[i])
    y[i].clamp(observed_fatalities[i])
}
</code></pre></div></div>

<p>Finally, we create the model object from the variables, add some
monitors and run the MCMC algorithm.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymodel = model( theta )

monitors[1] = mnModel(filename="output/coal_accidents.log",printgen=10, separator = "   ")
monitors[2] = mnScreen(printgen=10, eta, lambda, gamma, theta)

mymcmc = mcmc(mymodel, monitors, moves)

mymcmc.run(generations=3000)
</code></pre></div></div>

<h2 id="batch-mode">Batch Mode</h2>

<p>If you wish to run this exercise in batch mode, the files are provided
for you.</p>

<p>You can carry out these batch commands by providing the file name when
you execute the binary in your unix terminal (this will overwrite all of
your existing run files).</p>

      <ol class="bibliography"><li><span id="Hoehna2016b">Höhna S., Landis M.J., Heath T.A., Boussau B., Lartillot N., Moore B.R., Huelsenbeck J.P., Ronquist F. 2016. RevBayes: Bayesian Phylogenetic Inference Using Graphical Models and an Interactive Model-Specification Language. Systematic Biology. 65:726–736.</span>

<a href="https://doi.org/10.1093/sysbio/syw021">10.1093/sysbio/syw021</a>

</li>
<li><span id="Hoehna2014b">Höhna S., Heath T.A., Boussau B., Landis M.J., Ronquist F., Huelsenbeck J.P. 2014. Probabilistic Graphical Model Representation in Phylogenetics. Systematic Biology. 63:753–771.</span>

<a href="https://doi.org/10.1093/sysbio/syu039">10.1093/sysbio/syu039</a>

</li>
<li><span id="Gelman2003">Gelman A., Carlin J.B., Stern H.S., Rubin D.B. 2003. Bayesian Data Analysis. Chapman &amp; Hall/CRC.</span>

</li>
<li><span id="Rambaut2011">Rambaut A., Drummond A.J. 2011. Tracer v1.5. .</span>


<a href="http://tree.bio.ed.ac.uk/software/tracer/">http://tree.bio.ed.ac.uk/software/tracer/</a>
</li></ol>

<script type="text/javascript">
var _ol = document.querySelectorAll('ol');
for (var i = 0, elem_ol; elem_ol = _ol[i]; i++) {
	if ( elem_ol.classList == "bibliography" ) {
		var _li = elem_ol.getElementsByTagName("li");
		//for (var j = 0, elem_li; elem_li = _li[j]; j++)
		//{
		//	elem_li.innerHTML = elem_li.innerHTML.replace(/(https?:\/\/)([^\s<]+)/,"<a href=\"$1$2\">$2");
		//}
		if(_li.length > 0)
			elem_ol.outerHTML = "<h2>References</h2>"+elem_ol.outerHTML
	}
}
</script>
      <br>
<footer>
  <div class="container">
  <div class="row">
    <div class="col-sm-12" align="center">
      <a href="https://github.com/revbayes">GitHub</a> | <a href="/revbayes_tutorials/license">License</a> | <a href="/revbayes_tutorials/citation">Cite RevBayes</a> | <a href="https://groups.google.com/forum/#!forum/revbayes-users">Users' Forum</a>
    </div>
  </div>
  </div>
</footer>

    </div>
    <script src="/revbayes_tutorials/assets/js/vendor/jquery.min.js"></script>
<script src="/revbayes_tutorials/assets/js/vendor/FileSaver.min.js"></script>
<script src="/revbayes_tutorials/assets/js/vendor/jszip.min.js"></script>
<script src="/revbayes_tutorials/assets/js/vendor/bootstrap.min.js"></script>

<script type="text/javascript">
// Add default language
$(":not(code).highlighter-rouge").each(function() {
  
  if( this.classList == "highlighter-rouge") {
    this.classList = "default highlighter-rouge";
  }
  
});
// $("code.highlighter-rouge").each(function() {
//   
//   if( this.classList == "highlighter-rouge") {
//       this.classList = "default highlighter-rouge";
//   }
//   
// });
</script>

<script src="/revbayes_tutorials/assets/js/base.js"></script>

<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>

  </body>
</html>
